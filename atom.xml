<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>MasterYangBlog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://masteryang4.github.io/"/>
  <updated>2020-08-08T12:20:53.671Z</updated>
  <id>https://masteryang4.github.io/</id>
  
  <author>
    <name>Yang4</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>读书笔记节选一</title>
    <link href="https://masteryang4.github.io/2020/08/08/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E8%8A%82%E9%80%89%E4%B8%80/"/>
    <id>https://masteryang4.github.io/2020/08/08/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E8%8A%82%E9%80%89%E4%B8%80/</id>
    <published>2020-08-08T12:19:58.000Z</published>
    <updated>2020-08-08T12:20:53.671Z</updated>
    
    <content type="html"><![CDATA[<p>今天这个故事，关于人是如何废掉的。</p><p>1995年，美国旧金山举行过一个全球精英会议。500名政经精英在会上，为全球化的世界进行分析与规划。</p><p>大家一致认为：</p><p>1，八二定律真切地存在。</p><p>2，竞争会越来越激烈，而80%的人，将越来越贫穷，地位越来越下降，淘汰率也越来越高。</p><p>可是问题来了，这80%的loser，与其余20%的精英之间，必然存在冲突。</p><p>冲突如果剧烈，社会就会动荡。如何解决这一问题呢？</p><p>布热津斯基就此提出了著名的“奶头乐”理论。</p><p>所谓奶头乐，就是指采取娱乐化、低智化、游戏化、低成本、轻易就能获取刺激性快乐的办法，卸除底层人口的不满。</p><p>所以，娱乐要越多越好，游戏要越普及越好，综艺与真人秀要随处可见，低智的、无逻辑的、甚至堪称脑残的偶像剧要一部接一部。</p><p>当这些东西触手可及，底层人就会安分下来，快乐地、毫无怨言地、无知无觉地继续贫穷，继续无所得，然后虚度一生。</p><p>这虽然是大洋彼岸的事件，但奶头乐的现象，在我们身边同样存在。</p><p>曾有人说过，给一个人一根网线，一个小房间，一个外卖电话，就足以毁了一个人。</p><p>因为，当一个人置身于充满感官刺激的娱乐、碎片化的信息和无规则的游戏中时，你的注意力全部被占据，时间全被消耗，你的欲望能轻易被满足，自律会一点一点丧失，意志力逐渐瘫软。</p><p>你不会再思考。也不再向往艰难的事业。你会恐惧挑战，恐惧前行。</p><p>你会在一个接一个的综艺中，在一坨又一坨的微博资讯里，一阵接一阵的低质量欢娱中，走向你的颓废。</p><p>王尔德说过一句话：“人生只有两种悲剧，一种是：一直得不到。另一种是：太容易得到。”</p><p>一直得不到，是为永生遗憾。</p><p>太容易得到，要么会变味（边际效用递减），要么会上瘾（沉溺于直接刺激）。无论哪一种，都绝非善事。</p><p>要知道，人真正能成长，取决于不断的自我挑战。</p><p>是明知很苦，仍然前行；明知不易，仍然投入。</p><p>当一个人不断突破，在杂乱的信息里分花错柳，在活色生香的影像中保持高强度自律，目标明确，遇山开路，遇水搭桥，终于在某一天，他获得“我真的做到了”的大高潮，这种高峰体验，会令他受益一生。</p><p>因为，这才是真正的幸福——努力过，实现过，我无悔。</p><p>第二个故事，关于多任务执行所带来的消耗。</p><p>一个熟人，也是写字的。</p><p>文笔好，细节把控一流，出过多篇爆文。倘若一直专注写下去，不说成为名作家，至少，凭借一支笔，也能活得体面又阔绰。</p><p>可惜，她同时做了太多事。</p><p>前天她在做教材编改；今天她在接待领导，陪饭陪酒陪唱歌，想在就职的工作里混上一官半职；又过几天，她说自己正在和老公疯狂撕逼，打得天翻地覆，鸡犬不宁；没多久，开始做公益，准备去贫困山区支教；</p><p>但这个决心没下几天，又变了，说要开直播，做网红，教人搭配衣服和化妆；</p><p>直播没做几天，又生出一事儿，说有朋友拉她入伙，一起干房产，决定以后也同时做房地产销售……</p><p>就这样，她一份漂亮的成绩单也没有交出来。</p><p>几年过去了，我们早已经做出了或大或小的成就，只有她，仍然一无所成，甚至仍然赤贫。</p><p>是谁说，什么都做，就等于什么都没做。</p><p>人的精力是有限的。</p><p>假设精力总量是100，分在10个任务上，那就每个任务，都只能使上10%的精力与注意力。</p><p>这种低质的投入，当然带不来高回报。</p><p>而你还会一天到晚说：“好累啊，忙得没个停，也没赚到半个钱……”</p><p>为什么会这样？</p><p>就因为，当举足轻重的一件事VS无足轻重的多数事，理智的人，都会选择前者。他们知道，只有将80%以上的精力，都投注在前者上，才会真正有成效，也会真正为自己带来收益。</p><p>冯仑曾经讲过一件事。</p><p>一个著名的企业，大家都眼睁睁地看着它走下坡路。曾经的业内标杆，如今成了大家的负面教材。曾经金光闪闪放光彩，如今黯然无光危机重重。</p><p>为什么会这样？</p><p>因为老板的心散了。</p><p>为了资本，什么项目都开。这也做，那也做，主营项目停滞不前，结果哪个业务都没有精进。</p><p>企业如此，人也一样。</p><p>倘若你也不想穷忙而无所得，那么，谨记一条：用80%，甚至90%的精力，去做最最最最重要的那件事。</p><p>这样，才会带来真正的成长。</p><p>一个人的废掉，往往是悄无声息的。</p><p>它没有预警，也没有提示，但你反观自己，如果有如下症状，就应该警醒了：</p><p>一是太懒。懒于改变，懒于坚持。 </p><p>二是太闲。将时间与注意力，都放在索取低密度的信息、瞬间回报的快感中。 </p><p>三是太无逻辑。一旦做事，眉毛胡子一把抓，抓不到重点，也摸不清门路，光阴消耗，疲累不堪，一无所得。</p><p>如果你还喜欢找借口，推诿责任，不面对，不学习，不负责，那你就是行走的危险二字。 </p><p>时代正在大洗牌。贪图“奶头乐”的人，会逐渐被淘汰。迷恋感官愉悦的人，会在睁眼时，发现自己已跟不上时代的滚滚车流。</p><p>如果你想活得更无愧此生，改变趁当下，学习趁此时。不要再敷衍自己。因为生命已在追问：“你，到底赋予我什么意义？” </p><blockquote><p>*作者：周冲，2015年离开体制，放弃公职，从事自由写作。新书《我更喜欢努力的自己》正在热卖中。微博：周冲周冲 </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天这个故事，关于人是如何废掉的。&lt;/p&gt;
&lt;p&gt;1995年，美国旧金山举行过一个全球精英会议。500名政经精英在会上，为全球化的世界进行分析与规划。&lt;/p&gt;
&lt;p&gt;大家一致认为：&lt;/p&gt;
&lt;p&gt;1，八二定律真切地存在。&lt;/p&gt;
&lt;p&gt;2，竞争会越来越激烈，而80%的人，将
      
    
    </summary>
    
    
      <category term="其他" scheme="https://masteryang4.github.io/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="其他" scheme="https://masteryang4.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
      <category term="读书笔记" scheme="https://masteryang4.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>flink系列12电商用户行为分析</title>
    <link href="https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9712%E7%94%B5%E5%95%86%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/"/>
    <id>https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9712%E7%94%B5%E5%95%86%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/</id>
    <published>2020-07-02T03:56:47.000Z</published>
    <updated>2020-07-02T03:58:15.376Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据集解析"><a href="#数据集解析" class="headerlink" title="数据集解析"></a>数据集解析</h2><h3 id="淘宝数据集解析"><a href="#淘宝数据集解析" class="headerlink" title="淘宝数据集解析"></a>淘宝数据集解析</h3><p>我们准备了一份淘宝用户行为数据集，保存为csv文件。本数据集包含了淘宝上某一天随机一百万用户的所有行为（包括点击、购买、收藏、喜欢）。数据集的每一行表示一条用户行为，由用户ID、商品ID、商品类目ID、行为类型和时间戳组成，并以逗号分隔。关于数据集中每一列的详细描述如下：</p><table><thead><tr><th>字段名</th><th>数据类型</th><th>说明</th></tr></thead><tbody><tr><td>userId</td><td>Long</td><td>脱敏后的用户ID</td></tr><tr><td>itemId</td><td>Long</td><td>脱敏后的商品ID</td></tr><tr><td>categoryId</td><td>Int</td><td>脱敏后的商品所属类别ID</td></tr><tr><td>behavior</td><td>String</td><td>用户行为类型，包括：(‘pv’, ‘buy’, ‘cart’, ‘fav’)</td></tr><tr><td>timestamp</td><td>Long</td><td>行为发生的时间戳，单位秒</td></tr></tbody></table><h3 id="Apache服务器日志数据集解析"><a href="#Apache服务器日志数据集解析" class="headerlink" title="Apache服务器日志数据集解析"></a>Apache服务器日志数据集解析</h3><p>这里以apache服务器的一份log为例，每一行日志记录了访问者的IP、userId、访问时间、访问方法以及访问的url，具体描述如下：</p><table><thead><tr><th>字段名</th><th>数据类型</th><th>说明</th></tr></thead><tbody><tr><td>ip</td><td>String</td><td>访问的IP</td></tr><tr><td>userId</td><td>Long</td><td>访问的userId</td></tr><tr><td>eventTime</td><td>Long</td><td>访问时间</td></tr><tr><td>method</td><td>String</td><td>访问方法 GET/POST/PUT/DELETE</td></tr><tr><td>url</td><td>String</td><td>访问的url</td></tr></tbody></table><h2 id="实时热门商品统计"><a href="#实时热门商品统计" class="headerlink" title="实时热门商品统计"></a>实时热门商品统计</h2><p>首先要实现的是实时热门商品统计，我们将会基于UserBehavior数据集来进行分析。</p><p><em>基本需求</em></p><ul><li>每隔5分钟输出最近一小时内点击量最多的前N个商品</li><li>点击量用浏览次数(“pv”)来衡量</li></ul><p><em>解决思路</em></p><p>. 在所有用户行为数据中，过滤出浏览(“pv”)行为进行统计 . 构建滑动窗口，窗口长度为1小时，滑动距离为5分钟 . 窗口计算使用增量聚合函数和全窗口聚合函数相结合的方法 . 使用窗口结束时间作为key，对DataStream进行keyBy()操作 . 将KeyedStream中的元素存储到ListState中，当水位线超过窗口结束时间时，排序输出</p><p><em>数据准备</em></p><p>将数据文件UserBehavior.csv复制到资源文件目录src/main/resources下。</p><p><em>程序主体</em></p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 把数据需要ETL成UserBehavior类型</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserBehavior</span>(<span class="params">userId: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                        itemId: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                        categoryId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                        behavior: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                        timestamp: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">全窗口聚合函数输出的数据类型</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">ItemViewCount</span>(<span class="params">itemId: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                         windowEnd: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                         count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"> </span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">HotItems</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建一个 StreamExecutionEnvironment</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 设定Time类型为EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">// 为了打印到控制台的结果不乱序，</span></span><br><span class="line">    <span class="comment">// 我们配置全局的并发为1，这里改变并发对结果正确性没有影响</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> stream = env</span><br><span class="line">      <span class="comment">// 以window下为例，需替换成数据集的绝对路径</span></span><br><span class="line">      .readTextFile(<span class="string">"YOUR_PATH\\resources\\UserBehavior.csv"</span>)</span><br><span class="line">      .map(line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> linearray = line.split(<span class="string">","</span>)</span><br><span class="line">        <span class="type">UserBehavior</span>(linearray(<span class="number">0</span>).toLong,</span><br><span class="line">                     linearray(<span class="number">1</span>).toLong,</span><br><span class="line">                     linearray(<span class="number">2</span>).toInt,</span><br><span class="line">                     linearray(<span class="number">3</span>),</span><br><span class="line">                     linearray(<span class="number">4</span>).toLong)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">// 过滤出点击事件</span></span><br><span class="line">      .filter(_.behavior == <span class="string">"pv"</span>)</span><br><span class="line">      <span class="comment">// 指定时间戳和Watermark，这里我们已经知道了数据集的时间戳是单调递增的了。</span></span><br><span class="line">      .assignAscendingTimestamps(_.timestamp * <span class="number">1000</span>)</span><br><span class="line">      <span class="comment">// 根据商品Id分流</span></span><br><span class="line">      .keyBy(_.itemId)</span><br><span class="line">      <span class="comment">// 开窗操作</span></span><br><span class="line">      .timeWindow(<span class="type">Time</span>.minutes(<span class="number">60</span>), <span class="type">Time</span>.minutes(<span class="number">5</span>))</span><br><span class="line">      <span class="comment">// 窗口计算操作</span></span><br><span class="line">      .aggregate(<span class="keyword">new</span> <span class="type">CountAgg</span>(), <span class="keyword">new</span> <span class="type">WindowResultFunction</span>())</span><br><span class="line">      <span class="comment">// 根据窗口结束时间分流</span></span><br><span class="line">      .keyBy(_.windowEnd)</span><br><span class="line">      <span class="comment">// 求点击量前3名的商品</span></span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">TopNHotItems</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印结果</span></span><br><span class="line">    stream.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 别忘了执行</span></span><br><span class="line">    env.execute(<span class="string">"Hot Items Job"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><blockquote><p>真实业务场景一般都是乱序的，所以一般不用<code>assignAscendingTimestamps</code>，而是使用<code>BoundedOutOfOrdernessTimestampExtractor</code>。</p></blockquote><p><em>增量聚合函数逻辑编写</em></p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// COUNT统计的聚合函数实现，每出现一条记录就加一</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountAgg</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">UserBehavior</span>, <span class="type">Long</span>, <span class="type">Long</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">Long</span> = <span class="number">0</span>L</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(userBehavior: <span class="type">UserBehavior</span>, acc: <span class="type">Long</span>): <span class="type">Long</span> = acc + <span class="number">1</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResult</span></span>(acc: <span class="type">Long</span>): <span class="type">Long</span> = acc</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(acc1: <span class="type">Long</span>, acc2: <span class="type">Long</span>): <span class="type">Long</span> = acc1 + acc2</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p><em>全窗口聚合函数逻辑编写</em></p><p>其实就是将增量聚合的结果包上一层窗口信息和key的信息。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 用于输出窗口的结果</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WindowResultFunction</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[<span class="type">Long</span>, <span class="type">ItemViewCount</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(key: <span class="type">String</span>,</span><br><span class="line">                        context: <span class="type">Context</span>,</span><br><span class="line">                        elements: <span class="type">Iterable</span>[<span class="type">Long</span>],</span><br><span class="line">                        out: <span class="type">Collector</span>[<span class="type">ItemViewCount</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    out.collect(<span class="type">ItemViewCount</span>(key, context.window.getEnd, elements.iterator.next()))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>现在我们就得到了每个商品在每个窗口的点击量的数据流。</p><p><em>计算最热门TopN商品</em></p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopNHotItems</span>(<span class="params">topSize: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>[<span class="type">Long</span>, <span class="type">ItemViewCount</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 惰性赋值一个状态变量</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> itemState = getRuntimeContext.getListState(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ListStateDescriptor</span>[<span class="type">ItemViewCount</span>](<span class="string">"items"</span>, <span class="type">Types</span>.of[<span class="type">ItemViewCount</span>])</span><br><span class="line">  )</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 来一条数据都会调用一次</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: <span class="type">ItemViewCount</span>,</span><br><span class="line">                              ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">Long</span>,</span><br><span class="line">                                <span class="type">ItemViewCount</span>, <span class="type">String</span>]#<span class="type">Context</span>,</span><br><span class="line">                              out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    itemState.add(value)</span><br><span class="line">    ctx.timerService().registerEventTimeTimer(value.windowEnd + <span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 定时器事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(</span><br><span class="line">    ts: <span class="type">Long</span>,</span><br><span class="line">    ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">Long</span>, <span class="type">ItemViewCount</span>, <span class="type">String</span>]#<span class="type">OnTimerContext</span>,</span><br><span class="line">    out: <span class="type">Collector</span>[<span class="type">String</span>]</span><br><span class="line">  ): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> allItems: <span class="type">ListBuffer</span>[<span class="type">ItemViewCount</span>] = <span class="type">ListBuffer</span>()</span><br><span class="line">    <span class="comment">// 导入一些隐式类型转换</span></span><br><span class="line">    <span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line">    <span class="keyword">for</span> (item &lt;- itemState.get) &#123;</span><br><span class="line">      allItems += item</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 清空状态变量，释放空间</span></span><br><span class="line">    itemState.clear()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 降序排列</span></span><br><span class="line">    <span class="keyword">val</span> sortedItems = allItems.sortBy(-_.count).take(topSize)</span><br><span class="line">    <span class="keyword">val</span> result = <span class="keyword">new</span> <span class="type">StringBuilder</span></span><br><span class="line">    result.append(<span class="string">"====================================\n"</span>)</span><br><span class="line">    result.append(<span class="string">"时间: "</span>).append(<span class="keyword">new</span> <span class="type">Timestamp</span>(ts - <span class="number">1</span>)).append(<span class="string">"\n"</span>)</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- sortedItems.indices) &#123;</span><br><span class="line">      <span class="keyword">val</span> currentItem = sortedItems(i)</span><br><span class="line">      result.append(<span class="string">"No"</span>)</span><br><span class="line">        .append(i+<span class="number">1</span>)</span><br><span class="line">        .append(<span class="string">":"</span>)</span><br><span class="line">        .append(<span class="string">"  商品ID="</span>)</span><br><span class="line">        .append(currentItem.itemId)</span><br><span class="line">        .append(<span class="string">"  浏览量="</span>)</span><br><span class="line">        .append(currentItem.count)</span><br><span class="line">        .append(<span class="string">"\n"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(<span class="string">"====================================\n\n"</span>)</span><br><span class="line">    <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">    out.collect(result.toString())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p><em>更换Kafka作为数据源</em></p><p>实际生产环境中，我们的数据流往往是从Kafka获取到的。如果要让代码更贴近生产实际，我们只需将source更换为Kafka即可：</p><blockquote><p>注意：这里Kafka的版本要用2.2！</p></blockquote><p>添加依赖：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>编写代码：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"group.id"</span>, <span class="string">"consumer-group"</span>)</span><br><span class="line">properties.setProperty(</span><br><span class="line">  <span class="string">"key.deserializer"</span>,</span><br><span class="line">  <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">)</span><br><span class="line">properties.setProperty(</span><br><span class="line">  <span class="string">"value.deserializer"</span>,</span><br><span class="line">  <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">)</span><br><span class="line">properties.setProperty(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream = env</span><br><span class="line">  .addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](</span><br><span class="line">    <span class="string">"hotitems"</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(),</span><br><span class="line">    properties)</span><br><span class="line">  )</span><br></pre></td></tr></table></figure></div><p>当然，根据实际的需要，我们还可以将Sink指定为Kafka、ES、Redis或其它存储，这里就不一一展开实现了。</p><p><em>kafka生产者程序</em></p><p>添加依赖</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>编写代码：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.&#123;<span class="type">KafkaProducer</span>, <span class="type">ProducerRecord</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaProducerUtil</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    writeToKafka(<span class="string">"hotitems"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">writeToKafka</span></span>(topic: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">    props.put(</span><br><span class="line">      <span class="string">"key.serializer"</span>,</span><br><span class="line">      <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span></span><br><span class="line">    )</span><br><span class="line">    props.put(</span><br><span class="line">      <span class="string">"value.serializer"</span>,</span><br><span class="line">      <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](props)</span><br><span class="line">    <span class="keyword">val</span> bufferedSource = io.<span class="type">Source</span>.fromFile(<span class="string">"UserBehavior.csv文件的绝对路径"</span>)</span><br><span class="line">    <span class="keyword">for</span> (line &lt;- bufferedSource.getLines) &#123;</span><br><span class="line">      <span class="keyword">val</span> record = <span class="keyword">new</span> <span class="type">ProducerRecord</span>[<span class="type">String</span>, <span class="type">String</span>](topic, line)</span><br><span class="line">      producer.send(record)</span><br><span class="line">    &#125;</span><br><span class="line">    producer.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="实时流量统计"><a href="#实时流量统计" class="headerlink" title="实时流量统计"></a>实时流量统计</h2><ul><li>基本需求<ul><li>从web服务器的日志中，统计实时的访问流量</li><li>统计每分钟的ip访问量，取出访问量最大的5个地址，每5秒更新一次</li></ul></li><li>解决思路<ul><li>将apache服务器日志中的时间，转换为时间戳，作为Event Time</li><li>构建滑动窗口，窗口长度为1分钟，滑动距离为5秒</li></ul></li></ul><p><em>数据准备</em></p><p>将apache服务器的日志文件apache.log复制到资源文件目录src/main/resources下，我们将从这里读取数据。</p><p><em>代码实现</em></p><p>我们现在要实现的模块是“实时流量统计”。对于一个电商平台而言，用户登录的入口流量、不同页面的访问流量都是值得分析的重要数据，而这些数据，可以简单地从web服务器的日志中提取出来。我们在这里实现最基本的“页面浏览数”的统计，也就是读取服务器日志中的每一行log，统计在一段时间内用户访问url的次数。</p><p>具体做法为：每隔5秒，输出最近10分钟内访问量最多的前N个URL。可以看出，这个需求与之前“实时热门商品统计”非常类似，所以我们完全可以借鉴此前的代码。</p><p>完整代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.ysss.project</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Timestamp</span></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">AggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.<span class="type">ListStateDescriptor</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.typeutils.<span class="type">Types</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.<span class="type">KeyedProcessFunction</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions</span><br><span class="line">.timestamps.<span class="type">BoundedOutOfOrdernessTimestampExtractor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ApacheLogAnalysis</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ApacheLogEvent</span>(<span class="params">ip: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            userId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            eventTime: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            method: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            url: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">case</span> <span class="title">class</span> <span class="title">UrlViewCount</span>(<span class="params">url: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                          windowEnd: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                          count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> stream = env</span><br><span class="line">      <span class="comment">// 文件的绝对路径</span></span><br><span class="line">      .readTextFile(<span class="string">"apache.log的绝对路径"</span>)</span><br><span class="line">      .map(line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> linearray = line.split(<span class="string">" "</span>)</span><br><span class="line">        <span class="comment">// 把时间戳ETL成毫秒</span></span><br><span class="line">        <span class="keyword">val</span> simpleDateFormat = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"dd/MM/yyyy:HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">val</span> timestamp = simpleDateFormat.parse(linearray(<span class="number">3</span>)).getTime</span><br><span class="line">        <span class="type">ApacheLogEvent</span>(linearray(<span class="number">0</span>),</span><br><span class="line">                       linearray(<span class="number">2</span>),</span><br><span class="line">                       timestamp,</span><br><span class="line">                       linearray(<span class="number">5</span>),</span><br><span class="line">                       linearray(<span class="number">6</span>))</span><br><span class="line">      &#125;)</span><br><span class="line">      .assignTimestampsAndWatermarks(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">ApacheLogEvent</span>](</span><br><span class="line">          <span class="type">Time</span>.milliseconds(<span class="number">1000</span>)</span><br><span class="line">        ) &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(t: <span class="type">ApacheLogEvent</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">            t.eventTime</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      )</span><br><span class="line">      .keyBy(_.url)</span><br><span class="line">      .timeWindow(<span class="type">Time</span>.minutes(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">      .aggregate(<span class="keyword">new</span> <span class="type">CountAgg</span>(), <span class="keyword">new</span> <span class="type">WindowResultFunction</span>())</span><br><span class="line">      .keyBy(_.windowEnd)</span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">TopNHotUrls</span>(<span class="number">5</span>))</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"Traffic Analysis Job"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">CountAgg</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">ApacheLogEvent</span>, <span class="type">Long</span>, <span class="type">Long</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">Long</span> = <span class="number">0</span>L</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(apacheLogEvent: <span class="type">ApacheLogEvent</span>, acc: <span class="type">Long</span>): <span class="type">Long</span> = acc + <span class="number">1</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResult</span></span>(acc: <span class="type">Long</span>): <span class="type">Long</span> = acc</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(acc1: <span class="type">Long</span>, acc2: <span class="type">Long</span>): <span class="type">Long</span> = acc1 + acc2</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">WindowResultFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[<span class="type">Long</span>, <span class="type">UrlViewCount</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(key: <span class="type">String</span>, context: <span class="type">Context</span>, elements: <span class="type">Iterable</span>[<span class="type">Long</span>], out: <span class="type">Collector</span>[<span class="type">UrlViewCount</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      out.collect(<span class="type">UrlViewCount</span>(key, context.window.getEnd, elements.iterator.next()))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">TopNHotUrls</span>(<span class="params">topSize: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>[<span class="type">Long</span>, <span class="type">UrlViewCount</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> urlState = getRuntimeContext.getListState(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ListStateDescriptor</span>[<span class="type">UrlViewCount</span>](</span><br><span class="line">        <span class="string">"urlState-state"</span>,</span><br><span class="line">        <span class="type">Types</span>.of[<span class="type">UrlViewCount</span>]</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(</span><br><span class="line">      input: <span class="type">UrlViewCount</span>,</span><br><span class="line">      context: <span class="type">KeyedProcessFunction</span>[<span class="type">Long</span>, <span class="type">UrlViewCount</span>, <span class="type">String</span>]#<span class="type">Context</span>,</span><br><span class="line">      collector: <span class="type">Collector</span>[<span class="type">String</span>]</span><br><span class="line">    ): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 每条数据都保存到状态中</span></span><br><span class="line">      urlState.add(input)</span><br><span class="line">      context</span><br><span class="line">        .timerService</span><br><span class="line">        .registerEventTimeTimer(input.windowEnd + <span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(</span><br><span class="line">      timestamp: <span class="type">Long</span>,</span><br><span class="line">      ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">Long</span>, <span class="type">UrlViewCount</span>, <span class="type">String</span>]#<span class="type">OnTimerContext</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[<span class="type">String</span>]</span><br><span class="line">    ): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 获取收到的所有URL访问量</span></span><br><span class="line">      <span class="keyword">val</span> allUrlViews: <span class="type">ListBuffer</span>[<span class="type">UrlViewCount</span>] = <span class="type">ListBuffer</span>()</span><br><span class="line">      <span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line">      <span class="keyword">for</span> (urlView &lt;- urlState.get) &#123;</span><br><span class="line">        allUrlViews += urlView</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 提前清除状态中的数据，释放空间</span></span><br><span class="line">      urlState.clear()</span><br><span class="line">      <span class="comment">// 按照访问量从大到小排序</span></span><br><span class="line">      <span class="keyword">val</span> sortedUrlViews = allUrlViews.sortBy(_.count)(<span class="type">Ordering</span>.<span class="type">Long</span>.reverse)</span><br><span class="line">        .take(topSize)</span><br><span class="line">      <span class="comment">// 将排名信息格式化成 String, 便于打印</span></span><br><span class="line">      <span class="keyword">var</span> result: <span class="type">StringBuilder</span> = <span class="keyword">new</span> <span class="type">StringBuilder</span></span><br><span class="line">      result</span><br><span class="line">        .append(<span class="string">"====================================\n"</span>)</span><br><span class="line">        .append(<span class="string">"时间: "</span>)</span><br><span class="line">        .append(<span class="keyword">new</span> <span class="type">Timestamp</span>(timestamp - <span class="number">1</span>))</span><br><span class="line">        .append(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (i &lt;- sortedUrlViews.indices) &#123;</span><br><span class="line">        <span class="keyword">val</span> currentUrlView: <span class="type">UrlViewCount</span> = sortedUrlViews(i)</span><br><span class="line">        <span class="comment">// e.g.  No1：  URL=/blog/tags/firefox?flav=rss20  流量=55</span></span><br><span class="line">        result</span><br><span class="line">          .append(<span class="string">"No"</span>)</span><br><span class="line">          .append(i + <span class="number">1</span>)</span><br><span class="line">          .append(<span class="string">": "</span>)</span><br><span class="line">          .append(<span class="string">"  URL="</span>)</span><br><span class="line">          .append(currentUrlView.url)</span><br><span class="line">          .append(<span class="string">"  流量="</span>)</span><br><span class="line">          .append(currentUrlView.count)</span><br><span class="line">          .append(<span class="string">"\n"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      result</span><br><span class="line">        .append(<span class="string">"====================================\n\n"</span>)</span><br><span class="line">      <span class="comment">// 控制输出频率，模拟实时滚动结果</span></span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">      out.collect(result.toString)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="Uv统计的布隆过滤器实现"><a href="#Uv统计的布隆过滤器实现" class="headerlink" title="Uv统计的布隆过滤器实现"></a>Uv统计的布隆过滤器实现</h2><p>依赖：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>完整代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.ysss</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.ysss.<span class="type">UserBehavior</span>.<span class="type">UserAction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.triggers.&#123;<span class="type">Trigger</span>, <span class="type">TriggerResult</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.triggers.<span class="type">Trigger</span>.<span class="type">TriggerContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.<span class="type">Jedis</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UvWithBloomFilter</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="keyword">val</span> stream = env</span><br><span class="line">      .readTextFile(<span class="string">"UserBehavior.csv的绝对路径"</span>)</span><br><span class="line">      .map(line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">        <span class="type">UserAction</span>(arr(<span class="number">0</span>), arr(<span class="number">1</span>), arr(<span class="number">2</span>), arr(<span class="number">3</span>), arr(<span class="number">4</span>).toLong * <span class="number">1000</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      .assignAscendingTimestamps(_.ts)</span><br><span class="line">      .filter(_.behavior == <span class="string">"pv"</span>)</span><br><span class="line">      .map(r =&gt; (<span class="string">"dummyKey"</span>, r.userId))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line">      .timeWindow(<span class="type">Time</span>.minutes(<span class="number">60</span>), <span class="type">Time</span>.minutes(<span class="number">5</span>))</span><br><span class="line">      .trigger(<span class="keyword">new</span> <span class="type">MyTrigger123</span>)</span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">MyProcess</span>)</span><br><span class="line"></span><br><span class="line">    stream.print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyProcess</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[(<span class="type">String</span>, <span class="type">String</span>),</span></span><br><span class="line"><span class="class">      (<span class="type">Long</span>, <span class="type">Long</span>), <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> jedis = <span class="keyword">new</span> <span class="type">Jedis</span>(<span class="string">"localhost"</span>, <span class="number">6379</span>)</span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> bloom = <span class="keyword">new</span> <span class="type">Bloom</span>(<span class="number">1</span> &lt;&lt; <span class="number">29</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(key: <span class="type">String</span>,</span><br><span class="line">                         context: <span class="type">Context</span>,</span><br><span class="line">                         vals: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">String</span>)],</span><br><span class="line">                         out: <span class="type">Collector</span>[(<span class="type">Long</span>, <span class="type">Long</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> storeKey = context.window.getEnd.toString</span><br><span class="line">      <span class="keyword">var</span> count = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (jedis.hget(<span class="string">"UvCountHashTable"</span>, storeKey) != <span class="literal">null</span>) &#123;</span><br><span class="line">        count = jedis.hget(<span class="string">"UvCountHashTable"</span>, storeKey).toLong</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> userId = vals.last._2</span><br><span class="line">      <span class="keyword">val</span> offset = bloom.hash(userId, <span class="number">61</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> isExist = jedis.getbit(storeKey, offset)</span><br><span class="line">      <span class="keyword">if</span> (!isExist) &#123;</span><br><span class="line">        jedis.setbit(storeKey, offset, <span class="literal">true</span>)</span><br><span class="line">        jedis.hset(<span class="string">"UvCountHashTable"</span>, storeKey, (count + <span class="number">1</span>).toString)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//      out.collect((count, storeKey.toLong))</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyTrigger123</span> <span class="keyword">extends</span> <span class="title">Trigger</span>[(<span class="type">String</span>, <span class="type">String</span>), <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onEventTime</span></span>(time: <span class="type">Long</span>,</span><br><span class="line">                             window: <span class="type">TimeWindow</span>,</span><br><span class="line">                             ctx: <span class="type">TriggerContext</span>): <span class="type">TriggerResult</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (ctx.getCurrentWatermark &gt;= window.getEnd) &#123;</span><br><span class="line">        <span class="keyword">val</span> jedis = <span class="keyword">new</span> <span class="type">Jedis</span>(<span class="string">"localhost"</span>, <span class="number">6379</span>)</span><br><span class="line">        <span class="keyword">val</span> key = window.getEnd.toString</span><br><span class="line">        <span class="type">TriggerResult</span>.<span class="type">FIRE_AND_PURGE</span></span><br><span class="line">        println(key, jedis.hget(<span class="string">"UvCountHashTable"</span>, key))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="type">TriggerResult</span>.<span class="type">CONTINUE</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onProcessingTime</span></span>(</span><br><span class="line">      time: <span class="type">Long</span>,</span><br><span class="line">      window: <span class="type">TimeWindow</span>,</span><br><span class="line">      ctx: <span class="type">TriggerContext</span></span><br><span class="line">    ): <span class="type">TriggerResult</span> = &#123;</span><br><span class="line">      <span class="type">TriggerResult</span>.<span class="type">CONTINUE</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">clear</span></span>(</span><br><span class="line">      window: <span class="type">TimeWindow</span>,</span><br><span class="line">      ctx: <span class="type">Trigger</span>.<span class="type">TriggerContext</span></span><br><span class="line">    ): <span class="type">Unit</span> = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onElement</span></span>(element: (<span class="type">String</span>, <span class="type">String</span>),</span><br><span class="line">                           timestamp: <span class="type">Long</span>,</span><br><span class="line">                           window: <span class="type">TimeWindow</span>,</span><br><span class="line">                           ctx: <span class="type">TriggerContext</span>): <span class="type">TriggerResult</span> = &#123;</span><br><span class="line">      <span class="type">TriggerResult</span>.<span class="type">FIRE_AND_PURGE</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Bloom</span>(<span class="params">size: <span class="type">Long</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> cap = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span></span>(value: <span class="type">String</span>, seed: <span class="type">Int</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">      <span class="keyword">var</span> result = <span class="number">0</span></span><br><span class="line">      <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until value.length) &#123;</span><br><span class="line">        result = result * seed + value.charAt(i)</span><br><span class="line">      &#125;</span><br><span class="line">      (cap - <span class="number">1</span>) &amp; result</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="APP分渠道数据统计"><a href="#APP分渠道数据统计" class="headerlink" title="APP分渠道数据统计"></a>APP分渠道数据统计</h2><p>完整代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.ysss</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Calendar</span>, <span class="type">UUID</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">RichParallelSourceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span>.<span class="type">SourceContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AppMarketingByChannel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MarketingUserBehavior</span>(<span class="params">userId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                   behavior: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                   channel: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                   ts: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">class</span> <span class="title">SimulatedEventSource</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>[<span class="type">MarketingUserBehavior</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> running = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> channelSet = <span class="type">Seq</span>(<span class="string">"AppStore"</span>, <span class="string">"XiaomiStore"</span>)</span><br><span class="line">    <span class="keyword">val</span> behaviorTypes = <span class="type">Seq</span>(<span class="string">"BROWSE"</span>, <span class="string">"CLICK"</span>)</span><br><span class="line">    <span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">Random</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceContext</span>[<span class="type">MarketingUserBehavior</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">while</span> (running) &#123;</span><br><span class="line">        <span class="keyword">val</span> userId = <span class="type">UUID</span>.randomUUID().toString</span><br><span class="line">        <span class="keyword">val</span> behaviorType = behaviorTypes(rand.nextInt(behaviorTypes.size))</span><br><span class="line">        <span class="keyword">val</span> channel = channelSet(rand.nextInt(channelSet.size))</span><br><span class="line">        <span class="keyword">val</span> ts = <span class="type">Calendar</span>.getInstance().getTimeInMillis</span><br><span class="line"></span><br><span class="line">        ctx.collect(<span class="type">MarketingUserBehavior</span>(userId, behaviorType, channel, ts))</span><br><span class="line"></span><br><span class="line">        <span class="type">Thread</span>.sleep(<span class="number">10</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = running = <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="keyword">val</span> stream = env</span><br><span class="line">      .addSource(<span class="keyword">new</span> <span class="type">SimulatedEventSource</span>)</span><br><span class="line">      .assignAscendingTimestamps(_.ts)</span><br><span class="line">      .filter(_.behavior != <span class="string">"UNINSTALL"</span>)</span><br><span class="line">      .map(r =&gt; &#123;</span><br><span class="line">        ((r.channel, r.behavior), <span class="number">1</span>L)</span><br><span class="line">      &#125;)</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line">      .timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>), <span class="type">Time</span>.seconds(<span class="number">1</span>))</span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">MarketingCountByChannel</span>)</span><br><span class="line">    stream.print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MarketingCountByChannel</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Long</span>),</span></span><br><span class="line"><span class="class">      (<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Long</span>), (<span class="type">String</span>, <span class="type">String</span>), <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(key:  (<span class="type">String</span>,<span class="type">String</span>),</span><br><span class="line">                         context: <span class="type">Context</span>,</span><br><span class="line">                         elements: <span class="type">Iterable</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Long</span>)],</span><br><span class="line">                         out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Long</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      out.collect((key._1, elements.size, context.window.getEnd))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="APP不分渠道数据统计"><a href="#APP不分渠道数据统计" class="headerlink" title="APP不分渠道数据统计"></a>APP不分渠道数据统计</h2><p>完整代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.ysss</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.ysss.<span class="type">AppMarketingByChannel</span>.<span class="type">SimulatedEventSource</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AppMarketingStatistics</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="keyword">val</span> stream = env</span><br><span class="line">      .addSource(<span class="keyword">new</span> <span class="type">SimulatedEventSource</span>)</span><br><span class="line">      .assignAscendingTimestamps(_.ts)</span><br><span class="line">      .filter(_.behavior != <span class="string">"UNINSTALL"</span>)</span><br><span class="line">      .map(r =&gt; &#123;</span><br><span class="line">        (<span class="string">"dummyKey"</span>, <span class="number">1</span>L)</span><br><span class="line">      &#125;)</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line">      .timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>), <span class="type">Time</span>.seconds(<span class="number">1</span>))</span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">MarketingCountTotal</span>)</span><br><span class="line">    stream.print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MarketingCountTotal</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[(<span class="type">String</span>, <span class="type">Long</span>),</span></span><br><span class="line"><span class="class">      (<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Long</span>), <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(key: <span class="type">String</span>,</span><br><span class="line">                         context: <span class="type">Context</span>,</span><br><span class="line">                         elements: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)],</span><br><span class="line">                         out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Long</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      out.collect((key, elements.size, context.window.getEnd))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="恶意登陆实现"><a href="#恶意登陆实现" class="headerlink" title="恶意登陆实现"></a>恶意登陆实现</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.ysss</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.ysss.<span class="type">FlinkCepExample</span>.<span class="type">LoginEvent</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ListStateDescriptor</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.typeutils.<span class="type">Types</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.scala.pattern.<span class="type">Pattern</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.<span class="type">KeyedProcessFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LoginFailWithoutCEP</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = env</span><br><span class="line">      .fromElements(</span><br><span class="line">        <span class="type">LoginEvent</span>(<span class="string">"1"</span>, <span class="string">"0.0.0.0"</span>, <span class="string">"fail"</span>, <span class="string">"1"</span>),</span><br><span class="line">        <span class="type">LoginEvent</span>(<span class="string">"1"</span>, <span class="string">"0.0.0.0"</span>, <span class="string">"success"</span>, <span class="string">"2"</span>),</span><br><span class="line">        <span class="type">LoginEvent</span>(<span class="string">"1"</span>, <span class="string">"0.0.0.0"</span>, <span class="string">"fail"</span>, <span class="string">"3"</span>),</span><br><span class="line">        <span class="type">LoginEvent</span>(<span class="string">"1"</span>, <span class="string">"0.0.0.0"</span>, <span class="string">"fail"</span>, <span class="string">"4"</span>)</span><br><span class="line">      )</span><br><span class="line">      .assignAscendingTimestamps(_.ts.toLong * <span class="number">1000</span>)</span><br><span class="line">      .keyBy(_.userId)</span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">MatchFunction</span>)</span><br><span class="line"></span><br><span class="line">    stream.print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MatchFunction</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">LoginEvent</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> loginState = getRuntimeContext.getListState(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ListStateDescriptor</span>[<span class="type">LoginEvent</span>](<span class="string">"login-fail"</span>, <span class="type">Types</span>.of[<span class="type">LoginEvent</span>])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> timestamp = getRuntimeContext.getState(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Long</span>](<span class="string">"ts"</span>, <span class="type">Types</span>.of[<span class="type">Long</span>])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(</span><br><span class="line">      value: <span class="type">LoginEvent</span>,</span><br><span class="line">      ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">LoginEvent</span>, <span class="type">String</span>]#<span class="type">Context</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[<span class="type">String</span>]</span><br><span class="line">    ): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (value.loginStatus == <span class="string">"fail"</span>) &#123;</span><br><span class="line">        loginState.add(value)</span><br><span class="line">        <span class="keyword">if</span> (!timestamp.value()) &#123;</span><br><span class="line">          timestamp.update(value.ts.toLong * <span class="number">1000</span> + <span class="number">5000</span>L)</span><br><span class="line">          ctx</span><br><span class="line">            .timerService()</span><br><span class="line">            .registerEventTimeTimer(value.ts.toLong * <span class="number">1000</span> + <span class="number">5000</span>L)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (value.loginStatus == <span class="string">"success"</span>) &#123;</span><br><span class="line">        loginState.clear()</span><br><span class="line">        ctx</span><br><span class="line">          .timerService()</span><br><span class="line">          .deleteEventTimeTimer(timestamp.value())</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(</span><br><span class="line">      ts: <span class="type">Long</span>,</span><br><span class="line">      ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">LoginEvent</span>, <span class="type">String</span>]#<span class="type">OnTimerContext</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[<span class="type">String</span>]</span><br><span class="line">    ): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> allLogins = <span class="type">ListBuffer</span>[<span class="type">LoginEvent</span>]()</span><br><span class="line">      <span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line">      <span class="keyword">for</span> (login &lt;- loginState.get) &#123;</span><br><span class="line">        allLogins += login</span><br><span class="line">      &#125;</span><br><span class="line">      loginState.clear()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (allLogins.length &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        out.collect(<span class="string">"5s以内连续两次登陆失败"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="订单支付实时监控"><a href="#订单支付实时监控" class="headerlink" title="订单支付实时监控"></a>订单支付实时监控</h2><ul><li>基本需求<ul><li>用户下单之后，应设置订单失效时间，以提高用户支付的意愿，并降低系统风险</li><li>用户下单后15分钟未支付，则输出监控信息</li></ul></li><li>解决思路<ul><li>利用CEP库进行事件流的模式匹配，并设定匹配的时间间隔</li></ul></li></ul><h3 id="使用Flink-CEP来实现"><a href="#使用Flink-CEP来实现" class="headerlink" title="使用Flink CEP来实现"></a>使用Flink CEP来实现</h3><p>在电商平台中，最终创造收入和利润的是用户下单购买的环节；更具体一点，是用户真正完成支付动作的时候。用户下单的行为可以表明用户对商品的需求，但在现实中，并不是每次下单都会被用户立刻支付。当拖延一段时间后，用户支付的意愿会降低。所以为了让用户更有紧迫感从而提高支付转化率，同时也为了防范订单支付环节的安全风险，电商网站往往会对订单状态进行监控，设置一个失效时间（比如15分钟），如果下单后一段时间仍未支付，订单就会被取消。</p><p>我们将会利用CEP库来实现这个功能。我们先将事件流按照订单号orderId分流，然后定义这样的一个事件模式：在15分钟内，事件“create”与“pay”严格紧邻：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> orderPayPattern = <span class="type">Pattern</span>.begin[<span class="type">OrderEvent</span>](<span class="string">"begin"</span>)</span><br><span class="line">  .where(_.eventType == <span class="string">"create"</span>)</span><br><span class="line">  .next(<span class="string">"next"</span>)</span><br><span class="line">  .where(_.eventType == <span class="string">"pay"</span>)</span><br><span class="line">  .within(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br></pre></td></tr></table></figure></div><p>这样调用.select方法时，就可以同时获取到匹配出的事件和超时未匹配的事件了。 在src/main/scala下继续创建OrderTimeout.scala文件，新建一个单例对象。定义样例类OrderEvent，这是输入的订单事件流；另外还有OrderResult，这是输出显示的订单状态结果。由于没有现成的数据，我们还是用几条自定义的示例数据来做演示。 完整代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.cep.scala.<span class="type">CEP</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.scala.pattern.<span class="type">Pattern</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">Map</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderEvent</span>(<span class="params">orderId: <span class="type">String</span>, eventType: <span class="type">String</span>, eventTime: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">OrderTimeout</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> orderEventStream = env.fromCollection(<span class="type">List</span>(</span><br><span class="line">      <span class="type">OrderEvent</span>(<span class="string">"1"</span>, <span class="string">"create"</span>, <span class="string">"1558430842"</span>),</span><br><span class="line">      <span class="type">OrderEvent</span>(<span class="string">"2"</span>, <span class="string">"create"</span>, <span class="string">"1558430843"</span>),</span><br><span class="line">      <span class="type">OrderEvent</span>(<span class="string">"2"</span>, <span class="string">"pay"</span>, <span class="string">"1558430844"</span>),</span><br><span class="line">      <span class="type">OrderEvent</span>(<span class="string">"3"</span>, <span class="string">"pay"</span>, <span class="string">"1558430942"</span>),</span><br><span class="line">      <span class="type">OrderEvent</span>(<span class="string">"4"</span>, <span class="string">"pay"</span>, <span class="string">"1558430943"</span>)</span><br><span class="line">    )).assignAscendingTimestamps(_.eventTime.toLong * <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//    val orders: DataStream[String] = env</span></span><br><span class="line"><span class="comment">//      .socketTextStream("localhost", 9999)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//    val orderEventStream = orders</span></span><br><span class="line"><span class="comment">//      .map(s =&gt; &#123;</span></span><br><span class="line"><span class="comment">//        println(s)</span></span><br><span class="line"><span class="comment">//        val slist = s.split("\\|")</span></span><br><span class="line"><span class="comment">//        println(slist)</span></span><br><span class="line"><span class="comment">//        OrderEvent(slist(0), slist(1), slist(2))</span></span><br><span class="line"><span class="comment">//      &#125;)</span></span><br><span class="line"><span class="comment">//      .assignAscendingTimestamps(_.eventTime.toLong * 1000)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> orderPayPattern = <span class="type">Pattern</span>.begin[<span class="type">OrderEvent</span>](<span class="string">"begin"</span>)</span><br><span class="line">      .where(_.eventType.equals(<span class="string">"create"</span>))</span><br><span class="line">      .next(<span class="string">"next"</span>)</span><br><span class="line">      .where(_.eventType.equals(<span class="string">"pay"</span>))</span><br><span class="line">      .within(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> orderTimeoutOutput = <span class="type">OutputTag</span>[<span class="type">OrderEvent</span>](<span class="string">"orderTimeout"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> patternStream = <span class="type">CEP</span>.pattern(</span><br><span class="line">      orderEventStream.keyBy(<span class="string">"orderId"</span>), orderPayPattern)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> timeoutFunction = (</span><br><span class="line">      map: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">OrderEvent</span>]],</span><br><span class="line">      timestamp: <span class="type">Long</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[<span class="type">OrderEvent</span>]</span><br><span class="line">    ) =&gt; &#123;</span><br><span class="line">      print(timestamp)</span><br><span class="line">      <span class="keyword">val</span> orderStart = map.get(<span class="string">"begin"</span>).get.head</span><br><span class="line">      out.collect(orderStart)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> selectFunction = (</span><br><span class="line">      map: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">OrderEvent</span>]],</span><br><span class="line">      out: <span class="type">Collector</span>[<span class="type">OrderEvent</span>]</span><br><span class="line">    ) =&gt; &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> timeoutOrder = patternStream</span><br><span class="line">      .flatSelect(orderTimeoutOutput)(timeoutFunction)(selectFunction)</span><br><span class="line"></span><br><span class="line">    timeoutOrder.getSideOutput(orderTimeoutOutput).print()</span><br><span class="line"></span><br><span class="line">    env.execute</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="使用Process-Function实现订单超时需求"><a href="#使用Process-Function实现订单超时需求" class="headerlink" title="使用Process Function实现订单超时需求"></a>使用Process Function实现订单超时需求</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.ysss.project</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.<span class="type">ValueStateDescriptor</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.typeutils.<span class="type">Types</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.<span class="type">KeyedProcessFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OrderTimeoutWIthoutCep</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderEvent</span>(<span class="params">orderId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                        eventType: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                        eventTime: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = env</span><br><span class="line">      .fromElements(</span><br><span class="line">        <span class="type">OrderEvent</span>(<span class="string">"1"</span>, <span class="string">"create"</span>, <span class="string">"2"</span>),</span><br><span class="line">        <span class="type">OrderEvent</span>(<span class="string">"2"</span>, <span class="string">"create"</span>, <span class="string">"3"</span>),</span><br><span class="line">        <span class="type">OrderEvent</span>(<span class="string">"2"</span>, <span class="string">"pay"</span>, <span class="string">"4"</span>)</span><br><span class="line">      )</span><br><span class="line">      .assignAscendingTimestamps(_.eventTime.toLong * <span class="number">1000</span>L)</span><br><span class="line">      .keyBy(_.orderId)</span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">OrderMatchFunc</span>)</span><br><span class="line"></span><br><span class="line">    stream.print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">OrderMatchFunc</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">OrderEvent</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> orderState = getRuntimeContext.getState(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">OrderEvent</span>](<span class="string">"saved order"</span>, <span class="type">Types</span>.of[<span class="type">OrderEvent</span>])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: <span class="type">OrderEvent</span>,</span><br><span class="line">                                ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">OrderEvent</span>, <span class="type">String</span>]#<span class="type">Context</span>,</span><br><span class="line">                                out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (value.eventType.equals(<span class="string">"create"</span>)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (orderState.value() == <span class="literal">null</span>) &#123; <span class="comment">// 为什么要判空？因为可能出现`pay`先到的情况</span></span><br><span class="line">          <span class="comment">// 如果orderState为空，保存`create`事件</span></span><br><span class="line">          orderState.update(value)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 保存`pay`事件</span></span><br><span class="line">        orderState.update(value)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      ctx.timerService().registerEventTimeTimer(value.eventTime.toLong * <span class="number">1000</span> + <span class="number">5000</span>L)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(timestamp: <span class="type">Long</span>,</span><br><span class="line">                         ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">OrderEvent</span>, <span class="type">String</span>]#<span class="type">OnTimerContext</span>,</span><br><span class="line">                         out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> savedOrder = orderState.value()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (savedOrder != <span class="literal">null</span> &amp;&amp; savedOrder.eventType.equals(<span class="string">"create"</span>)) &#123;</span><br><span class="line">        out.collect(<span class="string">"超时订单的ID为："</span> + savedOrder.orderId)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      orderState.clear()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="实时对帐：实现两条流的Join"><a href="#实时对帐：实现两条流的Join" class="headerlink" title="实时对帐：实现两条流的Join"></a>实时对帐：实现两条流的Join</h2><p>完整代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.<span class="type">CoProcessFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">OutputTag</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderEvent</span>(<span class="params">orderId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                      eventType: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                      eventTime: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">PayEvent</span>(<span class="params">orderId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                    eventType: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                    eventTime: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">TwoStreamsJoin</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> unmatchedOrders = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">OrderEvent</span>](<span class="string">"unmatchedOrders"</span>)&#123;&#125;</span><br><span class="line">  <span class="keyword">val</span> unmatchedPays = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">PayEvent</span>](<span class="string">"unmatchedPays"</span>)&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> orders = env</span><br><span class="line">      .fromCollection(<span class="type">List</span>(</span><br><span class="line">      <span class="type">OrderEvent</span>(<span class="string">"1"</span>, <span class="string">"create"</span>, <span class="string">"1558430842"</span>),</span><br><span class="line">      <span class="type">OrderEvent</span>(<span class="string">"2"</span>, <span class="string">"create"</span>, <span class="string">"1558430843"</span>),</span><br><span class="line">      <span class="type">OrderEvent</span>(<span class="string">"1"</span>, <span class="string">"pay"</span>, <span class="string">"1558430844"</span>),</span><br><span class="line">      <span class="type">OrderEvent</span>(<span class="string">"2"</span>, <span class="string">"pay"</span>, <span class="string">"1558430845"</span>),</span><br><span class="line">      <span class="type">OrderEvent</span>(<span class="string">"3"</span>, <span class="string">"create"</span>, <span class="string">"1558430849"</span>),</span><br><span class="line">      <span class="type">OrderEvent</span>(<span class="string">"3"</span>, <span class="string">"pay"</span>, <span class="string">"1558430849"</span>)</span><br><span class="line">    )).assignAscendingTimestamps(_.eventTime.toLong * <span class="number">1000</span>)</span><br><span class="line">      .keyBy(<span class="string">"orderId"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pays = env.fromCollection(<span class="type">List</span>(</span><br><span class="line">      <span class="type">PayEvent</span>(<span class="string">"1"</span>, <span class="string">"weixin"</span>, <span class="string">"1558430847"</span>),</span><br><span class="line">      <span class="type">PayEvent</span>(<span class="string">"2"</span>, <span class="string">"zhifubao"</span>, <span class="string">"1558430848"</span>),</span><br><span class="line">      <span class="type">PayEvent</span>(<span class="string">"4"</span>, <span class="string">"zhifubao"</span>, <span class="string">"1558430850"</span>)</span><br><span class="line">    )).assignAscendingTimestamps(_.eventTime.toLong * <span class="number">1000</span>)</span><br><span class="line">      .keyBy(<span class="string">"orderId"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> processed = orders</span><br><span class="line">      .connect(pays)</span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">EnrichmentFunction</span>)</span><br><span class="line"></span><br><span class="line">    processed.getSideOutput[<span class="type">PayEvent</span>](unmatchedPays).print()</span><br><span class="line">    processed.getSideOutput[<span class="type">OrderEvent</span>](unmatchedOrders).print()</span><br><span class="line"></span><br><span class="line">    env.execute</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">EnrichmentFunction</span> <span class="keyword">extends</span> <span class="title">CoProcessFunction</span>[</span></span><br><span class="line"><span class="class">    <span class="type">OrderEvent</span>, <span class="type">PayEvent</span>, (<span class="type">OrderEvent</span>, <span class="type">PayEvent</span>)] </span>&#123;</span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> orderState: <span class="type">ValueState</span>[<span class="type">OrderEvent</span>] = getRuntimeContext</span><br><span class="line">      .getState(<span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">OrderEvent</span>](<span class="string">"saved order"</span>,</span><br><span class="line">        classOf[<span class="type">OrderEvent</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> payState: <span class="type">ValueState</span>[<span class="type">PayEvent</span>] = getRuntimeContext</span><br><span class="line">      .getState(<span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">PayEvent</span>](<span class="string">"saved pay"</span>,</span><br><span class="line">        classOf[<span class="type">PayEvent</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement1</span></span>(</span><br><span class="line">      order: <span class="type">OrderEvent</span>,</span><br><span class="line">      context: <span class="type">CoProcessFunction</span>[<span class="type">OrderEvent</span>,</span><br><span class="line">        <span class="type">PayEvent</span>, (<span class="type">OrderEvent</span>, <span class="type">PayEvent</span>)]#<span class="type">Context</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[(<span class="type">OrderEvent</span>, <span class="type">PayEvent</span>)]</span><br><span class="line">    ): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> pay = payState.value()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (pay != <span class="literal">null</span>) &#123;</span><br><span class="line">        payState.clear()</span><br><span class="line">        out.collect((order, pay))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        orderState.update(order)</span><br><span class="line">        <span class="comment">// as soon as the watermark arrives,</span></span><br><span class="line">        <span class="comment">// we can stop waiting for the corresponding pay</span></span><br><span class="line">        context.timerService</span><br><span class="line">          .registerEventTimeTimer(order.eventTime.toLong * <span class="number">1000</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement2</span></span>(</span><br><span class="line">      pay: <span class="type">PayEvent</span>,</span><br><span class="line">      context: <span class="type">CoProcessFunction</span>[<span class="type">OrderEvent</span>,</span><br><span class="line">        <span class="type">PayEvent</span>,(<span class="type">OrderEvent</span>, <span class="type">PayEvent</span>)]#<span class="type">Context</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[(<span class="type">OrderEvent</span>, <span class="type">PayEvent</span>)]</span><br><span class="line">    ): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> order = orderState.value()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (order != <span class="literal">null</span>) &#123;</span><br><span class="line">        orderState.clear()</span><br><span class="line">        out.collect((order, pay))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        payState.update(pay)</span><br><span class="line">        context</span><br><span class="line">          .timerService</span><br><span class="line">          .registerEventTimeTimer(pay.eventTime.toLong * <span class="number">1000</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(</span><br><span class="line">      timestamp: <span class="type">Long</span>,</span><br><span class="line">      ctx: <span class="type">CoProcessFunction</span>[<span class="type">OrderEvent</span>,</span><br><span class="line">        <span class="type">PayEvent</span>, (<span class="type">OrderEvent</span>, <span class="type">PayEvent</span>)]#<span class="type">OnTimerContext</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[(<span class="type">OrderEvent</span>, <span class="type">PayEvent</span>)]</span><br><span class="line">    ): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (payState.value != <span class="literal">null</span>) &#123;</span><br><span class="line">        ctx.output(unmatchedPays, payState.value)</span><br><span class="line">        payState.clear()</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (orderState.value != <span class="literal">null</span>) &#123;</span><br><span class="line">        ctx.output(unmatchedOrders, orderState.value)</span><br><span class="line">        orderState.clear()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据集解析&quot;&gt;&lt;a href=&quot;#数据集解析&quot; class=&quot;headerlink&quot; title=&quot;数据集解析&quot;&gt;&lt;/a&gt;数据集解析&lt;/h2&gt;&lt;h3 id=&quot;淘宝数据集解析&quot;&gt;&lt;a href=&quot;#淘宝数据集解析&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink系列11Table API 和 Flink SQL</title>
    <link href="https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9711Table-API-%E5%92%8C-Flink-SQL/"/>
    <id>https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9711Table-API-%E5%92%8C-Flink-SQL/</id>
    <published>2020-07-02T03:51:53.000Z</published>
    <updated>2020-07-02T03:56:32.076Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Table-API-和-Flink-SQL"><a href="#Table-API-和-Flink-SQL" class="headerlink" title="Table API 和 Flink SQL"></a>Table API 和 Flink SQL</h1><h2 id="整体介绍"><a href="#整体介绍" class="headerlink" title="整体介绍"></a>整体介绍</h2><h3 id="什么是-Table-API-和-Flink-SQL"><a href="#什么是-Table-API-和-Flink-SQL" class="headerlink" title="什么是 Table API 和 Flink SQL"></a>什么是 Table API 和 Flink SQL</h3><p>Flink本身是批流统一的处理框架，所以Table API和SQL，就是批流统一的上层处理API。目前功能尚未完善，处于活跃的开发阶段。</p><p>Table API是一套内嵌在Java和Scala语言中的查询API，它允许我们以非常直观的方式，组合来自一些关系运算符的查询（比如select、filter和join）。而对于Flink SQL，就是直接可以在代码中写SQL，来实现一些查询（Query）操作。Flink的SQL支持，基于实现了SQL标准的Apache Calcite（Apache开源SQL解析工具）。</p><p>无论输入是批输入还是流式输入，在这两套API中，指定的查询都具有相同的语义，得到相同的结果。</p><h3 id="需要引入的依赖"><a href="#需要引入的依赖" class="headerlink" title="需要引入的依赖"></a>需要引入的依赖</h3><p>Table API和SQL需要引入的依赖有两个：planner和bridge。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-scala-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><ul><li>flink-table-planner：planner计划器，是table API最主要的部分，提供了运行时环境和生成程序执行计划的planner；</li><li>flink-table-api-scala-bridge：bridge桥接器，主要负责table API和 DataStream/DataSet API的连接支持，按照语言分java和scala。</li></ul><p>这里的两个依赖，是IDE环境下运行需要添加的；如果是生产环境，lib目录下默认已经有了planner，就只需要有bridge就可以了。</p><p>当然，如果想使用用户自定义函数，或是跟kafka做连接，需要有一个SQL client，这个包含在flink-table-common里。</p><h3 id="两种planner（old-amp-blink）的区别"><a href="#两种planner（old-amp-blink）的区别" class="headerlink" title="两种planner（old &amp; blink）的区别"></a>两种planner（old &amp; blink）的区别</h3><ol><li>批流统一：Blink将批处理作业，视为流式处理的特殊情况。所以，blink不支持表和DataSet之间的转换，批处理作业将不转换为DataSet应用程序，而是跟流处理一样，转换为DataStream程序来处理。</li><li>因为批流统一，Blink planner也不支持BatchTableSource，而使用有界的StreamTableSource代替。</li><li>Blink planner只支持全新的目录，不支持已弃用的ExternalCatalog。</li><li>旧planner和Blink planner的FilterableTableSource实现不兼容。旧的planner会把PlannerExpressions下推到filterableTableSource中，而blink planner则会把Expressions下推。</li><li>基于字符串的键值配置选项仅适用于Blink planner。</li><li>PlannerConfig在两个planner中的实现不同。</li><li>Blink planner会将多个sink优化在一个DAG中（仅在TableEnvironment上受支持，而在StreamTableEnvironment上不受支持）。而旧planner的优化总是将每一个sink放在一个新的DAG中，其中所有DAG彼此独立。</li><li>旧的planner不支持目录统计，而Blink planner支持。</li></ol><h2 id="API调用"><a href="#API调用" class="headerlink" title="API调用"></a>API调用</h2><h3 id="基本程序结构"><a href="#基本程序结构" class="headerlink" title="基本程序结构"></a>基本程序结构</h3><p>Table API 和 SQL 的程序结构，与流式处理的程序结构类似；也可以近似地认为有这么几步：首先创建执行环境，然后定义source、transform和sink。</p><p>具体操作流程如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tableEnv = ...  <span class="comment">// 创建表的执行环境</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一张表，用于读取数据</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"inputTable"</span>)</span><br><span class="line"><span class="comment">// 注册一张表，用于把计算结果输出</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"outputTable"</span>)</span><br><span class="line"><span class="comment">// 通过 Table API 查询算子，得到一张结果表</span></span><br><span class="line"><span class="keyword">val</span> result = tableEnv.from(<span class="string">"inputTable"</span>).select(...)</span><br><span class="line"><span class="comment">// 通过 SQL查询语句，得到一张结果表</span></span><br><span class="line"><span class="keyword">val</span> sqlResult  = tableEnv.sqlQuery(<span class="string">"SELECT ... FROM inputTable ..."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将结果表写入输出表中</span></span><br><span class="line">result.insertInto(<span class="string">"outputTable"</span>)</span><br></pre></td></tr></table></figure></div><h3 id="创建表环境"><a href="#创建表环境" class="headerlink" title="创建表环境"></a>创建表环境</h3><p>创建表环境最简单的方式，就是基于流处理执行环境，调create方法直接创建：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create(env)</span><br></pre></td></tr></table></figure></div><p>表环境（TableEnvironment）是flink中集成Table API &amp; SQL的核心概念。它负责:</p><ul><li>注册catalog</li><li>在内部 catalog 中注册表</li><li>执行 SQL 查询</li><li>注册用户自定义函数</li><li>将 DataStream 或 DataSet 转换为表</li><li>保存对 ExecutionEnvironment 或 StreamExecutionEnvironment 的引用</li></ul><p>在创建TableEnv的时候，可以多传入一个EnvironmentSettings或者TableConfig参数，可以用来配置TableEnvironment的一些特性。</p><p>比如，配置老版本的流式查询（Flink-Streaming-Query）：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span></span><br><span class="line">  .newInstance()</span><br><span class="line">  .useOldPlanner()      <span class="comment">// 使用老版本planner</span></span><br><span class="line">  .inStreamingMode()    <span class="comment">// 流处理模式</span></span><br><span class="line">  .build()</span><br><span class="line">  </span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br></pre></td></tr></table></figure></div><p>基于老版本的批处理环境（Flink-Batch-Query）：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> batchEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> batchTableEnv = <span class="type">BatchTableEnvironment</span>.create(batchEnv)</span><br></pre></td></tr></table></figure></div><p>基于blink版本的流处理环境（Blink-Streaming-Query）：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> bsSettings = <span class="type">EnvironmentSettings</span></span><br><span class="line">  .newInstance()</span><br><span class="line">  .useBlinkPlanner()</span><br><span class="line">  .inStreamingMode()</span><br><span class="line">  .build()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> bsTableEnv = <span class="type">StreamTableEnvironment</span>.create(env, bsSettings)</span><br></pre></td></tr></table></figure></div><p>基于blink版本的批处理环境（Blink-Batch-Query）：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> bbSettings = <span class="type">EnvironmentSettings</span></span><br><span class="line">  .newInstance()</span><br><span class="line">  .useBlinkPlanner()</span><br><span class="line">  .inBatchMode().build()</span><br><span class="line">  </span><br><span class="line"><span class="keyword">val</span> bbTableEnv = <span class="type">TableEnvironment</span>.create(bbSettings)</span><br></pre></td></tr></table></figure></div><h3 id="在Catalog中注册表"><a href="#在Catalog中注册表" class="headerlink" title="在Catalog中注册表"></a>在Catalog中注册表</h3><h4 id="表（Table）的概念"><a href="#表（Table）的概念" class="headerlink" title="表（Table）的概念"></a>表（Table）的概念</h4><p>TableEnvironment可以注册目录Catalog，并可以基于Catalog注册表。它会维护一个Catalog-Table表之间的map。</p><p>表（Table）是由一个“标识符”来指定的，由3部分组成：Catalog名、数据库（database）名和对象名（表名）。如果没有指定目录或数据库，就使用当前的默认值。</p><p>表可以是常规的（Table，表），或者虚拟的（View，视图）。常规表（Table）一般可以用来描述外部数据，比如文件、数据库表或消息队列的数据，也可以直接从 DataStream转换而来。视图可以从现有的表中创建，通常是table API或者SQL查询的一个结果。</p><h4 id="连接到文件系统（Csv格式）"><a href="#连接到文件系统（Csv格式）" class="headerlink" title="连接到文件系统（Csv格式）"></a>连接到文件系统（Csv格式）</h4><p>连接外部系统在Catalog中注册表，直接调用tableEnv.connect()就可以，里面参数要传入一个ConnectorDescriptor，也就是connector描述器。对于文件系统的connector而言，flink内部已经提供了，就叫做FileSystem()。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tableEnv</span><br><span class="line">  .connect(<span class="keyword">new</span> <span class="type">FileSystem</span>().path(<span class="string">"sensor.txt"</span>))  <span class="comment">// 定义表数据来源，外部连接</span></span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">OldCsv</span>())    <span class="comment">// 定义从外部系统读取数据之后的格式化方法</span></span><br><span class="line">  .withSchema(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">      .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">      .field(<span class="string">"timestamp"</span>, <span class="type">DataTypes</span>.<span class="type">BIGINT</span>())</span><br><span class="line">      .field(<span class="string">"temperature"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">  )    <span class="comment">// 定义表结构</span></span><br><span class="line">  .createTemporaryTable(<span class="string">"inputTable"</span>)    <span class="comment">// 创建临时表</span></span><br></pre></td></tr></table></figure></div><p>这是旧版本的csv格式描述器。由于它是非标的，跟外部系统对接并不通用，所以将被弃用，以后会被一个符合RFC-4180标准的新format描述器取代。新的描述器就叫Csv()，但flink没有直接提供，需要引入依赖flink-csv：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-csv<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>代码非常类似，只需要把withFormat里的OldCsv改成Csv就可以了。</p><h4 id="连接到Kafka"><a href="#连接到Kafka" class="headerlink" title="连接到Kafka"></a>连接到Kafka</h4><p>kafka的连接器flink-kafka-connector中，1.10版本的已经提供了Table API的支持。我们可以在 connect方法中直接传入一个叫做Kafka的类，这就是kafka连接器的描述器ConnectorDescriptor。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tableEnv</span><br><span class="line">  .connect(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Kafka</span>()</span><br><span class="line">      .version(<span class="string">"0.11"</span>) <span class="comment">// 定义kafka的版本</span></span><br><span class="line">      .topic(<span class="string">"sensor"</span>) <span class="comment">// 定义主题</span></span><br><span class="line">      .property(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>)</span><br><span class="line">      .property(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">  )</span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">Csv</span>())</span><br><span class="line">  .withSchema(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">      .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">      .field(<span class="string">"timestamp"</span>, <span class="type">DataTypes</span>.<span class="type">BIGINT</span>())</span><br><span class="line">      .field(<span class="string">"temperature"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">  )</span><br><span class="line">  .createTemporaryTable(<span class="string">"kafkaInputTable"</span>)</span><br></pre></td></tr></table></figure></div><p>当然也可以连接到ElasticSearch、MySql、HBase、Hive等外部系统，实现方式基本上是类似的。</p><h3 id="表的查询"><a href="#表的查询" class="headerlink" title="表的查询"></a>表的查询</h3><p>利用外部系统的连接器connector，我们可以读写数据，并在环境的Catalog中注册表。接下来就可以对表做查询转换了。</p><p>Flink给我们提供了两种查询方式：Table API和 SQL。</p><h4 id="Table-API的调用"><a href="#Table-API的调用" class="headerlink" title="Table API的调用"></a>Table API的调用</h4><p>Table API是集成在Scala和Java语言内的查询API。与SQL不同，Table API的查询不会用字符串表示，而是在宿主语言中一步一步调用完成的。</p><p>Table API基于代表一张“表”的Table类，并提供一整套操作处理的方法API。这些方法会返回一个新的Table对象，这个对象就表示对输入表应用转换操作的结果。有些关系型转换操作，可以由多个方法调用组成，构成链式调用结构。例如table.select(…).filter(…)，其中select（…）表示选择表中指定的字段，filter(…)表示筛选条件。</p><p>代码中的实现如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorTable: <span class="type">Table</span> = tableEnv.from(<span class="string">"inputTable"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultTable: <span class="type">Table</span> = senorTable</span><br><span class="line">  .select(<span class="string">"id, temperature"</span>)</span><br><span class="line">  .filter(<span class="string">"id ='sensor_1'"</span>)</span><br></pre></td></tr></table></figure></div><h4 id="SQL查询"><a href="#SQL查询" class="headerlink" title="SQL查询"></a>SQL查询</h4><p>Flink的SQL集成，基于的是Apache Calcite，它实现了SQL标准。在Flink中，用常规字符串来定义SQL查询语句。SQL 查询的结果，是一个新的 Table。</p><p>代码实现如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultSqlTable: <span class="type">Table</span> = tableEnv</span><br><span class="line">  .sqlQuery(<span class="string">"select id, temperature from inputTable where id ='sensor_1'"</span>)</span><br></pre></td></tr></table></figure></div><p>或者：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultSqlTable: <span class="type">Table</span> = tableEnv.sqlQuery(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select id, temperature</span></span><br><span class="line"><span class="string">    |from inputTable</span></span><br><span class="line"><span class="string">    |where id = 'sensor_1'</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br></pre></td></tr></table></figure></div><p>当然，也可以加上聚合操作，比如我们统计每个sensor温度数据出现的个数，做个count统计：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> aggResultTable = sensorTable</span><br><span class="line">  .groupBy(<span class="symbol">'id</span>)</span><br><span class="line">  .select(<span class="symbol">'id</span>, <span class="symbol">'id</span>.count as <span class="symbol">'count</span>)</span><br></pre></td></tr></table></figure></div><p>SQL的实现：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> aggResultSqlTable = tableEnv</span><br><span class="line">  .sqlQuery(<span class="string">"select id, count(id) as cnt from inputTable group by id"</span>)</span><br></pre></td></tr></table></figure></div><p>这里Table API里指定的字段，前面加了一个单引号<code>&#39;</code>，这是Table API中定义的Expression类型的写法，可以很方便地表示一个表中的字段。</p><p>字段可以直接全部用双引号引起来，也可以用半边单引号+字段名的方式。以后的代码中，一般都用后一种形式。</p><h3 id="将DataStream转换成表"><a href="#将DataStream转换成表" class="headerlink" title="将DataStream转换成表"></a>将DataStream转换成表</h3><p>Flink允许我们把Table和DataStream做转换：我们可以基于一个DataStream，先流式地读取数据源，然后map成样例类，再把它转成Table。Table的列字段（column fields），就是样例类里的字段，这样就不用再麻烦地定义schema了。</p><h4 id="代码表达"><a href="#代码表达" class="headerlink" title="代码表达"></a>代码表达</h4><p>代码中实现非常简单，直接用tableEnv.fromDataStream()就可以了。默认转换后的 Table schema 和 DataStream 中的字段定义一一对应，也可以单独指定出来。</p><p>这就允许我们更换字段的顺序、重命名，或者只选取某些字段出来，相当于做了一次map操作（或者Table API的 select操作）。</p><p>代码具体如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">"sensor.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = inputStream</span><br><span class="line">  .map(data =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">    <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>), dataArray(<span class="number">1</span>).toLong, dataArray(<span class="number">2</span>).toDouble)</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sensorTable: <span class="type">Table</span> = tableEnv.fromDataStream(dataStream)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sensorTable2 = tableEnv.fromDataStream(dataStream, <span class="symbol">'id</span>, <span class="symbol">'timestamp</span> as <span class="symbol">'ts</span>)</span><br></pre></td></tr></table></figure></div><h4 id="数据类型与Table-schema的对应"><a href="#数据类型与Table-schema的对应" class="headerlink" title="数据类型与Table schema的对应"></a>数据类型与Table schema的对应</h4><p>在上节的例子中，DataStream 中的数据类型，与表的 Schema 之间的对应关系，是按照样例类中的字段名来对应的（name-based mapping），所以还可以用as做重命名。</p><p>另外一种对应方式是，直接按照字段的位置来对应（position-based mapping），对应的过程中，就可以直接指定新的字段名了。</p><p>基于名称的对应：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorTable = tableEnv</span><br><span class="line">  .fromDataStream(dataStream, <span class="symbol">'timestamp</span> as <span class="symbol">'ts</span>, <span class="symbol">'id</span> as <span class="symbol">'myId</span>, <span class="symbol">'temperature</span>)</span><br></pre></td></tr></table></figure></div><p>基于位置的对应：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorTable = tableEnv</span><br><span class="line">  .fromDataStream(dataStream, <span class="symbol">'myId</span>, <span class="symbol">'ts</span>)</span><br></pre></td></tr></table></figure></div><p>Flink的DataStream和 DataSet API支持多种类型。</p><p>组合类型，比如元组（内置Scala和Java元组）、POJO、Scala case类和Flink的Row类型等，允许具有多个字段的嵌套数据结构，这些字段可以在Table的表达式中访问。其他类型，则被视为原子类型。</p><p>元组类型和原子类型，一般用位置对应会好一些；如果非要用名称对应，也是可以的：</p><p>元组类型，默认的名称是 “_1”, “_2”；而原子类型，默认名称是 ”f0”。</p><h3 id="创建临时视图（Temporary-View）"><a href="#创建临时视图（Temporary-View）" class="headerlink" title="创建临时视图（Temporary View）"></a>创建临时视图（Temporary View）</h3><p>创建临时视图的第一种方式，就是直接从DataStream转换而来。同样，可以直接对应字段转换；也可以在转换的时候，指定相应的字段。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(<span class="string">"sensorView"</span>, dataStream)</span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"sensorView"</span>,</span><br><span class="line">  dataStream, <span class="symbol">'id</span>, <span class="symbol">'temperature</span>, <span class="symbol">'timestamp</span> as <span class="symbol">'ts</span>)</span><br></pre></td></tr></table></figure></div><p>另外，当然还可以基于Table创建视图：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(<span class="string">"sensorView"</span>, sensorTable)</span><br></pre></td></tr></table></figure></div><p>View和Table的Schema完全相同。事实上，在Table API中，可以认为View和Table是等价的。</p><h3 id="输出表"><a href="#输出表" class="headerlink" title="输出表"></a>输出表</h3><p>表的输出，是通过将数据写入 TableSink 来实现的。TableSink 是一个通用接口，可以支持不同的文件格式、存储数据库和消息队列。</p><p>具体实现，输出表最直接的方法，就是通过 Table.insertInto() 方法将一个 Table 写入注册过的 TableSink 中。</p><h4 id="输出到文件"><a href="#输出到文件" class="headerlink" title="输出到文件"></a>输出到文件</h4><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注册输出表</span></span><br><span class="line">tableEnv.connect(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">FileSystem</span>().path(<span class="string">"…\\resources\\out.txt"</span>)</span><br><span class="line">  ) <span class="comment">// 定义到文件系统的连接</span></span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">Csv</span>()) <span class="comment">// 定义格式化方法，Csv格式</span></span><br><span class="line">  .withSchema(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">      .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">      .field(<span class="string">"temp"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">  ) <span class="comment">// 定义表结构</span></span><br><span class="line">  .createTemporaryTable(<span class="string">"outputTable"</span>) <span class="comment">// 创建临时表</span></span><br><span class="line"></span><br><span class="line">resultSqlTable.insertInto(<span class="string">"outputTable"</span>)</span><br></pre></td></tr></table></figure></div><h4 id="更新模式（Update-Mode）"><a href="#更新模式（Update-Mode）" class="headerlink" title="更新模式（Update Mode）"></a>更新模式（Update Mode）</h4><p>在流处理过程中，表的处理并不像传统定义的那样简单。</p><p>对于流式查询（Streaming Queries），需要声明如何在（动态）表和外部连接器之间执行转换。与外部系统交换的消息类型，由更新模式（update mode）指定。</p><p>Flink Table API中的更新模式有以下三种：</p><ol><li>追加模式（Append Mode）</li></ol><p>在追加模式下，表（动态表）和外部连接器只交换插入（Insert）消息。</p><ol><li>撤回模式（Retract Mode）</li></ol><p>在撤回模式下，表和外部连接器交换的是：添加（Add）和撤回（Retract）消息。</p><ul><li>插入（Insert）会被编码为添加消息；</li><li>删除（Delete）则编码为撤回消息；</li><li>更新（Update）则会编码为，已更新行（上一行）的撤回消息，和更新行（新行）的添加消息。</li></ul><p>在此模式下，不能定义key，这一点跟upsert模式完全不同。</p><ol><li>Upsert（更新插入）模式</li></ol><p>在Upsert模式下，动态表和外部连接器交换Upsert和Delete消息。</p><p>这个模式需要一个唯一的key，通过这个key可以传递更新消息。为了正确应用消息，外部连接器需要知道这个唯一key的属性。</p><ul><li>插入（Insert）和更新（Update）都被编码为Upsert消息；</li><li>删除（Delete）编码为Delete信息。</li></ul><p>这种模式和Retract模式的主要区别在于，Update操作是用单个消息编码的，所以效率会更高。</p><h4 id="输出到Kafka"><a href="#输出到Kafka" class="headerlink" title="输出到Kafka"></a>输出到Kafka</h4><p>除了输出到文件，也可以输出到Kafka。我们可以结合前面Kafka作为输入数据，构建数据管道，kafka进，kafka出。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出到 kafka</span></span><br><span class="line">tableEnv.connect(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Kafka</span>()</span><br><span class="line">    .version(<span class="string">"0.11"</span>)</span><br><span class="line">    .topic(<span class="string">"sinkTest"</span>)</span><br><span class="line">    .property(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>)</span><br><span class="line">    .property(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">  )</span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">Csv</span>())</span><br><span class="line">  .withSchema(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">      .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">      .field(<span class="string">"temp"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">  )</span><br><span class="line">  .createTemporaryTable(<span class="string">"kafkaOutputTable"</span>)</span><br><span class="line">  </span><br><span class="line">resultTable.insertInto(<span class="string">"kafkaOutputTable"</span>)</span><br></pre></td></tr></table></figure></div><h4 id="输出到ElasticSearch"><a href="#输出到ElasticSearch" class="headerlink" title="输出到ElasticSearch"></a>输出到ElasticSearch</h4><p>ElasticSearch的connector可以在upsert（update+insert，更新插入）模式下操作，这样就可以使用Query定义的键（key）与外部系统交换UPSERT/DELETE消息。</p><p>另外，对于“仅追加”（append-only）的查询，connector还可以在append 模式下操作，这样就可以与外部系统只交换insert消息。</p><p>es目前支持的数据格式，只有Json，而flink本身并没有对应的支持，所以还需要引入依赖：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-json<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>代码实现如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出到es</span></span><br><span class="line">tableEnv.connect(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Elasticsearch</span>()</span><br><span class="line">    .version(<span class="string">"6"</span>)</span><br><span class="line">    .host(<span class="string">"localhost"</span>, <span class="number">9200</span>, <span class="string">"http"</span>)</span><br><span class="line">    .index(<span class="string">"sensor"</span>)</span><br><span class="line">    .documentType(<span class="string">"temp"</span>)</span><br><span class="line">  )</span><br><span class="line">  .inUpsertMode()           <span class="comment">// 指定是 Upsert 模式</span></span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">Json</span>())</span><br><span class="line">  .withSchema(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">      .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">      .field(<span class="string">"count"</span>, <span class="type">DataTypes</span>.<span class="type">BIGINT</span>())</span><br><span class="line">  )</span><br><span class="line">  .createTemporaryTable(<span class="string">"esOutputTable"</span>)</span><br><span class="line">  </span><br><span class="line">aggResultTable.insertInto(<span class="string">"esOutputTable"</span>)</span><br></pre></td></tr></table></figure></div><h4 id="输出到MySql"><a href="#输出到MySql" class="headerlink" title="输出到MySql"></a>输出到MySql</h4><p>Flink专门为Table API的jdbc连接提供了flink-jdbc连接器，我们需要先引入依赖：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-jdbc_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>jdbc连接的代码实现比较特殊，因为没有对应的java/scala类实现ConnectorDescriptor，所以不能直接tableEnv.connect()。不过Flink SQL留下了执行DDL的接口：tableEnv.sqlUpdate()。</p><p>对于jdbc的创建表操作，天生就适合直接写DDL来实现，所以我们的代码可以这样写：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出到 Mysql</span></span><br><span class="line"><span class="keyword">val</span> sinkDDL: <span class="type">String</span> =</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |create table jdbcOutputTable (</span></span><br><span class="line"><span class="string">    |  id varchar(20) not null,</span></span><br><span class="line"><span class="string">    |  cnt bigint not null</span></span><br><span class="line"><span class="string">    |) with (</span></span><br><span class="line"><span class="string">    |  'connector.type' = 'jdbc',</span></span><br><span class="line"><span class="string">    |  'connector.url' = 'jdbc:mysql://localhost:3306/test',</span></span><br><span class="line"><span class="string">    |  'connector.table' = 'sensor_count',</span></span><br><span class="line"><span class="string">    |  'connector.driver' = 'com.mysql.jdbc.Driver',</span></span><br><span class="line"><span class="string">    |  'connector.username' = 'root',</span></span><br><span class="line"><span class="string">    |  'connector.password' = '123456'</span></span><br><span class="line"><span class="string">    |)</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line">tableEnv.sqlUpdate(sinkDDL)</span><br><span class="line">aggResultSqlTable.insertInto(<span class="string">"jdbcOutputTable"</span>)</span><br></pre></td></tr></table></figure></div><h3 id="将表转换成DataStream"><a href="#将表转换成DataStream" class="headerlink" title="将表转换成DataStream"></a>将表转换成DataStream</h3><p>表可以转换为DataStream或DataSet。这样，自定义流处理或批处理程序就可以继续在 Table API或SQL查询的结果上运行了。</p><p>将表转换为DataStream或DataSet时，需要指定生成的数据类型，即要将表的每一行转换成的数据类型。通常，最方便的转换类型就是Row。当然，因为结果的所有字段类型都是明确的，我们也经常会用元组类型来表示。</p><p>表作为流式查询的结果，是动态更新的。所以，将这种动态查询转换成的数据流，同样需要对表的更新操作进行编码，进而有不同的转换模式。</p><p>Table API中表到DataStream有两种模式：</p><ul><li>追加模式（Append Mode）</li></ul><p>用于表只会被插入（Insert）操作更改的场景。</p><ul><li>撤回模式（Retract Mode）</li></ul><p>用于任何场景。有些类似于更新模式中Retract模式，它只有Insert和Delete两类操作。</p><p>得到的数据会增加一个Boolean类型的标识位（返回的第一个字段），用它来表示到底是新增的数据（Insert），还是被删除的数据（老数据，Delete）。</p><p>代码实现如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultStream: <span class="type">DataStream</span>[<span class="type">Row</span>] = tableEnv</span><br><span class="line">  .toAppendStream[<span class="type">Row</span>](resultTable)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> aggResultStream: <span class="type">DataStream</span>[(<span class="type">Boolean</span>, (<span class="type">String</span>, <span class="type">Long</span>))] = tableEnv</span><br><span class="line">  .toRetractStream[(<span class="type">String</span>, <span class="type">Long</span>)](aggResultTable)</span><br><span class="line"></span><br><span class="line">resultStream.print(<span class="string">"result"</span>)</span><br><span class="line">aggResultStream.print(<span class="string">"aggResult"</span>)</span><br></pre></td></tr></table></figure></div><p>所以，没有经过groupby之类聚合操作，可以直接用toAppendStream来转换；而如果经过了聚合，有更新操作，一般就必须用toRetractDstream。</p><h3 id="Query的解释和执行"><a href="#Query的解释和执行" class="headerlink" title="Query的解释和执行"></a>Query的解释和执行</h3><p>Table API提供了一种机制来解释（Explain）计算表的逻辑和优化查询计划。这是通过TableEnvironment.explain（table）方法或TableEnvironment.explain（）方法完成的。</p><p>explain方法会返回一个字符串，描述三个计划：</p><ul><li>未优化的逻辑查询计划</li><li>优化后的逻辑查询计划</li><li>实际执行计划</li></ul><p>我们可以在代码中查看执行计划：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> explaination: <span class="type">String</span> = tableEnv.explain(resultTable)</span><br><span class="line">println(explaination)</span><br></pre></td></tr></table></figure></div><p>Query的解释和执行过程，老planner和blink planner大体是一致的，又有所不同。整体来讲，Query都会表示成一个逻辑查询计划，然后分两步解释：</p><ol><li>优化查询计划</li><li>解释成 DataStream 或者 DataSet程序</li></ol><p>而Blink版本是批流统一的，所以所有的Query，只会被解释成DataStream程序；另外在批处理环境TableEnvironment下，Blink版本要到tableEnv.execute()执行调用才开始解释。</p><h2 id="流处理中的特殊概念"><a href="#流处理中的特殊概念" class="headerlink" title="流处理中的特殊概念"></a>流处理中的特殊概念</h2><p>Table API和SQL，本质上还是基于关系型表的操作方式；而关系型表、关系代数，以及SQL本身，一般是有界的，更适合批处理的场景。这就导致在进行流处理的过程中，理解会稍微复杂一些，需要引入一些特殊概念。</p><h3 id="流处理和关系代数（表，及SQL）的区别"><a href="#流处理和关系代数（表，及SQL）的区别" class="headerlink" title="流处理和关系代数（表，及SQL）的区别"></a>流处理和关系代数（表，及SQL）的区别</h3><table><thead><tr><th></th><th>关系代数（表）/SQL</th><th>流处理</th></tr></thead><tbody><tr><td>处理的数据对象</td><td>字段元组的有界集合</td><td>字段元组的无限序列</td></tr><tr><td>查询（Query）对数据的访问</td><td>可以访问到完整的数据输入</td><td>无法访问所有数据，必须持续等待流式输入</td></tr><tr><td>查询终止条件</td><td>生成固定大小的结果集后终止</td><td>永不停止，根据持续收到的数据不断更新查询结果</td></tr></tbody></table><p>可以看到，其实关系代数（主要就是指关系型数据库中的表）和SQL，主要就是针对批处理的，这和流处理有天生的隔阂。</p><h3 id="动态表（Dynamic-Tables）"><a href="#动态表（Dynamic-Tables）" class="headerlink" title="动态表（Dynamic Tables）"></a>动态表（Dynamic Tables）</h3><p>因为流处理面对的数据，是连续不断的，这和我们熟悉的关系型数据库中保存的“表”完全不同。所以，如果我们把流数据转换成Table，然后执行类似于table的select操作，结果就不是一成不变的，而是随着新数据的到来，会不停更新。</p><p>我们可以随着新数据的到来，不停地在之前的基础上更新结果。这样得到的表，在Flink Table API概念里，就叫做“动态表”（Dynamic Tables）。</p><p>动态表是Flink对流数据的Table API和SQL支持的核心概念。与表示批处理数据的静态表不同，动态表是随时间变化的。动态表可以像静态的批处理表一样进行查询，查询一个动态表会产生持续查询（Continuous Query）。连续查询永远不会终止，并会生成另一个动态表。查询（Query）会不断更新其动态结果表，以反映其动态输入表上的更改。</p><h3 id="流式持续查询的过程"><a href="#流式持续查询的过程" class="headerlink" title="流式持续查询的过程"></a>流式持续查询的过程</h3><p>下图显示了流、动态表和连续查询的关系：</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/stream-query-stream.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/stream-query-stream.png" class="lazyload"></a></p><p>流式持续查询的过程为：</p><ol><li>流被转换为动态表。</li><li>对动态表计算连续查询，生成新的动态表。</li><li>生成的动态表被转换回流。</li></ol><h4 id="将流转换成表（Table）"><a href="#将流转换成表（Table）" class="headerlink" title="将流转换成表（Table）"></a>将流转换成表（Table）</h4><p>为了处理带有关系查询的流，必须先将其转换为表。</p><p>从概念上讲，流的每个数据记录，都被解释为对结果表的插入（Insert）修改。因为流式持续不断的，而且之前的输出结果无法改变。本质上，我们其实是从一个、只有插入操作的changelog（更新日志）流，来构建一个表。</p><p>为了更好地说明动态表和持续查询的概念，我们来举一个具体的例子。</p><p>比如，我们现在的输入数据，就是用户在网站上的访问行为，数据类型（Schema）如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang"></div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  user:  VARCHAR,   // 用户名</span><br><span class="line">  cTime: TIMESTAMP, // 访问某个URL的时间戳</span><br><span class="line">  url:   VARCHAR    // 用户访问的URL</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>下图显示了如何将访问URL事件流，或者叫点击事件流（左侧）转换为表（右侧）。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/append-mode.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/append-mode.png" class="lazyload"></a></p><p>随着插入更多的访问事件流记录，生成的表将不断增长。</p><h4 id="持续查询（Continuous-Query）"><a href="#持续查询（Continuous-Query）" class="headerlink" title="持续查询（Continuous Query）"></a>持续查询（Continuous Query）</h4><p>持续查询，会在动态表上做计算处理，并作为结果生成新的动态表。与批处理查询不同，连续查询从不终止，并根据输入表上的更新更新其结果表。</p><p>在任何时间点，连续查询的结果在语义上，等同于在输入表的快照上，以批处理模式执行的同一查询的结果。</p><p>在下面的示例中，我们展示了对点击事件流中的一个持续查询。</p><p>这个Query很简单，是一个分组聚合做count统计的查询。它将用户字段上的clicks表分组，并统计访问的url数。图中显示了随着时间的推移，当clicks表被其他行更新时如何计算查询。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/query-groupBy-cnt.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/query-groupBy-cnt.png" class="lazyload"></a></p><h4 id="将动态表转换成流"><a href="#将动态表转换成流" class="headerlink" title="将动态表转换成流"></a>将动态表转换成流</h4><p>与常规的数据库表一样，动态表可以通过插入（Insert）、更新（Update）和删除（Delete）更改，进行持续的修改。将动态表转换为流或将其写入外部系统时，需要对这些更改进行编码。Flink的Table API和SQL支持三种方式对动态表的更改进行编码：</p><ol><li>仅追加（Append-only）流</li></ol><p>仅通过插入（Insert）更改，来修改的动态表，可以直接转换为“仅追加”流。这个流中发出的数据，就是动态表中新增的每一行。</p><ol><li>撤回（Retract）流</li></ol><p>Retract流是包含两类消息的流，添加（Add）消息和撤回（Retract）消息。</p><p>动态表通过将INSERT 编码为add消息、DELETE 编码为retract消息、UPDATE编码为被更改行（前一行）的retract消息和更新后行（新行）的add消息，转换为retract流。</p><p>下图显示了将动态表转换为Retract流的过程。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/undo-redo-mode.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/undo-redo-mode.png" class="lazyload"></a></p><ol><li>Upsert（更新插入）流</li></ol><p>Upsert流包含两种类型的消息：Upsert消息和delete消息。转换为upsert流的动态表，需要有唯一的键（key）。</p><p>通过将INSERT和UPDATE更改编码为upsert消息，将DELETE更改编码为DELETE消息，就可以将具有唯一键（Unique Key）的动态表转换为流。</p><p>下图显示了将动态表转换为upsert流的过程。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/redo-mode.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/redo-mode.png" class="lazyload"></a></p><p>这些概念我们之前都已提到过。需要注意的是，在代码里将动态表转换为DataStream时，仅支持Append和Retract流。而向外部系统输出动态表的TableSink接口，则可以有不同的实现，比如之前我们讲到的ES，就可以有Upsert模式。</p><h3 id="时间特性"><a href="#时间特性" class="headerlink" title="时间特性"></a>时间特性</h3><p>基于时间的操作（比如Table API和SQL中窗口操作），需要定义相关的时间语义和时间数据来源的信息。所以，Table可以提供一个逻辑上的时间字段，用于在表处理程序中，指示时间和访问相应的时间戳。</p><p>时间属性，可以是每个表schema的一部分。一旦定义了时间属性，它就可以作为一个字段引用，并且可以在基于时间的操作中使用。</p><p>时间属性的行为类似于常规时间戳，可以访问，并且进行计算。</p><h4 id="处理时间（Processing-Time）"><a href="#处理时间（Processing-Time）" class="headerlink" title="处理时间（Processing Time）"></a>处理时间（Processing Time）</h4><p>处理时间语义下，允许表处理程序根据机器的本地时间生成结果。它是时间的最简单概念。它既不需要提取时间戳，也不需要生成watermark。</p><p>定义处理时间属性有三种方法：在DataStream转化时直接指定；在定义Table Schema时指定；在创建表的DDL中指定。</p><ol><li>DataStream转化成Table时指定</li></ol><p>由DataStream转换成表时，可以在后面指定字段名来定义Schema。在定义Schema期间，可以使用.proctime，定义处理时间字段。</p><p>注意，这个proctime属性只能通过附加逻辑字段，来扩展物理schema。因此，只能在schema定义的末尾定义它。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义好 DataStream</span></span><br><span class="line"><span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">"\\sensor.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = inputStream</span><br><span class="line">  .map(data =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">    <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>), dataArray(<span class="number">1</span>).toLong, dataArray(<span class="number">2</span>).toDouble)</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream转换为 Table，并指定时间字段</span></span><br><span class="line"><span class="keyword">val</span> sensorTable = tableEnv</span><br><span class="line">  .fromDataStream(dataStream, <span class="symbol">'id</span>, <span class="symbol">'temperature</span>, <span class="symbol">'timestamp</span>, <span class="symbol">'pt</span>.proctime)</span><br></pre></td></tr></table></figure></div><ol><li>定义Table Schema时指定</li></ol><p>这种方法其实也很简单，只要在定义Schema的时候，加上一个新的字段，并指定成proctime就可以了。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tableEnv</span><br><span class="line">  .connect(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">FileSystem</span>().path(<span class="string">"..\\sensor.txt"</span>))</span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">Csv</span>())</span><br><span class="line">  .withSchema(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">      .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">      .field(<span class="string">"timestamp"</span>, <span class="type">DataTypes</span>.<span class="type">BIGINT</span>())</span><br><span class="line">      .field(<span class="string">"temperature"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">      .field(<span class="string">"pt"</span>, <span class="type">DataTypes</span>.<span class="type">TIMESTAMP</span>(<span class="number">3</span>))</span><br><span class="line">      .proctime()    <span class="comment">// 指定 pt字段为处理时间</span></span><br><span class="line">  ) <span class="comment">// 定义表结构</span></span><br><span class="line">  .createTemporaryTable(<span class="string">"inputTable"</span>) <span class="comment">// 创建临时表</span></span><br></pre></td></tr></table></figure></div><ol><li>创建表的DDL中指定</li></ol><p>在创建表的DDL中，增加一个字段并指定成proctime，也可以指定当前的时间字段。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sinkDDL: <span class="type">String</span> =</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |create table dataTable (</span></span><br><span class="line"><span class="string">    |  id varchar(20) not null,</span></span><br><span class="line"><span class="string">    |  ts bigint,</span></span><br><span class="line"><span class="string">    |  temperature double,</span></span><br><span class="line"><span class="string">    |  pt AS PROCTIME()</span></span><br><span class="line"><span class="string">    |) with (</span></span><br><span class="line"><span class="string">    |  'connector.type' = 'filesystem',</span></span><br><span class="line"><span class="string">    |  'connector.path' = 'file:///D:\\..\\sensor.txt',</span></span><br><span class="line"><span class="string">    |  'format.type' = 'csv'</span></span><br><span class="line"><span class="string">    |)</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line">tableEnv.sqlUpdate(sinkDDL) <span class="comment">// 执行 DDL</span></span><br></pre></td></tr></table></figure></div><p>注意：运行这段DDL，必须使用Blink Planner。</p><h4 id="事件时间（Event-Time）"><a href="#事件时间（Event-Time）" class="headerlink" title="事件时间（Event Time）"></a>事件时间（Event Time）</h4><p>事件时间语义，允许表处理程序根据每个记录中包含的时间生成结果。这样即使在有乱序事件或者延迟事件时，也可以获得正确的结果。</p><p>为了处理无序事件，并区分流中的准时和迟到事件；Flink需要从事件数据中，提取时间戳，并用来推进事件时间的进展（watermark）。</p><ol><li>DataStream转化成Table时指定</li></ol><p>在DataStream转换成Table，schema的定义期间，使用.rowtime可以定义事件时间属性。注意，必须在转换的数据流中分配时间戳和watermark。</p><p>在将数据流转换为表时，有两种定义时间属性的方法。根据指定的.rowtime字段名是否存在于数据流的架构中，timestamp字段可以：</p><ul><li>作为新字段追加到schema</li><li>替换现有字段</li></ul><p>在这两种情况下，定义的事件时间戳字段，都将保存DataStream中事件时间戳的值。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">"\\sensor.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = inputStream</span><br><span class="line">  .map(data =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">    <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>), dataArray(<span class="number">1</span>).toLong, dataArray(<span class="number">2</span>).toDouble)</span><br><span class="line">  &#125;)</span><br><span class="line">  .assignAscendingTimestamps(_.timestamp * <span class="number">1000</span>L)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream转换为 Table，并指定时间字段</span></span><br><span class="line"><span class="keyword">val</span> sensorTable = tableEnv</span><br><span class="line">  .fromDataStream(dataStream, <span class="symbol">'id</span>, <span class="symbol">'timestamp</span>.rowtime, <span class="symbol">'temperature</span>)</span><br><span class="line"><span class="comment">// 或者，直接追加字段</span></span><br><span class="line"><span class="keyword">val</span> sensorTable2 = tableEnv</span><br><span class="line">  .fromDataStream(dataStream, <span class="symbol">'id</span>, <span class="symbol">'temperature</span>, <span class="symbol">'timestamp</span>, <span class="symbol">'rt</span>.rowtime)</span><br></pre></td></tr></table></figure></div><ol><li>定义Table Schema时指定</li></ol><p>这种方法只要在定义Schema的时候，将事件时间字段，并指定成rowtime就可以了。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tableEnv</span><br><span class="line">  .connect(<span class="keyword">new</span> <span class="type">FileSystem</span>().path(<span class="string">"sensor.txt"</span>))</span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">Csv</span>())</span><br><span class="line">  .withSchema(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">      .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">      .field(<span class="string">"timestamp"</span>, <span class="type">DataTypes</span>.<span class="type">BIGINT</span>())</span><br><span class="line">      .rowtime(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Rowtime</span>()</span><br><span class="line">          .timestampsFromField(<span class="string">"timestamp"</span>)    <span class="comment">// 从字段中提取时间戳</span></span><br><span class="line">          .watermarksPeriodicBounded(<span class="number">1000</span>)    <span class="comment">// watermark延迟1秒</span></span><br><span class="line">      )</span><br><span class="line">      .field(<span class="string">"temperature"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">  ) <span class="comment">// 定义表结构</span></span><br><span class="line">  .createTemporaryTable(<span class="string">"inputTable"</span>) <span class="comment">// 创建临时表</span></span><br></pre></td></tr></table></figure></div><ol><li>创建表的DDL中指定</li></ol><p>事件时间属性，是使用CREATE TABLE DDL中的WARDMARK语句定义的。watermark语句，定义现有事件时间字段上的watermark生成表达式，该表达式将事件时间字段标记为事件时间属性。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sinkDDL: <span class="type">String</span> =</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |create table dataTable (</span></span><br><span class="line"><span class="string">    |  id varchar(20) not null,</span></span><br><span class="line"><span class="string">    |  ts bigint,</span></span><br><span class="line"><span class="string">    |  temperature double,</span></span><br><span class="line"><span class="string">    |  rt AS TO_TIMESTAMP( FROM_UNIXTIME(ts) ),</span></span><br><span class="line"><span class="string">    |  watermark for rt as rt - interval '1' second</span></span><br><span class="line"><span class="string">    |) with (</span></span><br><span class="line"><span class="string">    |  'connector.type' = 'filesystem',</span></span><br><span class="line"><span class="string">    |  'connector.path' = 'file:///D:\\..\\sensor.txt',</span></span><br><span class="line"><span class="string">    |  'format.type' = 'csv'</span></span><br><span class="line"><span class="string">    |)</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line">tableEnv.sqlUpdate(sinkDDL) <span class="comment">// 执行 DDL</span></span><br></pre></td></tr></table></figure></div><p>这里FROM_UNIXTIME是系统内置的时间函数，用来将一个整数（秒数）转换成“YYYY-MM-DD hh:mm:ss”格式（默认，也可以作为第二个String参数传入）的日期时间字符串（date time string）；然后再用TO_TIMESTAMP将其转换成Timestamp。</p><h2 id="窗口（Windows）"><a href="#窗口（Windows）" class="headerlink" title="窗口（Windows）"></a>窗口（Windows）</h2><p>时间语义，要配合窗口操作才能发挥作用。最主要的用途，当然就是开窗口、根据时间段做计算了。下面我们就来看看Table API和SQL中，怎么利用时间字段做窗口操作。</p><p>在Table API和SQL中，主要有两种窗口：Group Windows和Over Windows</p><h3 id="分组窗口（Group-Windows）"><a href="#分组窗口（Group-Windows）" class="headerlink" title="分组窗口（Group Windows）"></a>分组窗口（Group Windows）</h3><p>分组窗口（Group Windows）会根据时间或行计数间隔，将行聚合到有限的组（Group）中，并对每个组的数据执行一次聚合函数。</p><p>Table API中的Group Windows都是使用.window（w:GroupWindow）子句定义的，并且必须由as子句指定一个别名。为了按窗口对表进行分组，窗口的别名必须在group by子句中，像常规的分组字段一样引用。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> table = input</span><br><span class="line">  .window([w: <span class="type">GroupWindow</span>] as <span class="symbol">'w</span>) <span class="comment">// 定义窗口，别名 w</span></span><br><span class="line">  .groupBy(<span class="symbol">'w</span>, <span class="symbol">'a</span>)  <span class="comment">// 以属性a和窗口w作为分组的key</span></span><br><span class="line">  .select(<span class="symbol">'a</span>, <span class="symbol">'b</span>.sum)  <span class="comment">// 聚合字段b的值，求和</span></span><br></pre></td></tr></table></figure></div><p>或者，还可以把窗口的相关信息，作为字段添加到结果表中：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> table = input</span><br><span class="line">  .window([w: <span class="type">GroupWindow</span>] as <span class="symbol">'w</span>)</span><br><span class="line">  .groupBy(<span class="symbol">'w</span>, <span class="symbol">'a</span>) </span><br><span class="line">  .select(<span class="symbol">'a</span>, <span class="symbol">'w</span>.start, <span class="symbol">'w</span>.end, <span class="symbol">'w</span>.rowtime, <span class="symbol">'b</span>.count)</span><br></pre></td></tr></table></figure></div><p>Table API提供了一组具有特定语义的预定义Window类，这些类会被转换为底层DataStream或DataSet的窗口操作。</p><p>Table API支持的窗口定义，和我们熟悉的一样，主要也是三种：滚动（Tumbling）、滑动（Sliding）和会话（Session）。</p><h4 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h4><p>滚动窗口（Tumbling windows）要用Tumble类来定义，另外还有三个方法：</p><ul><li>over：定义窗口长度</li><li>on：用来分组（按时间间隔）或者排序（按行数）的时间字段</li><li>as：别名，必须出现在后面的groupBy中</li></ul><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Tumbling Event-time Window（事件时间字段rowtime</span></span><br><span class="line">.window(<span class="type">Tumble</span> over <span class="number">10.</span>minutes on <span class="symbol">'rowtime</span> as <span class="symbol">'w</span>)</span><br><span class="line"><span class="comment">// Tumbling Processing-time Window（处理时间字段proctime）</span></span><br><span class="line">.window(<span class="type">Tumble</span> over <span class="number">10.</span>minutes on <span class="symbol">'proctime</span> as <span class="symbol">'w</span>)</span><br><span class="line"><span class="comment">// Tumbling Row-count Window (类似于计数窗口，按处理时间排序，10行一组)</span></span><br><span class="line">.window(<span class="type">Tumble</span> over <span class="number">10.</span>rows on <span class="symbol">'proctime</span> as <span class="symbol">'w</span>)</span><br></pre></td></tr></table></figure></div><h4 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h4><p>滑动窗口（Sliding windows）要用Slide类来定义，另外还有四个方法：</p><ul><li>over：定义窗口长度</li><li>every：定义滑动步长</li><li>on：用来分组（按时间间隔）或者排序（按行数）的时间字段</li><li>as：别名，必须出现在后面的groupBy中</li></ul><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Sliding Event-time Window</span></span><br><span class="line">.window(<span class="type">Slide</span> over <span class="number">10.</span>minutes every <span class="number">5.</span>minutes on <span class="symbol">'rowtime</span> as <span class="symbol">'w</span>)</span><br><span class="line"><span class="comment">// Sliding Processing-time window</span></span><br><span class="line">.window(<span class="type">Slide</span> over <span class="number">10.</span>minutes every <span class="number">5.</span>minutes on <span class="symbol">'proctime</span> as <span class="symbol">'w</span>)</span><br><span class="line"><span class="comment">// Sliding Row-count window</span></span><br><span class="line">.window(<span class="type">Slide</span> over <span class="number">10.</span>rows every <span class="number">5.</span>rows on <span class="symbol">'proctime</span> as <span class="symbol">'w</span>)</span><br></pre></td></tr></table></figure></div><h4 id="会话窗口"><a href="#会话窗口" class="headerlink" title="会话窗口"></a>会话窗口</h4><p>会话窗口（Session windows）要用Session类来定义，另外还有三个方法：</p><ul><li>withGap：会话时间间隔</li><li>on：用来分组（按时间间隔）或者排序（按行数）的时间字段</li><li>as：别名，必须出现在后面的groupBy中</li></ul><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Session Event-time Window</span></span><br><span class="line">.window(<span class="type">Session</span> withGap <span class="number">10.</span>minutes on <span class="symbol">'rowtime</span> as <span class="symbol">'w</span>)</span><br><span class="line"><span class="comment">// Session Processing-time Window</span></span><br><span class="line">.window(<span class="type">Session</span> withGap <span class="number">10.</span>minutes on <span class="symbol">'proctime</span> as <span class="symbol">'w</span>)</span><br></pre></td></tr></table></figure></div><h3 id="Over-Windows"><a href="#Over-Windows" class="headerlink" title="Over Windows"></a>Over Windows</h3><p>Over window聚合是标准SQL中已有的（Over子句），可以在查询的SELECT子句中定义。Over window 聚合，会针对每个输入行，计算相邻行范围内的聚合。Over windows使用.window（w:overwindows*）子句定义，并在select()方法中通过别名来引用。</p><p>比如这样：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> table = input</span><br><span class="line">  .window([w: <span class="type">OverWindow</span>] as <span class="symbol">'w</span>)</span><br><span class="line">  .select(<span class="symbol">'a</span>, <span class="symbol">'b</span>.sum over <span class="symbol">'w</span>, <span class="symbol">'c</span>.min over <span class="symbol">'w</span>)</span><br></pre></td></tr></table></figure></div><p>Table API提供了Over类，来配置Over窗口的属性。可以在事件时间或处理时间，以及指定为时间间隔、或行计数的范围内，定义Over windows。</p><p>无界的over window是使用常量指定的。也就是说，时间间隔要指定UNBOUNDED_RANGE，或者行计数间隔要指定UNBOUNDED_ROW。而有界的over window是用间隔的大小指定的。</p><p>实际代码应用如下：</p><ol><li>无界的 over window</li></ol><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 无界的事件时间over window (时间字段 "rowtime")</span></span><br><span class="line">.window(<span class="type">Over</span> partitionBy <span class="symbol">'a</span> orderBy <span class="symbol">'rowtime</span> preceding <span class="type">UNBOUNDED_RANGE</span> as <span class="symbol">'w</span>)</span><br><span class="line"><span class="comment">//无界的处理时间over window (时间字段"proctime")</span></span><br><span class="line">.window(<span class="type">Over</span> partitionBy <span class="symbol">'a</span> orderBy <span class="symbol">'proctime</span> preceding <span class="type">UNBOUNDED_RANGE</span> as <span class="symbol">'w</span>)</span><br><span class="line"><span class="comment">// 无界的事件时间Row-count over window (时间字段 "rowtime")</span></span><br><span class="line">.window(<span class="type">Over</span> partitionBy <span class="symbol">'a</span> orderBy <span class="symbol">'rowtime</span> preceding <span class="type">UNBOUNDED_ROW</span> as <span class="symbol">'w</span>)</span><br><span class="line"><span class="comment">//无界的处理时间Row-count over window (时间字段 "rowtime")</span></span><br><span class="line">.window(<span class="type">Over</span> partitionBy <span class="symbol">'a</span> orderBy <span class="symbol">'proctime</span> preceding <span class="type">UNBOUNDED_ROW</span> as <span class="symbol">'w</span>)</span><br></pre></td></tr></table></figure></div><ol><li>有界的over window</li></ol><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 有界的事件时间over window (时间字段 "rowtime"，之前1分钟)</span></span><br><span class="line">.window(<span class="type">Over</span> partitionBy <span class="symbol">'a</span> orderBy <span class="symbol">'rowtime</span> preceding <span class="number">1.</span>minutes as <span class="symbol">'w</span>)</span><br><span class="line"><span class="comment">// 有界的处理时间over window (时间字段 "rowtime"，之前1分钟)</span></span><br><span class="line">.window(<span class="type">Over</span> partitionBy <span class="symbol">'a</span> orderBy <span class="symbol">'proctime</span> preceding <span class="number">1.</span>minutes as <span class="symbol">'w</span>)</span><br><span class="line"><span class="comment">// 有界的事件时间Row-count over window (时间字段 "rowtime"，之前10行)</span></span><br><span class="line">.window(<span class="type">Over</span> partitionBy <span class="symbol">'a</span> orderBy <span class="symbol">'rowtime</span> preceding <span class="number">10.</span>rows as <span class="symbol">'w</span>)</span><br><span class="line"><span class="comment">// 有界的处理时间Row-count over window (时间字段 "rowtime"，之前10行)</span></span><br><span class="line">.window(<span class="type">Over</span> partitionBy <span class="symbol">'a</span> orderBy <span class="symbol">'proctime</span> preceding <span class="number">10.</span>rows as <span class="symbol">'w</span>)</span><br></pre></td></tr></table></figure></div><h3 id="SQL中窗口的定义"><a href="#SQL中窗口的定义" class="headerlink" title="SQL中窗口的定义"></a>SQL中窗口的定义</h3><p>我们已经了解了在Table API里window的调用方式，同样，我们也可以在SQL中直接加入窗口的定义和使用。</p><h4 id="Group-Windows"><a href="#Group-Windows" class="headerlink" title="Group Windows"></a>Group Windows</h4><p>Group Windows在SQL查询的Group BY子句中定义。与使用常规GROUP BY子句的查询一样，使用GROUP BY子句的查询会计算每个组的单个结果行。</p><p>SQL支持以下Group窗口函数:</p><ul><li>TUMBLE(time_attr, interval)</li></ul><p>定义一个滚动窗口，第一个参数是时间字段，第二个参数是窗口长度。</p><ul><li>HOP(time_attr, interval, interval)</li></ul><p>定义一个滑动窗口，第一个参数是时间字段，第二个参数是窗口滑动步长，第三个是窗口长度。</p><ul><li>SESSION(time_attr, interval)</li></ul><p>定义一个会话窗口，第一个参数是时间字段，第二个参数是窗口间隔（Gap）。</p><p>另外还有一些辅助函数，可以用来选择Group Window的开始和结束时间戳，以及时间属性。</p><p>这里只写TUMBLE_<em>，滑动和会话窗口是类似的（HOP_</em>，SESSION_*）。</p><ul><li>TUMBLE_START(time_attr, interval)</li><li>TUMBLE_END(time_attr, interval)</li><li>TUMBLE_ROWTIME(time_attr, interval)</li><li>TUMBLE_PROCTIME(time_attr, interval)</li></ul><h4 id="Over-Windows-1"><a href="#Over-Windows-1" class="headerlink" title="Over Windows"></a>Over Windows</h4><p>由于Over本来就是SQL内置支持的语法，所以这在SQL中属于基本的聚合操作。所有聚合必须在同一窗口上定义，也就是说，必须是相同的分区、排序和范围。目前仅支持在当前行范围之前的窗口（无边界和有边界）。</p><p>注意，ORDER BY必须在单一的时间属性上指定。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">sql</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(amount) <span class="keyword">OVER</span> (</span><br><span class="line">  <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">user</span></span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> proctime</span><br><span class="line">  <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>)</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"></span><br><span class="line">// 也可以做多个聚合</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(amount) <span class="keyword">OVER</span> w, <span class="keyword">SUM</span>(amount) <span class="keyword">OVER</span> w</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="keyword">WINDOW</span> w <span class="keyword">AS</span> (</span><br><span class="line">  <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">user</span></span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> proctime</span><br><span class="line">  <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>)</span><br></pre></td></tr></table></figure></div><h3 id="代码练习（以分组滚动窗口为例）"><a href="#代码练习（以分组滚动窗口为例）" class="headerlink" title="代码练习（以分组滚动窗口为例）"></a>代码练习（以分组滚动窗口为例）</h3><p>我们可以综合学习过的内容，用一段完整的代码实现一个具体的需求。例如，可以开一个滚动窗口，统计10秒内出现的每个sensor的个数。</p><p>代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">  env.setParallelism(<span class="number">1</span>)</span><br><span class="line">  env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> streamFromFile: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">"sensor.txt"</span>)</span><br><span class="line">  <span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = streamFromFile</span><br><span class="line">    .map( data =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">      <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>).trim,</span><br><span class="line">        dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble)</span><br><span class="line">    &#125;)</span><br><span class="line">    .assignTimestampsAndWatermarks(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">SensorReading</span>](</span><br><span class="line">        <span class="type">Time</span>.seconds(<span class="number">1</span>)</span><br><span class="line">      ) &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(</span><br><span class="line">          element: <span class="type">SensorReading</span></span><br><span class="line">        ): <span class="type">Long</span> = element.timestamp * <span class="number">1000</span>L</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> settings: <span class="type">EnvironmentSettings</span> = <span class="type">EnvironmentSettings</span></span><br><span class="line">    .newInstance()</span><br><span class="line">    .useOldPlanner()</span><br><span class="line">    .inStreamingMode()</span><br><span class="line">    .build()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span></span><br><span class="line">    .create(env, settings)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> dataTable: <span class="type">Table</span> = tableEnv</span><br><span class="line">    .fromDataStream(dataStream, <span class="symbol">'id</span>, <span class="symbol">'temperature</span>, <span class="symbol">'timestamp</span>.rowtime)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> resultTable: <span class="type">Table</span> = dataTable</span><br><span class="line">    .window(<span class="type">Tumble</span> over <span class="number">10.</span>seconds on <span class="symbol">'timestamp</span> as <span class="symbol">'tw</span>)</span><br><span class="line">    .groupBy(<span class="symbol">'id</span>, <span class="symbol">'tw</span>)</span><br><span class="line">    .select(<span class="symbol">'id</span>, <span class="symbol">'id</span>.count)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sqlDataTable: <span class="type">Table</span> = dataTable</span><br><span class="line">    .select(<span class="symbol">'id</span>, <span class="symbol">'temperature</span>, <span class="symbol">'timestamp</span> as <span class="symbol">'ts</span>)</span><br><span class="line">  <span class="keyword">val</span> resultSqlTable: <span class="type">Table</span> = tableEnv</span><br><span class="line">    .sqlQuery(<span class="string">"select id, count(id) from "</span> </span><br><span class="line">      + sqlDataTable </span><br><span class="line">      + <span class="string">" group by id,tumble(ts,interval '10' second)"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 把 Table转化成数据流</span></span><br><span class="line">  <span class="keyword">val</span> resultDstream: <span class="type">DataStream</span>[(<span class="type">Boolean</span>, (<span class="type">String</span>, <span class="type">Long</span>))] = resultSqlTable</span><br><span class="line">    .toRetractStream[(<span class="type">String</span>, <span class="type">Long</span>)]</span><br><span class="line">  resultDstream.filter(_._1).print()</span><br><span class="line">  env.execute()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="函数（Functions）"><a href="#函数（Functions）" class="headerlink" title="函数（Functions）"></a>函数（Functions）</h2><p>Flink Table 和 SQL内置了很多SQL中支持的函数；如果有无法满足的需要，则可以实现用户自定义的函数（UDF）来解决。</p><h3 id="系统内置函数"><a href="#系统内置函数" class="headerlink" title="系统内置函数"></a>系统内置函数</h3><p>Flink Table API 和 SQL为用户提供了一组用于数据转换的内置函数。SQL中支持的很多函数，Table API和SQL都已经做了实现，其它还在快速开发扩展中。</p><p>以下是一些典型函数的举例，全部的内置函数，可以参考官网介绍。</p><ul><li>比较函数</li></ul><p>SQL：</p><p>value1 = value2</p><p>value1 &gt; value2</p><p>Table API：</p><p>ANY1 === ANY2</p><p>ANY1 &gt; ANY2</p><ul><li>逻辑函数</li></ul><p>SQL：</p><p>boolean1 OR boolean2</p><p>boolean IS FALSE</p><p>NOT boolean</p><p>Table API：</p><p>BOOLEAN1 || BOOLEAN2</p><p>BOOLEAN.isFalse</p><p>!BOOLEAN</p><ul><li>算术函数</li></ul><p>SQL：</p><p>numeric1 + numeric2</p><p>POWER(numeric1, numeric2)</p><p>Table API：</p><p>NUMERIC1 + NUMERIC2</p><p>NUMERIC1.power(NUMERIC2)</p><ul><li>字符串函数</li></ul><p>SQL：</p><p>string1 || string2</p><p>UPPER(string)</p><p>CHAR_LENGTH(string)</p><p>Table API：</p><p>STRING1 + STRING2</p><p>STRING.upperCase()</p><p>STRING.charLength()</p><ul><li>时间函数</li></ul><p>SQL：</p><p>DATE string</p><p>TIMESTAMP string</p><p>CURRENT_TIME</p><p>INTERVAL string range</p><p>Table API：</p><p>STRING.toDate</p><p>STRING.toTimestamp</p><p>currentTime()</p><p>NUMERIC.days</p><p>NUMERIC.minutes</p><ul><li>聚合函数</li></ul><p>SQL：</p><p>COUNT(*)</p><p>SUM([ ALL | DISTINCT ] expression)</p><p>RANK()</p><p>ROW_NUMBER()</p><p>Table API：</p><p>FIELD.count</p><p>FIELD.sum0</p><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><p>用户定义函数（User-defined Functions，UDF）是一个重要的特性，因为它们显著地扩展了查询（Query）的表达能力。一些系统内置函数无法解决的需求，我们可以用UDF来自定义实现。</p><h4 id="注册用户自定义函数UDF"><a href="#注册用户自定义函数UDF" class="headerlink" title="注册用户自定义函数UDF"></a>注册用户自定义函数UDF</h4><p>在大多数情况下，用户定义的函数必须先注册，然后才能在查询中使用。不需要专门为Scala 的Table API注册函数。</p><p>函数通过调用registerFunction（）方法在TableEnvironment中注册。当用户定义的函数被注册时，它被插入到TableEnvironment的函数目录中，这样Table API或SQL解析器就可以识别并正确地解释它。</p><h4 id="标量函数（Scalar-Functions）"><a href="#标量函数（Scalar-Functions）" class="headerlink" title="标量函数（Scalar Functions）"></a>标量函数（Scalar Functions）</h4><p>用户定义的标量函数，可以将0、1或多个标量值，映射到新的标量值。</p><p>为了定义标量函数，必须在org.apache.flink.table.functions中扩展基类Scalar Function，并实现（一个或多个）求值（evaluation，eval）方法。标量函数的行为由求值方法决定，求值方法必须公开声明并命名为eval（直接def声明，没有override）。求值方法的参数类型和返回类型，确定了标量函数的参数和返回类型。</p><p>在下面的代码中，我们定义自己的HashCode函数，在TableEnvironment中注册它，并在查询中调用它。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义一个标量函数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashCode</span>(<span class="params"> factor: <span class="type">Int</span> </span>) <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>( s: <span class="type">String</span> ): <span class="type">Int</span> = &#123;</span><br><span class="line">    s.hashCode * factor</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>主函数中调用，计算sensor id的哈希值（前面部分照抄，流环境、表环境、读取source、建表）：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">EnvironmentSettings</span>, <span class="type">Tumble</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.<span class="type">ScalarFunction</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableUDFExample1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">      .useBlinkPlanner()</span><br><span class="line">      .inStreamingMode()</span><br><span class="line">      .build()</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">SensorSource</span>)</span><br><span class="line">    <span class="keyword">val</span> hashCode = <span class="keyword">new</span> <span class="type">HashCode</span>(<span class="number">10</span>)</span><br><span class="line">    tEnv.registerFunction(<span class="string">"hashCode"</span>, <span class="keyword">new</span> <span class="type">HashCode</span>(<span class="number">10</span>))</span><br><span class="line">    <span class="keyword">val</span> table = tEnv.fromDataStream(stream, <span class="symbol">'id</span>)</span><br><span class="line">    <span class="comment">// table api 写法</span></span><br><span class="line">    table</span><br><span class="line">      .select(<span class="symbol">'id</span>, hashCode(<span class="symbol">'id</span>))</span><br><span class="line">      .toAppendStream[(<span class="type">String</span>, <span class="type">Int</span>)]</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sql 写法</span></span><br><span class="line">    tEnv.createTemporaryView(<span class="string">"t"</span>, table, <span class="symbol">'id</span>)</span><br><span class="line">    tEnv</span><br><span class="line">      .sqlQuery(<span class="string">"SELECT id, hashCode(id) FROM t"</span>)</span><br><span class="line">      .toAppendStream[(<span class="type">String</span>, <span class="type">Int</span>)]</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">HashCode</span>(<span class="params">factor: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(s: <span class="type">String</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      s.hashCode() * factor</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h4 id="表函数（Table-Functions）"><a href="#表函数（Table-Functions）" class="headerlink" title="表函数（Table Functions）"></a>表函数（Table Functions）</h4><p>与用户定义的标量函数类似，用户定义的表函数，可以将0、1或多个标量值作为输入参数；与标量函数不同的是，它可以返回任意数量的行作为输出，而不是单个值。</p><p>为了定义一个表函数，必须扩展org.apache.flink.table.functions中的基类TableFunction并实现（一个或多个）求值方法。表函数的行为由其求值方法决定，求值方法必须是public的，并命名为eval。求值方法的参数类型，决定表函数的所有有效参数。</p><p>返回表的类型由TableFunction的泛型类型确定。求值方法使用protected collect（T）方法发出输出行。</p><p>在Table API中，Table函数需要与.joinLateral或.leftOuterJoinLateral一起使用。</p><p>joinLateral算子，会将外部表中的每一行，与表函数（TableFunction，算子的参数是它的表达式）计算得到的所有行连接起来。</p><p>而leftOuterJoinLateral算子，则是左外连接，它同样会将外部表中的每一行与表函数计算生成的所有行连接起来；并且，对于表函数返回的是空表的外部行，也要保留下来。</p><p>在SQL中，则需要使用Lateral Table（），或者带有ON TRUE条件的左连接。</p><p>下面的代码中，我们将定义一个表函数，在表环境中注册它，并在查询中调用它。</p><p>自定义TableFunction：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义TableFunction</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Split</span>(<span class="params">separator: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">TableFunction</span>[(<span class="type">String</span>, <span class="type">Int</span>)]</span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    str.split(separator).foreach(</span><br><span class="line">      word =&gt; collect((word, word.length))</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>接下来，就是在代码中调用。首先是Table API的方式：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Table API中调用，需要用joinLateral</span></span><br><span class="line"><span class="keyword">val</span> resultTable = sensorTable</span><br><span class="line">  .joinLateral(split(<span class="symbol">'id</span>) as (<span class="symbol">'word</span>, <span class="symbol">'length</span>))   <span class="comment">// as对输出行的字段重命名</span></span><br><span class="line">  .select(<span class="symbol">'id</span>, <span class="symbol">'word</span>, <span class="symbol">'length</span>)</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 或者用leftOuterJoinLateral</span></span><br><span class="line"><span class="keyword">val</span> resultTable2 = sensorTable</span><br><span class="line">  .leftOuterJoinLateral(split(<span class="symbol">'id</span>) as (<span class="symbol">'word</span>, <span class="symbol">'length</span>))</span><br><span class="line">  .select(<span class="symbol">'id</span>, <span class="symbol">'word</span>, <span class="symbol">'length</span>)</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 转换成流打印输出</span></span><br><span class="line">resultTable.toAppendStream[<span class="type">Row</span>].print(<span class="string">"1"</span>)</span><br><span class="line">resultTable2.toAppendStream[<span class="type">Row</span>].print(<span class="string">"2"</span>)</span><br></pre></td></tr></table></figure></div><p>然后是SQL的方式：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(<span class="string">"sensor"</span>, sensorTable)</span><br><span class="line">tableEnv.registerFunction(<span class="string">"split"</span>, split)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultSqlTable = tableEnv.sqlQuery(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select id, word, length</span></span><br><span class="line"><span class="string">    |from</span></span><br><span class="line"><span class="string">    |sensor, LATERAL TABLE(split(id)) AS newsensor(word, length)</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 或者用左连接的方式</span></span><br><span class="line"><span class="keyword">val</span> resultSqlTable2 = tableEnv.sqlQuery(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |SELECT id, word, length</span></span><br><span class="line"><span class="string">    |FROM</span></span><br><span class="line"><span class="string">    |sensor</span></span><br><span class="line"><span class="string">    |  LEFT JOIN </span></span><br><span class="line"><span class="string">    |  LATERAL TABLE(split(id)) AS newsensor(word, length) </span></span><br><span class="line"><span class="string">    |  ON TRUE</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 转换成流打印输出</span></span><br><span class="line">resultSqlTable.toAppendStream[<span class="type">Row</span>].print(<span class="string">"1"</span>)</span><br><span class="line">resultSqlTable2.toAppendStream[<span class="type">Row</span>].print(<span class="string">"2"</span>)</span><br></pre></td></tr></table></figure></div><h4 id="聚合函数（Aggregate-Functions）"><a href="#聚合函数（Aggregate-Functions）" class="headerlink" title="聚合函数（Aggregate Functions）"></a>聚合函数（Aggregate Functions）</h4><p>用户自定义聚合函数（User-Defined Aggregate Functions，UDAGGs）可以把一个表中的数据，聚合成一个标量值。用户定义的聚合函数，是通过继承AggregateFunction抽象类实现的。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/udagg-mechanism.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/udagg-mechanism.png" class="lazyload"></a></p><p>上图中显示了一个聚合的例子。</p><p>假设现在有一张表，包含了各种饮料的数据。该表由三列（id、name和price）、五行组成数据。现在我们需要找到表中所有饮料的最高价格，即执行max（）聚合，结果将是一个数值。</p><p>AggregateFunction的工作原理如下。</p><ul><li>首先，它需要一个累加器，用来保存聚合中间结果的数据结构（状态）。可以通过调用AggregateFunction的createAccumulator（）方法创建空累加器。</li><li>随后，对每个输入行调用函数的accumulate（）方法来更新累加器。</li><li>处理完所有行后，将调用函数的getValue（）方法来计算并返回最终结果。</li></ul><p>AggregationFunction要求必须实现的方法：</p><ul><li>createAccumulator()</li><li>accumulate()</li><li>getValue()</li></ul><p>除了上述方法之外，还有一些可选择实现的方法。其中一些方法，可以让系统执行查询更有效率，而另一些方法，对于某些场景是必需的。例如，如果聚合函数应用在会话窗口（session group window）的上下文中，则merge（）方法是必需的。</p><ul><li>retract() </li><li>merge() </li><li>resetAccumulator()</li></ul><p>接下来我们写一个自定义AggregateFunction，计算一下每个sensor的平均温度值。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义AggregateFunction的Accumulator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AvgTempAcc</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> sum: <span class="type">Double</span> = <span class="number">0.0</span></span><br><span class="line">  <span class="keyword">var</span> count: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AvgTemp</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">Double</span>, <span class="type">AvgTempAcc</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>(accumulator: <span class="type">AvgTempAcc</span>): <span class="type">Double</span> = accumulator.sum / accumulator.count</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">AvgTempAcc</span> = <span class="keyword">new</span> <span class="type">AvgTempAcc</span></span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(accumulator: <span class="type">AvgTempAcc</span>, temp: <span class="type">Double</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    accumulator.sum += temp</span><br><span class="line">    accumulator.count += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>接下来就可以在代码中调用了。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个聚合函数实例</span></span><br><span class="line"><span class="keyword">val</span> avgTemp = <span class="keyword">new</span> <span class="type">AvgTemp</span>()</span><br><span class="line"><span class="comment">// Table API的调用</span></span><br><span class="line"><span class="keyword">val</span> resultTable = sensorTable</span><br><span class="line">  .groupBy(<span class="symbol">'id</span>)</span><br><span class="line">  .aggregate(avgTemp(<span class="symbol">'temperature</span>) as <span class="symbol">'avgTemp</span>)</span><br><span class="line">  .select(<span class="symbol">'id</span>, <span class="symbol">'avgTemp</span>)</span><br><span class="line">  </span><br><span class="line"><span class="comment">// SQL的实现</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"sensor"</span>, sensorTable)</span><br><span class="line">tableEnv.registerFunction(<span class="string">"avgTemp"</span>, avgTemp)</span><br><span class="line"><span class="keyword">val</span> resultSqlTable = tableEnv.sqlQuery(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |SELECT</span></span><br><span class="line"><span class="string">    |id, avgTemp(temperature)</span></span><br><span class="line"><span class="string">    |FROM</span></span><br><span class="line"><span class="string">    |sensor</span></span><br><span class="line"><span class="string">    |GROUP BY id</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 转换成流打印输出</span></span><br><span class="line">resultTable.toRetractStream[(<span class="type">String</span>, <span class="type">Double</span>)].print(<span class="string">"agg temp"</span>)</span><br><span class="line">resultSqlTable.toRetractStream[<span class="type">Row</span>].print(<span class="string">"agg temp sql"</span>)</span><br></pre></td></tr></table></figure></div><h4 id="表聚合函数（Table-Aggregate-Functions）"><a href="#表聚合函数（Table-Aggregate-Functions）" class="headerlink" title="表聚合函数（Table Aggregate Functions）"></a>表聚合函数（Table Aggregate Functions）</h4><p>用户定义的表聚合函数（User-Defined Table Aggregate Functions，UDTAGGs），可以把一个表中数据，聚合为具有多行和多列的结果表。这跟AggregateFunction非常类似，只是之前聚合结果是一个标量值，现在变成了一张表。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/udtagg-mechanism.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/udtagg-mechanism.png" class="lazyload"></a></p><p>比如现在我们需要找到表中所有饮料的前2个最高价格，即执行top2()表聚合。我们需要检查5行中的每一行，得到的结果将是一个具有排序后前2个值的表。</p><p>用户定义的表聚合函数，是通过继承TableAggregateFunction抽象类来实现的。</p><p>TableAggregateFunction的工作原理如下。</p><ul><li>首先，它同样需要一个累加器（Accumulator），它是保存聚合中间结果的数据结构。通过调用TableAggregateFunction的createAccumulator()方法可以创建空累加器。</li><li>随后，对每个输入行调用函数的accumulate()方法来更新累加器。</li><li>处理完所有行后，将调用函数的emitValue()方法来计算并返回最终结果。</li></ul><p>AggregationFunction要求必须实现的方法：</p><ul><li>createAccumulator()</li><li>accumulate()</li></ul><p>除了上述方法之外，还有一些可选择实现的方法。</p><ul><li>retract() </li><li>merge() </li><li>resetAccumulator() </li><li>emitValue() </li><li>emitUpdateWithRetract()</li></ul><p>接下来我们写一个自定义TableAggregateFunction，用来提取每个sensor最高的两个温度值。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 先定义一个 Accumulator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Top2TempAcc</span></span>&#123;</span><br><span class="line">  <span class="keyword">var</span> highestTemp: <span class="type">Double</span> = <span class="type">Int</span>.<span class="type">MinValue</span></span><br><span class="line">  <span class="keyword">var</span> secondHighestTemp: <span class="type">Double</span> = <span class="type">Int</span>.<span class="type">MinValue</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义 TableAggregateFunction</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Top2Temp</span> <span class="keyword">extends</span> <span class="title">TableAggregateFunction</span>[(<span class="type">Double</span>, <span class="type">Int</span>), <span class="type">Top2TempAcc</span>]</span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">Top2TempAcc</span> = <span class="keyword">new</span> <span class="type">Top2TempAcc</span></span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">Top2TempAcc</span>, temp: <span class="type">Double</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">if</span>( temp &gt; acc.highestTemp )&#123;</span><br><span class="line">      acc.secondHighestTemp = acc.highestTemp</span><br><span class="line">      acc.highestTemp = temp</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span>( temp &gt; acc.secondHighestTemp )&#123;</span><br><span class="line">      acc.secondHighestTemp = temp</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">emitValue</span></span>(acc: <span class="type">Top2TempAcc</span>, out: <span class="type">Collector</span>[(<span class="type">Double</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    out.collect(acc.highestTemp, <span class="number">1</span>)</span><br><span class="line">    out.collect(acc.secondHighestTemp, <span class="number">2</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>接下来就可以在代码中调用了。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个表聚合函数实例</span></span><br><span class="line"><span class="keyword">val</span> top2Temp = <span class="keyword">new</span> <span class="type">Top2Temp</span>()</span><br><span class="line"><span class="comment">// Table API的调用</span></span><br><span class="line"><span class="keyword">val</span> resultTable = sensorTable</span><br><span class="line">  .groupBy(<span class="symbol">'id</span>)</span><br><span class="line">  .flatAggregate( top2Temp(<span class="symbol">'temperature</span>) as (<span class="symbol">'temp</span>, <span class="symbol">'rank</span>) )</span><br><span class="line">  .select(<span class="symbol">'id</span>, <span class="symbol">'temp</span>, <span class="symbol">'rank</span>)</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 转换成流打印输出</span></span><br><span class="line">resultTable.toRetractStream[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Int</span>)].print(<span class="string">"agg temp"</span>)</span><br><span class="line">resultSqlTable.toRetractStream[<span class="type">Row</span>].print(<span class="string">"agg temp sql"</span>)</span><br></pre></td></tr></table></figure></div><h2 id="使用Table-API结合SQL实现TopN需求"><a href="#使用Table-API结合SQL实现TopN需求" class="headerlink" title="使用Table API结合SQL实现TopN需求"></a>使用Table API结合SQL实现TopN需求</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.ysss.project.topnhotitems</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Timestamp</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.ysss.project.util.<span class="type">UserBehavior</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">EnvironmentSettings</span>, <span class="type">Tumble</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HotItemsTable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 有关Blink的配置，样板代码</span></span><br><span class="line">    <span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">      .useBlinkPlanner()</span><br><span class="line">      .inStreamingMode()</span><br><span class="line">      .build()</span><br><span class="line">    <span class="comment">// 创建流式表的环境</span></span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 使用事件时间</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">// 过滤出pv事件，并抽取时间戳</span></span><br><span class="line">    <span class="keyword">val</span> stream = env</span><br><span class="line">      .readTextFile(<span class="string">"`UserBehavior.csv`的绝对路径"</span>)</span><br><span class="line">      .map(line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">        <span class="type">UserBehavior</span>(arr(<span class="number">0</span>).toLong,</span><br><span class="line">          arr(<span class="number">1</span>).toLong, arr(<span class="number">2</span>).toInt, arr(<span class="number">3</span>), arr(<span class="number">4</span>).toLong * <span class="number">1000</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      .filter(_.behavior == <span class="string">"pv"</span>)</span><br><span class="line">      .assignAscendingTimestamps(_.timestamp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从流中提取两个字段，时间戳；itemId，组成一张表</span></span><br><span class="line">    <span class="keyword">val</span> table = tEnv.fromDataStream(stream, <span class="symbol">'timestamp</span>.rowtime, <span class="symbol">'itemId</span>)</span><br><span class="line">    <span class="keyword">val</span> t = table</span><br><span class="line">      .window(<span class="type">Tumble</span> over <span class="number">60.</span>minutes on <span class="symbol">'timestamp</span> as <span class="symbol">'w</span>) <span class="comment">// 一小时滚动窗口</span></span><br><span class="line">      .groupBy(<span class="symbol">'itemId</span>, <span class="symbol">'w</span>)                               <span class="comment">// 根据itemId和窗口进行分组</span></span><br><span class="line">      .aggregate(<span class="symbol">'itemId</span>.count as <span class="symbol">'icount</span>)                <span class="comment">// 对itemId进行计数</span></span><br><span class="line">      .select(<span class="symbol">'itemId</span>, <span class="symbol">'icount</span>, <span class="symbol">'w</span>.end as <span class="symbol">'windowEnd</span>)     <span class="comment">// 查询三个字段</span></span><br><span class="line">      .toAppendStream[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Timestamp</span>)]            <span class="comment">// 转换成DataStream</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建临时表</span></span><br><span class="line">    tEnv.createTemporaryView(<span class="string">"topn"</span>, t, <span class="symbol">'itemId</span>, <span class="symbol">'icount</span>, <span class="symbol">'windowEnd</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// topN查询，Blink支持的特性</span></span><br><span class="line">    <span class="keyword">val</span> result = tEnv.sqlQuery(</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |SELECT *</span></span><br><span class="line"><span class="string">        |FROM (</span></span><br><span class="line"><span class="string">        |    SELECT *,</span></span><br><span class="line"><span class="string">        |        ROW_NUMBER() OVER</span></span><br><span class="line"><span class="string">        |        (PARTITION BY windowEnd ORDER BY icount DESC) as row_num</span></span><br><span class="line"><span class="string">        |    FROM topn)</span></span><br><span class="line"><span class="string">        |WHERE row_num &lt;= 5</span></span><br><span class="line"><span class="string">        |"</span><span class="string">""</span>.stripMargin</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 使用toRetractStream转换成DataStream，用来实时更新排行榜</span></span><br><span class="line">    <span class="comment">// true代表insert, false代表delete</span></span><br><span class="line">    result.toRetractStream[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Timestamp</span>, <span class="type">Long</span>)].print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="只使用Flink-SQL实现TopN需求"><a href="#只使用Flink-SQL实现TopN需求" class="headerlink" title="只使用Flink SQL实现TopN需求"></a>只使用Flink SQL实现TopN需求</h2><p>代码</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Timestamp</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.ysss.project.util.<span class="type">UserBehavior</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">EnvironmentSettings</span>, <span class="type">Tumble</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HotItemsSQL</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">      .useBlinkPlanner()</span><br><span class="line">      .inStreamingMode()</span><br><span class="line">      .build()</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="keyword">val</span> stream = env</span><br><span class="line">      .readTextFile(<span class="string">"`UserBehavior.csv`的绝对路径"</span>)</span><br><span class="line">      .map(line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">        <span class="type">UserBehavior</span>(arr(<span class="number">0</span>).toLong,</span><br><span class="line">          arr(<span class="number">1</span>).toLong, arr(<span class="number">2</span>).toInt, arr(<span class="number">3</span>), arr(<span class="number">4</span>).toLong * <span class="number">1000</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      .filter(_.behavior == <span class="string">"pv"</span>)</span><br><span class="line">      .assignAscendingTimestamps(_.timestamp)</span><br><span class="line"></span><br><span class="line">    tEnv.createTemporaryView(<span class="string">"t"</span>, stream, <span class="symbol">'itemId</span>, <span class="symbol">'timestamp</span>.rowtime as <span class="symbol">'ts</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = tEnv.sqlQuery(</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |SELECT *</span></span><br><span class="line"><span class="string">        |FROM (</span></span><br><span class="line"><span class="string">        |    SELECT *,</span></span><br><span class="line"><span class="string">        |        ROW_NUMBER() OVER</span></span><br><span class="line"><span class="string">        |        (PARTITION BY windowEnd ORDER BY icount DESC) as row_num</span></span><br><span class="line"><span class="string">        |    FROM</span></span><br><span class="line"><span class="string">        |    (SELECT count(itemId) as icount,</span></span><br><span class="line"><span class="string">        |     TUMBLE_START(ts, INTERVAL '1' HOUR) as windowEnd</span></span><br><span class="line"><span class="string">        |     FROM t GROUP BY TUMBLE(ts, INTERVAL '1' HOUR), itemId) topn)</span></span><br><span class="line"><span class="string">        |WHERE row_num &lt;= 5</span></span><br><span class="line"><span class="string">        |"</span><span class="string">""</span>.stripMargin</span><br><span class="line">    )</span><br><span class="line">    result.toRetractStream[(<span class="type">Long</span>, <span class="type">Timestamp</span>, <span class="type">Long</span>)].print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Table-API-和-Flink-SQL&quot;&gt;&lt;a href=&quot;#Table-API-和-Flink-SQL&quot; class=&quot;headerlink&quot; title=&quot;Table API 和 Flink SQL&quot;&gt;&lt;/a&gt;Table API 和 Flink SQL&lt;/
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink系列10Flink CEP简介</title>
    <link href="https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9710Flink-CEP%E7%AE%80%E4%BB%8B/"/>
    <id>https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9710Flink-CEP%E7%AE%80%E4%BB%8B/</id>
    <published>2020-07-02T03:50:56.000Z</published>
    <updated>2020-07-02T03:51:39.762Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink-CEP简介"><a href="#Flink-CEP简介" class="headerlink" title="Flink CEP简介"></a>Flink CEP简介</h1><p><em>什么是复杂事件CEP？</em></p><p>一个或多个由简单事件构成的事件流通过一定的规则匹配，然后输出用户想得到的数据，满足规则的复杂事件。</p><p><em>特征：</em></p><ul><li>目标：从有序的简单事件流中发现一些高阶特征</li><li>输入：一个或多个由简单事件构成的事件流</li><li>处理：识别简单事件之间的内在联系，多个符合一定规则的简单事件构成复杂事件</li><li>输出：满足规则的复杂事件</li></ul><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/cep1.jpg" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/cep1.jpg" class="lazyload"></a></p><p>CEP用于分析低延迟、频繁产生的不同来源的事件流。CEP可以帮助在复杂的、不相关的事件流中找出有意义的模式和复杂的关系，以接近实时或准实时的获得通知并阻止一些行为。</p><p>CEP支持在流上进行模式匹配，根据模式的条件不同，分为连续的条件或不连续的条件；模式的条件允许有时间的限制，当在条件范围内没有达到满足的条件时，会导致模式匹配超时。</p><p>看起来很简单，但是它有很多不同的功能：</p><ul><li>输入的流数据，尽快产生结果</li><li>在2个event流上，基于时间进行聚合类的计算</li><li>提供实时/准实时的警告和通知</li><li>在多样的数据源中产生关联并分析模式</li><li>高吞吐、低延迟的处理</li></ul><p>市场上有多种CEP的解决方案，例如Spark、Samza、Beam等，但他们都没有提供专门的library支持。但是Flink提供了专门的CEP library。</p><p><em>Flink CEP</em></p><p>Flink为CEP提供了专门的Flink CEP library，它包含如下组件：</p><ul><li>Event Stream</li><li>pattern定义</li><li>pattern检测</li><li>生成Alert</li></ul><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/cep6.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/cep6.png" class="lazyload"></a></p><p>首先，开发人员要在DataStream流上定义出模式条件，之后Flink CEP引擎进行模式检测，必要时生成告警。</p><p>为了使用Flink CEP，我们需要导入依赖：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-cep-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p><em>Event Streams</em></p><p>登录事件流</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginEvent</span>(<span class="params">userId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                      ip: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                      eventType: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                      eventTime: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">env</span> </span>= <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> loginEventStream = env</span><br><span class="line">  .fromCollection(<span class="type">List</span>(</span><br><span class="line">    <span class="type">LoginEvent</span>(<span class="string">"1"</span>, <span class="string">"192.168.0.1"</span>, <span class="string">"fail"</span>, <span class="string">"1558430842"</span>),</span><br><span class="line">    <span class="type">LoginEvent</span>(<span class="string">"1"</span>, <span class="string">"192.168.0.2"</span>, <span class="string">"fail"</span>, <span class="string">"1558430843"</span>),</span><br><span class="line">    <span class="type">LoginEvent</span>(<span class="string">"1"</span>, <span class="string">"192.168.0.3"</span>, <span class="string">"fail"</span>, <span class="string">"1558430844"</span>),</span><br><span class="line">    <span class="type">LoginEvent</span>(<span class="string">"2"</span>, <span class="string">"192.168.10.10"</span>, <span class="string">"success"</span>, <span class="string">"1558430845"</span>)</span><br><span class="line">  ))</span><br><span class="line">  .assignAscendingTimestamps(_.eventTime.toLong * <span class="number">1000</span>)</span><br></pre></td></tr></table></figure></div><p><em>Pattern API</em></p><p>每个Pattern都应该包含几个步骤，或者叫做state。从一个state到另一个state，通常我们需要定义一些条件，例如下列的代码：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> loginFailPattern = <span class="type">Pattern</span>.begin[<span class="type">LoginEvent</span>](<span class="string">"begin"</span>)</span><br><span class="line">  .where(_.eventType.equals(<span class="string">"fail"</span>))</span><br><span class="line">  .next(<span class="string">"next"</span>)</span><br><span class="line">  .where(_.eventType.equals(<span class="string">"fail"</span>))</span><br><span class="line">  .within(<span class="type">Time</span>.seconds(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></div><p>每个state都应该有一个标示：</p><p>例如: <code>.begin[LoginEvent](&quot;begin&quot;)</code>中的“begin”</p><p>每个state都需要有一个唯一的名字，而且需要一个filter来过滤条件，这个过滤条件定义事件需要符合的条件</p><p>例如: <code>.where(_.eventType.equals(&quot;fail&quot;))</code></p><p>我们也可以通过subtype来限制event的子类型：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start.subtype(<span class="type">SubEvent</span><span class="class">.<span class="keyword">class</span>).<span class="title">where</span>(<span class="params">...</span>)</span>;</span><br></pre></td></tr></table></figure></div><p>事实上，你可以多次调用subtype和where方法；而且如果where条件是不相关的，你可以通过or来指定一个单独的filter函数：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pattern.where(...).or(...);</span><br></pre></td></tr></table></figure></div><p>之后，我们可以在此条件基础上，通过next或者followedBy方法切换到下一个state，next的意思是说上一步符合条件的元素之后紧挨着的元素；而followedBy并不要求一定是挨着的元素。这两者分别称为严格近邻和非严格近邻。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> strictNext = start.next(<span class="string">"middle"</span>)</span><br><span class="line"><span class="keyword">val</span> nonStrictNext = start.followedBy(<span class="string">"middle"</span>)</span><br></pre></td></tr></table></figure></div><p>最后，我们可以将所有的Pattern的条件限定在一定的时间范围内：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">next.within(<span class="type">Time</span>.seconds(<span class="number">10</span>))</span><br></pre></td></tr></table></figure></div><p>这个时间可以是Processing Time，也可以是Event Time。</p><p><em>Pattern 检测</em></p><p>通过一个input DataStream以及刚刚我们定义的Pattern，我们可以创建一个PatternStream：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> input = ...</span><br><span class="line"><span class="keyword">val</span> pattern = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> patternStream = <span class="type">CEP</span>.pattern(input, pattern)</span><br><span class="line"><span class="keyword">val</span> patternStream = <span class="type">CEP</span></span><br><span class="line">  .pattern(</span><br><span class="line">    loginEventStream.keyBy(_.userId), loginFailPattern</span><br><span class="line">  )</span><br></pre></td></tr></table></figure></div><p>一旦获得PatternStream，我们就可以通过select或flatSelect，从一个Map序列找到我们需要的告警信息。</p><p><em>select</em></p><p>select方法需要实现一个PatternSelectFunction，通过select方法来输出需要的警告。它接受一个Map对，包含string/event，其中key为state的名字，event则为真是的Event。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> loginFailDataStream = patternStream</span><br><span class="line">  .select((pattern: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">LoginEvent</span>]]) =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> first = pattern.getOrElse(<span class="string">"begin"</span>, <span class="literal">null</span>).iterator.next()</span><br><span class="line">    <span class="keyword">val</span> second = pattern.getOrElse(<span class="string">"next"</span>, <span class="literal">null</span>).iterator.next()</span><br><span class="line"></span><br><span class="line">    (second.userId, second.ip, second.eventType)</span><br><span class="line">  &#125;)</span><br></pre></td></tr></table></figure></div><p>其返回值仅为1条记录。</p><p><em>flatSelect</em></p><p>通过实现PatternFlatSelectFunction，实现与select相似的功能。唯一的区别就是flatSelect方法可以返回多条记录。</p><p><em>超时事件的处理</em></p><p>通过within方法，我们的parttern规则限定在一定的窗口范围内。当有超过窗口时间后还到达的event，我们可以通过在select或flatSelect中，实现PatternTimeoutFunction/PatternFlatTimeoutFunction来处理这种情况。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> complexResult = patternStream.select(orderTimeoutOutput) &#123;</span><br><span class="line">  (pattern: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">OrderEvent</span>]], timestamp: <span class="type">Long</span>) =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> createOrder = pattern.get(<span class="string">"begin"</span>)</span><br><span class="line">    <span class="type">OrderTimeoutEvent</span>(createOrder.get.iterator.next().orderId, <span class="string">"timeout"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125; &#123;</span><br><span class="line">  pattern: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">OrderEvent</span>]] =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> payOrder = pattern.get(<span class="string">"next"</span>)</span><br><span class="line">    <span class="type">OrderTimeoutEvent</span>(payOrder.get.iterator.next().orderId, <span class="string">"success"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> timeoutResult = complexResult.getSideOutput(orderTimeoutOutput)</span><br><span class="line"></span><br><span class="line">complexResult.print()</span><br><span class="line">timeoutResult.print()</span><br></pre></td></tr></table></figure></div><p>完整例子:</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.cep.scala.<span class="type">CEP</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.scala.pattern.<span class="type">Pattern</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">Map</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFlinkLoginFail</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> loginEventStream = env.fromCollection(<span class="type">List</span>(</span><br><span class="line">      <span class="type">LoginEvent</span>(<span class="string">"1"</span>, <span class="string">"192.168.0.1"</span>, <span class="string">"fail"</span>, <span class="string">"1558430842"</span>),</span><br><span class="line">      <span class="type">LoginEvent</span>(<span class="string">"1"</span>, <span class="string">"192.168.0.2"</span>, <span class="string">"fail"</span>, <span class="string">"1558430843"</span>),</span><br><span class="line">      <span class="type">LoginEvent</span>(<span class="string">"1"</span>, <span class="string">"192.168.0.3"</span>, <span class="string">"fail"</span>, <span class="string">"1558430844"</span>),</span><br><span class="line">      <span class="type">LoginEvent</span>(<span class="string">"2"</span>, <span class="string">"192.168.10.10"</span>, <span class="string">"success"</span>, <span class="string">"1558430845"</span>)</span><br><span class="line">    )).assignAscendingTimestamps(_.eventTime.toLong)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> loginFailPattern = <span class="type">Pattern</span>.begin[<span class="type">LoginEvent</span>](<span class="string">"begin"</span>)</span><br><span class="line">      .where(_.eventType.equals(<span class="string">"fail"</span>))</span><br><span class="line">      .next(<span class="string">"next"</span>)</span><br><span class="line">      .where(_.eventType.equals(<span class="string">"fail"</span>))</span><br><span class="line">      .within(<span class="type">Time</span>.seconds(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> patternStream = <span class="type">CEP</span>.pattern(</span><br><span class="line">      loginEventStream.keyBy(_.userId), loginFailPattern</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> loginFailDataStream = patternStream</span><br><span class="line">      .select((pattern: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">LoginEvent</span>]]) =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> first = pattern.getOrElse(<span class="string">"begin"</span>, <span class="literal">null</span>).iterator.next()</span><br><span class="line">        <span class="keyword">val</span> second = pattern.getOrElse(<span class="string">"next"</span>, <span class="literal">null</span>).iterator.next()</span><br><span class="line"></span><br><span class="line">        (second.userId, second.ip, second.eventType)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    loginFailDataStream.print</span><br><span class="line"></span><br><span class="line">    env.execute</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginEvent</span>(<span class="params">userId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                      ip: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                      eventType: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                      eventTime: <span class="type">String</span></span>)</span></span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Flink-CEP简介&quot;&gt;&lt;a href=&quot;#Flink-CEP简介&quot; class=&quot;headerlink&quot; title=&quot;Flink CEP简介&quot;&gt;&lt;/a&gt;Flink CEP简介&lt;/h1&gt;&lt;p&gt;&lt;em&gt;什么是复杂事件CEP？&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;一个或多个由
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink系列09搭建Flink运行流式应用</title>
    <link href="https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9709%E6%90%AD%E5%BB%BAFlink%E8%BF%90%E8%A1%8C%E6%B5%81%E5%BC%8F%E5%BA%94%E7%94%A8/"/>
    <id>https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9709%E6%90%AD%E5%BB%BAFlink%E8%BF%90%E8%A1%8C%E6%B5%81%E5%BC%8F%E5%BA%94%E7%94%A8/</id>
    <published>2020-07-02T03:49:55.000Z</published>
    <updated>2020-07-02T03:50:41.204Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第九章，搭建Flink运行流式应用"><a href="#第九章，搭建Flink运行流式应用" class="headerlink" title="第九章，搭建Flink运行流式应用"></a>第九章，搭建Flink运行流式应用</h1><h2 id="部署方式"><a href="#部署方式" class="headerlink" title="部署方式"></a>部署方式</h2><h3 id="standalone集群"><a href="#standalone集群" class="headerlink" title="standalone集群"></a>standalone集群</h3><p>standalone集群包含至少一个master进程，以及至少一个TaskManager进程，TaskManager进程运行在一台或者多台机器上。所有的进程都是JVM进程。下图展示了standalone集群的部署。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0901.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0901.png" class="lazyload"></a></p><p>master进程在不同的线程中运行了一个Dispatcher和一个ResourceManager。一旦它们开始运行，所有TaskManager都将在Resourcemanager中进行注册。下图展示了一个任务如何提交到一个standalone集群中去。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0902.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0902.png" class="lazyload"></a></p><p>客户端向Dispatcher提交了一个任务，Dispatcher将会启动一个JobManager线程，并提供执行所需的JobGraph。JobManager向ResourceManager请求必要的task slots。一旦请求的slots分配好，JobManager就会部署job。</p><p>在standalone这种部署方式中，master和worker进程在失败以后，并不会自动重启。如果有足够的slots可供使用，job是可以从一次worker失败中恢复的。只要我们运行多个worker就好了。但如果job想从master失败中恢复的话，则需要进行高可用(HA)的配置了。</p><p><em>部署步骤</em></p><p>下载压缩包</p><p>链接：<a href="http://mirror.bit.edu.cn/apache/flink/flink-1.10.1/flink-1.10.1-bin-scala_2.11.tgz" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/flink/flink-1.10.1/flink-1.10.1-bin-scala_2.11.tgz</a></p><p>解压缩</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tar xvfz flink-1.10.1-bin-scala_2.11.tgz</span><br></pre></td></tr></table></figure></div><p>启动集群</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd flink-1.10.0</span><br><span class="line">$ .&#x2F;bin&#x2F;start-cluster.sh</span><br></pre></td></tr></table></figure></div><p>检查集群状态可以访问：<a href="http://localhost:8081" target="_blank" rel="noopener">http://localhost:8081</a></p><p>部署分布式集群</p><ol><li>所有运行TaskManager的机器的主机名（或者IP地址）都需要写入<code>./conf/slaves</code>文件中。</li><li><code>start-cluster.sh</code>脚本需要所有机器的无密码的SSH登录配置，方便启动TaskManager进程。</li><li>Flink的文件夹在所有的机器上都需要有相同的绝对路径。</li><li>运行master进程的机器的主机名或者IP地址需要写在<code>./conf/flink-conf.yaml</code>文件的<code>jobmanager.rpc.address</code>配置项。</li></ol><p>一旦部署好，我们就可以调用<code>./bin/start-cluster.sh</code>命令启动集群了，脚本会在本地机器启动一个JobManager，然后在每个slave机器上启动一个TaskManager。停止运行，请使用<code>./bin/stop-cluster.sh</code>。</p><h3 id="Apache-Hadoop-Yarn"><a href="#Apache-Hadoop-Yarn" class="headerlink" title="Apache Hadoop Yarn"></a>Apache Hadoop Yarn</h3><p>YARN是Apache Hadoop的资源管理组件。用来计算集群环境所需要的CPU和内存资源，然后提供给应用程序请求的资源。</p><p>Flink在YARN上运行，有两种模式：job模式和session模式。在job模式中，Flink集群用来运行一个单独的job。一旦job结束，Flink集群停止，并释放所有资源。下图展示了Flink的job如何提交到YARN集群。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0903.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0903.png" class="lazyload"></a></p><p>当客户端提交任务时，客户端将建立和YARN ResourceManager的连接，然后启动一个新的YARN应用的master进程，进程中包含一个JobManager线程和一个ResourceManager。JobManager向ResourceManager请求所需要的slots，用来运行Flink的job。接下来，Flink的ResourceManager将向Yarn的ResourceManager请求容器，然后启动TaskManager进程。一旦启动，TaskManager会将slots注册在Flink的ResourceManager中，Flink的ResourceManager将把slots提供给JobManager。最终，JobManager把job的任务提交给TaskManager执行。</p><p>sesison模式将启动一个长期运行的Flink集群，这个集群可以运行多个job，需要手动停止集群。如果以session模式启动，Flink将会连接到YARN的ResourceManager，然后启动一个master进程，包括一个Dispatcher线程和一个Flink的ResourceManager的线程。下图展示了一个Flink YARN session的启动。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0904.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0904.png" class="lazyload"></a></p><p>当一个job被提交运行，Dispatcher将启动一个JobManager线程，这个线程将向Flink的ResourceManager请求所需要的slots。如果没有足够的slots，Flink的ResourceManager将向YARN的ResourceManager请求额外的容器，来启动TaskManager进程，并在Flink的ResourceManager中注册。一旦所需slots可用，Flink的ResourceManager将吧slots分配给JobManager，然后开始执行job。下图展示了job如何在session模式下执行。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0905.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0905.png" class="lazyload"></a></p><p>无论是作业模式还是会话模式，Flink的ResourceManager都会自动对故障的TaskManager进行重启。你可以通过<code>./conf/flink-conf.yaml</code>配置文件来控制Flink在YARN上的故障恢复行为。例如，可以配置有多少容器发生故障后终止应用。</p><p>无论使用job模式还是sesison模式，都需要能够访问Hadoop。</p><p>job模式可以用以下命令来提交任务：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;flink run -m yarn-cluster .&#x2F;path&#x2F;to&#x2F;job.jar</span><br></pre></td></tr></table></figure></div><p>参数<code>-m</code>用来定义提交作业的目标主机。如果加上关键字<code>&quot;yarn-cluster&quot;</code>，客户端会将作业提交到由Hadoop配置所指定的YARN集群上。Flink的CLI客户端还支持很多参数，例如用于控制TaskManager容器内存大小的参数等。有关它们的详细信息，请参阅文档。Flink集群的Web UI由YARN集群某个节点上的主进程负责提供。你可以通过YARN的Web UI对其进行访问，具体链接位置在“Tracking URL: ApplicationMaster”下的Application Overview页面上。</p><p>session模式则是</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;yarn-session.sh # 启动一个yarn会话</span><br><span class="line">$ .&#x2F;bin&#x2F;flink run .&#x2F;path&#x2F;to&#x2F;job.jar # 向会话提交作业</span><br></pre></td></tr></table></figure></div><blockquote><p>Flink的Web UI链接可以从YARN Web UI的Application Overview页面上找到。</p></blockquote><h2 id="高可用配置-HA"><a href="#高可用配置-HA" class="headerlink" title="高可用配置(HA)"></a>高可用配置(HA)</h2><p>Flink的高可用配置需要Apache ZooKeeper组件，以及一个分布式文件系统，例如HDFS等等。JobManager将会把相关信息都存储在文件系统中，并将指向文件系统中相关信息的指针保存在ZooKeeper中。一旦失败，一个新的JobManager将从ZooKeeper中指向相关信息的指针所指向的文件系统中读取元数据，并恢复运行。</p><p>配置文件编写</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># REQUIRED: enable HA mode via ZooKeeper high-availability: zookeeper</span><br><span class="line"># REQUIRED: provide a list of all ZooKeeper servers of the quorum</span><br><span class="line">high-availability.zookeeper.quorum: address1:2181[,...],addressX:2181</span><br><span class="line"># REQUIRED: set storage location for job metadata in remote storage</span><br><span class="line">high-availability.storageDir: hdfs:&#x2F;&#x2F;&#x2F;flink&#x2F;recovery</span><br><span class="line"># RECOMMENDED: set the base path for all Flink clusters in ZooKeeper.</span><br><span class="line"># Isolates Flink from other frameworks using the ZooKeeper cluster.</span><br><span class="line">high-availability.zookeeper.path.root: &#x2F;flink</span><br></pre></td></tr></table></figure></div><h3 id="standalone集群高可用配置"><a href="#standalone集群高可用配置" class="headerlink" title="standalone集群高可用配置"></a>standalone集群高可用配置</h3><p>需要在配置文件中加一行集群标识符信息，因为可能多个集群共用一个zookeeper服务。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># RECOMMENDED: set the path for the Flink cluster in ZooKeeper.</span><br><span class="line"># Isolates multiple Flink clusters from each other.</span><br><span class="line"># The cluster id is required to look up the metadata of a failed cluster.</span><br><span class="line">high-availability.cluster-id: &#x2F;cluster-1</span><br></pre></td></tr></table></figure></div><h3 id="yarn集群高可用配置"><a href="#yarn集群高可用配置" class="headerlink" title="yarn集群高可用配置"></a>yarn集群高可用配置</h3><p>首先在yarn集群的配置文件<code>yarn-site.xml</code>中加入以下代码</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    The maximum number of application master execution attempts.</span><br><span class="line">    Default value is 2, i.e., an application is restarted at most once.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>然后在<code>./conf/flink-conf.yaml</code>加上</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Restart an application at most 3 times (+ the initial start).</span><br><span class="line"># Must be less or equal to the configured maximum number of attempts.</span><br><span class="line">yarn.application-attempts: 4</span><br></pre></td></tr></table></figure></div><h2 id="与Hadoop集成"><a href="#与Hadoop集成" class="headerlink" title="与Hadoop集成"></a>与Hadoop集成</h2><p>推荐两种方法</p><ol><li>下载包含hadoop的Flink版本。</li><li>使用我们之前下载的Flink，然后配置Hadoop的环境变量。 <code>export HADOOP_CLASSPATH={hadoop classpath}</code></li></ol><p>我们还需要提供Hadoop配置文件的路径。只需设置名为<code>HADOOP_CONF_DIR</code>的环境变量就可以了。这样Flink就能够连上YARN的ResourceManager和HDFS了。</p><h2 id="保存点操作"><a href="#保存点操作" class="headerlink" title="保存点操作"></a>保存点操作</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;flink savepoint &lt;jobId&gt; [savepointPath]</span><br></pre></td></tr></table></figure></div><p>例如</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;flink savepoint bc0b2ad61ecd4a615d92ce25390f61ad \</span><br><span class="line">hdfs:&#x2F;&#x2F;&#x2F;xxx:50070&#x2F;savepoints</span><br><span class="line">Triggering savepoint for job bc0b2ad61ecd4a615d92ce25390f61ad.</span><br><span class="line">Waiting for response...</span><br><span class="line">Savepoint completed. </span><br><span class="line">Path: hdfs:&#x2F;&#x2F;&#x2F;xxx:50070&#x2F;savepoints&#x2F;savepoint-bc0b2a-63cf5d5ccef8</span><br><span class="line">You can resume your program from this savepoint with the run command.</span><br></pre></td></tr></table></figure></div><p>删除保存点文件</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;flink savepoint -d &lt;savepointPath&gt;</span><br></pre></td></tr></table></figure></div><p>例子</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;flink savepoint -d \</span><br><span class="line">hdfs:&#x2F;&#x2F;&#x2F;xxx:50070&#x2F;savepoints&#x2F;savepoint-bc0b2a-63cf5d5ccef8</span><br><span class="line">Disposing savepoint &#39;hdfs:&#x2F;&#x2F;&#x2F;xxx:50070&#x2F;savepoints&#x2F;savepoint-bc0b2a-63cf5d5ccef8&#39;.</span><br><span class="line">Waiting for response...</span><br><span class="line">Savepoint &#39;hdfs:&#x2F;&#x2F;&#x2F;xxx:50070&#x2F;savepoints&#x2F;savepoint-bc0b2a-63cf5d5ccef8&#39; disposed.</span><br></pre></td></tr></table></figure></div><h2 id="取消一个应用"><a href="#取消一个应用" class="headerlink" title="取消一个应用"></a>取消一个应用</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;flink cancel &lt;jobId&gt;</span><br></pre></td></tr></table></figure></div><p>取消的同时做保存点操作</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;flink cancel -s [savepointPath] &lt;jobId&gt;</span><br></pre></td></tr></table></figure></div><p>例如</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;flink cancel -s \</span><br><span class="line">hdfs:&#x2F;&#x2F;&#x2F;xxx:50070&#x2F;savepoints d5fdaff43022954f5f02fcd8f25ef855</span><br><span class="line">Cancelling job bc0b2ad61ecd4a615d92ce25390f61ad </span><br><span class="line">with savepoint to hdfs:&#x2F;&#x2F;&#x2F;xxx:50070&#x2F;savepoints.</span><br><span class="line">Cancelled job bc0b2ad61ecd4a615d92ce25390f61ad. </span><br><span class="line">Savepoint stored in hdfs:&#x2F;&#x2F;&#x2F;xxx:50070&#x2F;savepoints&#x2F;savepoint-bc0b2a-d08de07fbb10.</span><br></pre></td></tr></table></figure></div><h2 id="从保存点启动应用程序"><a href="#从保存点启动应用程序" class="headerlink" title="从保存点启动应用程序"></a>从保存点启动应用程序</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;flink run -s &lt;savepointPath&gt; [options] &lt;jobJar&gt; [arguments]</span><br></pre></td></tr></table></figure></div><h2 id="扩容，改变并行度操作"><a href="#扩容，改变并行度操作" class="headerlink" title="扩容，改变并行度操作"></a>扩容，改变并行度操作</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;flink modify &lt;jobId&gt; -p &lt;newParallelism&gt;</span><br></pre></td></tr></table></figure></div><p>例子</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;flink modify bc0b2ad61ecd4a615d92ce25390f61ad -p 16</span><br><span class="line">Modify job bc0b2ad61ecd4a615d92ce25390f61ad.</span><br><span class="line">Rescaled job bc0b2ad61ecd4a615d92ce25390f61ad. Its new parallelism is 16.</span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第九章，搭建Flink运行流式应用&quot;&gt;&lt;a href=&quot;#第九章，搭建Flink运行流式应用&quot; class=&quot;headerlink&quot; title=&quot;第九章，搭建Flink运行流式应用&quot;&gt;&lt;/a&gt;第九章，搭建Flink运行流式应用&lt;/h1&gt;&lt;h2 id=&quot;部署方式&quot;
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink系列08读写外部系统</title>
    <link href="https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9708%E8%AF%BB%E5%86%99%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F/"/>
    <id>https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9708%E8%AF%BB%E5%86%99%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F/</id>
    <published>2020-07-02T03:48:50.000Z</published>
    <updated>2020-07-02T03:49:41.161Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第八章，读写外部系统"><a href="#第八章，读写外部系统" class="headerlink" title="第八章，读写外部系统"></a>第八章，读写外部系统</h1><p>数据可以存储在不同的系统中，例如：文件系统，对象存储系统（OSS），关系型数据库，Key-Value存储，搜索引擎索引，日志系统，消息队列，等等。每一种系统都是给特定的应用场景设计的，在某一个特定的目标上超越了其他系统。今天的数据架构，往往包含着很多不同的存储系统。在将一个组件加入到我们的系统中时，我们需要问一个问题：“这个组件和架构中的其他组件能多好的一起工作？”</p><p>添加一个像Flink这样的数据处理系统，需要仔细的考虑。因为Flink没有自己的存储层，而是读取数据和持久化数据都需要依赖外部存储。所以，对于Flink，针对外部系统提供良好的读取和写入的连接器就很重要了。尽管如此，仅仅能够读写外部系统对于Flink这样想要提供任务故障情况下一致性保证的流处理器来讲，是不够的。</p><p>在本章中，我们将会讨论source和sink的连接器。这些连接器影响了Flink的一致性保证，也提供了对于最流行的一些外部系统的读写的连接器。我们还将学习如何实现自定义source和sink连接器，以及如何实现可以向外部系统发送异步读写请求的函数。</p><h2 id="应用的一致性保证"><a href="#应用的一致性保证" class="headerlink" title="应用的一致性保证"></a>应用的一致性保证</h2><p>Flink的检查点和恢复机制定期的会保存应用程序状态的一致性检查点。在故障的情况下，应用程序的状态将会从最近一次完成的检查点恢复，并继续处理。尽管如此，可以使用检查点来重置应用程序的状态无法完全达到令人满意的一致性保证。相反，source和sink的连接器需要和Flink的检查点和恢复机制进行集成才能提供有意义的一致性保证。</p><p>为了给应用程序提供恰好处理一次语义的状态一致性保证，应用程序的source连接器需要能够将source的读位置重置到之前保存的检查点位置。当处理一次检查点时，source操作符将会把source的读位置持久化，并在恢复的时候从这些读位置开始重新读取。支持读位置的检查点的source连接器一般来说是基于文件的存储系统，如：文件流或者Kafka source（检查点会持久化某个正在消费的topic的读偏移量）。如果一个应用程序从一个无法存储和重置读位置的source连接器摄入数据，那么当任务出现故障的时候，数据就会丢失。也就是说我们只能提供at-most-once）的一致性保证。</p><p>Fink的检查点和恢复机制和可以重置读位置的source连接器结合使用，可以保证应用程序不会丢失任何数据。尽管如此，应用程序可能会发出两次计算结果，因为从上一次检查点恢复的应用程序所计算的结果将会被重新发送一次（一些结果已经发送出去了，这时任务故障，然后从上一次检查点恢复，这些结果将被重新计算一次然后发送出去）。所以，可重置读位置的source和Flink的恢复机制不足以提供端到端的恰好处理一次语义，即使应用程序的状态是恰好处理一次一致性级别。</p><p>一个志在提供端到端恰好处理一次语义一致性的应用程序需要特殊的sink连接器。sink连接器可以在不同的情况下使用两种技术来达到恰好处理一次一致性语义：幂等性写入和事务性写入。</p><h3 id="幂等性写入"><a href="#幂等性写入" class="headerlink" title="幂等性写入"></a>幂等性写入</h3><p>一个幂等操作无论执行多少次都会返回同样的结果。例如，重复的向hashmap中插入同样的key-value对就是幂等操作，因为头一次插入操作之后所有的插入操作都不会改变这个hashmap，因为hashmap已经包含这个key-value对了。另一方面，append操作就不是幂等操作了，因为多次append同一个元素将会导致列表每次都会添加一个元素。在流处理程序中，幂等写入操作是很有意思的，因为幂等写入操作可以执行多次但不改变结果。所以它们可以在某种程度上缓和Flink检查点机制带来的重播计算结果的效应。</p><p>需要注意的是，依赖于幂等性sink来达到exactly-once语义的应用程序，必须保证在从检查点恢复以后，它将会覆盖之前已经写入的结果。例如，一个包含有sink操作的应用在sink到一个key-value存储时必须保证它能够确定的计算出将要更新的key值。同时，从Flink程序sink到的key-value存储中读取数据的应用，在Flink从检查点恢复的过程中，可能会看到不想看到的结果。当重播开始时，之前已经发出的计算结果可能会被更早的结果所覆盖（因为在恢复过程中）。所以，一个消费Flink程序输出数据的应用，可能会观察到时间回退，例如读到了比之前小的计数。也就是说，当流处理程序处于恢复过程中时，流处理程序的结果将处于不稳定的状态，因为一些结果被覆盖掉，而另一些结果还没有被覆盖。一旦重播完成，也就是说应用程序已经通过了之前出故障的点，结果将会继续保持一致性。</p><h3 id="事务性写入"><a href="#事务性写入" class="headerlink" title="事务性写入"></a>事务性写入</h3><p>第二种实现端到端的恰好处理一次一致性语义的方法基于事务性写入。其思想是只将最近一次成功保存的检查点之前的计算结果写入到外部系统中去。这样就保证了在任务故障的情况下，端到端恰好处理一次语义。应用将被重置到最近一次的检查点，而在这个检查点之后并没有向外部系统发出任何计算结果。通过只有当检查点保存完成以后再写入数据这种方法，事务性的方法将不会遭受幂等性写入所遭受的重播不一致的问题。尽管如此，事务性写入却带来了延迟，因为只有在检查点完成以后，我们才能看到计算结果。</p><p>Flink提供了两种构建模块来实现事务性sink连接器：write-ahead-log（WAL，预写式日志）sink和两阶段提交sink。WAL式sink将会把所有计算结果写入到应用程序的状态中，等接到检查点完成的通知，才会将计算结果发送到sink系统。因为sink操作会把数据都缓存在状态后段，所以WAL可以使用在任何外部sink系统上。尽管如此，WAL还是无法提供刀枪不入的恰好处理一次语义的保证，再加上由于要缓存数据带来的状态后段的状态大小的问题，WAL模型并不十分完美。</p><p>与之形成对比的，2PC sink需要sink系统提供事务的支持或者可以模拟出事务特性的模块。对于每一个检查点，sink开始一个事务，然后将所有的接收到的数据都添加到事务中，并将这些数据写入到sink系统，但并没有提交（commit）它们。当事务接收到检查点完成的通知时，事务将被commit，数据将被真正的写入sink系统。这项机制主要依赖于一次sink可以在检查点完成之前开始事务，并在应用程序从一次故障中恢复以后再commit的能力。</p><p>2PC协议依赖于Flink的检查点机制。检查点屏障是开始一个新的事务的通知，所有操作符自己的检查点成功的通知是它们可以commit的投票，而JobManager通知一个检查点成功的消息是commit事务的指令。于WAL sink形成对比的是，2PC sinks依赖于sink系统和sink本身的实现可以实现恰好处理一次语义。更多的，2PC sink不断的将数据写入到sink系统中，而WAL写模型就会有之前所述的问题。</p><table><thead><tr><th></th><th>不可重置的源</th><th>可重置的源</th></tr></thead><tbody><tr><td>any sink</td><td>at-most-once</td><td>at-least-once</td></tr><tr><td>幂等性sink</td><td>at-most-once</td><td>exactly-once（当从任务失败中恢复时，存在暂时的不一致性）</td></tr><tr><td>预写式日志sink</td><td>at-most-once</td><td>at-least-once</td></tr><tr><td>2PC sink</td><td>at-most-once</td><td>exactly-once</td></tr></tbody></table><h2 id="Flink提供的连接器"><a href="#Flink提供的连接器" class="headerlink" title="Flink提供的连接器"></a>Flink提供的连接器</h2><p>Flink提供了读写很多存储系统的连接器。消息队列，日志系统，例如Apache Kafka, Kinesis, RabbitMQ等等这些是常用的数据源。在批处理环境中，数据流很可能是监听一个文件系统，而当新的数据落盘的时候，读取这些新数据。</p><p>在sink一端，数据流经常写入到消息队列中，以供接下来的流处理程序消费。数据流也可能写入到文件系统中做持久化，或者交给批处理程序来进行分析。数据流还可能被写入到key-value存储或者关系型数据库中，例如Cassandra，ElasticSearch或者MySQL中，这样数据可供查询，还可以在仪表盘中显示出来。</p><p>不幸的是，对于大多数存储系统并没有标准接口，除了针对DBMS的JDBC。相反，每一个存储系统都需要有自己的特定的连接器。所以，Flink需要维护针对不同存储系统（消息队列，日志系统，文件系统，k-v数据库，关系型数据库等等）的连接器实现。</p><p>Flink提供了针对Apache Kafka, Kinesis, RabbitMQ, Apache Nifi, 各种文件系统，Cassandra, Elasticsearch, 还有JDBC的连接器。除此之外，Apache Bahir项目还提供了额外的针对例如ActiveMQ, Akka, Flume, Netty, 和Redis等的连接器。</p><h3 id="Apache-Kafka-Source连接器"><a href="#Apache-Kafka-Source连接器" class="headerlink" title="Apache Kafka Source连接器"></a>Apache Kafka Source连接器</h3><p>Apache Kafka是一个分布式流式平台。它的核心是一个分布式的发布订阅消息系统。</p><p>Kafka将事件流组织为所谓的topics。一个主题就是一个事件日志系统，Kafka可以保证主题中的数据在被读取时和这些数据在被写入时相同的顺序。为了扩大读写的规模，主题可以分裂为多个分区，这些分区分布在一个集群上面。这时，读写顺序的保证就限制到了分区这个粒度， Kafka并没有提供从不同分区读取数据时的顺序保证。Kafka分区的读位置称为偏移量（offset）。</p><p>Kafka的依赖引入如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>Flink Kafka连接器并行的摄入事件流。每一个并行source任务可以从一个或者多个分区中读取数据。任务将会跟踪每一个分区当前的读偏移量，然后将读偏移量写入到检查点数据中。当从任务故障恢复时，读偏移量将被恢复，而source任务将从检查点保存的读偏移量开始重新读取数据。Flink Kafka连接器并不依赖Kafka自己的offset-tracking机制（基于消费者组实现）。下图展示了分区如何分配给source实例。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0801.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0801.png" class="lazyload"></a></p><p>Kafka source连接器使用如下代码创建</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"group.id"</span>, <span class="string">"test"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.addSource(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](</span><br><span class="line">    <span class="string">"topic"</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(),</span><br><span class="line">    properties))</span><br></pre></td></tr></table></figure></div><p>构造器接受三个参数。第一个参数定义了从哪些topic中读取数据，可以是一个topic，也可以是topic列表，还可以是匹配所有想要读取的topic的正则表达式。当从多个topic中读取数据时，Kafka连接器将会处理所有topic的分区，将这些分区的数据放到一条流中去。</p><p>第二个参数是一个DeserializationSchema或者KeyedDeserializationSchema。Kafka消息被存储为原始的字节数据，所以需要反序列化成Java或者Scala对象。上例中使用的SimpleStringSchema，是一个内置的DeserializationSchema，它仅仅是简单的将字节数组反序列化成字符串。DeserializationSchema和KeyedDeserializationSchema是公共的接口，所以我们可以自定义反序列化逻辑。</p><p>第三个参数是一个Properties对象，设置了用来读写的Kafka客户端的一些属性。</p><p>为了抽取事件时间的时间戳然后产生水印，我们可以通过调用</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">FlinkKafkaConsumer</span>.assignTimestampsAndWatermark()</span><br></pre></td></tr></table></figure></div><p>方法为Kafka消费者提供AssignerWithPeriodicWatermark或者AssignerWithPucntuatedWatermark。每一个assigner都将被应用到每个分区，来利用每一个分区的顺序保证特性。source实例将会根据水印的传播协议聚合所有分区的水印。</p><h3 id="Apache-Kafka-Sink连接器"><a href="#Apache-Kafka-Sink连接器" class="headerlink" title="Apache Kafka Sink连接器"></a>Apache Kafka Sink连接器</h3><p>添加依赖：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>下面的例子展示了如何创建一个Kafka sink</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">String</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> myProducer = <span class="keyword">new</span> <span class="type">FlinkKafkaProducer</span>[<span class="type">String</span>](</span><br><span class="line">  <span class="string">"localhost:9092"</span>,         <span class="comment">// broker list</span></span><br><span class="line">  <span class="string">"topic"</span>,                  <span class="comment">// target topic</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>)   <span class="comment">// serialization schema</span></span><br><span class="line"></span><br><span class="line">stream.addSink(myProducer)</span><br></pre></td></tr></table></figure></div><h3 id="Kakfa-Sink的at-least-once保证"><a href="#Kakfa-Sink的at-least-once保证" class="headerlink" title="Kakfa Sink的at-least-once保证"></a>Kakfa Sink的at-least-once保证</h3><p>Flink的Kafka sink提供了基于配置的一致性保证。Kafka sink使用下面的条件提供了至少处理一次保证：</p><ul><li>Flink检查点机制开启，所有的数据源都是可重置的。</li><li>当写入失败时，sink连接器将会抛出异常，使得应用程序挂掉然后重启。这是默认行为。应用程序内部的Kafka客户端还可以配置为重试写入，只要提前声明当写入失败时，重试几次这样的属性（retries property）。</li><li>sink连接器在完成它的检查点之前会等待Kafka发送已经将数据写入的通知。</li></ul><h3 id="Kafka-Sink的恰好处理一次语义保证"><a href="#Kafka-Sink的恰好处理一次语义保证" class="headerlink" title="Kafka Sink的恰好处理一次语义保证"></a>Kafka Sink的恰好处理一次语义保证</h3><p>Kafka 0.11版本引入了事务写特性。由于这个新特性，Flink Kafka sink可以为输出结果提供恰好处理一次语义的一致性保证，只要经过合适的配置就行。Flink程序必须开启检查点机制，并从可重置的数据源进行消费。FlinkKafkaProducer还提供了包含Semantic参数的构造器来控制sink提供的一致性保证。可能的取值如下：</p><ul><li>Semantic.NONE，不提供任何一致性保证。数据可能丢失或者被重写多次。</li><li>Semantic.AT_LEAST_ONCE，保证无数据丢失，但可能被处理多次。这个是默认设置。</li><li>Semantic.EXACTLY_ONCE，基于Kafka的事务性写入特性实现，保证每条数据恰好处理一次。</li></ul><h3 id="文件系统source连接器"><a href="#文件系统source连接器" class="headerlink" title="文件系统source连接器"></a>文件系统source连接器</h3><p>Apache Flink针对文件系统实现了一个可重置的source连接器，将文件看作流来读取数据。如下面的例子所示：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lineReader = <span class="keyword">new</span> <span class="type">TextInputFormat</span>(<span class="literal">null</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lineStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readFile[<span class="type">String</span>](</span><br><span class="line">  lineReader,                 <span class="comment">// The FileInputFormat</span></span><br><span class="line">  <span class="string">"hdfs:///path/to/my/data"</span>,  <span class="comment">// The path to read</span></span><br><span class="line">  <span class="type">FileProcessingMode</span></span><br><span class="line">    .<span class="type">PROCESS_CONTINUOUSLY</span>,    <span class="comment">// The processing mode</span></span><br><span class="line">  <span class="number">30000</span>L)                     <span class="comment">// The monitoring interval in ms</span></span><br></pre></td></tr></table></figure></div><p>StreamExecutionEnvironment.readFile()接收如下参数：</p><ul><li>FileInputFormat参数，负责读取文件中的内容。</li><li>文件路径。如果文件路径指向单个文件，那么将会读取这个文件。如果路径指向一个文件夹，FileInputFormat将会扫描文件夹中所有的文件。</li><li>PROCESS_CONTINUOUSLY将会周期性的扫描文件，以便扫描到文件新的改变。</li><li>30000L表示多久扫描一次监听的文件。</li></ul><p>FileInputFormat是一个特定的InputFormat，用来从文件系统中读取文件。FileInputFormat分两步读取文件。首先扫描文件系统的路径，然后为所有匹配到的文件创建所谓的input splits。一个input split将会定义文件上的一个范围，一般通过读取的开始偏移量和读取长度来定义。在将一个大的文件分割成一堆小的splits以后，这些splits可以分发到不同的读任务，这样就可以并行的读取文件了。FileInputFormat的第二步会接收一个input split，读取被split定义的文件范围，然后返回对应的数据。</p><p>DataStream应用中使用的FileInputFormat需要实现CheckpointableInputFormat接口。这个接口定义了方法来做检查点和重置文件片段的当前的读取位置。</p><p>在Flink 1.7中，Flink提供了一些类，这些类继承了FileInputFormat，并实现了CheckpointableInputFormat接口。TextInputFormat一行一行的读取文件，而CsvInputFormat使用逗号分隔符来读取文件。</p><h3 id="文件系统sink连接器"><a href="#文件系统sink连接器" class="headerlink" title="文件系统sink连接器"></a>文件系统sink连接器</h3><p>在将流处理应用配置成exactly-once检查点机制，以及配置成所有源数据都能在故障的情况下可以重置，Flink的StreamingFileSink提供了端到端的恰好处理一次语义保证。下面的例子展示了StreamingFileSink的使用方式。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataStream</span>[<span class="type">String</span>] = …</span><br><span class="line"><span class="keyword">val</span> sink: <span class="type">StreamingFileSink</span>[<span class="type">String</span>] = <span class="type">StreamingFileSink</span></span><br><span class="line">  .forRowFormat(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Path</span>(<span class="string">"/base/path"</span>), </span><br><span class="line">    <span class="keyword">new</span> <span class="type">SimpleStringEncoder</span>[<span class="type">String</span>](<span class="string">"UTF-8"</span>))</span><br><span class="line">  .build()</span><br><span class="line"></span><br><span class="line">input.addSink(sink)</span><br></pre></td></tr></table></figure></div><p>当StreamingFileSink接到一条数据，这条数据将被分配到一个桶（bucket）中。一个桶是我们配置的“/base/path”的子目录。</p><p>Flink使用BucketAssigner来分配桶。BucketAssigner是一个公共的接口，为每一条数据返回一个BucketId，BucketId决定了数据被分配到哪个子目录。如果没有指定BucketAssigner，Flink将使用DateTimeBucketAssigner来将每条数据分配到每个一个小时所产生的桶中去，基于数据写入的处理时间（机器时间，墙上时钟）。</p><p>StreamingFileSink提供了exactly-once输出的保证。sink通过一个commit协议来达到恰好处理一次语义的保证。这个commit协议会将文件移动到不同的阶段，有以下状态：in progress，pending，finished。这个协议基于Flink的检查点机制。当Flink决定roll a file时，这个文件将被关闭并移动到pending状态，通过重命名文件来实现。当下一个检查点完成时，pending文件将被移动到finished状态，同样是通过重命名来实现。</p><p>一旦任务故障，sink任务需要将处于in progress状态的文件重置到上一次检查点的写偏移量。这个可以通过关闭当前in progress的文件，并将文件结尾无效的部分丢弃掉来实现。</p><h2 id="实现自定义源函数"><a href="#实现自定义源函数" class="headerlink" title="实现自定义源函数"></a>实现自定义源函数</h2><p>DataStream API提供了两个接口来实现source连接器：</p><ul><li>SourceFunction和RichSourceFunction可以用来定义非并行的source连接器，source跑在单任务上。</li><li>ParallelSourceFunction和RichParallelSourceFunction可以用来定义跑在并行实例上的source连接器。</li></ul><p>除了并行于非并行的区别，这两种接口完全一样。就像process function的rich版本一样，RichSourceFunction和RichParallelSourceFunction的子类可以override open()和close()方法，也可以访问RuntimeContext，RuntimeContext提供了并行任务实例的数量，当前任务实例的索引，以及一些其他信息。</p><p>SourceFunction和ParallelSourceFunction定义了两种方法：</p><ul><li>void run(SourceContext ctx)</li><li>cancel()</li></ul><p>run()方法用来读取或者接收数据然后将数据摄入到Flink应用中。根据接收数据的系统，数据可能是推送的也可能是拉取的。Flink仅仅在特定的线程调用run()方法一次，通常情况下会是一个无限循环来读取或者接收数据并发送数据。任务可以在某个时间点被显式的取消，或者由于流是有限流，当数据被消费完毕时，任务也会停止。</p><p>当应用被取消或者关闭时，cancel()方法会被Flink调用。为了优雅的关闭Flink应用，run()方法需要在cancel()被调用以后，立即终止执行。下面的例子显示了一个简单的源函数的例子：从0数到Long.MaxValue。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountSource</span> <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">Long</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> isRunning: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Long</span>]) = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> cnt: <span class="type">Long</span> = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> (isRunning &amp;&amp; cnt &lt; <span class="type">Long</span>.<span class="type">MaxValue</span>) &#123;</span><br><span class="line">      cnt += <span class="number">1</span></span><br><span class="line">      ctx.collect(cnt)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>() = isRunning = <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="可重置的源函数"><a href="#可重置的源函数" class="headerlink" title="可重置的源函数"></a>可重置的源函数</h3><p>之前我们讲过，应用程序只有使用可以重播输出数据的数据源时，才能提供令人满意的一致性保证。如果外部系统暴露了获取和重置读偏移量的API，那么source函数就可以重播源数据。这样的例子包括一些能够提供文件流的偏移量的文件系统，或者提供seek方法用来移动到文件的特定位置的文件系统。或者Apache Kafka这种可以为每一个主题的分区提供偏移量并且可以设置分区的读位置的系统。一个反例就是source连接器连接的是socket，socket将会立即丢弃已经发送过的数据。</p><p>支持重播输出的源函数需要和Flink的检查点机制集成起来，还需要在检查点被处理时，持久化当前所有的读取位置。当应用从一个保存点（savepoint）恢复或者从故障恢复时，Flink会从最近一次的检查点或者保存点中获取读偏移量。如果程序开始时并不存在状态，那么读偏移量将会被设置到一个默认值。一个可重置的源函数需要实现CheckpointedFunction接口，还需要能够存储读偏移量和相关的元数据，例如文件的路径，分区的ID。这些数据将被保存在list state或者union list state中。</p><p>下面的例子将CountSource重写为可重置的数据源。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResettableCountSource</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">Long</span>] <span class="keyword">with</span> <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> isRunning: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">var</span> cnt: <span class="type">Long</span> = _</span><br><span class="line">  <span class="keyword">var</span> offsetState: <span class="type">ListState</span>[<span class="type">Long</span>] = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Long</span>]) = &#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning &amp;&amp; cnt &lt; <span class="type">Long</span>.<span class="type">MaxValue</span>) &#123;</span><br><span class="line">      <span class="comment">// synchronize data emission and checkpoints</span></span><br><span class="line">      ctx.getCheckpointLock.synchronized &#123;</span><br><span class="line">        cnt += <span class="number">1</span></span><br><span class="line">        ctx.collect(cnt)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>() = isRunning = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">snapshotState</span></span>(</span><br><span class="line">    snapshotCtx: <span class="type">FunctionSnapshotContext</span></span><br><span class="line">  ): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// remove previous cnt</span></span><br><span class="line">    offsetState.clear()</span><br><span class="line">    <span class="comment">// add current cnt</span></span><br><span class="line">    offsetState.add(cnt)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeState</span></span>(</span><br><span class="line">      initCtx: <span class="type">FunctionInitializationContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> desc = <span class="keyword">new</span> <span class="type">ListStateDescriptor</span>[<span class="type">Long</span>](</span><br><span class="line">      <span class="string">"offset"</span>, classOf[<span class="type">Long</span>])</span><br><span class="line">    offsetState = initCtx</span><br><span class="line">      .getOperatorStateStore</span><br><span class="line">      .getListState(desc)</span><br><span class="line">    <span class="comment">// initialize cnt variable</span></span><br><span class="line">    <span class="keyword">val</span> it = offsetState.get()</span><br><span class="line">    cnt = <span class="keyword">if</span> (<span class="literal">null</span> == it || !it.iterator().hasNext) &#123;</span><br><span class="line">      <span class="number">-1</span>L</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      it.iterator().next()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="实现自定义sink函数"><a href="#实现自定义sink函数" class="headerlink" title="实现自定义sink函数"></a>实现自定义sink函数</h2><p>DataStream API中，任何运算符或者函数都可以向外部系统发送数据。DataStream不需要最终流向sink运算符。例如，我们可能实现了一个FlatMapFunction，这个函数将每一个接收到的数据通过HTTP POST请求发送出去，而不使用Collector发送到下一个运算符。DataStream API也提供了SinkFunction接口以及对应的rich版本RichSinkFunction抽象类。SinkFunction接口提供了一个方法：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void invode(<span class="type">IN</span> value, <span class="type">Context</span> ctx)</span><br></pre></td></tr></table></figure></div><p>SinkFunction的Context可以访问当前处理时间，当前水位线，以及数据的时间戳。</p><p>下面的例子展示了一个简单的SinkFunction，可以将传感器读数写入到socket中去。需要注意的是，我们需要在启动Flink程序前启动一个监听相关端口的进程。否则将会抛出ConnectException异常。可以运行“nc -l localhost 9191”命令。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// write the sensor readings to a socket</span></span><br><span class="line">readings.addSink(<span class="keyword">new</span> <span class="type">SimpleSocketSink</span>(<span class="string">"localhost"</span>, <span class="number">9191</span>))</span><br><span class="line">  <span class="comment">// set parallelism to 1 because only one thread can write to a socket</span></span><br><span class="line">  .setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// -----</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleSocketSink</span>(<span class="params">val host: <span class="type">String</span>, val port: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[<span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> socket: <span class="type">Socket</span> = _</span><br><span class="line">  <span class="keyword">var</span> writer: <span class="type">PrintStream</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(config: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// open socket and writer</span></span><br><span class="line">    socket = <span class="keyword">new</span> <span class="type">Socket</span>(<span class="type">InetAddress</span>.getByName(host), port)</span><br><span class="line">    writer = <span class="keyword">new</span> <span class="type">PrintStream</span>(socket.getOutputStream)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(</span><br><span class="line">      value: <span class="type">SensorReading</span>,</span><br><span class="line">      ctx: <span class="type">SinkFunction</span>.<span class="type">Context</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// write sensor reading to socket</span></span><br><span class="line">    writer.println(value.toString)</span><br><span class="line">    writer.flush()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// close writer and socket</span></span><br><span class="line">    writer.close()</span><br><span class="line">    socket.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>之前我们讨论过，端到端的一致性保证建立在sink连接器的属性上面。为了达到端到端的恰好处理一次语义的目的，应用程序需要幂等性的sink连接器或者事务性的sink连接器。上面例子中的SinkFunction既不是幂等写入也不是事务性的写入。由于socket具有只能添加（append-only）这样的属性，所以不可能实现幂等性的写入。又因为socket不具备内置的事务支持，所以事务性写入就只能使用Flink的WAL sink特性来实现了。接下来我们将学习如何实现幂等sink连接器和事务sink连接器。</p><h3 id="幂等sink连接器"><a href="#幂等sink连接器" class="headerlink" title="幂等sink连接器"></a>幂等sink连接器</h3><p>对于大多数应用，SinkFunction接口足以实现一个幂等性写入的sink连接器了。需要以下两个条件：</p><ul><li>结果数据必须具有确定性的key，在这个key上面幂等性更新才能实现。例如一个计算每分钟每个传感器的平均温度值的程序，确定性的key值可以是传感器的ID和每分钟的时间戳。确定性的key值，对于在故障恢复的场景下，能够正确的覆盖结果非常的重要。</li><li>外部系统支持针对每个key的更新，例如关系型数据库或者key-value存储。</li></ul><p>下面的例子展示了如何实现一个针对JDBC数据库的幂等写入sink连接器，这里使用的是Apache Derby数据库。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// write the sensor readings to a Derby table</span></span><br><span class="line">readings.addSink(<span class="keyword">new</span> <span class="type">DerbyUpsertSink</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// -----</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DerbyUpsertSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[<span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> conn: <span class="type">Connection</span> = _</span><br><span class="line">  <span class="keyword">var</span> insertStmt: <span class="type">PreparedStatement</span> = _</span><br><span class="line">  <span class="keyword">var</span> updateStmt: <span class="type">PreparedStatement</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// connect to embedded in-memory Derby</span></span><br><span class="line">    conn = <span class="type">DriverManager</span>.getConnection(</span><br><span class="line">       <span class="string">"jdbc:derby:memory:flinkExample"</span>,</span><br><span class="line">       <span class="keyword">new</span> <span class="type">Properties</span>())</span><br><span class="line">    <span class="comment">// prepare insert and update statements</span></span><br><span class="line">    insertStmt = conn.prepareStatement(</span><br><span class="line">      <span class="string">"INSERT INTO Temperatures (sensor, temp) VALUES (?, ?)"</span>)</span><br><span class="line">    updateStmt = conn.prepareStatement(</span><br><span class="line">      <span class="string">"UPDATE Temperatures SET temp = ? WHERE sensor = ?"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(r: <span class="type">SensorReading</span>, context: <span class="type">Context</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// set parameters for update statement and execute it</span></span><br><span class="line">    updateStmt.setDouble(<span class="number">1</span>, r.temperature)</span><br><span class="line">    updateStmt.setString(<span class="number">2</span>, r.id)</span><br><span class="line">    updateStmt.execute()</span><br><span class="line">    <span class="comment">// execute insert statement</span></span><br><span class="line">    <span class="comment">// if update statement did not update any row</span></span><br><span class="line">    <span class="keyword">if</span> (updateStmt.getUpdateCount == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// set parameters for insert statement</span></span><br><span class="line">      insertStmt.setString(<span class="number">1</span>, r.id)</span><br><span class="line">      insertStmt.setDouble(<span class="number">2</span>, r.temperature)</span><br><span class="line">      <span class="comment">// execute insert statement</span></span><br><span class="line">      insertStmt.execute()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    insertStmt.close()</span><br><span class="line">    updateStmt.close()</span><br><span class="line">    conn.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>由于Apache Derby并没有提供内置的UPSERT方法，所以这个sink连接器实现了UPSERT写。具体实现方法是首先去尝试更新一行数据，如果这行数据不存在，则插入新的一行数据。</p><h3 id="事务性sink连接器"><a href="#事务性sink连接器" class="headerlink" title="事务性sink连接器"></a>事务性sink连接器</h3><p>事务写入sink连接器需要和Flink的检查点机制集成，因为只有在检查点成功完成以后，事务写入sink连接器才会向外部系统commit数据。</p><p>为了简化事务性sink的实现，Flink提供了两个模版用来实现自定义sink运算符。这两个模版都实现了CheckpointListener接口。CheckpointListener接口将会从JobManager接收到检查点完成的通知。</p><ul><li>GenericWriteAheadSink模版会收集检查点之前的所有的数据，并将数据存储到sink任务的运算符状态中。状态保存到了检查点中，并在任务故障的情况下恢复。当任务接收到检查点完成的通知时，任务会将所有的数据写入到外部系统中。</li><li>TwoPhaseCommitSinkFunction模版利用了外部系统的事务特性。对于每一个检查点，任务首先开始一个新的事务，并将接下来所有的数据都写到外部系统的当前事务上下文中去。当任务接收到检查点完成的通知时，sink连接器将会commit这个事务。</li></ul><p><em>GENERICWRITEAHEADSINK</em></p><p>GenericWriteAheadSink使得sink运算符可以很方便的实现。这个运算符和Flink的检查点机制集成使用，目标是将每一条数据恰好一次写入到外部系统中去。需要注意的是，在发生故障的情况下，write-ahead log sink可能会不止一次的发送相同的数据。所以GenericWriteAheadSink无法提供完美无缺的恰好处理一次语义的一致性保证，而是仅能提供at-least-once这样的保证。我们接下来详细的讨论这些场景。</p><p>GenericWriteAheadSink的原理是将接收到的所有数据都追加到有检查点分割好的预写式日志中去。每当sink运算符碰到检查点屏障，运算符将会开辟一个新的section，并将接下来的所有数据都追加到新的section中去。WAL（预写式日志）将会保存到运算符状态中。由于log能被恢复，所有不会有数据丢失。</p><p>当GenericWriteAheadSink接收到检查点完成的通知时，将会发送对应检查点的WAL中存储的所有数据。当所有数据发送成功，对应的检查点必须在内部提交。</p><p>检查点的提交分两步。第一步，sink持久化检查点被提交的信息。第二步，删除WAL中所有的数据。我们不能将commit信息保存在Flink应用程序状态中，因为状态不是持久化的，会在故障恢复时重置状态。相反，GenericWriteAheadSink依赖于可插拔的组件在一个外部持久化存储中存储和查找提交信息。这个组件就是CheckpointCommitter。</p><p>继承GenericWriteAheadSink的运算符需要提供三个构造器函数。</p><ul><li>CheckpointCommitter</li><li>TypeSerializer，用来序列化输入数据。</li><li>一个job ID，传给CheckpointCommitter，当应用重启时可以识别commit信息。</li></ul><p>还有，write-ahead运算符需要实现一个单独的方法：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boolean sendValues(<span class="type">Iterable</span>&lt;<span class="type">IN</span>&gt; values, long chkpntId, long timestamp)</span><br></pre></td></tr></table></figure></div><p>当检查点完成时，GenericWriteAheadSink调用sendValues()方法来将数据写入到外部存储系统中。这个方法接收一个检查点对应的所有数据的迭代器，检查点的ID，检查点被处理时的时间戳。当数据写入成功时，方法必须返回true，写入失败返回false。</p><p>下面的例子展示了如何实现一个写入到标准输出的write-ahead sink。它使用了FileCheckpointCommitter。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ???</span><br><span class="line"></span><br><span class="line"><span class="comment">// write the sensor readings to the standard out via a write-ahead log</span></span><br><span class="line">readings.transform(</span><br><span class="line">  <span class="string">"WriteAheadSink"</span>, <span class="keyword">new</span> <span class="type">SocketWriteAheadSink</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// ```-</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StdOutWriteAheadSink</span> <span class="keyword">extends</span> <span class="title">GenericWriteAheadSink</span>[<span class="type">SensorReading</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    // <span class="type">CheckpointCommitter</span> that commits</span></span></span><br><span class="line"><span class="class"><span class="params">    // checkpoints to the local filesystem</span></span></span><br><span class="line"><span class="class"><span class="params">    new <span class="type">FileCheckpointCommitter</span>(<span class="type">System</span>.getProperty("java.io.tmpdir"</span>)),</span></span><br><span class="line"><span class="class">    <span class="title">//</span> <span class="title">Serializer</span> <span class="title">for</span> <span class="title">records</span></span></span><br><span class="line"><span class="class">    <span class="title">createTypeInformation</span>[<span class="type">SensorReading</span>]</span></span><br><span class="line"><span class="class">      .<span class="title">createSerializer</span>(<span class="params">new <span class="type">ExecutionConfig</span></span>),</span></span><br><span class="line"><span class="class">    <span class="title">//</span> <span class="title">Random</span> <span class="title">JobID</span> <span class="title">used</span> <span class="title">by</span> <span class="title">the</span> <span class="title">CheckpointCommitter</span></span></span><br><span class="line"><span class="class">    <span class="title">UUID</span>.<span class="title">randomUUID</span>.<span class="title">toString</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">sendValues</span></span>(</span><br><span class="line">      readings: <span class="type">Iterable</span>[<span class="type">SensorReading</span>],</span><br><span class="line">      checkpointId: <span class="type">Long</span>,</span><br><span class="line">      timestamp: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (r &lt;- readings.asScala) &#123;</span><br><span class="line">      <span class="comment">// write record to standard out</span></span><br><span class="line">      println(r)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>之前我们讲过，GenericWriteAheadSink无法提供完美的exactly-once保证。有两个故障状况会导致数据可能被发送不止一次。</p><ul><li>当任务执行sendValues()方法时，程序挂掉了。如果外部系统无法原子性的写入所有数据（要么都写入要么都不写），一些数据可能会写入，而另一些数据并没有被写入。由于checkpoint还没有commit，所以在任务恢复的过程中一些数据可能会被再次写入。</li><li>所有数据都写入成功了，sendValues()方法也返回true了；但在CheckpointCommitter方法被调用之前程序挂了，或者CheckpointCommitter在commit检查点时失败了。那么在恢复的过程中，所有未被提交的检查点将会被重新写入。</li></ul><p><em>TWOPHASECOMMITSINKFUNCTION</em></p><p>Flink提供了TwoPhaseCommitSinkFunction接口来简化sink函数的实现。这个接口保证了端到端的exactly-once语义。2PC sink函数是否提供这样的一致性保证取决于我们的实现细节。我们需要讨论一个问题：“2PC协议是否开销太大？”</p><p>通常来讲，为了保证分布式系统的一致性，2PC是一个非常昂贵的方法。尽管如此，在Flink的语境下，2PC协议针对每一个检查点只运行一次。TwoPhaseCommitSinkFunction和WAL sink很相似，不同点在于前者不会将数据收集到state中，而是会写入到外部系统事务的上下文中。</p><p>TwoPhaseCommitSinkFunction实现了以下协议。在sink任务发送出第一条数据之前，任务将在外部系统中开始一个事务，所有接下来的数据将被写入这个事务的上下文中。当JobManager初始化检查点并将检查点屏障插入到流中的时候，2PC协议的投票阶段开始。当运算符接收到检查点屏障，运算符将保存它的状态，当保存完成时，运算符将发送一个acknowledgement信息给JobManager。当sink任务接收到检查点屏障时，运算符将会持久化它的状态，并准备提交当前的事务，以及acknowledge JobManager中的检查点。发送给JobManager的acknowledgement信息类似于2PC协议中的commit投票。sink任务还不能提交事务，因为它还没有保证所有的任务都已经完成了它们的检查点操作。sink任务也会为下一个检查点屏障之前的所有数据开始一个新的事务。</p><p>当JobManager成功接收到所有任务实例发出的检查点操作成功的通知时，JobManager将会把检查点完成的通知发送给所有感兴趣的任务。这里的通知对应于2PC协议的提交命令。当sink任务接收到通知时，它将commit所有处于开启状态的事务。一旦sink任务acknowledge了检查点操作，它必须能够commit对应的事务，即使任务发生故障。如果commit失败，数据将会丢失。</p><p>让我们总结一下外部系统需要满足什么样的要求：</p><ul><li>外部系统必须提供事务支持，或者sink的实现能在外部系统上模拟事务功能。</li><li>在检查点操作期间，事务必须处于open状态，并接收这段时间数据的持续写入。</li><li>事务必须等到检查点操作完成的通知到来才可以提交。在恢复周期中，可能需要一段时间等待。如果sink系统关闭了事务（例如超时了），那么未被commit的数据将会丢失。</li><li>sink必须在进程挂掉后能够恢复事务。一些sink系统会提供事务ID，用来commit或者abort一个开始的事务。</li><li>commit一个事务必须是一个幂等性操作。sink系统或者外部系统能够观察到事务已经被提交，或者重复提交并没有副作用。</li></ul><p>下面的例子可能会让上面的一些概念好理解一些。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransactionalFileSink</span>(<span class="params">val targetPath: <span class="type">String</span>, val tempPath: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">TwoPhaseCommitSinkFunction</span>[(<span class="type">String</span>, <span class="type">Double</span>), <span class="type">String</span>, <span class="type">Void</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">      createTypeInformation[<span class="type">String</span>].createSerializer(new <span class="type">ExecutionConfig</span></span>),</span></span><br><span class="line"><span class="class">      <span class="title">createTypeInformation</span>[<span class="type">Void</span>].<span class="title">createSerializer</span>(<span class="params">new <span class="type">ExecutionConfig</span></span>)) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> transactionWriter: <span class="type">BufferedWriter</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Creates a temporary file for a transaction into</span></span><br><span class="line">  <span class="comment">// which the records are written.</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">beginTransaction</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="comment">// path of transaction file</span></span><br><span class="line">    <span class="comment">// is built from current time and task index</span></span><br><span class="line">    <span class="keyword">val</span> timeNow = <span class="type">LocalDateTime</span>.now(<span class="type">ZoneId</span>.of(<span class="string">"UTC"</span>))</span><br><span class="line">      .format(<span class="type">DateTimeFormatter</span>.<span class="type">ISO_LOCAL_DATE_TIME</span>)</span><br><span class="line">    <span class="keyword">val</span> taskIdx = <span class="keyword">this</span>.getRuntimeContext.getIndexOfThisSubtask</span><br><span class="line">    <span class="keyword">val</span> transactionFile = <span class="string">s"<span class="subst">$timeNow</span>-<span class="subst">$taskIdx</span>"</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">// create transaction file and writer</span></span><br><span class="line">    <span class="keyword">val</span> tFilePath = <span class="type">Paths</span>.get(<span class="string">s"<span class="subst">$tempPath</span>/<span class="subst">$transactionFile</span>"</span>)</span><br><span class="line">    <span class="type">Files</span>.createFile(tFilePath)</span><br><span class="line">    <span class="keyword">this</span>.transactionWriter = <span class="type">Files</span>.newBufferedWriter(tFilePath)</span><br><span class="line">    println(<span class="string">s"Creating Transaction File: <span class="subst">$tFilePath</span>"</span>)</span><br><span class="line">    <span class="comment">// name of transaction file is returned to</span></span><br><span class="line">    <span class="comment">// later identify the transaction</span></span><br><span class="line">    transactionFile</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Write record into the current transaction file. */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(</span><br><span class="line">      transaction: <span class="type">String</span>,</span><br><span class="line">      value: (<span class="type">String</span>, <span class="type">Double</span>),</span><br><span class="line">      context: <span class="type">Context</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    transactionWriter.write(value.toString)</span><br><span class="line">    transactionWriter.write('\n')</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Flush and close the current transaction file. */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">preCommit</span></span>(transaction: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    transactionWriter.flush()</span><br><span class="line">    transactionWriter.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Commit a transaction by moving</span></span><br><span class="line">  <span class="comment">// the precommitted transaction file</span></span><br><span class="line">  <span class="comment">// to the target directory.</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">commit</span></span>(transaction: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> tFilePath = <span class="type">Paths</span>.get(<span class="string">s"<span class="subst">$tempPath</span>/<span class="subst">$transaction</span>"</span>)</span><br><span class="line">    <span class="comment">// check if the file exists to ensure</span></span><br><span class="line">    <span class="comment">// that the commit is idempotent</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="type">Files</span>.exists(tFilePath)) &#123;</span><br><span class="line">      <span class="keyword">val</span> cFilePath = <span class="type">Paths</span>.get(<span class="string">s"<span class="subst">$targetPath</span>/<span class="subst">$transaction</span>"</span>)</span><br><span class="line">      <span class="type">Files</span>.move(tFilePath, cFilePath)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Aborts a transaction by deleting the transaction file.</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">abort</span></span>(transaction: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> tFilePath = <span class="type">Paths</span>.get(<span class="string">s"<span class="subst">$tempPath</span>/<span class="subst">$transaction</span>"</span>)</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">Files</span>.exists(tFilePath)) &#123;</span><br><span class="line">      <span class="type">Files</span>.delete(tFilePath)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>TwoPhaseCommitSinkFunction[IN, TXN, CONTEXT]包含如下三个范型参数：</p><ul><li>IN表示输入数据的类型。</li><li>TXN定义了一个事务的标识符，可以用来识别和恢复事务。</li><li>CONTEXT定义了自定义的上下文。</li></ul><p>TwoPhaseCommitSinkFunction的构造器需要两个TypeSerializer。一个是TXN的类型，另一个是CONTEXT的类型。</p><p>最后，TwoPhaseCommitSinkFunction定义了五个需要实现的方法：</p><ul><li>beginTransaction(): TXN开始一个事务，并返回事务的标识符。</li><li>invoke(txn: TXN, value: IN, context: Context[_]): Unit将值写入到当前事务中。</li><li>preCommit(txn: TXN): Unit预提交一个事务。一个预提交的事务不会接收新的写入。</li><li>commit(txn: TXN): Unit提交一个事务。这个操作必须是幂等的。</li><li>abort(txn: TXN): Unit终止一个事务。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第八章，读写外部系统&quot;&gt;&lt;a href=&quot;#第八章，读写外部系统&quot; class=&quot;headerlink&quot; title=&quot;第八章，读写外部系统&quot;&gt;&lt;/a&gt;第八章，读写外部系统&lt;/h1&gt;&lt;p&gt;数据可以存储在不同的系统中，例如：文件系统，对象存储系统（OSS），关系型数
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink系列07有状态算子和应用</title>
    <link href="https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9707%E6%9C%89%E7%8A%B6%E6%80%81%E7%AE%97%E5%AD%90%E5%92%8C%E5%BA%94%E7%94%A8/"/>
    <id>https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9707%E6%9C%89%E7%8A%B6%E6%80%81%E7%AE%97%E5%AD%90%E5%92%8C%E5%BA%94%E7%94%A8/</id>
    <published>2020-07-02T03:47:39.000Z</published>
    <updated>2020-07-02T03:48:27.490Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第七章，有状态算子和应用"><a href="#第七章，有状态算子和应用" class="headerlink" title="第七章，有状态算子和应用"></a>第七章，有状态算子和应用</h1><p>状态操作符和用户自定义函数都是我们在写流处理程序时，常用的工具。事实上，大部分稍微复杂一点的逻辑都需要保存数据或者保存计算结果。很多Flink内置的操作符例如：source操作符，sink操作符等等都是有状态的，也就是说会缓存流数据或者计算结果。例如，窗口操作符将会为ProcessWindowFunction收集输入的数据，或者收集ReduceFunction计算的结果。而ProcessFunction也会保存定时器事件，一些sink方法为了做到exactly-once，会将事务保存下来。除了内置的操作符以及提供的source和sink操作符，Flink的DataStream API还在UDF函数中暴露了可以注册、保存和访问状态的接口。</p><p>本章重点讨论有状态的用户自定义函数的实现，以及讨论有状态应用的性能和健壮性。特别的，我们将解释在用户自定义函数中，如何定义不同类型的状态，以及如何与状态进行交互。我们还讨论了性能方面的问题以及如何控制状态大小的问题。</p><h2 id="实现有状态的用户自定义函数"><a href="#实现有状态的用户自定义函数" class="headerlink" title="实现有状态的用户自定义函数"></a>实现有状态的用户自定义函数</h2><p>我们知道函数有两种状态，键控状态(keyed state)和操作符状态(operator state)。</p><h3 id="在RuntimeContext中定义键控状态-keyed-state"><a href="#在RuntimeContext中定义键控状态-keyed-state" class="headerlink" title="在RuntimeContext中定义键控状态(keyed state)"></a>在RuntimeContext中定义键控状态(keyed state)</h3><p>用户自定义函数可以使用keyed state来存储和访问key对应的状态。对于每一个key，Flink将会维护一个状态实例。一个操作符的状态实例将会被分发到操作符的所有并行任务中去。这表明函数的每一个并行任务只为所有key的某一部分key保存key对应的状态实例。所以keyed state和分布式key-value map数据结构非常类似。</p><p>keyed state仅可用于KeyedStream。Flink支持以下数据类型的状态变量：</p><ul><li>ValueState[T]保存单个的值，值的类型为T。<ul><li>get操作: ValueState.value()</li><li>set操作: ValueState.update(value: T)</li></ul></li><li>ListState[T]保存一个列表，列表里的元素的数据类型为T。基本操作如下：<ul><li>ListState.add(value: T)</li><li>ListState.addAll(values: java.util.List[T])</li><li>ListState.get()返回Iterable[T]</li><li>ListState.update(values: java.util.List[T])</li></ul></li><li>MapState[K, V]保存Key-Value对。<ul><li>MapState.get(key: K)</li><li>MapState.put(key: K, value: V)</li><li>MapState.contains(key: K)</li><li>MapState.remove(key: K)</li></ul></li><li>ReducingState[T]</li><li>AggregatingState[I, O]</li></ul><p>State.clear()是清空操作。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorData: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> keyedData: <span class="type">KeyedStream</span>[<span class="type">SensorReading</span>, <span class="type">String</span>] = sensorData.keyBy(_.id)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> alerts: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] = keyedData</span><br><span class="line">  .flatMap(<span class="keyword">new</span> <span class="type">TemperatureAlertFunction</span>(<span class="number">1.7</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemperatureAlertFunction</span>(<span class="params">val threshold: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>[<span class="type">SensorReading</span>, (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> lastTempState: <span class="type">ValueState</span>[<span class="type">Double</span>] = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> lastTempDescriptor = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Double</span>](</span><br><span class="line">      <span class="string">"lastTemp"</span>, classOf[<span class="type">Double</span>])</span><br><span class="line"></span><br><span class="line">    lastTempState = getRuntimeContext.getState[<span class="type">Double</span>](lastTempDescriptor)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(</span><br><span class="line">    reading: <span class="type">SensorReading</span>,</span><br><span class="line">    out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)]</span><br><span class="line">  ): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> lastTemp = lastTempState.value()</span><br><span class="line">    <span class="keyword">val</span> tempDiff = (reading.temperature - lastTemp).abs</span><br><span class="line">    <span class="keyword">if</span> (tempDiff &gt; threshold) &#123;</span><br><span class="line">      out.collect((reading.id, reading.temperature, tempDiff))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.lastTempState.update(reading.temperature)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>上面例子中的FlatMapFunction只能访问当前处理的元素所包含的key所对应的状态变量。</p><blockquote><p>不同key对应的keyed state是相互隔离的。</p></blockquote><ul><li>通过RuntimeContext注册StateDescriptor。StateDescriptor以状态state的名字和存储的数据类型为参数。数据类型必须指定，因为Flink需要选择合适的序列化器。</li><li>在open()方法中创建state变量。注意复习之前的RichFunction相关知识。</li></ul><p>当一个函数注册了StateDescriptor描述符，Flink会检查状态后端是否已经存在这个状态。这种情况通常出现在应用挂掉要从检查点或者保存点恢复的时候。在这两种情况下，Flink会将注册的状态连接到已经存在的状态。如果不存在状态，则初始化一个空的状态。</p><p>使用FlatMap with keyed ValueState的快捷方式flatMapWithState也可以实现以上需求。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> alerts: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] = keyedSensorData</span><br><span class="line">  .flatMapWithState[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>), <span class="type">Double</span>] &#123;</span><br><span class="line">    <span class="keyword">case</span> (in: <span class="type">SensorReading</span>, <span class="type">None</span>) =&gt;</span><br><span class="line">      <span class="comment">// no previous temperature defined.</span></span><br><span class="line">      <span class="comment">// Just update the last temperature</span></span><br><span class="line">      (<span class="type">List</span>.empty, <span class="type">Some</span>(in.temperature))</span><br><span class="line">    <span class="keyword">case</span> (r: <span class="type">SensorReading</span>, lastTemp: <span class="type">Some</span>[<span class="type">Double</span>]) =&gt;</span><br><span class="line">      <span class="comment">// compare temperature difference with threshold</span></span><br><span class="line">      <span class="keyword">val</span> tempDiff = (r.temperature - lastTemp.get).abs</span><br><span class="line">      <span class="keyword">if</span> (tempDiff &gt; <span class="number">1.7</span>) &#123;</span><br><span class="line">        <span class="comment">// threshold exceeded.</span></span><br><span class="line">        <span class="comment">// Emit an alert and update the last temperature</span></span><br><span class="line">        (<span class="type">List</span>((r.id, r.temperature, tempDiff)), <span class="type">Some</span>(r.temperature))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// threshold not exceeded. Just update the last temperature</span></span><br><span class="line">        (<span class="type">List</span>.empty, <span class="type">Some</span>(r.temperature))</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div><h3 id="使用ListCheckpointed接口来实现操作符的列表状态-List-State"><a href="#使用ListCheckpointed接口来实现操作符的列表状态-List-State" class="headerlink" title="使用ListCheckpointed接口来实现操作符的列表状态(List State)"></a>使用ListCheckpointed接口来实现操作符的列表状态(List State)</h3><p>操作符状态会在操作符的每一个并行实例中去维护。一个操作符并行实例上的所有事件都可以访问同一个状态。Flink支持三种操作符状态：list state, list union state, broadcast state。</p><p>一个函数可以实现ListCheckpointed接口来处理操作符的list state。ListCheckpointed接口无法处理ValueState和ListState，因为这些状态是注册在状态后端的。操作符状态类似于成员变量，和状态后端的交互通过ListCheckpointed接口的回调函数实现。接口提供了两个方法：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 返回函数状态的快照，返回值为列表</span></span><br><span class="line">snapshotState(checkpointId: <span class="type">Long</span>, timestamp: <span class="type">Long</span>): java.util.<span class="type">List</span>[<span class="type">T</span>]</span><br><span class="line"><span class="comment">// 从列表恢复函数状态</span></span><br><span class="line">restoreState(java.util.<span class="type">List</span>[<span class="type">T</span>] state): <span class="type">Unit</span></span><br></pre></td></tr></table></figure></div><p>当Flink触发stateful functon的一次checkpoint时，snapshotState()方法会被调用。方法接收两个参数，checkpointId为唯一的单调递增的检查点Id，timestamp为当master机器开始做检查点操作时的墙上时钟（机器时间）。方法必须返回序列化好的状态对象的列表。</p><p>当宕机程序从检查点或者保存点恢复时会调用restoreState()方法。restoreState使用snapshotState保存的列表来恢复。</p><p>下面的例子展示了如何实现ListCheckpointed接口。业务场景为：一个对每一个并行实例的超过阈值的温度的计数程序。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HighTempCounter</span>(<span class="params">val threshold: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>[<span class="type">SensorReading</span>, (<span class="type">Int</span>, <span class="type">Long</span>)]</span></span><br><span class="line"><span class="class">    <span class="keyword">with</span> <span class="title">ListCheckpointed</span>[java.lang.<span class="type">Long</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// index of the subtask</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">lazy</span> <span class="keyword">val</span> subtaskIdx = getRuntimeContext</span><br><span class="line">    .getIndexOfThisSubtask</span><br><span class="line">  <span class="comment">// local count variable</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> highTempCnt = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(</span><br><span class="line">      in: <span class="type">SensorReading</span>, </span><br><span class="line">      out: <span class="type">Collector</span>[(<span class="type">Int</span>, <span class="type">Long</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (in.temperature &gt; threshold) &#123;</span><br><span class="line">      <span class="comment">// increment counter if threshold is exceeded</span></span><br><span class="line">      highTempCnt += <span class="number">1</span></span><br><span class="line">      <span class="comment">// emit update with subtask index and counter</span></span><br><span class="line">      out.collect((subtaskIdx, highTempCnt))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">restoreState</span></span>(</span><br><span class="line">      state: util.<span class="type">List</span>[java.lang.<span class="type">Long</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    highTempCnt = <span class="number">0</span></span><br><span class="line">    <span class="comment">// restore state by adding all longs of the list</span></span><br><span class="line">    <span class="keyword">for</span> (cnt &lt;- state.asScala) &#123;</span><br><span class="line">      highTempCnt += cnt</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">snapshotState</span></span>(</span><br><span class="line">      chkpntId: <span class="type">Long</span>, </span><br><span class="line">      ts: <span class="type">Long</span>): java.util.<span class="type">List</span>[java.lang.<span class="type">Long</span>] = &#123;</span><br><span class="line">    <span class="comment">// snapshot state as list with a single count</span></span><br><span class="line">    java.util.<span class="type">Collections</span>.singletonList(highTempCnt)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>上面的例子中，每一个并行实例都计数了本实例有多少温度值超过了设定的阈值。例子中使用了操作符状态，并且每一个并行实例都拥有自己的状态变量，这个状态变量将会被检查点操作保存下来，并且可以通过使用ListCheckpointed接口来恢复状态变量。</p><p>看了上面的例子，我们可能会有疑问，那就是为什么操作符状态是状态对象的列表。这是因为列表数据结构支持包含操作符状态的函数的并行度改变的操作。为了增加或者减少包含了操作符状态的函数的并行度，操作符状态需要被重新分区到更多或者更少的并行任务实例中去。而这样的操作需要合并或者分割状态对象。而对于每一个有状态的函数，分割和合并状态对象都是很常见的操作，所以这显然不是任何类型的状态都能自动完成的。</p><p>通过提供一个状态对象的列表，拥有操作符状态的函数可以使用snapshotState()方法和restoreState()方法来实现以上所说的逻辑。snapshotState()方法将操作符状态分割成多个部分，restoreState()方法从所有的部分中将状态对象收集起来。当函数的操作符状态恢复时，状态变量将被分区到函数的所有不同的并行实例中去，并作为参数传递给restoreState()方法。如果并行任务的数量大于状态对象的数量，那么一些并行任务在开始的时候是没有状态的，所以restoreState()函数的参数为空列表。</p><p>再来看一下上面的程序，我们可以看到操作符的每一个并行实例都暴露了一个状态对象的列表。如果我们增加操作符的并行度，那么一些并行任务将会从0开始计数。为了获得更好的状态分区的行为，当HighTempCounter函数扩容时，我们可以按照下面的程序来实现snapshotState()方法，这样就可以把计数值分配到不同的并行计数中去了。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">snapshotState</span></span>(</span><br><span class="line">    chkpntId: <span class="type">Long</span>, </span><br><span class="line">    ts: <span class="type">Long</span>): java.util.<span class="type">List</span>[java.lang.<span class="type">Long</span>] = &#123;</span><br><span class="line">  <span class="comment">// split count into ten partial counts</span></span><br><span class="line">  <span class="keyword">val</span> div = highTempCnt / <span class="number">10</span></span><br><span class="line">  <span class="keyword">val</span> mod = (highTempCnt % <span class="number">10</span>).toInt</span><br><span class="line">  <span class="comment">// return count as ten parts</span></span><br><span class="line">  (<span class="type">List</span>.fill(mod)(<span class="keyword">new</span> java.lang.<span class="type">Long</span>(div + <span class="number">1</span>)) ++</span><br><span class="line">    <span class="type">List</span>.fill(<span class="number">10</span> - mod)(<span class="keyword">new</span> java.lang.<span class="type">Long</span>(div))).asJava</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="使用连接的广播状态-using-connected-broadcast-state"><a href="#使用连接的广播状态-using-connected-broadcast-state" class="headerlink" title="使用连接的广播状态(using connected broadcast state)"></a>使用连接的广播状态(using connected broadcast state)</h3><p>一个常见的需求就是流应用需要将同样的事件分发到操作符的所有的并行实例中，而这样的分发操作还得是可恢复的。</p><p>我们举个例子：一条流是一个规则(比如5秒钟内连续两个超过阈值的温度)，另一条流是待匹配的流。也就是说，规则流和事件流。所以每一个操作符的并行实例都需要把规则流保存在操作符状态中。也就是说，规则流需要被广播到所有的并行实例中去。</p><p>在Flink中，这样的状态叫做广播状态(broadcast state)。广播状态和DataStream或者KeyedStream都可以做连接操作。</p><p>下面的例子实现了一个温度报警应用，应用有可以动态设定的阈值，动态设定通过广播流来实现。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorData: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> thresholds: <span class="type">DataStream</span>[<span class="type">ThresholdUpdate</span>] = ...</span><br><span class="line"><span class="keyword">val</span> keyedSensorData: <span class="type">KeyedStream</span>[<span class="type">SensorReading</span>, <span class="type">String</span>] = sensorData</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line"></span><br><span class="line"><span class="comment">// the descriptor of the broadcast state</span></span><br><span class="line"><span class="keyword">val</span> broadcastStateDescriptor =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapStateDescriptor</span>[<span class="type">String</span>, <span class="type">Double</span>](</span><br><span class="line">    <span class="string">"thresholds"</span>, classOf[<span class="type">String</span>], classOf[<span class="type">Double</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> broadcastThresholds: <span class="type">BroadcastStream</span>[<span class="type">ThresholdUpdate</span>] = thresholds</span><br><span class="line">  .broadcast(broadcastStateDescriptor)</span><br><span class="line"></span><br><span class="line"><span class="comment">// connect keyed sensor stream and broadcasted rules stream</span></span><br><span class="line"><span class="keyword">val</span> alerts: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] = keyedSensorData</span><br><span class="line">  .connect(broadcastThresholds)</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">UpdatableTemperatureAlertFunction</span>())</span><br></pre></td></tr></table></figure></div><p>带有广播状态的函数在应用到两条流上时分三个步骤：</p><ul><li>调用DataStream.broadcast()来创建BroadcastStream，定义一个或者多个MapStateDescriptor对象。</li><li>将BroadcastStream和DataStream/KeyedStream做connect操作。</li><li>在connected streams上调用KeyedBroadcastProcessFunction/BroadcastProcessFunction。</li></ul><p>下面的例子实现了动态设定温度阈值的功能。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpdatableTemperatureAlertFunction</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">KeyedBroadcastProcessFunction</span>[<span class="type">String</span>,</span></span><br><span class="line"><span class="class">      <span class="type">SensorReading</span>, <span class="type">ThresholdUpdate</span>, (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// the descriptor of the broadcast state</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">lazy</span> <span class="keyword">val</span> thresholdStateDescriptor =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapStateDescriptor</span>[<span class="type">String</span>, <span class="type">Double</span>](</span><br><span class="line">      <span class="string">"thresholds"</span>, classOf[<span class="type">String</span>], classOf[<span class="type">Double</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment">// the keyed state handle</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> lastTempState: <span class="type">ValueState</span>[<span class="type">Double</span>] = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// create keyed state descriptor</span></span><br><span class="line">    <span class="keyword">val</span> lastTempDescriptor = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Double</span>](</span><br><span class="line">      <span class="string">"lastTemp"</span>, classOf[<span class="type">Double</span>])</span><br><span class="line">    <span class="comment">// obtain the keyed state handle</span></span><br><span class="line">    lastTempState = getRuntimeContext</span><br><span class="line">      .getState[<span class="type">Double</span>](lastTempDescriptor)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processBroadcastElement</span></span>(</span><br><span class="line">      update: <span class="type">ThresholdUpdate</span>,</span><br><span class="line">      ctx: <span class="type">KeyedBroadcastProcessFunction</span>[<span class="type">String</span>,</span><br><span class="line">        <span class="type">SensorReading</span>, <span class="type">ThresholdUpdate</span>,</span><br><span class="line">        (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)]#<span class="type">Context</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// get broadcasted state handle</span></span><br><span class="line">    <span class="keyword">val</span> thresholds = ctx</span><br><span class="line">      .getBroadcastState(thresholdStateDescriptor)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (update.threshold != <span class="number">0.0</span>d) &#123;</span><br><span class="line">      <span class="comment">// configure a new threshold for the sensor</span></span><br><span class="line">      thresholds.put(update.id, update.threshold)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// remove threshold for the sensor</span></span><br><span class="line">      thresholds.remove(update.id)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(</span><br><span class="line">      reading: <span class="type">SensorReading</span>,</span><br><span class="line">      readOnlyCtx: <span class="type">KeyedBroadcastProcessFunction</span></span><br><span class="line">        [<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">ThresholdUpdate</span>, </span><br><span class="line">        (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)]#<span class="type">ReadOnlyContext</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// get read-only broadcast state</span></span><br><span class="line">    <span class="keyword">val</span> thresholds = readOnlyCtx</span><br><span class="line">      .getBroadcastState(thresholdStateDescriptor)</span><br><span class="line">    <span class="comment">// check if we have a threshold</span></span><br><span class="line">    <span class="keyword">if</span> (thresholds.contains(reading.id)) &#123;</span><br><span class="line">      <span class="comment">// get threshold for sensor</span></span><br><span class="line">      <span class="keyword">val</span> sensorThreshold: <span class="type">Double</span> = thresholds.get(reading.id)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// fetch the last temperature from state</span></span><br><span class="line">      <span class="keyword">val</span> lastTemp = lastTempState.value()</span><br><span class="line">      <span class="comment">// check if we need to emit an alert</span></span><br><span class="line">      <span class="keyword">val</span> tempDiff = (reading.temperature - lastTemp).abs</span><br><span class="line">      <span class="keyword">if</span> (tempDiff &gt; sensorThreshold) &#123;</span><br><span class="line">        <span class="comment">// temperature increased by more than the threshold</span></span><br><span class="line">        out.collect((reading.id, reading.temperature, tempDiff))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// update lastTemp state</span></span><br><span class="line">    <span class="keyword">this</span>.lastTempState.update(reading.temperature)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="配置检查点"><a href="#配置检查点" class="headerlink" title="配置检查点"></a>配置检查点</h2><p>10秒钟保存一次检查点。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">// set checkpointing interval to 10 seconds (10000 milliseconds)</span></span><br><span class="line">env.enableCheckpointing(<span class="number">10000</span>L)</span><br></pre></td></tr></table></figure></div><h3 id="将hdfs配置为状态后端"><a href="#将hdfs配置为状态后端" class="headerlink" title="将hdfs配置为状态后端"></a>将hdfs配置为状态后端</h3><p>首先在IDEA的pom文件中添加依赖：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>在<code>hdfs-site.xml</code>添加:</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></div><blockquote><p>别忘了重启hdfs文件系统！</p></blockquote><p>然后添加本地文件夹和hdfs文件的映射：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs getconf -confKey fs.default.name</span><br><span class="line">hdfs dfs -put &#x2F;home&#x2F;parallels&#x2F;flink&#x2F;checkpoint hdfs:&#x2F;&#x2F;localhost:9000&#x2F;flink</span><br></pre></td></tr></table></figure></div><p>然后在代码中添加：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">env.enableCheckpointing(5000)</span><br><span class="line">env.setStateBackend(new FsStateBackend(&quot;hdfs:&#x2F;&#x2F;localhost:9000&#x2F;flink&quot;))</span><br></pre></td></tr></table></figure></div><p>检查一下检查点正确保存了没有：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -ls hdfs:&#x2F;&#x2F;localhost:9000&#x2F;flink</span><br></pre></td></tr></table></figure></div><h2 id="保证有状态应用的可维护性"><a href="#保证有状态应用的可维护性" class="headerlink" title="保证有状态应用的可维护性"></a>保证有状态应用的可维护性</h2><h3 id="指定唯一的操作符标识符-operator-identifiers"><a href="#指定唯一的操作符标识符-operator-identifiers" class="headerlink" title="指定唯一的操作符标识符(operator identifiers)"></a>指定唯一的操作符标识符(operator identifiers)</h3><p>每一个操作符都可以指定唯一的标识符。标识符将会作为操作符的元数据和状态数据一起保存到savepoint中去。当应用从保存点恢复时，标识符可以用来在savepoint中查找标识符对应的操作符的状态数据。标识符必须是唯一的，否则应用不知道从哪一个标识符恢复。</p><p>强烈建议为应用的每一个操作符定义唯一标识符。例子：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> alerts: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] = keyedSensorData</span><br><span class="line">  .flatMap(<span class="keyword">new</span> <span class="type">TemperatureAlertFunction</span>(<span class="number">1.1</span>))  </span><br><span class="line">  .uid(<span class="string">"TempAlert"</span>)</span><br></pre></td></tr></table></figure></div><h3 id="指定操作符的最大并行度"><a href="#指定操作符的最大并行度" class="headerlink" title="指定操作符的最大并行度"></a>指定操作符的最大并行度</h3><p>操作符的最大并行度定义了操作符的keyed state可以被分到多少个key groups中。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">// set the maximum parallelism for this application</span></span><br><span class="line">env.setMaxParallelism(<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> alerts: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] = keyedSensorData</span><br><span class="line">  .flatMap(<span class="keyword">new</span> <span class="type">TemperatureAlertFunction</span>(<span class="number">1.1</span>))</span><br><span class="line">  <span class="comment">// set the maximum parallelism for this operator and</span></span><br><span class="line">  <span class="comment">// override the application-wide value</span></span><br><span class="line">  .setMaxParallelism(<span class="number">1024</span>)</span><br></pre></td></tr></table></figure></div><h2 id="有状态应用的性能和健壮性"><a href="#有状态应用的性能和健壮性" class="headerlink" title="有状态应用的性能和健壮性"></a>有状态应用的性能和健壮性</h2><h3 id="选择一个状态后端"><a href="#选择一个状态后端" class="headerlink" title="选择一个状态后端"></a>选择一个状态后端</h3><ul><li>MemoryStateBackend将状态当作Java的对象(没有序列化操作)存储在TaskManager JVM进程的堆上。</li><li>FsStateBackend将状态存储在本地的文件系统或者远程的文件系统如HDFS。</li><li>RocksDBStateBackend将状态存储在RocksDB 中。</li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> checkpointPath: <span class="type">String</span> = ???</span><br><span class="line"><span class="comment">// configure path for checkpoints on the remote filesystem</span></span><br><span class="line"><span class="comment">// env.setStateBackend(new FsStateBackend("file:///tmp/checkpoints"))</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(checkpointPath)</span><br><span class="line"><span class="comment">// configure the state backend</span></span><br><span class="line">env.setStateBackend(backend)</span><br></pre></td></tr></table></figure></div><h3 id="防止状态泄露"><a href="#防止状态泄露" class="headerlink" title="防止状态泄露"></a>防止状态泄露</h3><p>流应用通常需要运行几个月或者几年。如果state数据不断增长的话，会爆炸。所以控制state数据的大小十分重要。而Flink并不会清理state和gc。所以所有的stateful operator都需要控制他们各自的状态数据大小，保证不爆炸。</p><p>例如我们之前讲过增量聚合函数ReduceFunction/AggregateFunction，就可以提前聚合而不给state太多压力。</p><p>我们来看一个例子，我们实现了一个KeyedProcessFunction，用来计算连续两次的温度的差值，如果差值超过阈值，报警。</p><p>我们之前实现过这个需求，但没有清理掉状态数据。比如一小时内不再产生温度数据的传感器对应的状态数据就可以清理掉了。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SelfCleaningTemperatureAlertFunction</span>(<span class="params">val threshold: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>[<span class="type">String</span>,</span></span><br><span class="line"><span class="class">      <span class="type">SensorReading</span>, (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// the keyed state handle for the last temperature</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> lastTempState: <span class="type">ValueState</span>[<span class="type">Double</span>] = _</span><br><span class="line">  <span class="comment">// the keyed state handle for the last registered timer</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> lastTimerState: <span class="type">ValueState</span>[<span class="type">Long</span>] = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// register state for last temperature</span></span><br><span class="line">    <span class="keyword">val</span> lastTempDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Double</span>](</span><br><span class="line">      <span class="string">"lastTemp"</span>, classOf[<span class="type">Double</span>])</span><br><span class="line">    lastTempState = getRuntimeContext</span><br><span class="line">      .getState[<span class="type">Double</span>](lastTempDescriptor)</span><br><span class="line">    <span class="comment">// register state for last timer</span></span><br><span class="line">    <span class="keyword">val</span> lastTimerDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Long</span>](</span><br><span class="line">      <span class="string">"lastTimer"</span>, classOf[<span class="type">Long</span>])</span><br><span class="line">    lastTimerState = getRuntimeContext</span><br><span class="line">      .getState(timestampDescriptor)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(</span><br><span class="line">      reading: <span class="type">SensorReading</span>,</span><br><span class="line">      ctx: <span class="type">KeyedProcessFunction</span></span><br><span class="line">        [<span class="type">String</span>, <span class="type">SensorReading</span>, (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)]#<span class="type">Context</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// compute timestamp of new clean up timer</span></span><br><span class="line">    <span class="comment">// as record timestamp + one hour</span></span><br><span class="line">    <span class="keyword">val</span> newTimer = ctx.timestamp() + (<span class="number">3600</span> * <span class="number">1000</span>)</span><br><span class="line">    <span class="comment">// get timestamp of current timer</span></span><br><span class="line">    <span class="keyword">val</span> curTimer = lastTimerState.value()</span><br><span class="line">    <span class="comment">// delete previous timer and register new timer</span></span><br><span class="line">    ctx.timerService().deleteEventTimeTimer(curTimer)</span><br><span class="line">    ctx.timerService().registerEventTimeTimer(newTimer)</span><br><span class="line">    <span class="comment">// update timer timestamp state</span></span><br><span class="line">    lastTimerState.update(newTimer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// fetch the last temperature from state</span></span><br><span class="line">    <span class="keyword">val</span> lastTemp = lastTempState.value()</span><br><span class="line">    <span class="comment">// check if we need to emit an alert</span></span><br><span class="line">    <span class="keyword">val</span> tempDiff = (reading.temperature - lastTemp).abs</span><br><span class="line">    <span class="keyword">if</span> (tempDiff &gt; threshold) &#123;</span><br><span class="line">      <span class="comment">// temperature increased by more than the threshold</span></span><br><span class="line">      out.collect((reading.id, reading.temperature, tempDiff))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// update lastTemp state</span></span><br><span class="line">    <span class="keyword">this</span>.lastTempState.update(reading.temperature)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(</span><br><span class="line">      timestamp: <span class="type">Long</span>,</span><br><span class="line">      ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>,</span><br><span class="line">        <span class="type">SensorReading</span>, (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)]#<span class="type">OnTimerContext</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// clear all state for the key</span></span><br><span class="line">    lastTempState.clear()</span><br><span class="line">    lastTimerState.clear()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第七章，有状态算子和应用&quot;&gt;&lt;a href=&quot;#第七章，有状态算子和应用&quot; class=&quot;headerlink&quot; title=&quot;第七章，有状态算子和应用&quot;&gt;&lt;/a&gt;第七章，有状态算子和应用&lt;/h1&gt;&lt;p&gt;状态操作符和用户自定义函数都是我们在写流处理程序时，常用的工
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink系列06基于时间和窗口的操作符</title>
    <link href="https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9706%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E5%92%8C%E7%AA%97%E5%8F%A3%E7%9A%84%E6%93%8D%E4%BD%9C%E7%AC%A6/"/>
    <id>https://masteryang4.github.io/2020/07/02/flink%E7%B3%BB%E5%88%9706%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E5%92%8C%E7%AA%97%E5%8F%A3%E7%9A%84%E6%93%8D%E4%BD%9C%E7%AC%A6/</id>
    <published>2020-07-02T03:45:19.000Z</published>
    <updated>2020-07-02T03:47:24.167Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第六章，基于时间和窗口的操作符"><a href="#第六章，基于时间和窗口的操作符" class="headerlink" title="第六章，基于时间和窗口的操作符"></a>第六章，基于时间和窗口的操作符</h1><p>在本章，我们将要学习DataStream API中处理时间和基于时间的操作符，例如窗口操作符。</p><p>首先，我们会学习如何定义时间属性，时间戳和水位线。然后我们将会学习底层操作process function，它可以让我们访问时间戳和水位线，以及注册定时器事件。接下来，我们将会使用Flink的window API，它提供了通常使用的各种窗口类型的内置实现。我们将会学到如何进行用户自定义窗口操作符，以及窗口的核心功能：assigners（分配器）、triggers（触发器）和evictors（清理器）。最后，我们将讨论如何基于时间来做流的联结查询，以及处理迟到事件的策略。</p><h2 id="设置时间属性"><a href="#设置时间属性" class="headerlink" title="设置时间属性"></a>设置时间属性</h2><p>如果我们想要在分布式流处理应用程序中定义有关时间的操作，彻底理解时间的语义是非常重要的。当我们指定了一个窗口去收集某1分钟内的数据时，这个长度为1分钟的桶中，到底应该包含哪些数据？在DataStream API中，我们将使用时间属性来告诉Flink：当我们创建窗口时，我们如何定义时间。时间属性是<code>StreamExecutionEnvironment</code>的一个属性，有以下值：</p><p><em>ProcessingTime</em></p><blockquote><p>机器时间在分布式系统中又叫做“墙上时钟”。</p></blockquote><p>当操作符执行时，此操作符看到的时间是操作符所在机器的机器时间。Processing-time window的触发取决于机器时间，窗口包含的元素也是那个机器时间段内到达的元素。通常情况下，窗口操作符使用processing time会导致不确定的结果，因为基于机器时间的窗口中收集的元素取决于元素到达的速度快慢。使用processing time会为程序提供极低的延迟，因为无需等待水位线的到达。</p><blockquote><p>如果要追求极限的低延迟，请使用processing time。</p></blockquote><p><em>EventTime</em></p><p>当操作符执行时，操作符看的当前时间是由流中元素所携带的信息决定的。流中的每一个元素都必须包含时间戳信息。而系统的逻辑时钟由水位线(Watermark)定义。我们之前学习过，时间戳要么在事件进入流处理程序之前已经存在，要么就需要在程序的数据源（source）处进行分配。当水位线宣布特定时间段的数据都已经到达，事件时间窗口将会被触发计算。即使数据到达的顺序是乱序的，事件时间窗口的计算结果也将是确定性的。窗口的计算结果并不取决于元素到达的快与慢。</p><blockquote><p>当水位线超过事件时间窗口的结束时间时，窗口将会闭合，不再接收数据，并触发计算。</p></blockquote><p><em>IngestionTime</em></p><p>当事件进入source操作符时，source操作符所在机器的机器时间，就是此事件的“摄入时间”（IngestionTime），并同时产生水位线。IngestionTime相当于EventTime和ProcessingTime的混合体。一个事件的IngestionTime其实就是它进入流处理器中的时间。</p><blockquote><p>IngestionTime没什么价值，既有EventTime的执行效率（比较低），有没有EventTime计算结果的准确性。</p></blockquote><p>下面的例子展示了如何设置事件时间。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AverageSensorReadings</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="keyword">val</span> sensorData: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = env.addSource(...)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>如果要使用processing time，将<code>TimeCharacteristic.EventTime</code>替换为<code>TimeCharacteristic.ProcessingTIme</code>就可以了。</p><h3 id="指定时间戳和产生水位线"><a href="#指定时间戳和产生水位线" class="headerlink" title="指定时间戳和产生水位线"></a>指定时间戳和产生水位线</h3><p>如果使用事件时间，那么流中的事件必须包含这个事件真正发生的时间。使用了事件时间的流必须携带水位线。</p><p>时间戳和水位线的单位是毫秒，记时从<code>1970-01-01T00:00:00Z</code>开始。到达某个操作符的水位线就会告知这个操作符：小于等于水位线中携带的时间戳的事件都已经到达这个操作符了。时间戳和水位线可以由<code>SourceFunction</code>产生，或者由用户自定义的时间戳分配器和水位线产生器来生成。</p><p>Flink暴露了TimestampAssigner接口供我们实现，使我们可以自定义如何从事件数据中抽取时间戳。一般来说，时间戳分配器需要在source操作符后马上进行调用。</p><blockquote><p>因为时间戳分配器看到的元素的顺序应该和source操作符产生数据的顺序是一样的，否则就乱了。这就是为什么我们经常将source操作符的并行度设置为1的原因。</p></blockquote><p>也就是说，任何分区操作都会将元素的顺序打乱，例如：并行度改变，keyBy()操作等等。</p><p>所以最佳实践是：在尽量接近数据源source操作符的地方分配时间戳和产生水位线，甚至最好在SourceFunction中分配时间戳和产生水位线。当然在分配时间戳和产生水位线之前可以对流进行map和filter操作是没问题的，也就是说必须是窄依赖。</p><p>以下这种写法是可以的。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream = env</span><br><span class="line">  .addSource(...)</span><br><span class="line">  .map(...)</span><br><span class="line">  .filter(...)</span><br><span class="line">  .assignTimestampsAndWatermarks(...)</span><br></pre></td></tr></table></figure></div><p>下面的例子展示了首先filter流，然后再分配时间戳和水位线。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 从调用时刻开始给env创建的每一个stream追加时间特征</span></span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = env</span><br><span class="line">  .addSource(<span class="keyword">new</span> <span class="type">SensorSource</span>)</span><br><span class="line">  .filter(r =&gt; r.temperature &gt; <span class="number">25</span>)</span><br><span class="line">  .assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">MyAssigner</span>())</span><br></pre></td></tr></table></figure></div><p>MyAssigner有两种类型</p><ul><li>AssignerWithPeriodicWatermarks</li><li>AssignerWithPunctuatedWatermarks</li></ul><p>以上两个接口都继承自TimestampAssigner。</p><h3 id="周期性的生成水位线"><a href="#周期性的生成水位线" class="headerlink" title="周期性的生成水位线"></a>周期性的生成水位线</h3><p>周期性的生成水位线：系统会周期性的将水位线插入到流中（水位线也是一种特殊的事件!）。默认周期是200毫秒，也就是说，系统会每隔200毫秒就往流中插入一次水位线。</p><blockquote><p>这里的200毫秒是机器时间！</p></blockquote><p>可以使用<code>ExecutionConfig.setAutoWatermarkInterval()</code>方法进行设置。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"><span class="comment">// 每隔5秒产生一个水位线</span></span><br><span class="line">env.getConfig.setAutoWatermarkInterval(<span class="number">5000</span>)</span><br></pre></td></tr></table></figure></div><p>上面的例子产生水位线的逻辑：每隔5秒钟，Flink会调用AssignerWithPeriodicWatermarks中的getCurrentWatermark()方法。如果方法返回的时间戳大于之前水位线的时间戳，新的水位线会被插入到流中。这个检查保证了水位线是单调递增的。如果方法返回的时间戳小于等于之前水位线的时间戳，则不会产生新的水位线。</p><p>例子，自定义一个周期性的时间戳抽取</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PeriodicAssigner</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">AssignerWithPeriodicWatermarks</span>[<span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> bound: <span class="type">Long</span> = <span class="number">60</span> * <span class="number">1000</span> <span class="comment">// 延时为1分钟</span></span><br><span class="line">  <span class="keyword">var</span> maxTs: <span class="type">Long</span> = <span class="type">Long</span>.<span class="type">MinValue</span> + bound <span class="comment">// 观察到的最大时间戳</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCurrentWatermark</span></span>: <span class="type">Watermark</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Watermark</span>(maxTs - bound)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(r: <span class="type">SensorReading</span>, previousTS: <span class="type">Long</span>) = &#123;</span><br><span class="line">    maxTs = maxTs.max(r.timestamp)</span><br><span class="line">    r.timestamp</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>如果我们事先得知数据流的时间戳是单调递增的，也就是说没有乱序。我们可以使用assignAscendingTimestamps，方法会直接使用数据的时间戳生成水位线。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val stream: DataStream[SensorReading] &#x3D; ...</span><br><span class="line">val withTimestampsAndWatermarks &#x3D; stream</span><br><span class="line">  .assignAscendingTimestamps(e &#x3D;&gt; e.timestamp)</span><br></pre></td></tr></table></figure></div><p>如果我们能大致估算出数据流中的事件的最大延迟时间，可以使用如下代码：</p><blockquote><p>最大延迟时间就是当前到达的事件的事件时间和之前所有到达的事件中最大时间戳的差。</p></blockquote><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">SensorTimeAssigner</span> </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SensorTimeAssigner</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">SensorReading</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    <span class="type">Time</span>.seconds(5</span>)</span></span><br><span class="line"><span class="class">  ) </span>&#123;</span><br><span class="line">    <span class="comment">// 抽取时间戳</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(r: <span class="type">SensorReading</span>): <span class="type">Long</span> = r.timestamp</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>以上代码设置了最大延迟时间为5秒。</p><h3 id="如何产生不规则的水位线"><a href="#如何产生不规则的水位线" class="headerlink" title="如何产生不规则的水位线"></a>如何产生不规则的水位线</h3><p>有时候输入流中会包含一些用于指示系统进度的特殊元组或标记。Flink为此类情形以及可根据输入元素生成水位线的情形提供了<code>AssignerWithPunctuatedWatermarks</code>接口。该接口中的<code>checkAndGetNextWatermark()</code>方法会在针对每个事件的<code>extractTimestamp()</code>方法后立即调用。它可以决定是否生成一个新的水位线。如果该方法返回一个非空、且大于之前值的水位线，算子就会将这个新水位线发出。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PunctuatedAssigner</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">AssignerWithPunctuatedWatermarks</span>[<span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> bound: <span class="type">Long</span> = <span class="number">60</span> * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 每来一条数据就调用一次</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">checkAndGetNextWatermark</span></span>(r: <span class="type">SensorReading</span>,</span><br><span class="line">                                        extractedTS: <span class="type">Long</span>): <span class="type">Watermark</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (r.id == <span class="string">"sensor_1"</span>) &#123;</span><br><span class="line">      <span class="comment">// 抽取的时间戳 - 最大延迟时间</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">Watermark</span>(extractedTS - bound)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="literal">null</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 每来一条数据就调用一次</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(r: <span class="type">SensorReading</span>,</span><br><span class="line">                                previousTS: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">    r.timestamp</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>现在我们已经知道如何使用 <code>TimestampAssigner</code> 来产生水位线了。现在我们要讨论一下水位线会对我们的程序产生什么样的影响。</p><p>水位线用来平衡延迟和计算结果的正确性。水位线告诉我们，在触发计算（例如关闭窗口并触发窗口计算）之前，我们需要等待事件多长时间。基于事件时间的操作符根据水位线来衡量系统的逻辑时间的进度。</p><p>完美的水位线永远不会错：时间戳小于水位线的事件不会再出现。在特殊情况下(例如非乱序事件流)，最近一次事件的时间戳就可能是完美的水位线。启发式水位线则相反，它只估计时间，因此有可能出错，即迟到的事件(其时间戳小于水位线标记时间)晚于水位线出现。针对启发式水位线，Flink提供了处理迟到元素的机制。</p><p>设定水位线通常需要用到领域知识。举例来说，如果知道事件的迟到时间不会超过5秒，就可以将水位线标记时间设为收到的最大时间戳减去5秒。另一种做法是，采用一个Flink作业监控事件流，学习事件的迟到规律，并以此构建水位线生成模型。</p><p>如果最大延迟时间设置的很大，计算出的结果会更精确，但收到计算结果的速度会很慢，同时系统会缓存大量的数据，并对系统造成比较大的压力。如果最大延迟时间设置的很小，那么收到计算结果的速度会很快，但可能收到错误的计算结果。不过Flink处理迟到数据的机制可以解决这个问题。上述问题看起来很复杂，但是恰恰符合现实世界的规律：大部分真实的事件流都是乱序的，并且通常无法了解它们的乱序程度(因为理论上不能预见未来)。水位线是唯一让我们直面乱序事件流并保证正确性的机制; 否则只能选择忽视事实，假装错误的结果是正确的。</p><blockquote><p>思考题一：实时程序，要求实时性非常高，并且结果并不一定要求非常准确，那么应该怎么办？ 直接使用处理时间。 思考题二：如果要进行时间旅行，也就是要还原以前的数据集当时的流的状态，应该怎么办？ 使用事件时间。使用Hive将数据集先按照时间戳升序排列，再将最大延迟时间设置为0。</p></blockquote><h2 id="Process-Function-Low-Level-API"><a href="#Process-Function-Low-Level-API" class="headerlink" title="Process Function(Low-Level API)"></a>Process Function(Low-Level API)</h2><p>我们之前学习的转换算子是无法访问事件的时间戳信息和水位线信息的。而这在一些应用场景下，极为重要。例如MapFunction这样的map转换算子就无法访问时间戳或者当前事件的事件时间。</p><p>基于此，DataStream API提供了一系列的Low-Level转换算子。可以访问时间戳、水位线以及注册定时事件。还可以输出特定的一些事件，例如超时事件等。Process Function用来构建事件驱动的应用以及实现自定义的业务逻辑(使用之前的window函数和转换算子无法实现)。例如，Flink-SQL就是使用Process Function实现的。</p><p>Flink提供了8个Process Function：</p><ul><li>ProcessFunction</li><li>KeyedProcessFunction</li><li>CoProcessFunction</li><li>ProcessJoinFunction</li><li>BroadcastProcessFunction</li><li>KeyedBroadcastProcessFunction</li><li>ProcessWindowFunction</li><li>ProcessAllWindowFunction</li></ul><p>我们这里详细介绍一下KeyedProcessFunction。</p><p>KeyedProcessFunction用来操作KeyedStream。KeyedProcessFunction会处理流的每一个元素，输出为0个、1个或者多个元素。所有的Process Function都继承自RichFunction接口，所以都有open()、close()和getRuntimeContext()等方法。而KeyedProcessFunction[KEY, IN, OUT]还额外提供了两个方法:</p><ul><li>processElement(v: IN, ctx: Context, out: Collector[OUT]), 流中的每一个元素都会调用这个方法，调用结果将会放在Collector数据类型中输出。Context可以访问元素的时间戳，元素的key，以及TimerService时间服务。Context还可以将结果输出到别的流(side outputs)。</li><li>onTimer(timestamp: Long, ctx: OnTimerContext, out: Collector[OUT])是一个回调函数。当之前注册的定时器触发时调用。参数timestamp为定时器所设定的触发的时间戳。Collector为输出结果的集合。OnTimerContext和processElement的Context参数一样，提供了上下文的一些信息，例如firing trigger的时间信息(事件时间或者处理时间)。</li></ul><h3 id="TimerService-and-Timers"><a href="#TimerService-and-Timers" class="headerlink" title="TimerService and Timers"></a>TimerService and Timers</h3><p>Context和OnTimerContext所持有的TimerService对象拥有以下方法:</p><ul><li><code>currentProcessingTime(): Long</code> 返回当前处理时间</li><li><code>currentWatermark(): Long</code> 返回当前水位线的时间戳</li><li><code>registerProcessingTimeTimer(timestamp: Long): Unit</code> 会注册当前key的processing time的timer。当processing time到达定时时间时，触发timer。</li><li><code>registerEventTimeTimer(timestamp: Long): Unit</code> 会注册当前key的event time timer。当水位线大于等于定时器注册的时间时，触发定时器执行回调函数。</li><li><code>deleteProcessingTimeTimer(timestamp: Long): Unit</code> 删除之前注册处理时间定时器。如果没有这个时间戳的定时器，则不执行。</li><li><code>deleteEventTimeTimer(timestamp: Long): Unit</code> 删除之前注册的事件时间定时器，如果没有此时间戳的定时器，则不执行。</li></ul><p>当定时器timer触发时，执行回调函数onTimer()。processElement()方法和onTimer()方法是同步（不是异步）方法，这样可以避免并发访问和操作状态。</p><blockquote><p>定时器timer只能在KeyedStream上面使用。</p></blockquote><p>针对每一个key和timestamp，只能注册一个定期器。也就是说，每一个key可以注册多个定时器，但在每一个时间戳只能注册一个定时器。KeyedProcessFunction默认将所有定时器的时间戳放在一个优先队列中。在Flink做检查点操作时，定时器也会被保存到状态后端中。</p><p>举个例子说明KeyedProcessFunction如何操作KeyedStream。</p><p>下面的程序展示了如何监控温度传感器的温度值，如果温度值在一秒钟之内(processing time)连续上升，报警。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> warnings = readings</span><br><span class="line">  <span class="comment">// key by sensor id</span></span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  <span class="comment">// apply ProcessFunction to monitor temperatures</span></span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">TempIncreaseAlertFunction</span>)</span><br></pre></td></tr></table></figure></div><p>看一下TempIncreaseAlertFunction如何实现, 程序中使用了ValueState这样一个状态变量, 后面会详细讲解。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TempIncreaseAlertFunction</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 保存上一个传感器温度值</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> lastTemp: <span class="type">ValueState</span>[<span class="type">Double</span>] = getRuntimeContext.getState(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Double</span>](<span class="string">"lastTemp"</span>, <span class="type">Types</span>.of[<span class="type">Double</span>])</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 保存注册的定时器的时间戳</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> currentTimer: <span class="type">ValueState</span>[<span class="type">Long</span>] = getRuntimeContext.getState(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Long</span>](<span class="string">"timer"</span>, <span class="type">Types</span>.of[<span class="type">Long</span>])</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(r: <span class="type">SensorReading</span>,</span><br><span class="line">                              ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>,</span><br><span class="line">                                <span class="type">SensorReading</span>, <span class="type">String</span>]#<span class="type">Context</span>,</span><br><span class="line">                              out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// get previous temperature</span></span><br><span class="line">    <span class="comment">// 取出上一次的温度</span></span><br><span class="line">    <span class="keyword">val</span> prevTemp = lastTemp.value()</span><br><span class="line">    <span class="comment">// update last temperature</span></span><br><span class="line">    <span class="comment">// 将当前温度更新到上一次的温度这个变量中</span></span><br><span class="line">    lastTemp.update(r.temperature)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> curTimerTimestamp = currentTimer.value()</span><br><span class="line">    <span class="keyword">if</span> (prevTemp == <span class="number">0.0</span> || r.temperature &lt; prevTemp) &#123;</span><br><span class="line">      <span class="comment">// temperature decreased; delete current timer</span></span><br><span class="line">      <span class="comment">// 温度下降或者是第一个温度值，删除定时器</span></span><br><span class="line">      ctx.timerService().deleteProcessingTimeTimer(curTimerTimestamp)</span><br><span class="line">      <span class="comment">// 清空状态变量</span></span><br><span class="line">      currentTimer.clear()</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (r.temperature &gt; prevTemp &amp;&amp; curTimerTimestamp == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// temperature increased and we have not set a timer yet</span></span><br><span class="line">      <span class="comment">// set processing time timer for now + 1 second</span></span><br><span class="line">      <span class="comment">// 温度上升且我们并没有设置定时器</span></span><br><span class="line">      <span class="keyword">val</span> timerTs = ctx.timerService().currentProcessingTime() + <span class="number">1000</span></span><br><span class="line">      ctx.timerService().registerProcessingTimeTimer(timerTs)</span><br><span class="line">      <span class="comment">// remember current timer</span></span><br><span class="line">      currentTimer.update(timerTs)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(ts: <span class="type">Long</span>,</span><br><span class="line">                       ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>,</span><br><span class="line">                        <span class="type">SensorReading</span>, <span class="type">String</span>]#<span class="type">OnTimerContext</span>,</span><br><span class="line">                       out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    out.collect(<span class="string">"传感器id为: "</span></span><br><span class="line">      + ctx.getCurrentKey</span><br><span class="line">      + <span class="string">"的传感器温度值已经连续1s上升了。"</span>)</span><br><span class="line">    currentTimer.clear()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="将事件发送到侧输出-Emitting-to-Side-Outputs"><a href="#将事件发送到侧输出-Emitting-to-Side-Outputs" class="headerlink" title="将事件发送到侧输出(Emitting to Side Outputs)"></a>将事件发送到侧输出(Emitting to Side Outputs)</h3><p>大部分的DataStream API的算子的输出是单一输出，也就是某种数据类型的流。除了split算子，可以将一条流分成多条流，这些流的数据类型也都相同。process function的side outputs功能可以产生多条流，并且这些流的数据类型可以不一样。一个side output可以定义为OutputTag[X]对象，X是输出流的数据类型。process function可以通过Context对象发射一个事件到一个或者多个side outputs。</p><p>例子</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> monitoredReadings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = readings</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">FreezingMonitor</span>)</span><br><span class="line"></span><br><span class="line">monitoredReadings</span><br><span class="line">  .getSideOutput(<span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">"freezing-alarms"</span>))</span><br><span class="line">  .print()</span><br><span class="line"></span><br><span class="line">readings.print()</span><br></pre></td></tr></table></figure></div><p>接下来我们实现FreezingMonitor函数，用来监控传感器温度值，将温度值低于32F的温度输出到side output。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FreezingMonitor</span> <span class="keyword">extends</span> <span class="title">ProcessFunction</span>[<span class="type">SensorReading</span>, <span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// define a side output tag</span></span><br><span class="line">  <span class="comment">// 定义一个侧输出标签</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> freezingAlarmOutput: <span class="type">OutputTag</span>[<span class="type">String</span>] =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">"freezing-alarms"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(r: <span class="type">SensorReading</span>,</span><br><span class="line">                              ctx: <span class="type">ProcessFunction</span>[<span class="type">SensorReading</span>,</span><br><span class="line">                                <span class="type">SensorReading</span>]#<span class="type">Context</span>,</span><br><span class="line">                              out: <span class="type">Collector</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// emit freezing alarm if temperature is below 32F</span></span><br><span class="line">    <span class="keyword">if</span> (r.temperature &lt; <span class="number">32.0</span>) &#123;</span><br><span class="line">      ctx.output(freezingAlarmOutput, <span class="string">s"Freezing Alarm for <span class="subst">$&#123;r.id&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// forward all readings to the regular output</span></span><br><span class="line">    out.collect(r)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="CoProcessFunction"><a href="#CoProcessFunction" class="headerlink" title="CoProcessFunction"></a>CoProcessFunction</h3><p>对于两条输入流，DataStream API提供了CoProcessFunction这样的low-level操作。CoProcessFunction提供了操作每一个输入流的方法: processElement1()和processElement2()。类似于ProcessFunction，这两种方法都通过Context对象来调用。这个Context对象可以访问事件数据，定时器时间戳，TimerService，以及side outputs。CoProcessFunction也提供了onTimer()回调函数。下面的例子展示了如何使用CoProcessFunction来合并两条流。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ingest sensor stream</span></span><br><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// filter switches enable forwarding of readings</span></span><br><span class="line"><span class="keyword">val</span> filterSwitches: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = env</span><br><span class="line">  .fromCollection(<span class="type">Seq</span>(</span><br><span class="line">    (<span class="string">"sensor_2"</span>, <span class="number">10</span> * <span class="number">1000</span>L),</span><br><span class="line">    (<span class="string">"sensor_7"</span>, <span class="number">60</span> * <span class="number">1000</span>L)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> forwardedReadings = readings</span><br><span class="line">  <span class="comment">// connect readings and switches</span></span><br><span class="line">  .connect(filterSwitches)</span><br><span class="line">  <span class="comment">// key by sensor ids</span></span><br><span class="line">  .keyBy(_.id, _._1)</span><br><span class="line">  <span class="comment">// apply filtering CoProcessFunction</span></span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">ReadingFilter</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReadingFilter</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">CoProcessFunction</span>[<span class="type">SensorReading</span>,</span></span><br><span class="line"><span class="class">    (<span class="type">String</span>, <span class="type">Long</span>), <span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// switch to enable forwarding</span></span><br><span class="line">  <span class="comment">// 传送数据的开关</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> forwardingEnabled: <span class="type">ValueState</span>[<span class="type">Boolean</span>] = getRuntimeContext</span><br><span class="line">    .getState(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Boolean</span>](<span class="string">"filterSwitch"</span>, <span class="type">Types</span>.of[<span class="type">Boolean</span>])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">  <span class="comment">// hold timestamp of currently active disable timer</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> disableTimer: <span class="type">ValueState</span>[<span class="type">Long</span>] = getRuntimeContext</span><br><span class="line">    .getState(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Long</span>](<span class="string">"timer"</span>, <span class="type">Types</span>.of[<span class="type">Long</span>])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement1</span></span>(reading: <span class="type">SensorReading</span>,</span><br><span class="line">                               ctx: <span class="type">CoProcessFunction</span>[<span class="type">SensorReading</span>,</span><br><span class="line">                                (<span class="type">String</span>, <span class="type">Long</span>), <span class="type">SensorReading</span>]#<span class="type">Context</span>,</span><br><span class="line">                               out: <span class="type">Collector</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// check if we may forward the reading</span></span><br><span class="line">    <span class="comment">// 决定我们是否要将数据继续传下去</span></span><br><span class="line">    <span class="keyword">if</span> (forwardingEnabled.value()) &#123;</span><br><span class="line">      out.collect(reading)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement2</span></span>(switch: (<span class="type">String</span>, <span class="type">Long</span>),</span><br><span class="line">                               ctx: <span class="type">CoProcessFunction</span>[<span class="type">SensorReading</span>,</span><br><span class="line">                                (<span class="type">String</span>, <span class="type">Long</span>), <span class="type">SensorReading</span>]#<span class="type">Context</span>,</span><br><span class="line">                               out: <span class="type">Collector</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// enable reading forwarding</span></span><br><span class="line">    <span class="comment">// 允许继续传输数据</span></span><br><span class="line">    forwardingEnabled.update(<span class="literal">true</span>)</span><br><span class="line">    <span class="comment">// set disable forward timer</span></span><br><span class="line">    <span class="keyword">val</span> timerTimestamp = ctx.timerService().currentProcessingTime()</span><br><span class="line">     + switch._2</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">val</span> curTimerTimestamp = disableTimer.value()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (timerTimestamp &gt; curTimerTimestamp) &#123;</span><br><span class="line">      <span class="comment">// remove current timer and register new timer</span></span><br><span class="line">      ctx.timerService().deleteProcessingTimeTimer(curTimerTimestamp)</span><br><span class="line">      ctx.timerService().registerProcessingTimeTimer(timerTimestamp)</span><br><span class="line">      disableTimer.update(timerTimestamp)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(ts: <span class="type">Long</span>,</span><br><span class="line">                       ctx: <span class="type">CoProcessFunction</span>[<span class="type">SensorReading</span>,</span><br><span class="line">                        (<span class="type">String</span>, <span class="type">Long</span>), <span class="type">SensorReading</span>]#<span class="type">OnTimerContext</span>,</span><br><span class="line">                       out: <span class="type">Collector</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="comment">// remove all state; forward switch will be false by default</span></span><br><span class="line">     forwardingEnabled.clear()</span><br><span class="line">     disableTimer.clear()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="窗口操作符-Window-Operators"><a href="#窗口操作符-Window-Operators" class="headerlink" title="窗口操作符(Window Operators)"></a>窗口操作符(Window Operators)</h2><p>窗口操作是流处理程序中很常见的操作。窗口操作允许我们在无限流上的一段有界区间上面做聚合之类的操作。而我们使用基于时间的逻辑来定义区间。窗口操作符提供了一种将数据放进一个桶，并根据桶中的数据做计算的方法。例如，我们可以将事件放进5分钟的滚动窗口中，然后计数。</p><blockquote><p>无限流转化成有限数据的方法：使用窗口。</p></blockquote><h3 id="定义窗口操作符"><a href="#定义窗口操作符" class="headerlink" title="定义窗口操作符"></a>定义窗口操作符</h3><p>Window算子可以在keyed stream或者nokeyed stream上面使用。</p><p>创建一个Window算子，需要指定两个部分：</p><ol><li><code>window assigner</code>定义了流的元素如何分配到window中。window assigner将会产生一条WindowedStream(或者AllWindowedStream，如果是nonkeyed DataStream的话)</li><li>window function用来处理WindowedStream(AllWindowedStream)中的元素。</li></ol><p>下面的代码说明了如何使用窗口操作符。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">  .keyBy(...)</span><br><span class="line">  .window(...)  <span class="comment">// 指定window assigner</span></span><br><span class="line">  .reduce/aggregate/process(...) <span class="comment">// 指定window function</span></span><br><span class="line"></span><br><span class="line">stream</span><br><span class="line">  .windowAll(...) <span class="comment">// 指定window assigner</span></span><br><span class="line">  .reduce/aggregate/process(...) <span class="comment">// 指定window function</span></span><br></pre></td></tr></table></figure></div><blockquote><p>我们的学习重点是Keyed WindowedStream。</p></blockquote><h3 id="内置的窗口分配器-built-in-window-assigner"><a href="#内置的窗口分配器-built-in-window-assigner" class="headerlink" title="内置的窗口分配器(built-in window assigner)"></a>内置的窗口分配器(built-in window assigner)</h3><p>窗口分配器将会根据事件的事件时间或者处理时间来将事件分配到对应的窗口中去。窗口包含开始时间和结束时间这两个时间戳。</p><p>所有的窗口分配器都包含一个默认的触发器：</p><ul><li>对于事件时间：当水位线超过窗口结束时间，触发窗口的求值操作。</li><li>对于处理时间：当机器时间超过窗口结束时间，触发窗口的求值操作。</li></ul><blockquote><p>需要注意的是：当处于某个窗口的第一个事件到达的时候，这个窗口才会被创建。Flink不会对空窗口求值。</p></blockquote><p>Flink创建的窗口类型是<code>TimeWindow</code>，包含开始时间和结束时间，区间是左闭右开的，也就是说包含开始时间戳，不包含结束时间戳。</p><p><em>滚动窗口(tumbling windows)</em></p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0601.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0601.png" class="lazyload"></a></p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorData: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> avgTemp = sensorData</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  <span class="comment">// group readings in 1s event-time windows</span></span><br><span class="line">  .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">1</span>)))</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">TemperatureAverager</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> avgTemp = sensorData</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  <span class="comment">// group readings in 1s processing-time windows</span></span><br><span class="line">  .window(<span class="type">TumblingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">1</span>)))</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">TemperatureAverager</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其实就是之前的</span></span><br><span class="line"><span class="comment">// shortcut for window.(TumblingEventTimeWindows.of(size))</span></span><br><span class="line"><span class="keyword">val</span> avgTemp = sensorData</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  .timeWindow(<span class="type">Time</span>.seconds(<span class="number">1</span>))</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">TemperatureAverager</span>)</span><br></pre></td></tr></table></figure></div><p>默认情况下，滚动窗口会和<code>1970-01-01-00:00:00.000</code>对齐，例如一个1小时的滚动窗口将会定义以下开始时间的窗口：00:00:00，01:00:00，02:00:00，等等。</p><p><em>滑动窗口(sliding window)</em></p><p>对于滑动窗口，我们需要指定窗口的大小和滑动的步长。当滑动步长小于窗口大小时，窗口将会出现重叠，而元素会被分配到不止一个窗口中去。当滑动步长大于窗口大小时，一些元素可能不会被分配到任何窗口中去，会被直接丢弃。</p><p>下面的代码定义了窗口大小为1小时，滑动步长为15分钟的窗口。每一个元素将被分配到4个窗口中去。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0602.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0602.png" class="lazyload"></a></p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> slidingAvgTemp = sensorData</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  .window(</span><br><span class="line">    <span class="type">SlidingEventTimeWindows</span>.of(<span class="type">Time</span>.hours(<span class="number">1</span>), <span class="type">Time</span>.minutes(<span class="number">15</span>))</span><br><span class="line">  )</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">TemperatureAverager</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> slidingAvgTemp = sensorData</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  .window(</span><br><span class="line">    <span class="type">SlidingProcessingTimeWindows</span>.of(<span class="type">Time</span>.hours(<span class="number">1</span>), <span class="type">Time</span>.minutes(<span class="number">15</span>))</span><br><span class="line">  )</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">TemperatureAverager</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> slidingAvgTemp = sensorData</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  .timeWindow(<span class="type">Time</span>.hours(<span class="number">1</span>), <span class="type">Time</span>.minutes(<span class="number">15</span>))</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">TemperatureAverager</span>)</span><br></pre></td></tr></table></figure></div><p><em>会话窗口(session windows)</em></p><p>会话窗口不可能重叠，并且会话窗口的大小也不是固定的。不活跃的时间长度定义了会话窗口的界限。不活跃的时间是指这段时间没有元素到达。下图展示了元素如何被分配到会话窗口。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0603.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0603.png" class="lazyload"></a></p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sessionWindows = sensorData</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  .window(<span class="type">EventTimeSessionWindows</span>.withGap(<span class="type">Time</span>.minutes(<span class="number">15</span>)))</span><br><span class="line">  .process(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sessionWindows = sensorData</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  .window(<span class="type">ProcessingTimeSessionWindows</span>.withGap(<span class="type">Time</span>.minutes(<span class="number">15</span>)))</span><br><span class="line">  .process(...)</span><br></pre></td></tr></table></figure></div><p>由于会话窗口的开始时间和结束时间取决于接收到的元素，所以窗口分配器无法立即将所有的元素分配到正确的窗口中去。相反，会话窗口分配器最开始时先将每一个元素分配到它自己独有的窗口中去，窗口开始时间是这个元素的时间戳，窗口大小是session gap的大小。接下来，会话窗口分配器会将出现重叠的窗口合并成一个窗口。</p><h3 id="调用窗口计算函数"><a href="#调用窗口计算函数" class="headerlink" title="调用窗口计算函数"></a>调用窗口计算函数</h3><p>window functions定义了窗口中数据的计算逻辑。有两种计算逻辑：</p><ol><li>增量聚合函数(Incremental aggregation functions)：当一个事件被添加到窗口时，触发函数计算，并且更新window的状态(单个值)。最终聚合的结果将作为输出。ReduceFunction和AggregateFunction是增量聚合函数。</li><li>全窗口函数(Full window functions)：这个函数将会收集窗口中所有的元素，可以做一些复杂计算。ProcessWindowFunction是window function。</li></ol><p><em>ReduceFunction</em></p><p>例子: 计算每个传感器15s窗口中的温度最小值</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> minTempPerWindow: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = sensorData</span><br><span class="line">  .map(r =&gt; (r.id, r.temperature))</span><br><span class="line">  .keyBy(_._1)</span><br><span class="line">  .timeWindow(<span class="type">Time</span>.seconds(<span class="number">15</span>))</span><br><span class="line">  .reduce((r1, r2) =&gt; (r1._1, r1._2.min(r2._2)))</span><br></pre></td></tr></table></figure></div><p><em>AggregateFunction</em></p><p>先来看接口定义</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public interface <span class="type">AggregateFunction</span>&lt;<span class="type">IN</span>, <span class="type">ACC</span>, <span class="type">OUT</span>&gt;</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Function</span>, <span class="type">Serializable</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// create a new accumulator to start a new aggregate</span></span><br><span class="line">  <span class="type">ACC</span> createAccumulator();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// add an input element to the accumulator and return the accumulator</span></span><br><span class="line">  <span class="type">ACC</span> add(<span class="type">IN</span> value, <span class="type">ACC</span> accumulator);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// compute the result from the accumulator and return it.</span></span><br><span class="line">  <span class="type">OUT</span> getResult(<span class="type">ACC</span> accumulator);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// merge two accumulators and return the result.</span></span><br><span class="line">  <span class="type">ACC</span> merge(<span class="type">ACC</span> a, <span class="type">ACC</span> b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>IN是输入元素的类型，ACC是累加器的类型，OUT是输出元素的类型。</p><p>例子</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> avgTempPerWindow: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = sensorData</span><br><span class="line">  .map(r =&gt; (r.id, r.temperature))</span><br><span class="line">  .keyBy(_._1)</span><br><span class="line">  .timeWindow(<span class="type">Time</span>.seconds(<span class="number">15</span>))</span><br><span class="line">  .aggregate(<span class="keyword">new</span> <span class="type">AvgTempFunction</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// An AggregateFunction to compute the average temperature per sensor.</span></span><br><span class="line"><span class="comment">// The accumulator holds the sum of temperatures and an event count.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AvgTempFunction</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[(<span class="type">String</span>, <span class="type">Double</span>),</span></span><br><span class="line"><span class="class">    (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Int</span>), (<span class="type">String</span>, <span class="type">Double</span>)] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>() = &#123;</span><br><span class="line">    (<span class="string">""</span>, <span class="number">0.0</span>, <span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(in: (<span class="type">String</span>, <span class="type">Double</span>), acc: (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Int</span>)) = &#123;</span><br><span class="line">    (in._1, in._2 + acc._2, <span class="number">1</span> + acc._3)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResult</span></span>(acc: (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Int</span>)) = &#123;</span><br><span class="line">    (acc._1, acc._2 / acc._3)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(acc1: (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Int</span>),</span><br><span class="line">    acc2: (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Int</span>)) = &#123;</span><br><span class="line">    (acc1._1, acc1._2 + acc2._2, acc1._3 + acc2._3)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p><em>ProcessWindowFunction</em></p><p>一些业务场景，我们需要收集窗口内所有的数据进行计算，例如计算窗口数据的中位数，或者计算窗口数据中出现频率最高的值。这样的需求，使用ReduceFunction和AggregateFunction就无法实现了。这个时候就需要ProcessWindowFunction了。</p><p>先来看接口定义</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessWindowFunction&lt;IN</span>, <span class="title">OUT</span>, <span class="title">KEY</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window&gt;</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">AbstractRichFunction</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Evaluates the window</span></span><br><span class="line">  void process(<span class="type">KEY</span> key, <span class="type">Context</span> ctx, <span class="type">Iterable</span>&lt;<span class="type">IN</span>&gt; vals, <span class="type">Collector</span>&lt;<span class="type">OUT</span>&gt; out)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">Exception</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Deletes any custom per-window state when the window is purged</span></span><br><span class="line">  public void clear(<span class="type">Context</span> ctx) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The context holding window metadata</span></span><br><span class="line">  public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> <span class="title">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Returns the metadata of the window</span></span><br><span class="line">    public <span class="keyword">abstract</span> <span class="type">W</span> window();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Returns the current processing time</span></span><br><span class="line">    public <span class="keyword">abstract</span> long currentProcessingTime();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Returns the current event-time watermark</span></span><br><span class="line">    public <span class="keyword">abstract</span> long currentWatermark();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// State accessor for per-window state</span></span><br><span class="line">    public <span class="keyword">abstract</span> <span class="type">KeyedStateStore</span> windowState();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// State accessor for per-key global state</span></span><br><span class="line">    public <span class="keyword">abstract</span> <span class="type">KeyedStateStore</span> globalState();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Emits a record to the side output identified by the OutputTag.</span></span><br><span class="line">    public <span class="keyword">abstract</span> &lt;<span class="type">X</span>&gt; void output(<span class="type">OutputTag</span>&lt;<span class="type">X</span>&gt; outputTag, <span class="type">X</span> value);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>process()方法接受的参数为：window的key，Iterable迭代器包含窗口的所有元素，Collector用于输出结果流。Context参数和别的process方法一样。而ProcessWindowFunction的Context对象还可以访问window的元数据(窗口开始和结束时间)，当前处理时间和水位线，per-window state和per-key global state，side outputs。</p><ul><li>per-window state: 用于保存一些信息，这些信息可以被process()访问，只要process所处理的元素属于这个窗口。</li><li>per-key global state: 同一个key，也就是在一条KeyedStream上，不同的window可以访问per-key global state保存的值。</li></ul><p>例子：计算5s滚动窗口中的最低和最高的温度。输出的元素包含了(流的Key, 最低温度, 最高温度, 窗口结束时间)。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> minMaxTempPerWindow: <span class="type">DataStream</span>[<span class="type">MinMaxTemp</span>] = sensorData</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  .timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">HighAndLowTempProcessFunction</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MinMaxTemp</span>(<span class="params">id: <span class="type">String</span>, min: <span class="type">Double</span>, max: <span class="type">Double</span>, endTs: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">HighAndLowTempProcessFunction</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[<span class="type">SensorReading</span>,</span></span><br><span class="line"><span class="class">    <span class="type">MinMaxTemp</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(key: <span class="type">String</span>,</span><br><span class="line">                       ctx: <span class="type">Context</span>,</span><br><span class="line">                       vals: <span class="type">Iterable</span>[<span class="type">SensorReading</span>],</span><br><span class="line">                       out: <span class="type">Collector</span>[<span class="type">MinMaxTemp</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> temps = vals.map(_.temperature)</span><br><span class="line">    <span class="keyword">val</span> windowEnd = ctx.window.getEnd</span><br><span class="line"></span><br><span class="line">    out.collect(<span class="type">MinMaxTemp</span>(key, temps.min, temps.max, windowEnd))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>我们还可以将ReduceFunction/AggregateFunction和ProcessWindowFunction结合起来使用。ReduceFunction/AggregateFunction做增量聚合，ProcessWindowFunction提供更多的对数据流的访问权限。如果只使用ProcessWindowFunction(底层的实现为将事件都保存在ListState中)，将会非常占用空间。分配到某个窗口的元素将被提前聚合，而当窗口的trigger触发时，也就是窗口收集完数据关闭时，将会把聚合结果发送到ProcessWindowFunction中，这时Iterable参数将会只有一个值，就是前面聚合的值。</p><p>例子</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">input</span><br><span class="line">  .keyBy(...)</span><br><span class="line">  .timeWindow(...)</span><br><span class="line">  .reduce(</span><br><span class="line">    incrAggregator: <span class="type">ReduceFunction</span>[<span class="type">IN</span>],</span><br><span class="line">    function: <span class="type">ProcessWindowFunction</span>[<span class="type">IN</span>, <span class="type">OUT</span>, <span class="type">K</span>, <span class="type">W</span>])</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(...)</span><br><span class="line">  .timeWindow(...)</span><br><span class="line">  .aggregate(</span><br><span class="line">    incrAggregator: <span class="type">AggregateFunction</span>[<span class="type">IN</span>, <span class="type">ACC</span>, <span class="type">V</span>],</span><br><span class="line">    windowFunction: <span class="type">ProcessWindowFunction</span>[<span class="type">V</span>, <span class="type">OUT</span>, <span class="type">K</span>, <span class="type">W</span>])</span><br></pre></td></tr></table></figure></div><p>我们把之前的需求重新使用以上两种方法实现一下。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MinMaxTemp</span>(<span class="params">id: <span class="type">String</span>, min: <span class="type">Double</span>, max: <span class="type">Double</span>, endTs: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">minMaxTempPerWindow2</span></span>: <span class="type">DataStream</span>[<span class="type">MinMaxTemp</span>] = sensorData</span><br><span class="line">  .map(r =&gt; (r.id, r.temperature, r.temperature))</span><br><span class="line">  .keyBy(_._1)</span><br><span class="line">  .timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">  .reduce(</span><br><span class="line">    (r1: (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>), r2: (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)) =&gt; &#123;</span><br><span class="line">      (r1._1, r1._2.min(r2._2), r1._3.max(r2._3))</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">AssignWindowEndProcessFunction</span>()</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AssignWindowEndProcessFunction</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>),</span></span><br><span class="line"><span class="class">    <span class="type">MinMaxTemp</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(key: <span class="type">String</span>,</span><br><span class="line">                       ctx: <span class="type">Context</span>,</span><br><span class="line">                       minMaxIt: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)],</span><br><span class="line">                       out: <span class="type">Collector</span>[<span class="type">MinMaxTemp</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> minMax = minMaxIt.head</span><br><span class="line">    <span class="keyword">val</span> windowEnd = ctx.window.getEnd</span><br><span class="line">    out.collect(<span class="type">MinMaxTemp</span>(key, minMax._2, minMax._3, windowEnd))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="自定义窗口操作符-windows-operators"><a href="#自定义窗口操作符-windows-operators" class="headerlink" title="自定义窗口操作符(windows operators)"></a>自定义窗口操作符(windows operators)</h3><p>Flink内置的window operators分配器已经已经足够应付大多数应用场景。尽管如此，如果我们需要实现一些复杂的窗口逻辑，例如：可以发射早到的事件或者碰到迟到的事件就更新窗口的结果，或者窗口的开始和结束决定于特定事件的接收。</p><p>DataStream API暴露了接口和方法来自定义窗口操作符。</p><ul><li>自定义窗口分配器</li><li>自定义窗口计算触发器(trigger)</li><li>自定义窗口数据清理功能(evictor)</li></ul><p>当一个事件来到窗口操作符，首先将会传给WindowAssigner来处理。WindowAssigner决定了事件将被分配到哪些窗口。如果窗口不存在，WindowAssigner将会创建一个新的窗口。</p><p>如果一个window operator接受了一个增量聚合函数作为参数，例如ReduceFunction或者AggregateFunction，新到的元素将会立即被聚合，而聚合结果result将存储在window中。如果window operator没有使用增量聚合函数，那么新元素将被添加到ListState中，ListState中保存了所有分配给窗口的元素。</p><p>新元素被添加到窗口时，这个新元素同时也被传给了window的trigger。trigger定义了window何时准备好求值，何时window被清空。trigger可以基于window被分配的元素和注册的定时器来对窗口的所有元素求值或者在特定事件清空window中所有的元素。</p><p>当window operator只接收一个增量聚合函数作为参数时：</p><p>当window operator只接收一个全窗口函数作为参数时：</p><p>当window operator接收一个增量聚合函数和一个全窗口函数作为参数时：</p><p>evictor是一个可选的组件，可以被注入到ProcessWindowFunction之前或者之后调用。evictor可以清除掉window中收集的元素。由于evictor需要迭代所有的元素，所以evictor只能使用在没有增量聚合函数作为参数的情况下。</p><p>下面的代码说明了如果使用自定义的trigger和evictor定义一个window operator：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">  .keyBy(...)</span><br><span class="line">  .window(...)</span><br><span class="line"> [.trigger(...)]</span><br><span class="line"> [.evictor(...)]</span><br><span class="line">  .reduce/aggregate/process(...)</span><br></pre></td></tr></table></figure></div><p>注意：每个WindowAssigner都有一个默认的trigger。</p><p><em>窗口生命周期</em></p><p>当WindowAssigner分配某个窗口的第一个元素时，这个窗口才会被创建。所以不存在没有元素的窗口。</p><p>一个窗口包含了如下状态：</p><ul><li>Window content<ul><li>分配到这个窗口的元素</li><li>增量聚合的结果(如果window operator接收了ReduceFunction或者AggregateFunction作为参数)。</li></ul></li><li>Window object<ul><li>WindowAssigner返回0个，1个或者多个window object。</li><li>window operator根据返回的window object来聚合元素。</li><li>每一个window object包含一个windowEnd时间戳，来区别于其他窗口。</li></ul></li><li>触发器的定时器：一个触发器可以注册定时事件，到了定时的时间可以执行相应的回调函数，例如：对窗口进行求值或者清空窗口。</li><li>触发器中的自定义状态：触发器可以定义和使用自定义的、per-window或者per-key状态。这个状态完全被触发器所控制。而不是被window operator控制。</li></ul><p>当窗口结束时间来到，window operator将删掉这个窗口。窗口结束时间是由window object的end timestamp所定义的。无论是使用processing time还是event time，窗口结束时间是什么类型可以调用WindowAssigner.isEventTime()方法获得。</p><p><em>窗口分配器(window assigners)</em></p><p>WindowAssigner将会把元素分配到0个，1个或者多个窗口中去。我们看一下WindowAssigner接口：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowAssigner&lt;T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window&gt;</span></span></span><br><span class="line"><span class="class">    <span class="title">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  public <span class="keyword">abstract</span> <span class="type">Collection</span>&lt;<span class="type">W</span>&gt; assignWindows(</span><br><span class="line">    <span class="type">T</span> element,</span><br><span class="line">    long timestamp,</span><br><span class="line">    <span class="type">WindowAssignerContext</span> context);</span><br><span class="line"></span><br><span class="line">  public <span class="keyword">abstract</span> <span class="type">Trigger</span>&lt;<span class="type">T</span>, <span class="type">W</span>&gt; getDefaultTriger(</span><br><span class="line">    <span class="type">StreamExecutionEnvironment</span> env);</span><br><span class="line"></span><br><span class="line">  public <span class="keyword">abstract</span> <span class="type">TypeSerializer</span>&lt;<span class="type">W</span>&gt; getWindowSerializer(</span><br><span class="line">    <span class="type">ExecutionConfig</span> executionConfig);</span><br><span class="line"></span><br><span class="line">  public <span class="keyword">abstract</span> boolean isEventTime();</span><br><span class="line"></span><br><span class="line">  public <span class="keyword">abstract</span> static <span class="class"><span class="keyword">class</span> <span class="title">WindowAssignerContext</span> </span>&#123;</span><br><span class="line">    public <span class="keyword">abstract</span> long getCurrentProcessingTime();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>WindowAssigner有两个泛型参数：</p><ul><li>T: 事件的数据类型</li><li>W: 窗口的类型</li></ul><p>下面的代码创建了一个自定义窗口分配器，是一个30秒的滚动事件时间窗口。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ThirtySecondsWindows</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">WindowAssigner</span>[<span class="type">Object</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> windowSize: <span class="type">Long</span> = <span class="number">30</span> * <span class="number">1000</span>L</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">assignWindows</span></span>(</span><br><span class="line">    o: <span class="type">Object</span>,</span><br><span class="line">    ts: <span class="type">Long</span>,</span><br><span class="line">    ctx: <span class="type">WindowAssigner</span>.<span class="type">WindowAssignerContext</span></span><br><span class="line">  ): java.util.<span class="type">List</span>[<span class="type">TimeWindow</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> startTime = ts - (ts % windowSize)</span><br><span class="line">    <span class="keyword">val</span> endTime = startTime + windowSize</span><br><span class="line">    <span class="type">Collections</span>.singletonList(<span class="keyword">new</span> <span class="type">TimeWindow</span>(startTime, endTime))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getDefaultTrigger</span></span>(</span><br><span class="line">    env: environment.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line">  ): <span class="type">Trigger</span>[<span class="type">Object</span>, <span class="type">TimeWindow</span>] = &#123;</span><br><span class="line">      <span class="type">EventTimeTrigger</span>.create()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getWindowSerializer</span></span>(</span><br><span class="line">    executionConfig: <span class="type">ExecutionConfig</span></span><br><span class="line">  ): <span class="type">TypeSerializer</span>[<span class="type">TimeWindow</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">TimeWindow</span>.<span class="type">Serializer</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isEventTime</span> </span>= <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>增量聚合示意图</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0604.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0604.png" class="lazyload"></a></p><p>全窗口聚合示意图</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0605.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0605.png" class="lazyload"></a></p><p>增量聚合和全窗口聚合结合使用的示意图</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0606.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0606.png" class="lazyload"></a></p><p><em>触发器(Triggers)</em></p><p>触发器定义了window何时会被求值以及何时发送求值结果。触发器可以到了特定的时间触发也可以碰到特定的事件触发。例如：观察到事件数量符合一定条件或者观察到了特定的事件。</p><p>默认的触发器将会在两种情况下触发</p><ul><li>处理时间：机器时间到达处理时间</li><li>事件时间：水位线超过了窗口的结束时间</li></ul><p>触发器可以访问流的时间属性以及定时器，还可以对state状态编程。所以触发器和process function一样强大。例如我们可以实现一个触发逻辑：当窗口接收到一定数量的元素时，触发器触发。再比如当窗口接收到一个特定元素时，触发器触发。还有就是当窗口接收到的元素里面包含特定模式(5秒钟内接收到了两个同样类型的事件)，触发器也可以触发。在一个事件时间的窗口中，一个自定义的触发器可以提前(在水位线没过窗口结束时间之前)计算和发射计算结果。这是一个常见的低延迟计算策略，尽管计算不完全，但不像默认的那样需要等待水位线没过窗口结束时间。</p><p>每次调用触发器都会产生一个TriggerResult来决定窗口接下来发生什么。TriggerResult可以取以下结果：</p><ul><li>CONTINUE：什么都不做</li><li>FIRE：如果window operator有ProcessWindowFunction这个参数，将会调用这个ProcessWindowFunction。如果窗口仅有增量聚合函数(ReduceFunction或者AggregateFunction)作为参数，那么当前的聚合结果将会被发送。窗口的state不变。</li><li>PURGE：窗口所有内容包括窗口的元数据都将被丢弃。</li><li>FIRE_AND_PURGE：先对窗口进行求值，再将窗口中的内容丢弃。</li></ul><p>TriggerResult可能的取值使得我们可以实现很复杂的窗口逻辑。一个自定义触发器可以触发多次，可以计算或者更新结果，可以在发送结果之前清空窗口。</p><p>接下来我们看一下Trigger API：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Trigger&lt;T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window&gt;</span></span></span><br><span class="line"><span class="class">    <span class="title">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="type">TriggerResult</span> onElement(</span><br><span class="line">    long timestamp,</span><br><span class="line">    <span class="type">W</span> window,</span><br><span class="line">    <span class="type">TriggerContext</span> ctx);</span><br><span class="line"></span><br><span class="line">  public <span class="keyword">abstract</span> <span class="type">TriggerResult</span> onProcessingTime(</span><br><span class="line">    long timestamp,</span><br><span class="line">    <span class="type">W</span> window,</span><br><span class="line">    <span class="type">TriggerContext</span> ctx);</span><br><span class="line"></span><br><span class="line">  public <span class="keyword">abstract</span> <span class="type">TriggerResult</span> onEventTime(</span><br><span class="line">    long timestamp,</span><br><span class="line">    <span class="type">W</span> window,</span><br><span class="line">    <span class="type">TriggerContext</span> ctx);</span><br><span class="line">  </span><br><span class="line">  public boolean canMerge();</span><br><span class="line"></span><br><span class="line">  public void onMerge(<span class="type">W</span> window, <span class="type">OnMergeContext</span> ctx);</span><br><span class="line"></span><br><span class="line">  public <span class="keyword">abstract</span> void clear(<span class="type">W</span> window, <span class="type">TriggerContext</span> ctx);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public interface <span class="type">TriggerContext</span> &#123;</span><br><span class="line"></span><br><span class="line">  long getCurrentProcessingTime();</span><br><span class="line"></span><br><span class="line">  long getCurrentWatermark();</span><br><span class="line"></span><br><span class="line">  void registerProcessingTimeTimer(long time);</span><br><span class="line"></span><br><span class="line">  void registerEventTimeTimer(long time);</span><br><span class="line"></span><br><span class="line">  void deleteProcessingTimeTimer(long time);</span><br><span class="line"></span><br><span class="line">  void deleteEventTimeTimer(long time);</span><br><span class="line"></span><br><span class="line">  &lt;<span class="type">S</span> <span class="keyword">extends</span> <span class="type">State</span>&gt; <span class="type">S</span> getPartitionedState(</span><br><span class="line">    <span class="type">StateDescriptor</span>&lt;<span class="type">S</span>, ?&gt; stateDescriptor);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public interface <span class="type">OnMergeContext</span> <span class="keyword">extends</span> <span class="type">TriggerContext</span> &#123;</span><br><span class="line"></span><br><span class="line">  void mergePartitionedState(</span><br><span class="line">    <span class="type">StateDescriptor</span>&lt;<span class="type">S</span>, ?&gt; stateDescriptor</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>这里要注意两个地方：清空state和merging合并触发器。</p><p>当在触发器中使用per-window state时，这里我们需要保证当窗口被删除时state也要被删除，否则随着时间的推移，window operator将会积累越来越多的数据，最终可能使应用崩溃。</p><p>当窗口被删除时，为了清空所有状态，触发器的clear()方法需要需要删掉所有的自定义per-window state，以及使用TriggerContext对象将处理时间和事件时间的定时器都删除。</p><p>下面的例子展示了一个触发器在窗口结束时间之前触发。当第一个事件被分配到窗口时，这个触发器注册了一个定时器，定时时间为水位线之前一秒钟。当定时事件执行，将会注册一个新的定时事件，这样，这个触发器每秒钟最多触发一次。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneSecondIntervalTrigger</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Trigger</span>[<span class="type">SensorReading</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onElement</span></span>(</span><br><span class="line">    r: <span class="type">SensorReading</span>,</span><br><span class="line">    timestamp: <span class="type">Long</span>,</span><br><span class="line">    window: <span class="type">TimeWindow</span>,</span><br><span class="line">    ctx: <span class="type">Trigger</span>.<span class="type">TriggerContext</span></span><br><span class="line">  ): <span class="type">TriggerResult</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> firstSeen: <span class="type">ValueState</span>[<span class="type">Boolean</span>] = ctx</span><br><span class="line">      .getPartitionedState(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Boolean</span>](</span><br><span class="line">          <span class="string">"firstSeen"</span>, classOf[<span class="type">Boolean</span>]</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!firstSeen.value()) &#123;</span><br><span class="line">      <span class="keyword">val</span> t = ctx.getCurrentWatermark</span><br><span class="line">       + (<span class="number">1000</span> - (ctx.getCurrentWatermark % <span class="number">1000</span>))</span><br><span class="line">      ctx.registerEventTimeTimer(t)</span><br><span class="line">      ctx.registerEventTimeTimer(window.getEnd)</span><br><span class="line">      firstSeen.update(<span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">TriggerResult</span>.<span class="type">CONTINUE</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onEventTime</span></span>(</span><br><span class="line">    timestamp: <span class="type">Long</span>,</span><br><span class="line">    window: <span class="type">TimeWindow</span>,</span><br><span class="line">    ctx: <span class="type">Trigger</span>.<span class="type">TriggerContext</span></span><br><span class="line">  ): <span class="type">TriggerResult</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (timestamp == window.getEnd) &#123;</span><br><span class="line">      <span class="type">TriggerResult</span>.<span class="type">FIRE_AND_PURGE</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> t = ctx.getCurrentWatermark</span><br><span class="line">       + (<span class="number">1000</span> - (ctx.getCurrentWatermark % <span class="number">1000</span>))</span><br><span class="line">      <span class="keyword">if</span> (t &lt; window.getEnd) &#123;</span><br><span class="line">        ctx.registerEventTimeTimer(t)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="type">TriggerResult</span>.<span class="type">FIRE</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onProcessingTime</span></span>(</span><br><span class="line">    timestamp: <span class="type">Long</span>,</span><br><span class="line">    window: <span class="type">TimeWindow</span>,</span><br><span class="line">    ctx: <span class="type">Trigger</span>.<span class="type">TriggerContext</span></span><br><span class="line">  ): <span class="type">TriggerResult</span> = &#123;</span><br><span class="line">    <span class="type">TriggerResult</span>.<span class="type">CONTINUE</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">clear</span></span>(</span><br><span class="line">    window: <span class="type">TimeWindow</span>,</span><br><span class="line">    ctx: <span class="type">Trigger</span>.<span class="type">TriggerContext</span></span><br><span class="line">  ): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> firstSeen: <span class="type">ValueState</span>[<span class="type">Boolean</span>] = ctx</span><br><span class="line">      .getPartitionedState(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Boolean</span>](</span><br><span class="line">          <span class="string">"firstSeen"</span>, classOf[<span class="type">Boolean</span>]</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    firstSeen.clear()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p><em>清理器(EVICTORS)</em></p><p>evictor可以在window function求值之前或者之后移除窗口中的元素。</p><p>我们看一下Evictor的接口定义：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public interface <span class="type">Evictor</span>&lt;<span class="type">T</span>, <span class="type">W</span> <span class="keyword">extends</span> <span class="type">Window</span>&gt;</span><br><span class="line">    <span class="keyword">extends</span> <span class="type">Serializable</span> &#123;</span><br><span class="line">  void evictBefore(</span><br><span class="line">    <span class="type">Iterable</span>&lt;<span class="type">TimestampedValue</span>&lt;<span class="type">T</span>&gt;&gt; elements,</span><br><span class="line">    int size,</span><br><span class="line">    <span class="type">W</span> window,</span><br><span class="line">    <span class="type">EvictorContext</span> evictorContext);</span><br><span class="line"></span><br><span class="line">  void evictAfter(</span><br><span class="line">    <span class="type">Iterable</span>&lt;<span class="type">TimestampedValue</span>&lt;<span class="type">T</span>&gt;&gt; elements,</span><br><span class="line">    int size,</span><br><span class="line">    <span class="type">W</span> window,</span><br><span class="line">    <span class="type">EvictorContext</span> evictorContext);</span><br><span class="line"></span><br><span class="line">  interface <span class="type">EvictorContext</span> &#123;</span><br><span class="line"></span><br><span class="line">    long getCurrentProcessingTime();</span><br><span class="line"></span><br><span class="line">    long getCurrentWatermark();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>evictBefore()和evictAfter()分别在window function计算之前或者之后调用。Iterable迭代器包含了窗口所有的元素，size为窗口中元素的数量，window object和EvictorContext可以访问当前处理时间和水位线。可以对Iterator调用remove()方法来移除窗口中的元素。</p><p>evictor也经常被用在GlobalWindow上，用来清除部分元素，而不是将窗口中的元素全部清空。</p><h2 id="基于时间的双流Join"><a href="#基于时间的双流Join" class="headerlink" title="基于时间的双流Join"></a>基于时间的双流Join</h2><p>数据流操作的另一个常见需求是对两条数据流中的事件进行联结（connect）或Join。Flink DataStream API中内置有两个可以根据时间条件对数据流进行Join的算子：基于间隔的Join和基于窗口的Join。本节我们会对它们进行介绍。</p><p>如果Flink内置的Join算子无法表达所需的Join语义，那么你可以通过CoProcessFunction、BroadcastProcessFunction或KeyedBroadcastProcessFunction实现自定义的Join逻辑。</p><blockquote><p>注意，你要设计的Join算子需要具备高效的状态访问模式及有效的状态清理策略。</p></blockquote><h3 id="基于间隔的Join"><a href="#基于间隔的Join" class="headerlink" title="基于间隔的Join"></a>基于间隔的Join</h3><p>基于间隔的Join会对两条流中拥有相同键值以及彼此之间时间戳不超过某一指定间隔的事件进行Join。</p><p>下图展示了两条流（A和B）上基于间隔的Join，如果B中事件的时间戳相较于A中事件的时间戳不早于1小时且不晚于15分钟，则会将两个事件Join起来。Join间隔具有对称性，因此上面的条件也可以表示为A中事件的时间戳相较B中事件的时间戳不早于15分钟且不晚于1小时。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0607.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0607.png" class="lazyload"></a></p><p>基于间隔的Join目前只支持事件时间以及INNER JOIN语义（无法发出未匹配成功的事件）。下面的例子定义了一个基于间隔的Join。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input1</span><br><span class="line">  .keyBy(...)</span><br><span class="line">  .between(&lt;lower-bound&gt;, &lt;upper-bound&gt;) <span class="comment">// 相对于input1的上下界</span></span><br><span class="line">  .process(<span class="type">ProcessJoinFunction</span>) <span class="comment">// 处理匹配的事件对</span></span><br></pre></td></tr></table></figure></div><p>Join成功的事件对会发送给ProcessJoinFunction。下界和上界分别由负时间间隔和正时间间隔来定义，例如between(Time.hour(-1), Time.minute(15))。在满足下界值小于上界值的前提下，你可以任意对它们赋值。例如，允许出现B中事件的时间戳相较A中事件的时间戳早1～2小时这样的条件。</p><p>基于间隔的Join需要同时对双流的记录进行缓冲。对第一个输入而言，所有时间戳大于当前水位线减去间隔上界的数据都会被缓冲起来；对第二个输入而言，所有时间戳大于当前水位线加上间隔下界的数据都会被缓冲起来。注意，两侧边界值都有可能为负。上图中的Join需要存储数据流A中所有时间戳大于当前水位线减去15分钟的记录，以及数据流B中所有时间戳大于当前水位线减去1小时的记录。不难想象，如果两条流的事件时间不同步，那么Join所需的存储就会显著增加，因为水位线总是由“较慢”的那条流来决定。</p><p>例子：每个用户的点击Join这个用户最近10分钟内的浏览</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.<span class="type">ProcessJoinFunction</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions</span><br><span class="line">.timestamps.<span class="type">BoundedOutOfOrdernessTimestampExtractor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"><span class="keyword">import</span> org.joda.time.<span class="type">DateTime</span></span><br><span class="line"><span class="keyword">import</span> org.joda.time.format.<span class="type">DateTimeFormat</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 需求：每个用户的点击Join这个用户最近10分钟内的浏览</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据流clickStream</span></span><br><span class="line"><span class="comment">// 某个用户在某个时刻点击了某个页面</span></span><br><span class="line"><span class="comment">// &#123;"userID": "user_2", "eventTime": "2019-11-16 17:30:02", "eventType": "click", "pageID": "page_1"&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据流browseStream</span></span><br><span class="line"><span class="comment">// 某个用户在某个时刻浏览了某个商品，以及商品的价值</span></span><br><span class="line"><span class="comment">// &#123;"userID": "user_2", "eventTime": "2019-11-16 17:30:01", "eventType": "browse", "productID": "product_1", "productPrice": 10&#125;</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">IntervalJoinExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserClickLog</span>(<span class="params">userID: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                          eventTime: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                          eventType: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                          pageID: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">case</span> <span class="title">class</span> <span class="title">UserBrowseLog</span>(<span class="params">userID: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                           eventTime: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                           eventType: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                           productID: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                           productPrice: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> clickStream = env</span><br><span class="line">      .fromElements(</span><br><span class="line">        <span class="type">UserClickLog</span>(<span class="string">"user_2"</span>, <span class="string">"2019-11-16 17:30:00"</span>, <span class="string">"click"</span>, <span class="string">"page_1"</span>)</span><br><span class="line">      )</span><br><span class="line">      .assignTimestampsAndWatermarks(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">UserClickLog</span>](<span class="type">Time</span>.seconds(<span class="number">0</span>)) &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(t: <span class="type">UserClickLog</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> dateTimeFormatter = <span class="type">DateTimeFormat</span>.forPattern(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">            <span class="keyword">val</span> dateTime = <span class="type">DateTime</span>.parse(t.eventTime, dateTimeFormatter)</span><br><span class="line">            dateTime.getMillis</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> browseStream = env</span><br><span class="line">      .fromElements(</span><br><span class="line">        <span class="type">UserBrowseLog</span>(<span class="string">"user_2"</span>, <span class="string">"2019-11-16 17:19:00"</span>, <span class="string">"browse"</span>, <span class="string">"product_1"</span>, <span class="string">"10"</span>),</span><br><span class="line">        <span class="type">UserBrowseLog</span>(<span class="string">"user_2"</span>, <span class="string">"2019-11-16 17:20:00"</span>, <span class="string">"browse"</span>, <span class="string">"product_1"</span>, <span class="string">"10"</span>),</span><br><span class="line">        <span class="type">UserBrowseLog</span>(<span class="string">"user_2"</span>, <span class="string">"2019-11-16 17:22:00"</span>, <span class="string">"browse"</span>, <span class="string">"product_1"</span>, <span class="string">"10"</span>),</span><br><span class="line">        <span class="type">UserBrowseLog</span>(<span class="string">"user_2"</span>, <span class="string">"2019-11-16 17:26:00"</span>, <span class="string">"browse"</span>, <span class="string">"product_1"</span>, <span class="string">"10"</span>),</span><br><span class="line">        <span class="type">UserBrowseLog</span>(<span class="string">"user_2"</span>, <span class="string">"2019-11-16 17:30:00"</span>, <span class="string">"browse"</span>, <span class="string">"product_1"</span>, <span class="string">"10"</span>),</span><br><span class="line">        <span class="type">UserBrowseLog</span>(<span class="string">"user_2"</span>, <span class="string">"2019-11-16 17:31:00"</span>, <span class="string">"browse"</span>, <span class="string">"product_1"</span>, <span class="string">"10"</span>)</span><br><span class="line">      )</span><br><span class="line">      .assignTimestampsAndWatermarks(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">UserBrowseLog</span>](<span class="type">Time</span>.seconds(<span class="number">0</span>)) &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(t: <span class="type">UserBrowseLog</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> dateTimeFormatter = <span class="type">DateTimeFormat</span>.forPattern(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">            <span class="keyword">val</span> dateTime = <span class="type">DateTime</span>.parse(t.eventTime, dateTimeFormatter)</span><br><span class="line">            dateTime.getMillis</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">    clickStream</span><br><span class="line">      .keyBy(<span class="string">"userID"</span>)</span><br><span class="line">      .intervalJoin(browseStream.keyBy(<span class="string">"userID"</span>))</span><br><span class="line">      .between(<span class="type">Time</span>.minutes(<span class="number">-10</span>),<span class="type">Time</span>.seconds(<span class="number">0</span>))</span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">MyIntervalJoin</span>)</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyIntervalJoin</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessJoinFunction</span>[<span class="type">UserClickLog</span>, <span class="type">UserBrowseLog</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(</span><br><span class="line">      left: <span class="type">UserClickLog</span>,</span><br><span class="line">      right: <span class="type">UserBrowseLog</span>,</span><br><span class="line">      context: <span class="type">ProcessJoinFunction</span>[<span class="type">UserClickLog</span>, <span class="type">UserBrowseLog</span>, <span class="type">String</span>]#<span class="type">Context</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[<span class="type">String</span>]</span><br><span class="line">    ): <span class="type">Unit</span> = &#123;</span><br><span class="line">      out.collect(left +<span class="string">" =Interval Join=&gt; "</span>+right)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="基于窗口的Join"><a href="#基于窗口的Join" class="headerlink" title="基于窗口的Join"></a>基于窗口的Join</h3><p>顾名思义，基于窗口的Join需要用到Flink中的窗口机制。其原理是将两条输入流中的元素分配到公共窗口中并在窗口完成时进行Join（或Cogroup）。</p><p>下面的例子展示了如何定义基于窗口的Join。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">input1.join(input2)</span><br><span class="line">  .where(...)       <span class="comment">// 为input1指定键值属性</span></span><br><span class="line">  .equalTo(...)     <span class="comment">// 为input2指定键值属性</span></span><br><span class="line">  .window(...)      <span class="comment">// 指定WindowAssigner</span></span><br><span class="line">  [.trigger(...)]   <span class="comment">// 选择性的指定Trigger</span></span><br><span class="line">  [.evictor(...)]   <span class="comment">// 选择性的指定Evictor</span></span><br><span class="line">  .apply(...)       <span class="comment">// 指定JoinFunction</span></span><br></pre></td></tr></table></figure></div><p>下图展示了DataStream API中基于窗口的Join是如何工作的。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0608.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0608.png" class="lazyload"></a></p><p>两条输入流都会根据各自的键值属性进行分区，公共窗口分配器会将二者的事件映射到公共窗口内（其中同时存储了两条流中的数据）。当窗口的计时器触发时，算子会遍历两个输入中元素的每个组合（叉乘积）去调用JoinFunction。同时你也可以自定义触发器或移除器。由于两条流中的事件会被映射到同一个窗口中，因此该过程中的触发器和移除器与常规窗口算子中的完全相同。</p><p>除了对窗口中的两条流进行Join，你还可以对它们进行Cogroup，只需将算子定义开始位置的join改为coGroup()即可。Join和Cogroup的总体逻辑相同，二者的唯一区别是：Join会为两侧输入中的每个事件对调用JoinFunction；而Cogroup中用到的CoGroupFunction会以两个输入的元素遍历器为参数，只在每个窗口中被调用一次。</p><blockquote><p>注意，对划分窗口后的数据流进行Join可能会产生意想不到的语义。例如，假设你为执行Join操作的算子配置了1小时的滚动窗口，那么一旦来自两个输入的元素没有被划分到同一窗口，它们就无法Join在一起，即使二者彼此仅相差1秒钟。</p></blockquote><h2 id="处理迟到的元素-Handling-Late-Data"><a href="#处理迟到的元素-Handling-Late-Data" class="headerlink" title="处理迟到的元素(Handling Late Data)"></a>处理迟到的元素(Handling Late Data)</h2><p>水位线可以用来平衡计算的完整性和延迟两方面。除非我们选择一种非常保守的水位线策略(最大延时设置的非常大，以至于包含了所有的元素，但结果是非常大的延迟)，否则我们总需要处理迟到的元素。</p><p>迟到的元素是指当这个元素来到时，这个元素所对应的窗口已经计算完毕了(也就是说水位线已经没过窗口结束时间了)。这说明迟到这个特性只针对事件时间。</p><p>DataStream API提供了三种策略来处理迟到元素</p><ul><li>直接抛弃迟到的元素</li><li>将迟到的元素发送到另一条流中去</li><li>可以更新窗口已经计算完的结果，并发出计算结果。</li></ul><h3 id="抛弃迟到元素"><a href="#抛弃迟到元素" class="headerlink" title="抛弃迟到元素"></a>抛弃迟到元素</h3><p>抛弃迟到的元素是event time window operator的默认行为。也就是说一个迟到的元素不会创建一个新的窗口。</p><p>process function可以通过比较迟到元素的时间戳和当前水位线的大小来很轻易的过滤掉迟到元素。</p><h3 id="重定向迟到元素"><a href="#重定向迟到元素" class="headerlink" title="重定向迟到元素"></a>重定向迟到元素</h3><p>迟到的元素也可以使用侧输出(side output)特性被重定向到另外的一条流中去。迟到元素所组成的侧输出流可以继续处理或者sink到持久化设施中去。</p><p>例子</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> readings = env</span><br><span class="line">  .socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>, '\n')</span><br><span class="line">  .map(line =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> arr = line.split(<span class="string">" "</span>)</span><br><span class="line">    (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong * <span class="number">1000</span>)</span><br><span class="line">  &#125;)</span><br><span class="line">  .assignAscendingTimestamps(_._2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> countPer10Secs = readings</span><br><span class="line">  .keyBy(_._1)</span><br><span class="line">  .timeWindow(<span class="type">Time</span>.seconds(<span class="number">10</span>))</span><br><span class="line">  .sideOutputLateData(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">OutputTag</span>[(<span class="type">String</span>, <span class="type">Long</span>)](<span class="string">"late-readings"</span>)</span><br><span class="line">  )</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">CountFunction</span>())</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lateStream = countPer10Secs</span><br><span class="line">  .getSideOutput(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">OutputTag</span>[(<span class="type">String</span>, <span class="type">Long</span>)](<span class="string">"late-readings"</span>)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">lateStream.print()</span><br></pre></td></tr></table></figure></div><p>实现<code>CountFunction</code>:</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountFunction</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[(<span class="type">String</span>, <span class="type">Long</span>),</span></span><br><span class="line"><span class="class">  <span class="type">String</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(key: <span class="type">String</span>,</span><br><span class="line">                       context: <span class="type">Context</span>,</span><br><span class="line">                       elements: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)],</span><br><span class="line">                       out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    out.collect(<span class="string">"窗口共有"</span> + elements.size + <span class="string">"条数据"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>下面这个例子展示了ProcessFunction如何过滤掉迟到的元素然后将迟到的元素发送到侧输出流中去。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ???</span><br><span class="line"><span class="keyword">val</span> filteredReadings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = readings</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">LateReadingsFilter</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// retrieve late readings</span></span><br><span class="line"><span class="keyword">val</span> lateReadings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = filteredReadings</span><br><span class="line">  .getSideOutput(<span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">SensorReading</span>](<span class="string">"late-readings"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/** A ProcessFunction that filters out late sensor readings and </span></span><br><span class="line"><span class="comment">  * re-directs them to a side output */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LateReadingsFilter</span> </span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessFunction</span>[<span class="type">SensorReading</span>, <span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lateReadingsOut = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">SensorReading</span>](<span class="string">"late-readings"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(</span><br><span class="line">      r: <span class="type">SensorReading</span>,</span><br><span class="line">      ctx: <span class="type">ProcessFunction</span>[<span class="type">SensorReading</span>, <span class="type">SensorReading</span>]#<span class="type">Context</span>,</span><br><span class="line">      out: <span class="type">Collector</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// compare record timestamp with current watermark</span></span><br><span class="line">    <span class="keyword">if</span> (r.timestamp &lt; ctx.timerService().currentWatermark()) &#123;</span><br><span class="line">      <span class="comment">// this is a late reading =&gt; redirect it to the side output</span></span><br><span class="line">      ctx.output(lateReadingsOut, r)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      out.collect(r)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="使用迟到元素更新窗口计算结果-Updating-Results-by-Including-Late-Events"><a href="#使用迟到元素更新窗口计算结果-Updating-Results-by-Including-Late-Events" class="headerlink" title="使用迟到元素更新窗口计算结果(Updating Results by Including Late Events)"></a>使用迟到元素更新窗口计算结果(Updating Results by Including Late Events)</h3><p>由于存在迟到的元素，所以已经计算出的窗口结果是不准确和不完全的。我们可以使用迟到元素更新已经计算完的窗口结果。</p><p>如果我们要求一个operator支持重新计算和更新已经发出的结果，就需要在第一次发出结果以后也要保存之前所有的状态。但显然我们不能一直保存所有的状态，肯定会在某一个时间点将状态清空，而一旦状态被清空，结果就再也不能重新计算或者更新了。而迟到的元素只能被抛弃或者发送到侧输出流。</p><p>window operator API提供了方法来明确声明我们要等待迟到元素。当使用event-time window，我们可以指定一个时间段叫做allowed lateness。window operator如果设置了allowed lateness，这个window operator在水位线没过窗口结束时间时也将不会删除窗口和窗口中的状态。窗口会在一段时间内(allowed lateness设置的)保留所有的元素。</p><p>当迟到元素在allowed lateness时间内到达时，这个迟到元素会被实时处理并发送到触发器(trigger)。当水位线没过了窗口结束时间+allowed lateness时间时，窗口会被删除，并且所有后来的迟到的元素都会被丢弃。</p><p>Allowed lateness可以使用allowedLateness()方法来指定，如下所示：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> countPer10Secs: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>, <span class="type">String</span>)] = readings</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  .timeWindow(<span class="type">Time</span>.seconds(<span class="number">10</span>))</span><br><span class="line">  <span class="comment">// process late readings for 5 additional seconds</span></span><br><span class="line">  .allowedLateness(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">  <span class="comment">// count readings and update results if late readings arrive</span></span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">UpdatingWindowCountFunction</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** A counting WindowProcessFunction that distinguishes between </span></span><br><span class="line"><span class="comment">  * first results and updates. */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpdatingWindowCountFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[<span class="type">SensorReading</span>,</span></span><br><span class="line"><span class="class">      (<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>, <span class="type">String</span>), <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(</span><br><span class="line">      id: <span class="type">String</span>,</span><br><span class="line">      ctx: <span class="type">Context</span>,</span><br><span class="line">      elements: <span class="type">Iterable</span>[<span class="type">SensorReading</span>],</span><br><span class="line">      out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>, <span class="type">String</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// count the number of readings</span></span><br><span class="line">    <span class="keyword">val</span> cnt = elements.count(_ =&gt; <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// state to check if this is</span></span><br><span class="line">    <span class="comment">// the first evaluation of the window or not</span></span><br><span class="line">    <span class="keyword">val</span> isUpdate = ctx.windowState.getState(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Boolean</span>](</span><br><span class="line">        <span class="string">"isUpdate"</span>,</span><br><span class="line">        <span class="type">Types</span>.of[<span class="type">Boolean</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!isUpdate.value()) &#123;</span><br><span class="line">      <span class="comment">// first evaluation, emit first result</span></span><br><span class="line">      out.collect((id, ctx.window.getEnd, cnt, <span class="string">"first"</span>))</span><br><span class="line">      isUpdate.update(<span class="literal">true</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// not the first evaluation, emit an update</span></span><br><span class="line">      out.collect((id, ctx.window.getEnd, cnt, <span class="string">"update"</span>))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第六章，基于时间和窗口的操作符&quot;&gt;&lt;a href=&quot;#第六章，基于时间和窗口的操作符&quot; class=&quot;headerlink&quot; title=&quot;第六章，基于时间和窗口的操作符&quot;&gt;&lt;/a&gt;第六章，基于时间和窗口的操作符&lt;/h1&gt;&lt;p&gt;在本章，我们将要学习DataStre
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flume源码修改之flumeTailDirSource兼容log4j</title>
    <link href="https://masteryang4.github.io/2020/06/30/flume%E6%BA%90%E7%A0%81%E4%BF%AE%E6%94%B9%E4%B9%8BflumeTailDirSource%E5%85%BC%E5%AE%B9log4j/"/>
    <id>https://masteryang4.github.io/2020/06/30/flume%E6%BA%90%E7%A0%81%E4%BF%AE%E6%94%B9%E4%B9%8BflumeTailDirSource%E5%85%BC%E5%AE%B9log4j/</id>
    <published>2020-06-30T13:41:57.000Z</published>
    <updated>2020-06-30T14:49:36.440Z</updated>
    
    <content type="html"><![CDATA[<h1 id="tailDir-Source"><a href="#tailDir-Source" class="headerlink" title="tailDir Source"></a>tailDir Source</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p>1）断点续传</p><p>2）同时监控多目录</p><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>1）说明：使用<strong>正则表达式</strong>监控文件名时，当修改文件名称之后，会重复读取数据。</p><p>2）示例：</p><p>配置信息 test.conf</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; TAILDIR</span><br><span class="line">a1.sources.r1.filegroups &#x3D; f1</span><br><span class="line">a1.sources.r1.filegroups.f1 &#x3D; &#x2F;opt&#x2F;module&#x2F;data&#x2F;flume.*</span><br><span class="line">a1.sources.r1.positionFile &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;taildir&#x2F;taildir_flume.json</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure></div><p>3）启动任务</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ysss@hadoop102 flume]$ bin/flume-ng agent -n a1 -c conf -f conf/test.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></div><p>4）测试</p><p>（1）在/opt/module/data目录下创建flume.log</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ysss@hadoop102 data]$ pwd</span><br><span class="line">/opt/module/data</span><br><span class="line">[ysss@hadoop102 data]$ touch flume.log</span><br></pre></td></tr></table></figure></div><p>（2）向flume.log文件中添加数据</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ysss@hadoop102 data]$ echo hello &gt;&gt; flume.log </span><br><span class="line">[ysss@hadoop102 data]$ echo ysss &gt;&gt; flume.log</span><br></pre></td></tr></table></figure></div><p>（3）查看监控Flume控制台</p><p>（4）修改flume.log为flume.2020-06-09.log</p><p>（5）再次查看监控Flume控制台</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>1）方案一</p><p>跟公司后台人员协商；</p><p>让他们使用类似<code>logback</code>不更名打印日志框架，不要使用<code>log4j</code>会更名的打印日志框架。对于不想协商、项目经理或组长偏向JAVA组的，只能使用方案二了。</p><p>2）方案二</p><p>修改TailDirSource源码:</p><p>1、flume-taildir-source\src\main\java\org\apache\flume\source\taildir\TailFile.java</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">updatePos</span><span class="params">(String path, <span class="keyword">long</span> inode, <span class="keyword">long</span> pos)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">//    if (this.inode == inode &amp;&amp; this.path.equals(path)) &#123;</span></span><br><span class="line">        <span class="comment">//    ysss</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.inode == inode) &#123;</span><br><span class="line">            setPos(pos);</span><br><span class="line">            updateFilePos(pos);</span><br><span class="line">            logger.info(<span class="string">"Updated position, file: "</span> + path + <span class="string">", inode: "</span> + inode + <span class="string">", pos: "</span> + pos);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></div><p>2、\src\main\java\org\apache\flume\source\taildir\ReliableTaildirEventReader.java</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">**</span><br><span class="line">     * Update tailFiles mapping <span class="keyword">if</span> a <span class="keyword">new</span> file is created or appends are detected</span><br><span class="line">     * to the existing file.</span><br><span class="line">     */</span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Long&gt; <span class="title">updateTailFiles</span><span class="params">(<span class="keyword">boolean</span> skipToEnd)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        updateTime = System.currentTimeMillis();</span><br><span class="line">        List&lt;Long&gt; updatedInodes = Lists.newArrayList();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (TaildirMatcher taildir : taildirCache) &#123;</span><br><span class="line">            Map&lt;String, String&gt; headers = headerTable.row(taildir.getFileGroup());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (File f : taildir.getMatchingFiles()) &#123;</span><br><span class="line">                <span class="keyword">long</span> inode;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    inode = getInode(f);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (NoSuchFileException e) &#123;</span><br><span class="line">                    logger.info(<span class="string">"File has been deleted in the meantime: "</span> + e.getMessage());</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                TailFile tf = tailFiles.get(inode);</span><br><span class="line">                <span class="comment">//if (tf == null || !tf.getPath().equals(f.getAbsolutePath())) &#123;</span></span><br><span class="line">                <span class="comment">//ysss</span></span><br><span class="line">                <span class="keyword">if</span> (tf == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">long</span> startPos = skipToEnd ? f.length() : <span class="number">0</span>;</span><br><span class="line">                    tf = openFile(f, headers, inode, startPos);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">boolean</span> updated = tf.getLastUpdated() &lt; f.lastModified() || tf.getPos() != f.length();</span><br><span class="line">                    <span class="keyword">if</span> (updated) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (tf.getRaf() == <span class="keyword">null</span>) &#123;</span><br><span class="line">                            tf = openFile(f, headers, inode, tf.getPos());</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">if</span> (f.length() &lt; tf.getPos()) &#123;</span><br><span class="line">                            logger.info(<span class="string">"Pos "</span> + tf.getPos() + <span class="string">" is larger than file size! "</span></span><br><span class="line">                                    + <span class="string">"Restarting from pos 0, file: "</span> + tf.getPath() + <span class="string">", inode: "</span> + inode);</span><br><span class="line">                            tf.updatePos(tf.getPath(), inode, <span class="number">0</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    tf.setNeedTail(updated);</span><br><span class="line">                &#125;</span><br><span class="line">                tailFiles.put(inode, tf);</span><br><span class="line">                updatedInodes.add(inode);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> updatedInodes;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></div><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><blockquote><p>taildir和logback配合使用，为什么不是log4j？</p><p>logback的日志：ysss.2020-05-18.log，ysss.2020-05-19.log</p><p>Log4j的日志：ysss.log -&gt; ysss.2020-05-18.log，在一天过去之后，改名为后者存盘</p><p>Linux对于文件而言</p><p>​        (1) 全路径</p><p>​        (2) Inode(Linux文件的唯一标识,修改名称不会改动INode值)</p><p>但是tailDirSource的工作机制：文件更名或者INode改变都会被识别为一个新文件！也就是说，如果使用log4j，日志会更名，被tailDirSource识别为一个新文件，重复读取。</p><p>如果非要使用log4j怎么办呢？</p><p>改flume源码！只有INode改变才会是被为一个新文件！</p></blockquote><ul><li><p><code>source\taildir\TailFile.java</code> 的 <code>updatePos</code> 方法</p></li><li><p><code>source\taildir\ReliableTaildirEventReader.java</code> 的 <code>updateTailFiles</code> 方法</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;tailDir-Source&quot;&gt;&lt;a href=&quot;#tailDir-Source&quot; class=&quot;headerlink&quot; title=&quot;tailDir Source&quot;&gt;&lt;/a&gt;tailDir Source&lt;/h1&gt;&lt;h2 id=&quot;优点&quot;&gt;&lt;a href=&quot;#优点&quot;
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flume" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flume/"/>
    
    
      <category term="源码" scheme="https://masteryang4.github.io/tags/%E6%BA%90%E7%A0%81/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flume" scheme="https://masteryang4.github.io/tags/flume/"/>
    
  </entry>
  
  <entry>
    <title>flume拦截器之flumeHDFS_Sink时间问题</title>
    <link href="https://masteryang4.github.io/2020/06/30/flume%E6%8B%A6%E6%88%AA%E5%99%A8%E4%B9%8BflumeHDFS-Sink%E6%97%B6%E9%97%B4%E9%97%AE%E9%A2%98/"/>
    <id>https://masteryang4.github.io/2020/06/30/flume%E6%8B%A6%E6%88%AA%E5%99%A8%E4%B9%8BflumeHDFS-Sink%E6%97%B6%E9%97%B4%E9%97%AE%E9%A2%98/</id>
    <published>2020-06-30T13:38:37.000Z</published>
    <updated>2020-06-30T13:43:23.484Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HDFS-Sink存在的问题"><a href="#HDFS-Sink存在的问题" class="headerlink" title="HDFS Sink存在的问题"></a>HDFS Sink存在的问题</h1><p><code>hdfs.useLocalTimeStamp</code>设置为<code>true</code>，也会在Event头信息中添加”timestamp”的key</p><p>我们一般设置为false，因为我们目前使用的是KafkaSource，会根据<strong>当前系统时间</strong>添加该头信息。</p><p><a href="https://pic.downk.cc/item/5efb3b1414195aa5949f15ae.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5efb3b1414195aa5949f15ae.png" class="lazyload"></a></p><blockquote><p>说明：HDFS Sink要想根据时间滚动文件夹，必须在Event头信息中添加”timestamp”的key用于提供给HDFS Sink使用。</p><p>我们目前使用的是KafkaSource，会根据当前系统时间添加该头信息。</p></blockquote><p><strong>问题：我们使用的是按照每天的具体时间来创建新的目录，假如我们Flume任务在夜间11点多挂了，零点以后任务才被重新启动，那么昨天的挂掉之后的数据就会被算作第二天的数据了。</strong></p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p><strong>我们需要根据事件内部时间来控制HDFS目录时间的创建，</strong></p><p>思路为<strong>自定义拦截器</strong>来修改KafkaSource自动添加的时间戳。</p><blockquote><p>使用事件内部时间【替换】KafkaSource自动添加的时间戳</p></blockquote><p>1）创建Maven工程flume-interceptor</p><p>2）创建包名：com.ysss.flume.interceptor</p><p>3）在pom.xml文件中添加如下配置</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.62<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>4）在com.ysss.flume.interceptor包下创建TimeStampInterceptor类</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.ysss.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeStampInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;Event&gt; events = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line">        String log = <span class="keyword">new</span> String(event.getBody(), StandardCharsets.UTF_8);</span><br><span class="line"></span><br><span class="line">        JSONObject jsonObject = JSONObject.parseObject(log);</span><br><span class="line"></span><br><span class="line">        String ts = jsonObject.getString(<span class="string">"ts"</span>);</span><br><span class="line">        headers.put(<span class="string">"timestamp"</span>, ts);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; list)</span> </span>&#123;</span><br><span class="line">        events.clear();</span><br><span class="line">        <span class="keyword">for</span> (Event event : list) &#123;</span><br><span class="line">            events.add(intercept(event));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> events;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span> </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> TimeStampInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>5）打包</p><p>flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar</p><p>6）需要先将打好的包放入到hadoop102的/opt/module/flume/lib文件夹下面。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ysss@hadoop102 lib]$ ls | grep interceptor</span><br><span class="line">flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure></div><p>7）分发Flume到hadoop103、hadoop104</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ysss@hadoop102 module]$ xsync flume/</span><br></pre></td></tr></table></figure></div><h1 id="调整消费Flume配置文件"><a href="#调整消费Flume配置文件" class="headerlink" title="调整消费Flume配置文件"></a>调整消费Flume配置文件</h1><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ysss@hadoop104 conf]$ pwd</span><br><span class="line">/opt/module/flume/conf</span><br></pre></td></tr></table></figure></div><p>修改配置文件kafka-flume-hdfs.conf</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#组件</span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1</span><br><span class="line"></span><br><span class="line">#source</span><br><span class="line">a1.sources.r1.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">a1.sources.r1.batchSize &#x3D; 5000</span><br><span class="line">a1.sources.r1.batchDurationMillis &#x3D; 2000</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers &#x3D; hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line">a1.sources.r1.kafka.topics&#x3D;topic_log</span><br><span class="line">a1.sources.r1.interceptors &#x3D; i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type &#x3D; com.ysss.interceptor.TimeStampInterceptor$Builder  #【注意】</span><br><span class="line"></span><br><span class="line">#channel</span><br><span class="line">a1.channels.c1.type &#x3D; file</span><br><span class="line">a1.channels.c1.checkpointDir &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;checkpoint&#x2F;behavior1</span><br><span class="line">a1.channels.c1.dataDirs &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;data&#x2F;behavior1&#x2F;</span><br><span class="line">a1.channels.c1.maxFileSize &#x3D; 2146435071</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000000</span><br><span class="line">a1.channels.c1.keep-alive &#x3D; 6</span><br><span class="line"></span><br><span class="line">#sink</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; &#x2F;origin_data&#x2F;gmall&#x2F;log&#x2F;topic_log&#x2F;%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; log-</span><br><span class="line">a1.sinks.k1.hdfs.round &#x3D; false</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 10</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 0</span><br><span class="line"></span><br><span class="line">#控制输出文件是原生文件。</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; CompressedStream</span><br><span class="line">a1.sinks.k1.hdfs.codeC &#x3D; lzop</span><br><span class="line"></span><br><span class="line">#拼装</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel&#x3D; c1</span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;HDFS-Sink存在的问题&quot;&gt;&lt;a href=&quot;#HDFS-Sink存在的问题&quot; class=&quot;headerlink&quot; title=&quot;HDFS Sink存在的问题&quot;&gt;&lt;/a&gt;HDFS Sink存在的问题&lt;/h1&gt;&lt;p&gt;&lt;code&gt;hdfs.useLocalTim
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flume" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flume/"/>
    
    
      <category term="源码" scheme="https://masteryang4.github.io/tags/%E6%BA%90%E7%A0%81/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flume" scheme="https://masteryang4.github.io/tags/flume/"/>
    
  </entry>
  
  <entry>
    <title>OLAP和OLTP的区别</title>
    <link href="https://masteryang4.github.io/2020/06/29/OLAP%E5%92%8COLTP%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://masteryang4.github.io/2020/06/29/OLAP%E5%92%8COLTP%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2020-06-28T17:12:12.000Z</published>
    <updated>2020-06-28T17:14:10.668Z</updated>
    
    <content type="html"><![CDATA[<h1 id="OLAP和OLTP的区别"><a href="#OLAP和OLTP的区别" class="headerlink" title="OLAP和OLTP的区别"></a>OLAP和OLTP的区别</h1><p>OLAP（On-Line Analytical Processing）联机分析处理，也称为面向交易的处理过程，其基本特征是前台接收的用户数据可以立即传送到计算中心进行处理，并在很短的时间内给出处理结果，是对用户操作快速响应的方式之一。应用在数据仓库，使用对象是决策者。OLAP系统强调的是数据分析，响应速度要求没那么高。</p><p>OLTP（On-Line Transaction Processing）联机事务处理，它使分析人员能够迅速、一致、交互地从各个方面观察信息，以达到深入理解数据的目的。它具有FASMI(Fast Analysis of Shared Multidimensional Information)，即共享多维信息的快速分析的特征。主要应用是传统关系型数据库。OLTP系统强调的是内存效率，实时性比较高。</p><p> 以下是OLAP和OLTP的比较图1： </p><p><a href="https://pic.downk.cc/item/5ef8ce0514195aa594e85521.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5ef8ce0514195aa594e85521.png" class="lazyload"></a></p><p>图2：</p><p><a href="https://pic.downk.cc/item/5ef8ce1d14195aa594e86d3b.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5ef8ce1d14195aa594e86d3b.png" class="lazyload"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;OLAP和OLTP的区别&quot;&gt;&lt;a href=&quot;#OLAP和OLTP的区别&quot; class=&quot;headerlink&quot; title=&quot;OLAP和OLTP的区别&quot;&gt;&lt;/a&gt;OLAP和OLTP的区别&lt;/h1&gt;&lt;p&gt;OLAP（On-Line Analytical Proces
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="数据库" scheme="https://masteryang4.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>flink系列05Flink DataStream API</title>
    <link href="https://masteryang4.github.io/2020/06/27/flink%E7%B3%BB%E5%88%9705Flink-DataStream-API/"/>
    <id>https://masteryang4.github.io/2020/06/27/flink%E7%B3%BB%E5%88%9705Flink-DataStream-API/</id>
    <published>2020-06-27T15:28:18.000Z</published>
    <updated>2020-06-27T15:29:00.972Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第五章，Flink-DataStream-API"><a href="#第五章，Flink-DataStream-API" class="headerlink" title="第五章，Flink DataStream API"></a>第五章，Flink DataStream API</h1><p>本章介绍了Flink DataStream API的基本知识。我们展示了典型的Flink流处理程序的结构和组成部分，还讨论了Flink的类型系统以及支持的数据类型，还展示了数据和分区转换操作。窗口操作符，基于时间语义的转换操作，有状态的操作符，以及和外部系统的连接器将在接下来的章节进行介绍。阅读完这一章后，我们将会知道如何去实现一个具有基本功能的流处理程序。我们的示例程序采用Scala语言，因为Scala语言相对比较简洁。但Java API也是十分类似的（特殊情况，我们将会指出）。在我们的Github仓库里，我们所写的应用程序具有Scala和Java两种版本。</p><h2 id="你好，Flink！"><a href="#你好，Flink！" class="headerlink" title="你好，Flink！"></a>你好，Flink！</h2><p>让我们写一个简单的例子来获得使用DataStream API编写流处理应用程序的粗浅印象。我们将使用这个简单的示例来展示一个Flink程序的基本结构，以及介绍一些DataStream API的重要特性。我们的示例程序摄取了一条（来自多个传感器的）温度测量数据流。</p><p>首先让我们看一下表示传感器读数的数据结构：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SensorReading</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  id: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  timestamp: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  temperature: <span class="type">Double</span></span>)</span></span><br></pre></td></tr></table></figure></div><p>示例程序5-1将温度从华氏温度读数转换成摄氏温度读数，然后针对每一个传感器，每5秒钟计算一次平均温度纸。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Scala object that defines</span></span><br><span class="line"><span class="comment">// the DataStream program in the main() method.</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AverageSensorReadings</span> </span>&#123;</span><br><span class="line">  <span class="comment">// main() defines and executes the DataStream program</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">// set up the streaming execution environment</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// use event time for the application</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">// create a DataStream[SensorReading] from a stream source</span></span><br><span class="line">    <span class="keyword">val</span> sensorData: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = env</span><br><span class="line">      <span class="comment">// ingest sensor readings with a SensorSource SourceFunction</span></span><br><span class="line">      .addSource(<span class="keyword">new</span> <span class="type">SensorSource</span>)</span><br><span class="line">      <span class="comment">// assign timestamps and watermarks (required for event time)</span></span><br><span class="line">    <span class="keyword">val</span> avgTemp: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = sensorData</span><br><span class="line">      <span class="comment">// convert Fahrenheit to Celsius with an inline lambda function</span></span><br><span class="line">      .map( r =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> celsius = (r.temperature - <span class="number">32</span>) * (<span class="number">5.0</span> / <span class="number">9.0</span>)</span><br><span class="line">        <span class="type">SensorReading</span>(r.id, r.timestamp, celsius)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">// organize readings by sensor id</span></span><br><span class="line">      .keyBy(_.id)</span><br><span class="line">      <span class="comment">// group readings in 5 second tumbling windows</span></span><br><span class="line">      .timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">      <span class="comment">// compute average temperature using a user-defined function</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">TemperatureAverager</span>)</span><br><span class="line">      <span class="comment">// print result stream to standard out</span></span><br><span class="line">      avgTemp.print()</span><br><span class="line">    <span class="comment">// execute application</span></span><br><span class="line">    env.execute(<span class="string">"Compute average sensor temperature"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>你可能已经注意到Flink程序的定义和提交执行使用的就是正常的Scala或者Java的方法。大多数情况下，这些代码都写在一个静态main方法中。在我们的例子中，我们定义了AverageSensorReadings对象，然后将大多数的应用程序逻辑放在了main()中。</p><p>Flink流处理程序的结构如下：</p><ol><li>创建Flink程序执行环境。</li><li>从数据源读取一条或者多条流数据</li><li>使用流转换算子实现业务逻辑</li><li>将计算结果输出到一个或者多个外部设备（可选）</li><li>执行程序</li></ol><p>接下来我们详细的学习一下这些部分。</p><h2 id="搭建执行环境"><a href="#搭建执行环境" class="headerlink" title="搭建执行环境"></a>搭建执行环境</h2><p>编写Flink程序的第一件事情就是搭建执行环境。执行环境决定了程序是运行在单机上还是集群上。在DataStream API中，程序的执行环境是由StreamExecutionEnvironment设置的。在我们的例子中，我们通过调用静态getExecutionEnvironment()方法来获取执行环境。这个方法根据调用方法的上下文，返回一个本地的或者远程的环境。如果这个方法是一个客户端提交到远程集群的代码调用的，那么这个方法将会返回一个远程的执行环境。否则，将返回本地执行环境。</p><p>也可以用下面的方法来显式的创建本地或者远程执行环境：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create a local stream execution environment</span></span><br><span class="line"><span class="keyword">val</span> localEnv = <span class="type">StreamExecutionEnvironment</span></span><br><span class="line">  .createLocalEnvironment()</span><br><span class="line"><span class="comment">// create a remote stream execution environment</span></span><br><span class="line"><span class="keyword">val</span> remoteEnv = <span class="type">StreamExecutionEnvironment</span></span><br><span class="line">  .createRemoteEnvironment(</span><br><span class="line">    <span class="string">"host"</span>, <span class="comment">// hostname of JobManager</span></span><br><span class="line">    <span class="number">1234</span>, <span class="comment">// port of JobManager process</span></span><br><span class="line">    <span class="string">"path/to/jarFile.jar"</span></span><br><span class="line">  ) <span class="comment">// JAR file to ship to the JobManager</span></span><br></pre></td></tr></table></figure></div><p>接下来，我们使用<code>env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</code>来将我们程序的时间语义设置为事件时间。执行环境提供了很多配置选项，例如：设置程序的并行度和程序是否开启容错机制。</p><h2 id="读取输入流"><a href="#读取输入流" class="headerlink" title="读取输入流"></a>读取输入流</h2><p>一旦执行环境设置好，就该写业务逻辑了。<code>StreamExecutionEnvironment</code>提供了创建数据源的方法，这些方法可以从数据流中将数据摄取到程序中。数据流可以来自消息队列或者文件系统，也可能是实时产生的（例如socket）。</p><p>在我们的例子里面，我们这样写：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorData: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = env</span><br><span class="line">  .addSource(<span class="keyword">new</span> <span class="type">SensorSource</span>)</span><br></pre></td></tr></table></figure></div><p>这样就可以连接到传感器测量数据的数据源并创建一个类型为<code>SensorReading</code>的<code>DataStream</code>了。Flink支持很多数据类型，我们将在接下来的章节里面讲解。在我们的例子里面，我们的数据类型是一个定义好的Scala样例类。<code>SensorReading</code>样例类包含了传感器ID，数据的测量时间戳，以及测量温度值。<code>assignTimestampsAndWatermarks(new SensorTimeAssigner)</code>方法指定了如何设置事件时间语义的时间戳和水位线。有关<code>SensorTimeAssigner</code>我们后面再讲。</p><h2 id="转换算子的使用"><a href="#转换算子的使用" class="headerlink" title="转换算子的使用"></a>转换算子的使用</h2><p>一旦我们有一条DataStream，我们就可以在这条数据流上面使用转换算子了。转换算子有很多种。一些转换算子可以产生一条新的DataStream，当然这个DataStream的类型可能是新类型。还有一些转换算子不会改变原有DataStream的数据，但会将数据流分区或者分组。业务逻辑就是由转换算子串起来组合而成的。</p><p>在我们的例子中，我们首先使用<code>map()</code>转换算子将传感器的温度值转换成了摄氏温度单位。然后，我们使用<code>keyBy()</code>转换算子将传感器读数流按照传感器ID进行分区。接下来，我们定义了一个<code>timeWindow()</code>转换算子，这个算子将每个传感器ID所对应的分区的传感器读数分配到了5秒钟的滚动窗口中。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> avgTemp: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = sensorData</span><br><span class="line">  .map(r =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> celsius = (r.temperature - <span class="number">32</span>) * (<span class="number">5.0</span> / <span class="number">9.0</span>)</span><br><span class="line">    <span class="type">SensorReading</span>(r.id, r.timestamp, celsius)</span><br><span class="line">  &#125;)</span><br><span class="line">  .keyBy(_.id)</span><br><span class="line">  .timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">  .apply(<span class="keyword">new</span> <span class="type">TemperatureAverager</span>)</span><br></pre></td></tr></table></figure></div><p>窗口转换算子将在“窗口操作符”一章中讲解。最后，我们使用了一个UDF函数来计算每个窗口的温度的平均值。我们稍后将会讨论UDF函数的实现。</p><h2 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h2><p>流处理程序经常将它们的计算结果发送到一些外部系统中去，例如：Apache Kafka，文件系统，或者数据库中。Flink提供了一个维护的很好的sink算子的集合，这些sink算子可以用来将数据写入到不同的系统中去。我们也可以实现自己的sink算子。也有一些Flink程序并不会向第三方外部系统发送数据，而是将数据存储到Flink系统内部，然后可以使用Flink的可查询状态的特性来查询数据。</p><p>在我们的例子中，计算结果是一个<code>DataStream[SensorReading]</code>数据记录。每一条数据记录包含了一个传感器在5秒钟的周期里面的平均温度。计算结果组成的数据流将会调用<code>print()</code>将计算结果写到标准输出。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">avgTemp.print()</span><br></pre></td></tr></table></figure></div><blockquote><p>要注意一点，流的Sink算子的选择将会影响应用程序端到端(<code>end-to-end</code>)的一致性，具体就是应用程序的计算提供的到底是<code>at-least-once</code>还是<code>exactly-once</code>的一致性语义。应用程序端到端的一致性依赖于所选择的流的Sink算子和Flink的检查点算法的集成使用。</p></blockquote><h2 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h2><p>当应用程序完全写好时，我们可以调用<code>StreamExecutionEnvironment.execute()</code>来执行应用程序。在我们的例子中就是我们的最后一行调用：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.execute(<span class="string">"Compute average sensor temperature"</span>)</span><br></pre></td></tr></table></figure></div><p>Flink程序是惰性执行的。也就是说创建数据源和转换算子的API调用并不会立刻触发任何数据处理逻辑。API调用仅仅是在执行环境中构建了一个执行计划，这个执行计划包含了执行环境创建的数据源和所有的将要用在数据源上的转换算子。只有当<code>execute()</code>被调用时，系统才会触发程序的执行。</p><p>构建好的执行计划将被翻译成一个<code>JobGraph</code>并提交到<code>JobManager</code>上面去执行。根据执行环境的种类，一个<code>JobManager</code>将会运行在一个本地线程中（如果是本地执行环境的化）或者<code>JobGraph</code>将会被发送到一个远程的<code>JobManager</code>上面去。如果<code>JobManager</code>远程运行，那么<code>JobGraph</code>必须和一个包含有所有类和应用程序的依赖的JAR包一起发送到远程<code>JobManager</code>。</p><h2 id="产生传感器读数代码编写"><a href="#产生传感器读数代码编写" class="headerlink" title="产生传感器读数代码编写"></a>产生传感器读数代码编写</h2><h3 id="从批读取数据"><a href="#从批读取数据" class="headerlink" title="从批读取数据"></a>从批读取数据</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream = env</span><br><span class="line">  .fromCollection(<span class="type">List</span>(</span><br><span class="line">    <span class="type">SensorReading</span>(<span class="string">"sensor_1"</span>, <span class="number">1547718199</span>, <span class="number">35.80018327300259</span>),</span><br><span class="line">    <span class="type">SensorReading</span>(<span class="string">"sensor_6"</span>, <span class="number">1547718199</span>, <span class="number">15.402984393403084</span>),</span><br><span class="line">    <span class="type">SensorReading</span>(<span class="string">"sensor_7"</span>, <span class="number">1547718199</span>, <span class="number">6.720945201171228</span>),</span><br><span class="line">    <span class="type">SensorReading</span>(<span class="string">"sensor_10"</span>, <span class="number">1547718199</span>, <span class="number">38.101067604893444</span>)</span><br><span class="line">  ))</span><br></pre></td></tr></table></figure></div><h3 id="从文件读取数据"><a href="#从文件读取数据" class="headerlink" title="从文件读取数据"></a>从文件读取数据</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream = env.readTextFile(filePath)</span><br></pre></td></tr></table></figure></div><h3 id="以Kafka消息队列的数据为数据来源"><a href="#以Kafka消息队列的数据为数据来源" class="headerlink" title="以Kafka消息队列的数据为数据来源"></a>以Kafka消息队列的数据为数据来源</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"group.id"</span>, <span class="string">"consumer-group"</span>)</span><br><span class="line">properties.setProperty(</span><br><span class="line">  <span class="string">"key.deserializer"</span>,</span><br><span class="line">  <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">)</span><br><span class="line">properties.setProperty(</span><br><span class="line">  <span class="string">"value.deserializer"</span>,</span><br><span class="line">  <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">)</span><br><span class="line">properties.setProperty(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>)</span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">env.setParallelism(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> stream = env</span><br><span class="line">  <span class="comment">// source为来自Kafka的数据，这里我们实例化一个消费者，topic为hotitems</span></span><br><span class="line">  .addSource(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](</span><br><span class="line">      <span class="string">"hotitems"</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(),</span><br><span class="line">      properties</span><br><span class="line">    )</span><br><span class="line">  )</span><br></pre></td></tr></table></figure></div><blockquote><p>注意，Kafka的版本为<code>2.2</code>。</p></blockquote><h3 id="自定义数据源"><a href="#自定义数据源" class="headerlink" title="自定义数据源"></a>自定义数据源</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Calendar</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">RichParallelSourceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span>.<span class="type">SourceContext</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 传感器id，时间戳，温度</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SensorReading</span>(<span class="params">id: <span class="type">String</span>, timestamp: <span class="type">Long</span>, temperature: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">需要extends</span> <span class="title">RichParallelSourceFunction</span>, <span class="title">泛型为SensorReading</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">SensorSource</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>[<span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// flag indicating whether source is still running.</span></span><br><span class="line">  <span class="comment">// flag: 表示数据源是否还在正常运行</span></span><br><span class="line">  <span class="keyword">var</span> running: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// run()函数连续的发送SensorReading数据，使用SourceContext</span></span><br><span class="line">  <span class="comment">// 需要override</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(srcCtx: <span class="type">SourceContext</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize random number generator</span></span><br><span class="line">    <span class="comment">// 初始化随机数发生器</span></span><br><span class="line">    <span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">    <span class="comment">// look up index of this parallel task</span></span><br><span class="line">    <span class="comment">// 查找当前运行时上下文的任务的索引</span></span><br><span class="line">    <span class="keyword">val</span> taskIdx = <span class="keyword">this</span>.getRuntimeContext.getIndexOfThisSubtask</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize sensor ids and temperatures</span></span><br><span class="line">    <span class="comment">// 初始化10个(温度传感器的id, 温度值)元组</span></span><br><span class="line">    <span class="keyword">var</span> curFTemp = (<span class="number">1</span> to <span class="number">10</span>).map &#123;</span><br><span class="line">      <span class="comment">// nextGaussian产生高斯随机数</span></span><br><span class="line">      i =&gt; (<span class="string">"sensor_"</span> + (taskIdx * <span class="number">10</span> + i), <span class="number">65</span> + (rand.nextGaussian() * <span class="number">20</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// emit data until being canceled</span></span><br><span class="line">    <span class="comment">// 无限循环，产生数据流</span></span><br><span class="line">    <span class="keyword">while</span> (running) &#123;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// update temperature</span></span><br><span class="line">      <span class="comment">// 更新温度</span></span><br><span class="line">      curFTemp = curFTemp.map(t =&gt; (t._1, t._2 + (rand.nextGaussian() * <span class="number">0.5</span>)) )</span><br><span class="line">      <span class="comment">// get current time</span></span><br><span class="line">      <span class="comment">// 获取当前时间戳</span></span><br><span class="line">      <span class="keyword">val</span> curTime = <span class="type">Calendar</span>.getInstance.getTimeInMillis</span><br><span class="line"></span><br><span class="line">      <span class="comment">// emit new SensorReading</span></span><br><span class="line">      <span class="comment">// 发射新的传感器数据, 注意这里srcCtx.collect</span></span><br><span class="line">      curFTemp.foreach(t =&gt; srcCtx.collect(<span class="type">SensorReading</span>(t._1, curTime, t._2)))</span><br><span class="line"></span><br><span class="line">      <span class="comment">// wait for 100 ms</span></span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">100</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// override cancel函数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    running = <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>使用方法</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ingest sensor stream</span></span><br><span class="line"><span class="keyword">val</span> sensorData: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = env</span><br><span class="line">  <span class="comment">// SensorSource generates random temperature readings</span></span><br><span class="line">  .addSource(<span class="keyword">new</span> <span class="type">SensorSource</span>)</span><br></pre></td></tr></table></figure></div><blockquote><p>注意，在我们本教程中，我们一直会使用这个自定义的数据源。</p></blockquote><h2 id="转换算子"><a href="#转换算子" class="headerlink" title="转换算子"></a>转换算子</h2><p>在这一小节我们将大概看一下DataStream API的基本转换算子。与时间有关的操作符（例如窗口操作符和其他特殊的转换算子）将会在后面的章节叙述。一个流的转换操作将会应用在一个或者多个流上面，这些转换操作将流转换成一个或者多个输出流。编写一个DataStream API简单来说就是将这些转换算子组合在一起来构建一个数据流图，这个数据流图就实现了我们的业务逻辑。</p><p>大部分的流转换操作都基于用户自定义函数UDF。UDF函数打包了一些业务逻辑并定义了输入流的元素如何转换成输出流的元素。像<code>MapFunction</code>这样的函数，将会被定义为类，这个类实现了Flink针对特定的转换操作暴露出来的接口。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMapFunction</span> <span class="keyword">extends</span> <span class="title">MapFunction</span>[<span class="type">Int</span>, <span class="type">Int</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: <span class="type">Int</span>): <span class="type">Int</span> = value + <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>函数接口定义了需要由用户实现的转换方法，例如上面例子中的<code>map()</code>方法。</p><p>大部分函数接口被设计为<code>Single Abstract Method</code>（单独抽象方法）接口，并且接口可以使用Java 8匿名函数来实现。Scala DataStream API也内置了对匿名函数的支持。当讲解DataStream API的转换算子时，我们展示了针对所有函数类的接口，但为了简洁，大部分接口的实现使用匿名函数而不是函数类的方式。</p><p>DataStream API针对大多数数据转换操作提供了转换算子。如果你很熟悉批处理API、函数式编程语言或者SQL，那么你将会发现这些API很容易学习。我们会将DataStream API的转换算子分成四类：</p><ul><li>基本转换算子：将会作用在数据流中的每一条单独的数据上。</li><li>KeyedStream转换算子：在数据有key的情况下，对数据应用转换算子。</li><li>多流转换算子：合并多条流为一条流或者将一条流分割为多条流。</li><li>分布式转换算子：将重新组织流里面的事件。</li></ul><h3 id="基本转换算子"><a href="#基本转换算子" class="headerlink" title="基本转换算子"></a>基本转换算子</h3><p>基本转换算子会针对流中的每一个单独的事件做处理，也就是说每一个输入数据会产生一个输出数据。单值转换，数据的分割，数据的过滤，都是基本转换操作的典型例子。我们将解释这些算子的语义并提供示例代码。</p><p><em>MAP</em></p><p><code>map</code>算子通过调用<code>DataStream.map()</code>来指定。<code>map</code>算子的使用将会产生一条新的数据流。它会将每一个输入的事件传送到一个用户自定义的mapper，这个mapper只返回一个输出事件，这个输出事件和输入事件的类型可能不一样。图5-1展示了一个map算子，这个map将每一个正方形转化成了圆形。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0501.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0501.png" class="lazyload"></a></p><p><code>MapFunction</code>的类型与输入事件和输出事件的类型相关，可以通过实现<code>MapFunction</code>接口来定义。接口包含<code>map()</code>函数，这个函数将一个输入事件恰好转换为一个输出事件。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// T: the type of input elements</span></span><br><span class="line"><span class="comment">// O: the type of output elements</span></span><br><span class="line"><span class="type">MapFunction</span>[<span class="type">T</span>, <span class="type">O</span>]</span><br><span class="line">    &gt; map(<span class="type">T</span>): <span class="type">O</span></span><br></pre></td></tr></table></figure></div><p>下面的代码实现了将SensorReading中的id字段抽取出来的功能。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> sensorIds: <span class="type">DataStream</span>[<span class="type">String</span>] = readings.map(<span class="keyword">new</span> <span class="type">MyMapFunction</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMapFunction</span> <span class="keyword">extends</span> <span class="title">MapFunction</span>[<span class="type">SensorReading</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(r: <span class="type">SensorReading</span>): <span class="type">String</span> = r.id</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>当然我们更推荐匿名函数的写法。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> sensorIds: <span class="type">DataStream</span>[<span class="type">String</span>] = readings.map(r =&gt; r.id)</span><br></pre></td></tr></table></figure></div><p><em>FILTER</em></p><p><code>filter</code>转换算子通过在每个输入事件上对一个布尔条件进行求值来过滤掉一些元素，然后将剩下的元素继续发送。一个<code>true</code>的求值结果将会把输入事件保留下来并发送到输出，而如果求值结果为<code>false</code>，则输入事件会被抛弃掉。我们通过调用<code>DataStream.filter()</code>来指定流的<code>filter</code>算子，<code>filter</code>操作将产生一条新的流，其类型和输入流中的事件类型是一样的。图5-2展示了只产生白色方框的<code>filter</code>操作。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0502.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0502.png" class="lazyload"></a></p><p>布尔条件可以使用函数、FilterFunction接口或者匿名函数来实现。FilterFunction中的泛型是输入事件的类型。定义的<code>filter()</code>方法会作用在每一个输入元素上面，并返回一个布尔值。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// T: the type of elements</span></span><br><span class="line"><span class="type">FilterFunction</span>[<span class="type">T</span>]</span><br><span class="line">    &gt; filter(<span class="type">T</span>): <span class="type">Boolean</span></span><br></pre></td></tr></table></figure></div><p>下面的例子展示了如何使用filter来从传感器数据中过滤掉温度值小于25华氏温度的读数。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> filteredSensors = readings.filter(r =&gt; r.temperature &gt;= <span class="number">25</span>)</span><br></pre></td></tr></table></figure></div><p><em>FLATMAP</em></p><p><code>flatMap</code>算子和<code>map</code>算子很类似，不同之处在于针对每一个输入事件<code>flatMap</code>可以生成0个、1个或者多个输出元素。事实上，<code>flatMap</code>转换算子是<code>filter</code>和<code>map</code>的泛化。所以<code>flatMap</code>可以实现<code>map</code>和<code>filter</code>算子的功能。图5-3展示了<code>flatMap</code>如何根据输入事件的颜色来做不同的处理。如果输入事件是白色方框，则直接输出。输入元素是黑框，则复制输入。灰色方框会被过滤掉。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0503.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0503.png" class="lazyload"></a></p><p>flatMap算子将会应用在每一个输入事件上面。对应的<code>FlatMapFunction</code>定义了<code>flatMap()</code>方法，这个方法返回0个、1个或者多个事件到一个<code>Collector</code>集合中，作为输出结果。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// T: the type of input elements</span></span><br><span class="line"><span class="comment">// O: the type of output elements</span></span><br><span class="line"><span class="type">FlatMapFunction</span>[<span class="type">T</span>, <span class="type">O</span>]</span><br><span class="line">    &gt; flatMap(<span class="type">T</span>, <span class="type">Collector</span>[<span class="type">O</span>]): <span class="type">Unit</span></span><br></pre></td></tr></table></figure></div><p>下面的例子展示了在数据分析教程中经常用到的例子，我们用<code>flatMap</code>来实现。这个函数应用在一个语句流上面，将每个句子用空格切分，然后把切分出来的单词作为单独的事件发送出去。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sentences: <span class="type">DataStream</span>[<span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> words: <span class="type">DataStream</span>[<span class="type">String</span>] = sentences</span><br><span class="line">  .flatMap(id =&gt; id.split(<span class="string">" "</span>))</span><br></pre></td></tr></table></figure></div><h3 id="键控流转换算子"><a href="#键控流转换算子" class="headerlink" title="键控流转换算子"></a>键控流转换算子</h3><p>很多流处理程序的一个基本要求就是要能对数据进行分组，分组后的数据共享某一个相同的属性。DataStream API提供了一个叫做<code>KeyedStream</code>的抽象，此抽象会从逻辑上对DataStream进行分区，分区后的数据拥有同样的<code>Key</code>值，分区后的流互不相关。</p><p>针对KeyedStream的状态转换操作可以读取数据或者写入数据到当前事件Key所对应的状态中。这表明拥有同样Key的所有事件都可以访问同样的状态，也就是说所以这些事件可以一起处理。</p><blockquote><p>要小心使用状态转换操作和基于Key的聚合操作。如果Key的值越来越多，例如：Key是订单ID，我们必须及时清空Key所对应的状态，以免引起内存方面的问题。稍后我们会详细讲解。</p></blockquote><p>KeyedStream可以使用map，flatMap和filter算子来处理。接下来我们会使用keyBy算子来将DataStream转换成KeyedStream，并讲解基于key的转换操作：滚动聚合和reduce算子。</p><p><em>KEYBY</em></p><p>keyBy通过指定key来将DataStream转换成KeyedStream。基于不同的key，流中的事件将被分配到不同的分区中去。所有具有相同key的事件将会在接下来的操作符的同一个子任务槽中进行处理。拥有不同key的事件可以在同一个任务中处理。但是算子只能访问当前事件的key所对应的状态。</p><p>如图5-4所示，把输入事件的颜色作为key，黑色的事件输出到了一个分区，其他颜色输出到了另一个分区。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0504.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0504.png" class="lazyload"></a></p><p><code>keyBy()</code>方法接收一个参数，这个参数指定了key或者keys，有很多不同的方法来指定key。我们将在后面讲解。下面的代码声明了<code>id</code>这个字段为SensorReading流的key。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> keyed: <span class="type">KeyedStream</span>[<span class="type">SensorReading</span>, <span class="type">String</span>] = readings</span><br><span class="line">  .keyBy(r =&gt; r.id)</span><br></pre></td></tr></table></figure></div><p>匿名函数<code>r =&gt; r.id</code>抽取了传感器读数SensorReading的id值。</p><p><em>滚动聚合</em></p><p>滚动聚合算子由<code>KeyedStream</code>调用，并生成一个聚合以后的DataStream，例如：sum，minimum，maximum。一个滚动聚合算子会为每一个观察到的key保存一个聚合的值。针对每一个输入事件，算子将会更新保存的聚合结果，并发送一个带有更新后的值的事件到下游算子。滚动聚合不需要用户自定义函数，但需要接受一个参数，这个参数指定了在哪一个字段上面做聚合操作。DataStream API提供了以下滚动聚合方法。</p><blockquote><p>滚动聚合算子只能用在滚动窗口，不能用在滑动窗口。</p></blockquote><ul><li>sum()：在输入流上对指定的字段做滚动相加操作。</li><li>min()：在输入流上对指定的字段求最小值。</li><li>max()：在输入流上对指定的字段求最大值。</li><li>minBy()：在输入流上针对指定字段求最小值，并返回包含当前观察到的最小值的事件。</li><li>maxBy()：在输入流上针对指定字段求最大值，并返回包含当前观察到的最大值的事件。</li></ul><p>滚动聚合算子无法组合起来使用，每次计算只能使用一个单独的滚动聚合算子。</p><p>下面的例子根据第一个字段来对类型为<code>Tuple3[Int, Int, Int]</code>的流做分流操作，然后针对第二个字段做滚动求和操作。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>)] = env.fromElements(</span><br><span class="line">  (<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), (<span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultStream: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>)] = inputStream</span><br><span class="line">  .keyBy(<span class="number">0</span>) <span class="comment">// key on first field of the tuple</span></span><br><span class="line">  .sum(<span class="number">1</span>)   <span class="comment">// sum the second field of the tuple in place</span></span><br></pre></td></tr></table></figure></div><p>在这个例子里面，输入流根据第一个字段来分流，然后在第二个字段上做计算。对于key 1，输出结果是(1,2,2),(1,7,2)。对于key 2，输出结果是(2,3,1),(2,5,1)。第一个字段是key，第二个字段是求和的数值，第三个字段未定义。</p><blockquote><p>滚动聚合操作会对每一个key都保存一个状态。因为状态从来不会被清空，所以我们在使用滚动聚合算子时只能使用在含有有限个key的流上面。</p></blockquote><p><em>REDUCE</em></p><p>reduce算子是滚动聚合的泛化实现。它将一个ReduceFunction应用到了一个KeyedStream上面去。reduce算子将会把每一个输入事件和当前已经reduce出来的值做聚合计算。reduce操作不会改变流的事件类型。输出流数据类型和输入流数据类型是一样的。</p><p>reduce函数可以通过实现接口ReduceFunction来创建一个类。ReduceFunction接口定义了<code>reduce()</code>方法，此方法接收两个输入事件，输入一个相同类型的事件。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// T: the element type</span></span><br><span class="line"><span class="type">ReduceFunction</span>[<span class="type">T</span>]</span><br><span class="line">    &gt; reduce(<span class="type">T</span>, <span class="type">T</span>): <span class="type">T</span></span><br></pre></td></tr></table></figure></div><p>下面的例子，流根据语言这个key来分区，输出结果为针对每一种语言都实时更新的单词列表。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">List</span>[<span class="type">String</span>])] = env.fromElements(</span><br><span class="line">  (<span class="string">"en"</span>, <span class="type">List</span>(<span class="string">"tea"</span>)), (<span class="string">"fr"</span>, <span class="type">List</span>(<span class="string">"vin"</span>)), (<span class="string">"en"</span>, <span class="type">List</span>(<span class="string">"cake"</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">List</span>[<span class="type">String</span>])] = inputStream</span><br><span class="line">  .keyBy(<span class="number">0</span>)</span><br><span class="line">  .reduce((x, y) =&gt; (x._1, x._2 ::: y._2))</span><br></pre></td></tr></table></figure></div><p>reduce匿名函数将连续两个tuple的第一个字段(key字段)继续发送出去，然后将两个tuple的第二个字段List[String]连接。</p><blockquote><p>reduce作为滚动聚合的泛化实现，同样也要针对每一个key保存状态。因为状态从来不会清空，所以我们需要将reduce算子应用在一个有限key的流上。</p></blockquote><h3 id="多流转换算子"><a href="#多流转换算子" class="headerlink" title="多流转换算子"></a>多流转换算子</h3><p>许多应用需要摄入多个流并将流合并处理，还可能需要将一条流分割成多条流然后针对每一条流应用不同的业务逻辑。接下来，我们将讨论DataStream API中提供的能够处理多条输入流或者发送多条输出流的操作算子。</p><p><em>UNION</em></p><p>DataStream.union()方法将两条或者多条DataStream合并成一条具有与输入流相同类型的输出DataStream。接下来的转换算子将会处理输入流中的所有元素。图5-5展示了union操作符如何将黑色和白色的事件流合并成一个单一输出流。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0505.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0505.png" class="lazyload"></a></p><p>事件合流的方式为FIFO方式。操作符并不会产生一个特定顺序的事件流。union操作符也不会进行去重。每一个输入事件都被发送到了下一个操作符。</p><p>下面的例子展示了如何将三条类型为SensorReading的数据流合并成一条流。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> parisStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> tokyoStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> rioStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> allCities: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = parisStream</span><br><span class="line">  .union(tokyoStream, rioStream)</span><br></pre></td></tr></table></figure></div><p><em>CONNECT, COMAP和COFLATMAP</em></p><p>联合两条流的事件是非常常见的流处理需求。例如监控一片森林然后发出高危的火警警报。报警的Application接收两条流，一条是温度传感器传回来的数据，一条是烟雾传感器传回来的数据。当两条流都超过各自的阈值时，报警。</p><p>DataStream API提供了<code>connect</code>操作来支持以上的应用场景。<code>DataStream.connect()</code>方法接收一条<code>DataStream</code>，然后返回一个<code>ConnectedStreams</code>类型的对象，这个对象表示了两条连接的流。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// first stream</span></span><br><span class="line"><span class="keyword">val</span> first: <span class="type">DataStream</span>[<span class="type">Int</span>] = ...</span><br><span class="line"><span class="comment">// second stream</span></span><br><span class="line"><span class="keyword">val</span> second: <span class="type">DataStream</span>[<span class="type">String</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// connect streams</span></span><br><span class="line"><span class="keyword">val</span> connected: <span class="type">ConnectedStreams</span>[<span class="type">Int</span>, <span class="type">String</span>] = first.connect(second)</span><br></pre></td></tr></table></figure></div><p>ConnectedStreams提供了<code>map()</code>和<code>flatMap()</code>方法，分别需要接收类型为<code>CoMapFunction</code>和<code>CoFlatMapFunction</code>的参数。</p><p>以上两个函数里面的泛型是第一条流的事件类型和第二条流的事件类型，以及输出流的事件类型。还定义了两个方法，每一个方法针对一条流来调用。<code>map1()</code>和<code>flatMap1()</code>会调用在第一条流的元素上面，<code>map2()</code>和<code>flatMap2()</code>会调用在第二条流的元素上面。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// IN1: 第一条流的事件类型</span></span><br><span class="line"><span class="comment">// IN2: 第二条流的事件类型</span></span><br><span class="line"><span class="comment">// OUT: 输出流的事件类型</span></span><br><span class="line"><span class="type">CoMapFunction</span>[<span class="type">IN1</span>, <span class="type">IN2</span>, <span class="type">OUT</span>]</span><br><span class="line">    &gt; map1(<span class="type">IN1</span>): <span class="type">OUT</span></span><br><span class="line">    &gt; map2(<span class="type">IN2</span>): <span class="type">OUT</span></span><br><span class="line"></span><br><span class="line"><span class="type">CoFlatMapFunction</span>[<span class="type">IN1</span>, <span class="type">IN2</span>, <span class="type">OUT</span>]</span><br><span class="line">    &gt; flatMap1(<span class="type">IN1</span>, <span class="type">Collector</span>[<span class="type">OUT</span>]): <span class="type">Unit</span></span><br><span class="line">    &gt; flatMap2(<span class="type">IN2</span>, <span class="type">Collector</span>[<span class="type">OUT</span>]): <span class="type">Unit</span></span><br></pre></td></tr></table></figure></div><blockquote><p>函数无法选择读某一条流。我们是无法控制函数中的两个方法的调用顺序的。当一条流中的元素到来时，将会调用相对应的方法。</p></blockquote><p>对两条流做连接查询通常需要这两条流基于某些条件被确定性的路由到操作符中相同的并行实例里面去。在默认情况下，connect()操作将不会对两条流的事件建立任何关系，所以两条流的事件将会随机的被发送到下游的算子实例里面去。这样的行为会产生不确定性的计算结果，显然不是我们想要的。为了针对ConnectedStreams进行确定性的转换操作，connect()方法可以和keyBy()或者broadcast()组合起来使用。我们首先看一下keyBy()的示例。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> one: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Long</span>)] = ...</span><br><span class="line"><span class="keyword">val</span> two: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// keyBy two connected streams</span></span><br><span class="line"><span class="keyword">val</span> keyedConnect1: <span class="type">ConnectedStreams</span>[(<span class="type">Int</span>, <span class="type">Long</span>), (<span class="type">Int</span>, <span class="type">String</span>)] = one</span><br><span class="line">  .connect(two)</span><br><span class="line">  .keyBy(<span class="number">0</span>, <span class="number">0</span>) <span class="comment">// key both input streams on first attribute</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// alternative: connect two keyed streams</span></span><br><span class="line"><span class="keyword">val</span> keyedConnect2: <span class="type">ConnectedStreams</span>[(<span class="type">Int</span>, <span class="type">Long</span>), (<span class="type">Int</span>, <span class="type">String</span>)] = one</span><br><span class="line">  .keyBy(<span class="number">0</span>)</span><br><span class="line">  .connect(two.keyBy(<span class="number">0</span>))</span><br></pre></td></tr></table></figure></div><p>无论使用keyBy()算子操作ConnectedStreams还是使用connect()算子连接两条KeyedStreams，connect()算子会将两条流的含有相同Key的所有事件都发送到相同的算子实例。两条流的key必须是一样的类型和值，就像SQL中的JOIN。在connected和keyed stream上面执行的算子有访问keyed state的权限。</p><p>下面的例子展示了如何连接一条DataStream和广播过的流。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> first: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Long</span>)] = ...</span><br><span class="line"><span class="keyword">val</span> second: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// connect streams with broadcast</span></span><br><span class="line"><span class="keyword">val</span> keyedConnect: <span class="type">ConnectedStreams</span>[(<span class="type">Int</span>, <span class="type">Long</span>), (<span class="type">Int</span>, <span class="type">String</span>)] = first</span><br><span class="line">  <span class="comment">// broadcast second input stream</span></span><br><span class="line">  .connect(second.broadcast())</span><br></pre></td></tr></table></figure></div><p>一条被广播过的流中的所有元素将会被复制然后发送到下游算子的所有并行实例中去。未被广播过的流仅仅向前发送。所以两条流的元素显然会被连接处理。</p><p><em>SPLIT和SELECT</em></p><p>Split是Union的反函数。Split将输入的流分成两条或者多条流。每一个输入的元素都可以被路由到0、1或者多条流中去。所以，split可以用来过滤或者复制元素。图5-6展示了split操作符将所有的白色事件都路由到同一条流中去了，剩下的元素去往另一条流。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0506.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0506.png" class="lazyload"></a></p><p>DataStream.split()方法接受<code>OutputSelector</code>类型，此类型定义了输入流中的元素被分配到哪个名字的流中去。<code>OutputSelector</code>定义了<code>select()</code>方法，此方法将被每一个元素调用，并返回<code>java.lang.Iterable[String]</code>类型的数据。返回的<code>String</code>类型的值将指定元素将被路由到哪一条流。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; IN: the type of the split elements</span><br><span class="line">OutputSelector[IN]</span><br><span class="line">    &gt; select(IN): Iterable[String]</span><br></pre></td></tr></table></figure></div><p>DataStream.split()方法返回<code>SplitStream</code>类型，此类型提供<code>select()</code>方法，可以根据分流后不同流的名字，将某个名字对应的流提取出来。</p><p>例5-2将一条整数流分成了不同的流，大的整数一条流，小的整数一条流。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> splitted: <span class="type">SplitStream</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = inputStream</span><br><span class="line">  .split(t =&gt; <span class="keyword">if</span> (t._1 &gt; <span class="number">1000</span>) <span class="type">Seq</span>(<span class="string">"large"</span>) <span class="keyword">else</span> <span class="type">Seq</span>(<span class="string">"small"</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> large: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = splitted.select(<span class="string">"large"</span>)</span><br><span class="line"><span class="keyword">val</span> small: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = splitted.select(<span class="string">"small"</span>)</span><br><span class="line"><span class="keyword">val</span> all: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = splitted.select(<span class="string">"small"</span>, <span class="string">"large"</span>)</span><br></pre></td></tr></table></figure></div><blockquote><p>不推荐使用split方法，推荐使用Flink的侧输出（side-output）特性。</p></blockquote><h3 id="分布式转换算子"><a href="#分布式转换算子" class="headerlink" title="分布式转换算子"></a>分布式转换算子</h3><p>分区操作对应于我们之前讲过的“数据交换策略”这一节。这些操作定义了事件如何分配到不同的任务中去。当我们使用DataStream API来编写程序时，系统将自动的选择数据分区策略，然后根据操作符的语义和设置的并行度将数据路由到正确的地方去。有些时候，我们需要在应用程序的层面控制分区策略，或者自定义分区策略。例如，如果我们知道会发生数据倾斜，那么我们想要针对数据流做负载均衡，将数据流平均发送到接下来的操作符中去。又或者，应用程序的业务逻辑可能需要一个算子所有的并行任务都需要接收同样的数据。再或者，我们需要自定义分区策略的时候。在这一小节，我们将展示DataStream的一些方法，可以使我们来控制或者自定义数据分区策略。</p><blockquote><p>keyBy()方法不同于分布式转换算子。所有的分布式转换算子将产生DataStream数据类型。而keyBy()产生的类型是KeyedStream，它拥有自己的keyed state。</p></blockquote><p><em>Random</em></p><p>随机数据交换由<code>DataStream.shuffle()</code>方法实现。shuffle方法将数据随机的分配到下游算子的并行任务中去。</p><p><em>Round-Robin</em></p><p><code>rebalance()</code>方法使用Round-Robin负载均衡算法将输入流平均分配到随后的并行运行的任务中去。图5-7为round-robin分布式转换算子的示意图。</p><p><em>Rescale</em></p><p><code>rescale()</code>方法使用的也是round-robin算法，但只会将数据发送到接下来的并行运行的任务中的一部分任务中。本质上，当发送者任务数量和接收者任务数量不一样时，rescale分区策略提供了一种轻量级的负载均衡策略。如果接收者任务的数量是发送者任务的数量的倍数时，rescale操作将会效率更高。</p><p><code>rebalance()</code>和<code>rescale()</code>的根本区别在于任务之间连接的机制不同。 <code>rebalance()</code>将会针对所有发送者任务和所有接收者任务之间建立通信通道，而<code>rescale()</code>仅仅针对每一个任务和下游算子的一部分子并行任务之间建立通信通道。rescale的示意图为图5-7。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0507.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0507.png" class="lazyload"></a></p><p><em>Broadcast</em></p><p><code>broadcast()</code>方法将输入流的所有数据复制并发送到下游算子的所有并行任务中去。</p><p><em>Global</em></p><p><code>global()</code>方法将所有的输入流数据都发送到下游算子的第一个并行任务中去。这个操作需要很谨慎，因为将所有数据发送到同一个task，将会对应用程序造成很大的压力。</p><p><em>Custom</em></p><p>当Flink提供的分区策略都不适用时，我们可以使用<code>partitionCustom()</code>方法来自定义分区策略。这个方法接收一个<code>Partitioner</code>对象，这个对象需要实现分区逻辑以及定义针对流的哪一个字段或者key来进行分区。下面的例子将一条整数流做partition，使得所有的负整数都发送到第一个任务中，剩下的数随机分配。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> numbers: <span class="type">DataStream</span>[(<span class="type">Int</span>)] = ...</span><br><span class="line">numbers.partitionCustom(myPartitioner, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">myPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>[<span class="type">Int</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> r = scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">partition</span></span>(key: <span class="type">Int</span>, numPartitions: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (key &lt; <span class="number">0</span>) <span class="number">0</span> <span class="keyword">else</span> r.nextInt(numPartitions)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="设置并行度"><a href="#设置并行度" class="headerlink" title="设置并行度"></a>设置并行度</h2><p>Flink应用程序在一个像集群这样的分布式环境中并行执行。当一个数据流程序提交到JobManager执行时，系统将会创建一个数据流图，然后准备执行需要的操作符。每一个操作符将会并行化到一个或者多个任务中去。每个算子的并行任务都会处理这个算子的输入流中的一份子集。一个算子并行任务的个数叫做算子的并行度。它决定了算子执行的并行化程度，以及这个算子能处理多少数据量。</p><p>算子的并行度可以在执行环境这个层级来控制，也可以针对每个不同的算子设置不同的并行度。默认情况下，应用程序中所有算子的并行度都将设置为执行环境的并行度。执行环境的并行度（也就是所有算子的默认并行度）将在程序开始运行时自动初始化。如果应用程序在本地执行环境中运行，并行度将被设置为CPU的核数。当我们把应用程序提交到一个处于运行中的Flink集群时，执行环境的并行度将被设置为集群默认的并行度，除非我们在客户端提交应用程序时显式的设置好并行度。</p><p>通常情况下，将算子的并行度定义为和执行环境并行度相关的数值会是个好主意。这允许我们通过在客户端调整应用程序的并行度就可以将程序水平扩展了。我们可以使用以下代码来访问执行环境的默认并行度。</p><p>我们还可以重写执行环境的默认并行度，但这样的话我们将再也不能通过客户端来控制应用程序的并行度了。</p><p>算子默认的并行度也可以通过重写来明确指定。在下面的例子里面，数据源的操作符将会按照环境默认的并行度来并行执行，map操作符的并行度将会是默认并行度的2倍，sink操作符的并行度为2。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> defaultP = env.getParallelism</span><br><span class="line"><span class="keyword">val</span> result = env.addSource(<span class="keyword">new</span> <span class="type">CustomSource</span>)</span><br><span class="line">  .map(<span class="keyword">new</span> <span class="type">MyMapper</span>).setParallelism(defaultP * <span class="number">2</span>)</span><br><span class="line">  .print().setParallelism(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></div><p>当我们通过客户端将应用程序的并行度设置为16并提交执行时，source操作符的并行度为16，mapper并行度为32，sink并行度为2。如果我们在本地环境运行应用程序的话，例如在IDE中运行，机器是8核，那么source任务将会并行执行在8个任务上面，mapper运行在16个任务上面，sink运行在2个任务上面。</p><blockquote><p>并行度是动态概念，任务槽数量是静态概念。并行度&lt;=任务槽数量。一个任务槽最多运行一个并行度。</p></blockquote><h2 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h2><p>Flink程序所处理的流中的事件一般是对象类型。操作符接收对象输出对象。所以Flink的内部机制需要能够处理事件的类型。在网络中传输数据，或者将数据写入到状态后端、检查点和保存点中，都需要我们对数据进行序列化和反序列化。为了高效的进行此类操作，Flink需要流中事件类型的详细信息。Flink使用了<code>Type Information</code>的概念来表达数据类型，这样就能针对不同的数据类型产生特定的序列化器，反序列化器和比较操作符。</p><blockquote><p>有点像泛型。</p></blockquote><p>Flink也能够通过分析输入数据和输出数据来自动获取数据的类型信息以及序列化器和反序列化器。尽管如此，在一些特定的情况下，例如匿名函数或者使用泛型的情况下，我们需要明确的提供数据的类型信息，来提高我们程序的性能。</p><p>在这一节中，我们将讨论Flink支持的类型，以及如何为数据类型创建相应的类型信息，还有就是在Flink无法推断函数返回类型的情况下，如何帮助Flink的类型系统去做类型推断。</p><h3 id="支持的数据类型"><a href="#支持的数据类型" class="headerlink" title="支持的数据类型"></a>支持的数据类型</h3><p>Flink支持Java和Scala提供的所有普通数据类型。最常用的数据类型可以做以下分类：</p><ul><li>Primitives（原始数据类型）</li><li>Java和Scala的Tuples（元组）</li><li>Scala的样例类</li><li>POJO类型</li><li>一些特殊的类型</li></ul><p>接下来让我们一探究竟。</p><p><em>Primitives</em></p><p>Java和Scala提供的所有原始数据类型都支持，例如<code>Int</code>(Java的<code>Integer</code>)，String，Double等等。下面举一个例子：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> numbers: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.fromElements(<span class="number">1</span>L, <span class="number">2</span>L, <span class="number">3</span>L, <span class="number">4</span>L)</span><br><span class="line">numbers.map(n =&gt; n + <span class="number">1</span>)</span><br></pre></td></tr></table></figure></div><p><em>Tuples</em></p><p>元组是一种组合数据类型，由固定数量的元素组成。</p><p>DataStream的Scala API直接使用Scala内置的Tuple。举个例子：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> persons: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Integer</span>)] =</span><br><span class="line">env.fromElements(</span><br><span class="line">  (<span class="string">"Adam"</span>, <span class="number">17</span>),</span><br><span class="line">  (<span class="string">"Sarah"</span>, <span class="number">23</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">persons.filter(p =&gt; p._2 &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure></div><p>Flink为Java的Tuple同样提供了高效的实现。Flink实现的Java Tuple最多可以有25个元素，根据元素数量的不同，Tuple都被实现成了不同的类：Tuple1，Tuple2，一直到Tuple25。Tuple类是强类型。</p><p>我们可以将上面的例子用Java的DataStream API重写：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStream</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt;&gt; persons = env</span><br><span class="line">  .fromElements(</span><br><span class="line">    <span class="type">Tuple2</span>.of(<span class="string">"Adam"</span>, <span class="number">17</span>),</span><br><span class="line">    <span class="type">Tuple2</span>.of(<span class="string">"Sarah"</span>, <span class="number">23</span>)</span><br><span class="line">  );</span><br><span class="line"></span><br><span class="line">persons.filter(p -&gt; p.f1 &gt; <span class="number">18</span>);</span><br></pre></td></tr></table></figure></div><p>Tuple的元素可以通过它们的public属性访问–f0，f1，f2等等。或者使用<code>getField(int pos)</code>方法来访问，元素下标从0开始：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple2</span></span><br><span class="line"></span><br><span class="line"><span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; personTuple = <span class="type">Tuple2</span>.of(<span class="string">"Alex"</span>, <span class="number">42</span>);</span><br><span class="line"><span class="type">Integer</span> age = personTuple.getField(<span class="number">1</span>); <span class="comment">// age = 42</span></span><br></pre></td></tr></table></figure></div><p>不同于Scala的Tuple，Java的Tuple是可变数据结构，所以Tuple中的元素可以重新进行赋值。重复利用Java的Tuple可以减轻垃圾收集的压力。举个例子：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">personTuple.f1 = <span class="number">42</span>; <span class="comment">// set the 2nd field to 42</span></span><br><span class="line">personTuple.setField(<span class="number">43</span>, <span class="number">1</span>); <span class="comment">// set the 2nd field to 43</span></span><br></pre></td></tr></table></figure></div><p><em>Scala case classes</em></p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">persons</span></span>: <span class="type">DataStream</span>[<span class="type">Person</span>] = env.fromElements(</span><br><span class="line">  <span class="type">Person</span>(<span class="string">"Adam"</span>, <span class="number">17</span>),</span><br><span class="line">  <span class="type">Person</span>(<span class="string">"Sarah"</span>, <span class="number">23</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">persons.filter(p =&gt; p.age &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure></div><p><em>POJO</em></p><p>POJO类的定义：</p><ul><li>公有类</li><li>无参数的公有构造器</li><li>所有的字段都是公有的，可以通过getters和setters访问。</li><li>所有字段的数据类型都必须是Flink支持的数据类型。</li></ul><p>举个例子：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  public <span class="type">String</span> name;</span><br><span class="line">  public int age;</span><br><span class="line"></span><br><span class="line">  public <span class="type">Person</span>() &#123;&#125;</span><br><span class="line"></span><br><span class="line">  public <span class="type">Person</span>(<span class="type">String</span> name, int age) &#123;</span><br><span class="line">    <span class="keyword">this</span>.name = name;</span><br><span class="line">    <span class="keyword">this</span>.age = age;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">DataStream</span>&lt;<span class="type">Person</span>&gt; persons = env.fromElements(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"Alex"</span>, <span class="number">42</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"Wendy"</span>, <span class="number">23</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></div><p><em>其他数据类型</em></p><ul><li>Array, ArrayList, HashMap, Enum</li><li>Hadoop Writable types</li><li>Either, Option, Try</li></ul><h3 id="为数据类型创建类型信息"><a href="#为数据类型创建类型信息" class="headerlink" title="为数据类型创建类型信息"></a>为数据类型创建类型信息</h3><p>Flink类型系统的核心类是<code>TypeInformation</code>。它为系统在产生序列化器和比较操作符时，提供了必要的类型信息。例如，如果我们想使用某个key来做联结查询或者分组操作，<code>TypeInformation</code>可以让Flink做更严格的类型检查。</p><p>Flink针对Java和Scala分别提供了类来产生类型信息。在Java中，类是</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.flink.api.common.typeinfo.<span class="type">Types</span></span><br></pre></td></tr></table></figure></div><p>举个例子：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">TypeInformation</span>&lt;<span class="type">Integer</span>&gt; intType = <span class="type">Types</span>.<span class="type">INT</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">TypeInformation</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt;&gt; tupleType = <span class="type">Types</span></span><br><span class="line">  .<span class="type">TUPLE</span>(<span class="type">Types</span>.<span class="type">LONG</span>, <span class="type">Types</span>.<span class="type">STRING</span>);</span><br><span class="line"></span><br><span class="line"><span class="type">TypeInformation</span>&lt;<span class="type">Person</span>&gt; personType = <span class="type">Types</span></span><br><span class="line">  .<span class="type">POJO</span>(<span class="type">Person</span><span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure></div><p>在Scala中，类是 <code>org.apache.flink.api.scala.typeutils.Types</code> ，举个例子：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TypeInformation for primitive types</span></span><br><span class="line"><span class="keyword">val</span> stringType: <span class="type">TypeInformation</span>[<span class="type">String</span>] = <span class="type">Types</span>.<span class="type">STRING</span></span><br><span class="line"><span class="comment">// TypeInformation for Scala Tuples</span></span><br><span class="line"><span class="keyword">val</span> tupleType: <span class="type">TypeInformation</span>[(<span class="type">Int</span>, <span class="type">Long</span>)] = <span class="type">Types</span>.<span class="type">TUPLE</span>[(<span class="type">Int</span>, <span class="type">Long</span>)]</span><br><span class="line"><span class="comment">// TypeInformation for case classes</span></span><br><span class="line"><span class="keyword">val</span> caseClassType: <span class="type">TypeInformation</span>[<span class="type">Person</span>] = <span class="type">Types</span>.<span class="type">CASE_CLASS</span>[<span class="type">Person</span>]</span><br></pre></td></tr></table></figure></div><blockquote><p>别忘了导入<code>import org.apache.flink.streaming.api.scala._</code></p></blockquote><h2 id="定义Key以及引用字段"><a href="#定义Key以及引用字段" class="headerlink" title="定义Key以及引用字段"></a>定义Key以及引用字段</h2><p>在Flink中，我们必须明确指定输入流中的元素中的哪一个字段是key。</p><h3 id="使用字段位置进行keyBy"><a href="#使用字段位置进行keyBy" class="headerlink" title="使用字段位置进行keyBy"></a>使用字段位置进行keyBy</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Long</span>)] = ...</span><br><span class="line"><span class="keyword">val</span> keyed = input.keyBy(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></div><blockquote><p>注意，要么明确写清楚类型注释，要么让Scala去做类型推断，不要用IDEA的类型推断功能。</p></blockquote><p>如果我们想要用元组的第2个字段和第3个字段做keyBy，可以看下面的例子。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> keyed2 = input.keyBy(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></div><h3 id="使用字段表达式来进行keyBy"><a href="#使用字段表达式来进行keyBy" class="headerlink" title="使用字段表达式来进行keyBy"></a>使用字段表达式来进行keyBy</h3><p>对于样例类：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SensorReading</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  id: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  timestamp: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  temperature: <span class="type">Double</span></span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">sensorStream</span></span>: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> keyedSensors = sensorStream.keyBy(<span class="string">"id"</span>)</span><br></pre></td></tr></table></figure></div><p>对于元组：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Long</span>)] = ...</span><br><span class="line"><span class="keyword">val</span> keyed1 = input.keyBy(<span class="string">"2"</span>) <span class="comment">// key by 3rd field</span></span><br><span class="line"><span class="keyword">val</span> keyed2 = input.keyBy(<span class="string">"_1"</span>) <span class="comment">// key by 1st field</span></span><br><span class="line"></span><br><span class="line"><span class="type">DataStream</span>&lt;<span class="type">Tuple3</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>, <span class="type">Long</span>&gt;&gt; javaInput = ...</span><br><span class="line">javaInput.keyBy(<span class="string">"f2"</span>) <span class="comment">// key Java tuple by 3rd field</span></span><br></pre></td></tr></table></figure></div><p>对于存在嵌套的样例类：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Address</span> (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  address: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  zip: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  country: <span class="type">String</span></span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Person</span> (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  name: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  birthday: (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span></span>), <span class="title">//</span> <span class="title">year</span>, <span class="title">month</span>, <span class="title">day</span></span></span><br><span class="line"><span class="class">  <span class="title">address</span></span>: <span class="type">Address</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> persons: <span class="type">DataStream</span>[<span class="type">Person</span>] = ...</span><br><span class="line">persons.keyBy(<span class="string">"address.zip"</span>) <span class="comment">// key by nested POJO field</span></span><br><span class="line">persons.keyBy(<span class="string">"birthday._1"</span>) <span class="comment">// key by field of nested tuple</span></span><br><span class="line">persons.keyBy(<span class="string">"birthday._"</span>) <span class="comment">// key by all fields of nested tuple</span></span><br></pre></td></tr></table></figure></div><h3 id="Key选择器"><a href="#Key选择器" class="headerlink" title="Key选择器"></a>Key选择器</h3><p>方法类型</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">KeySelector[IN, KEY]</span><br><span class="line">  &gt; getKey(IN): KEY</span><br></pre></td></tr></table></figure></div><p>两个例子</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorData: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> byId: <span class="type">KeyedStream</span>[<span class="type">SensorReading</span>, <span class="type">String</span>] = sensorData.keyBy(r =&gt; r.id)</span><br><span class="line"><span class="keyword">val</span> input: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = ...</span><br><span class="line"><span class="keyword">val</span> keyedStream = input.keyBy(value =&gt; math.max(value._1, value._2))</span><br></pre></td></tr></table></figure></div><h2 id="实现UDF函数，更细粒度的控制流"><a href="#实现UDF函数，更细粒度的控制流" class="headerlink" title="实现UDF函数，更细粒度的控制流"></a>实现UDF函数，更细粒度的控制流</h2><h3 id="函数类-Function-Classes"><a href="#函数类-Function-Classes" class="headerlink" title="函数类(Function Classes)"></a>函数类(Function Classes)</h3><p>Flink暴露了所有udf函数的接口(实现方式为接口或者抽象类)。例如MapFunction, FilterFunction, ProcessFunction等等。</p><p>例子实现了FilterFunction接口</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FilterFilter</span> <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    value.contains(<span class="string">"flink"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> flinkTweets = tweets.filter(<span class="keyword">new</span> <span class="type">FlinkFilter</span>)</span><br></pre></td></tr></table></figure></div><p>还可以将函数实现成匿名类</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> flinkTweets = tweets.filter(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">RichFilterFunction</span>[<span class="type">String</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">      value.contains(<span class="string">"flink"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></div><p>我们filter的字符串“flink”还可以当作参数传进去。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tweets: <span class="type">DataStream</span>[<span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> flinkTweets = tweets.filter(<span class="keyword">new</span> <span class="type">KeywordFilter</span>(<span class="string">"flink"</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KeywordFilter</span>(<span class="params">keyWord: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    value.contains(keyWord)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="匿名函数-Lambda-Functions"><a href="#匿名函数-Lambda-Functions" class="headerlink" title="匿名函数(Lambda Functions)"></a>匿名函数(Lambda Functions)</h3><p>匿名函数可以实现一些简单的逻辑，但无法实现一些高级功能，例如访问状态等等。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tweets: <span class="type">DataStream</span>[<span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> flinkTweets = tweets.filter(_.contains(<span class="string">"flink"</span>))</span><br></pre></td></tr></table></figure></div><h3 id="富函数-Rich-Functions"><a href="#富函数-Rich-Functions" class="headerlink" title="富函数(Rich Functions)"></a>富函数(Rich Functions)</h3><p>我们经常会有这样的需求：在函数处理数据之前，需要做一些初始化的工作；或者需要在处理数据时可以获得函数执行上下文的一些信息；以及在处理完数据时做一些清理工作。而DataStream API就提供了这样的机制。</p><p>DataStream API提供的所有转换操作函数，都拥有它们的“富”版本，并且我们在使用常规函数或者匿名函数的地方来使用富函数。例如下面就是富函数的一些例子，可以看出，只需要在常规函数的前面加上<code>Rich</code>前缀就是富函数了。</p><ul><li>RichMapFunction</li><li>RichFlatMapFunction</li><li>RichFilterFunction</li><li>…</li></ul><p>当我们使用富函数时，我们可以实现两个额外的方法：</p><ul><li>open()方法是rich function的初始化方法，当一个算子例如map或者filter被调用之前open()会被调用。open()函数通常用来做一些只需要做一次即可的初始化工作。</li><li>close()方法是生命周期中的最后一个调用的方法，通常用来做一些清理工作。</li></ul><p>另外，getRuntimeContext()方法提供了函数的RuntimeContext的一些信息，例如函数执行的并行度，当前子任务的索引，当前子任务的名字。同时还它还包含了访问<strong>分区状态</strong>的方法。下面看一个例子：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyFlatMap</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>[<span class="type">Int</span>, (<span class="type">Int</span>, <span class="type">Int</span>)] </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> subTaskIndex = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(configuration: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    subTaskIndex = getRuntimeContext.getIndexOfThisSubtask</span><br><span class="line">    <span class="comment">// 做一些初始化工作</span></span><br><span class="line">    <span class="comment">// 例如建立一个和HDFS的连接</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(in: <span class="type">Int</span>, out: <span class="type">Collector</span>[(<span class="type">Int</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (in % <span class="number">2</span> == subTaskIndex) &#123;</span><br><span class="line">      out.collect((subTaskIndex, in))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 清理工作，断开和HDFS的连接。</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><p>Flink没有类似于spark中foreach方法，让用户进行迭代的操作。所有对外的输出操作都要利用Sink完成。最后通过类似如下方式完成整个任务最终输出操作。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.addSink(<span class="keyword">new</span> <span class="type">MySink</span>(xxxx))</span><br></pre></td></tr></table></figure></div><p>官方提供了一部分的框架的sink。除此以外，需要用户自定义实现sink。</p><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><p>Kafka版本为0.11</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>Kafka版本为2.0以上</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>主函数中添加sink：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> union = high</span><br><span class="line">  .union(low)</span><br><span class="line">  .map(_.temperature.toString)</span><br><span class="line"></span><br><span class="line">union.addSink(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">FlinkKafkaProducer011</span>[<span class="type">String</span>](</span><br><span class="line">    <span class="string">"localhost:9092"</span>,</span><br><span class="line">    <span class="string">"test"</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure></div><h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>定义一个redis的mapper类，用于定义保存到redis时调用的命令：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">extends</span> <span class="title">RedisMapper</span>[<span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCommandDescription</span></span>: <span class="type">RedisCommandDescription</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">RedisCommandDescription</span>(<span class="type">RedisCommand</span>.<span class="type">HSET</span>, <span class="string">"sensor_temperature"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValueFromData</span></span>(t: <span class="type">SensorReading</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    t.temperature.toString</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKeyFromData</span></span>(t: <span class="type">SensorReading</span>): <span class="type">String</span> = t.id</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="ElasticSearch"><a href="#ElasticSearch" class="headerlink" title="ElasticSearch"></a>ElasticSearch</h3><p>在主函数中调用：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>在主函数中调用：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> httpHosts = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">HttpHost</span>]()</span><br><span class="line">httpHosts.add(<span class="keyword">new</span> <span class="type">HttpHost</span>(<span class="string">"localhost"</span>, <span class="number">9200</span>))</span><br><span class="line"><span class="keyword">val</span> esSinkBuilder = <span class="keyword">new</span> <span class="type">ElasticsearchSink</span>.<span class="type">Builder</span>[<span class="type">SensorReading</span>](</span><br><span class="line">  httpHosts,</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ElasticsearchSinkFunction</span>[<span class="type">SensorReading</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(t: <span class="type">SensorReading</span>,</span><br><span class="line">                         runtimeContext: <span class="type">RuntimeContext</span>,</span><br><span class="line">                         requestIndexer: <span class="type">RequestIndexer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">"saving data: "</span> + t)</span><br><span class="line">      <span class="keyword">val</span> json = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</span><br><span class="line">      json.put(<span class="string">"data"</span>, t.toString)</span><br><span class="line">      <span class="keyword">val</span> indexRequest = <span class="type">Requests</span></span><br><span class="line">        .indexRequest()</span><br><span class="line">        .index(<span class="string">"sensor"</span>)</span><br><span class="line">        .`<span class="class"><span class="keyword">type</span>`(<span class="params">"readingData"</span>)</span></span><br><span class="line"><span class="class">        .<span class="title">source</span>(<span class="params">json</span>)</span></span><br><span class="line"><span class="class">      <span class="title">requestIndexer</span>.<span class="title">add</span>(<span class="params">indexRequest</span>)</span></span><br><span class="line"><span class="class">      <span class="title">println</span>(<span class="params">"saved successfully"</span>)</span></span><br><span class="line"><span class="class">    &#125;</span></span><br><span class="line"><span class="class">  &#125;)</span></span><br><span class="line"><span class="class"><span class="title">dataStream</span>.<span class="title">addSink</span>(<span class="params">esSinkBuilder.build(</span>))</span></span><br></pre></td></tr></table></figure></div><h3 id="JDBC自定义sink"><a href="#JDBC自定义sink" class="headerlink" title="JDBC自定义sink"></a>JDBC自定义sink</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.44<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>添加MyJdbcSink</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyJdbcSink</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[<span class="type">SensorReading</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> conn: <span class="type">Connection</span> = _</span><br><span class="line">  <span class="keyword">var</span> insertStmt: <span class="type">PreparedStatement</span> = _</span><br><span class="line">  <span class="keyword">var</span> updateStmt: <span class="type">PreparedStatement</span> = _</span><br><span class="line">  <span class="comment">// open 主要是创建连接</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">super</span>.open(parameters)</span><br><span class="line">    conn = <span class="type">DriverManager</span>.getConnection(</span><br><span class="line">      <span class="string">"jdbc:mysql://localhost:3306/test"</span>,</span><br><span class="line">      <span class="string">"root"</span>,</span><br><span class="line">      <span class="string">"123456"</span>)</span><br><span class="line">    insertStmt = conn.prepareStatement(</span><br><span class="line">      <span class="string">"INSERT INTO temperatures (sensor, temp) VALUES (?, ?)"</span></span><br><span class="line">    )</span><br><span class="line">    updateStmt = conn.prepareStatement(</span><br><span class="line">      <span class="string">"UPDATE temperatures SET temp = ? WHERE sensor = ?"</span></span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 调用连接，执行sql</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(value: <span class="type">SensorReading</span>,</span><br><span class="line">                      context: <span class="type">SinkFunction</span>.<span class="type">Context</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    updateStmt.setDouble(<span class="number">1</span>, value.temperature)</span><br><span class="line">    updateStmt.setString(<span class="number">2</span>, value.id)</span><br><span class="line">    updateStmt.execute()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (updateStmt.getUpdateCount == <span class="number">0</span>) &#123;</span><br><span class="line">      insertStmt.setString(<span class="number">1</span>, value.id)</span><br><span class="line">      insertStmt.setDouble(<span class="number">2</span>, value.temperature)</span><br><span class="line">      insertStmt.execute()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    insertStmt.close()</span><br><span class="line">    updateStmt.close()</span><br><span class="line">    conn.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>在main方法中增加，把明细保存到mysql中</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.addSink(<span class="keyword">new</span> <span class="type">MyJdbcSink</span>())</span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第五章，Flink-DataStream-API&quot;&gt;&lt;a href=&quot;#第五章，Flink-DataStream-API&quot; class=&quot;headerlink&quot; title=&quot;第五章，Flink DataStream API&quot;&gt;&lt;/a&gt;第五章，Flink Data
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink系列04第一个Flink程序</title>
    <link href="https://masteryang4.github.io/2020/06/27/flink%E7%B3%BB%E5%88%9704%E7%AC%AC%E4%B8%80%E4%B8%AAFlink%E7%A8%8B%E5%BA%8F/"/>
    <id>https://masteryang4.github.io/2020/06/27/flink%E7%B3%BB%E5%88%9704%E7%AC%AC%E4%B8%80%E4%B8%AAFlink%E7%A8%8B%E5%BA%8F/</id>
    <published>2020-06-27T15:27:31.000Z</published>
    <updated>2020-06-27T15:28:02.624Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第四章，编写第一个Flink程序"><a href="#第四章，编写第一个Flink程序" class="headerlink" title="第四章，编写第一个Flink程序"></a>第四章，编写第一个Flink程序</h1><h2 id="在IDEA中编写Flink程序"><a href="#在IDEA中编写Flink程序" class="headerlink" title="在IDEA中编写Flink程序"></a>在IDEA中编写Flink程序</h2><p>本项目使用的Flink版本为最新版本，也就是1.10.0。现在提供maven项目的配置文件。</p><ol><li>使用Intellij IDEA创建一个Maven新项目</li><li>勾选<code>Create from archetype</code>，然后点击<code>Add Archetype</code>按钮</li><li><code>GroupId</code>中输入<code>org.apache.flink</code>，<code>ArtifactId</code>中输入<code>flink-quickstart-scala</code>，<code>Version</code>中输入<code>1.10.0</code>，然后点击<code>OK</code></li><li>点击向右箭头，出现下拉列表，选中<code>flink-quickstart-scala:1.10.0</code>，点击<code>Next</code></li><li><code>Name</code>中输入<code>FlinkTutorial</code>，<code>GroupId</code>中输入<code>com.atguigu</code>，<code>ArtifactId</code>中输入<code>FlinkTutorial</code>，点击<code>Next</code></li><li>最好使用IDEA默认的Maven工具：Bundled（Maven 3），点击<code>Finish</code>，等待一会儿，项目就创建好了</li></ol><p>编写<code>WordCount.scala</code>程序</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingJob</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Main program method */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) : <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get the execution environment</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span></span><br><span class="line">      .getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get input data by connecting to the socket</span></span><br><span class="line">    <span class="keyword">val</span> text: <span class="type">DataStream</span>[<span class="type">String</span>] = env</span><br><span class="line">      .socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>, '\n')</span><br><span class="line"></span><br><span class="line">    <span class="comment">// parse the data, group it, window it, and aggregate the counts</span></span><br><span class="line">    <span class="keyword">val</span> windowCounts = text</span><br><span class="line">      .flatMap &#123; w =&gt; w.split(<span class="string">"\\s"</span>) &#125;</span><br><span class="line">      .map &#123; w =&gt; <span class="type">WordWithCount</span>(w, <span class="number">1</span>) &#125;</span><br><span class="line">      .keyBy(<span class="string">"word"</span>)</span><br><span class="line">      .timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">      .sum(<span class="string">"count"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// print the results with a single thread, rather than in parallel</span></span><br><span class="line">    windowCounts</span><br><span class="line">      .print()</span><br><span class="line">      .setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"Socket Window WordCount"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Data type for words with count */</span></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">WordWithCount</span>(<span class="params">word: <span class="type">String</span>, count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure></div><p>打开一个终端（Terminal），运行以下命令</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nc -lk 9999</span><br></pre></td></tr></table></figure></div><p>接下来使用<code>IDEA</code>运行就可以了。</p><h2 id="下载Flink运行时环境，提交Jar包的运行方式"><a href="#下载Flink运行时环境，提交Jar包的运行方式" class="headerlink" title="下载Flink运行时环境，提交Jar包的运行方式"></a>下载Flink运行时环境，提交Jar包的运行方式</h2><p>下载链接：<a href="http://mirror.bit.edu.cn/apache/flink/flink-1.10.1/flink-1.10.1-bin-scala_2.11.tgz" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/flink/flink-1.10.1/flink-1.10.1-bin-scala_2.11.tgz</a></p><p>然后解压</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tar xvfz flink-1.10.0-bin-scala_2.11.tgz</span><br></pre></td></tr></table></figure></div><p>启动Flink集群</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd flink-1.10.0</span><br><span class="line">$ .&#x2F;bin&#x2F;start-cluster.sh</span><br></pre></td></tr></table></figure></div><p>可以打开Flink WebUI查看集群状态：<a href="http://localhost:8081" target="_blank" rel="noopener">http://localhost:8081</a></p><p>在<code>IDEA</code>中使用<code>maven package</code>打包。</p><p>提交打包好的<code>JAR</code>包</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd flink-1.10.0</span><br><span class="line">$ .&#x2F;bin&#x2F;flink run 打包好的JAR包的绝对路径</span><br></pre></td></tr></table></figure></div><p>停止Flink集群</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;stop-cluster.sh</span><br></pre></td></tr></table></figure></div><p>查看标准输出日志的位置，在<code>log</code>文件夹中。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cd flink-1.10.0&#x2F;log</span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第四章，编写第一个Flink程序&quot;&gt;&lt;a href=&quot;#第四章，编写第一个Flink程序&quot; class=&quot;headerlink&quot; title=&quot;第四章，编写第一个Flink程序&quot;&gt;&lt;/a&gt;第四章，编写第一个Flink程序&lt;/h1&gt;&lt;h2 id=&quot;在IDEA中编写F
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink系列03Flink运行架构</title>
    <link href="https://masteryang4.github.io/2020/06/27/flink%E7%B3%BB%E5%88%9703Flink%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/"/>
    <id>https://masteryang4.github.io/2020/06/27/flink%E7%B3%BB%E5%88%9703Flink%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/</id>
    <published>2020-06-27T15:26:18.000Z</published>
    <updated>2020-06-27T15:27:16.346Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第三章，Flink运行架构"><a href="#第三章，Flink运行架构" class="headerlink" title="第三章，Flink运行架构"></a>第三章，Flink运行架构</h1><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p>Flink是一个用于有状态的并行数据流处理的分布式系统。它由多个进程构成，这些进程一般会分布运行在不同的机器上。对于分布式系统来说，面对的常见问题有：集群中资源的分配和管理、进程协调调度、持久化和高可用的数据存储，以及故障恢复。</p><p>对于这些分布式系统的经典问题，业内已有比较成熟的解决方案和服务。所以Flink并不会自己去处理所有的问题，而是利用了现有的集群架构和服务，这样它就可以把精力集中在核心工作——分布式数据流处理上了。Flink与一些集群资源管理工具有很好的集成，比如Apache Mesos、YARN和Kubernetes；同时，也可以配置为独立（stand-alone）集群运行。Flink自己并不提供持久化的分布式存储，而是直接利用了已有的分布式文件系统（比如HDFS）或者对象存储（比如S3）。对于高可用的配置，Flink需要依靠Apache ZooKeeper来完成。</p><p>在本节中，我们将介绍Flink的不同组件，以及在运行程序时它们如何相互作用。我们会讨论部署Flink应用程序的两种模式，并且了解每种模式下分发和执行任务的方式。最后，我们还会解释一下Flink的高可用性模式是如何工作的。</p><h3 id="Flink运行时组件"><a href="#Flink运行时组件" class="headerlink" title="Flink运行时组件"></a>Flink运行时组件</h3><p>Flink运行时架构主要包括四个不同的组件，它们会在运行流处理应用程序时协同工作：作业管理器（JobManager）、资源管理器（ResourceManager）、任务管理器（TaskManager），以及分发器（Dispatcher）。因为Flink是用Java和Scala实现的，所以所有组件都会运行在Java虚拟机（JVMs）上。每个组件的职责如下：</p><ul><li>作业管理器（JobManager）是控制一个应用程序执行的主进程，也就是说，每个应用程序都会被一个不同的JobManager所控制执行。JobManager会先接收到要执行的应用程序。这个应用程序会包括：作业图（JobGraph）、逻辑数据流图（logical dataflow graph）和打包了所有的类、库和其它资源的JAR包。JobManager会把JobGraph转换成一个物理层面的数据流图，这个图被叫做“执行图”（ExecutionGraph），包含了所有可以并发执行的任务。JobManager会向资源管理器（ResourceManager）请求执行任务必要的资源，也就是任务管理器（TaskManager）上的插槽（slot）。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的TaskManager上。而在运行过程中，JobManager会负责所有需要中央协调的操作，比如说检查点（checkpoints）的协调。</li><li>ResourceManager主要负责管理任务管理器（TaskManager）的插槽（slot），TaskManger插槽是Flink中定义的处理资源单元。Flink为不同的环境和资源管理工具提供了不同资源管理器（ResourceManager），比如YARN、Mesos、K8s，以及standalone部署。当JobManager申请插槽资源时，ResourceManager会将有空闲插槽的TaskManager分配给JobManager。如果ResourceManager没有足够的插槽来满足JobManager的请求，它还可以向资源提供平台发起会话，以提供启动TaskManager进程的容器。另外，ResourceManager还负责终止空闲的TaskManager，释放计算资源。</li><li>任务管理器（TaskManager）是Flink中的工作进程。通常在Flink中会有多个TaskManager运行，每一个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager能够执行的任务数量。启动之后，TaskManager会向资源管理器注册它的插槽；收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用。JobManager就可以向插槽分配任务（tasks）来执行了。在执行过程中，一个TaskManager可以跟其它运行同一应用程序的TaskManager交换数据。任务的执行和插槽的概念会在“任务执行”一节做具体讨论。</li><li>分发器（Dispatcher）可以跨作业运行，它为应用提交提供了REST接口。当一个应用被提交执行时，分发器就会启动并将应用移交给一个JobManager。由于是REST接口，所以Dispatcher可以作为集群的一个HTTP接入点，这样就能够不受防火墙阻挡。Dispatcher也会启动一个Web UI，用来方便地展示和监控作业执行的信息。Dispatcher在架构中可能并不是必需的，这取决于应用提交运行的方式。</li></ul><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0301.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0301.png" class="lazyload"></a></p><blockquote><p>上图是从一个较为高层级的视角，来看应用中各组件的交互协作。如果部署的集群环境不同（例如YARN，Mesos，Kubernetes，standalone等），其中一些步骤可以被省略，或是有些组件会运行在同一个JVM进程中。</p></blockquote><h3 id="应用部署"><a href="#应用部署" class="headerlink" title="应用部署"></a>应用部署</h3><p>Flink应用程序可以用以下两种不同的方式部署：</p><p><em>框架（Framework）方式</em></p><p>在这个模式下，Flink应用被打包成一个Jar文件，并由客户端提交给一个运行服务（running service）。这个服务可以是一个Flink的Dispatcher，也可以是一个Flink的JobManager，或是Yarn的ResourceManager。如果application被提交给一个JobManager，则它会立即开始执行这个application。如果application被提交给了一个Dispatcher，或是Yarn ResourceManager，则它会启动一个JobManager，然后将application交给它，再由JobManager开始执行此应用。</p><p><em>库（Library）方式</em></p><p>在这个模式下，Flink Application 会被打包在一个容器（container） 镜像里，例如一个Docker 镜像。此镜像包含了运行JobManager和ResourceManager的代码。当一个容器从镜像启动后，它会自动启动ResourceManager和JobManager，并提交打包好的应用。另一种方法是：将应用打包到镜像后，只用于部署TaskManager容器。从镜像启动的容器会自动启动一个TaskManager，然后连接ResourceManager并注册它的slots。这些镜像的启动以及失败重启，通常都会由一个外部的资源管理器管理（比如Kubernetes）。</p><p>框架模式遵循了传统的任务提交方式，从客户端提交到Flink运行服务。而在库模式下，没有运行的Flink服务。它是将Flink作为一个库，与应用程序一同打包到了一个容器镜像。这种部署方式在微服务架构中较为常见。我们会在“运行管理流式应用程序”一节对这个话题做详细讨论。</p><h3 id="任务执行"><a href="#任务执行" class="headerlink" title="任务执行"></a>任务执行</h3><p>一个TaskManager可以同时执行多个任务（tasks）。这些任务可以是同一个算子（operator）的子任务（数据并行），也可以是来自不同算子的（任务并行），甚至可以是另一个不同应用程序的（作业并行）。TaskManager提供了一定数量的处理插槽（processing slots），用于控制可以并行执行的任务数。一个slot可以执行应用的一个分片，也就是应用中每一个算子的一个并行任务。图3-2展示了TaskManagers，slots，tasks以及operators之间的关系：</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0302.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0302.png" class="lazyload"></a></p><p>最左边是一个“作业图”（JobGraph），包含了5个算子——它是应用程序的非并行表示。其中算子A和C是数据源（source），E是输出端（sink）。C和E并行度为2，而其他的算子并行度为4。因为最高的并行度是4，所以应用需要至少四个slot来执行任务。现在有两个TaskManager，每个又各有两个slot，所以我们的需求是满足的。JobManager将JobGraph转化为“执行图”（ExecutionGraph），并将任务分配到四个可用的slot上。对于有4个并行任务的算子，它的task会分配到每个slot上。而对于并行度为2的operator C和E，它们的任务被分配到slot 1.1、2.1 以及 slot 1.2、2.2。将tasks调度到slots上，可以让多个tasks跑在同一个TaskManager内，也就可以是的tasks之间的数据交换更高效。然而将太多任务调度到同一个TaskManager上会导致TaskManager过载，继而影响效率。之后我们会在“控制任务调度”一节继续讨论如何控制任务的调度。</p><p>TaskManager在同一个JVM中以多线程的方式执行任务。线程较进程会更轻量级，但是线程之间并没有对任务进行严格隔离。所以，单个任务的异常行为有可能会导致整个TaskManager进程挂掉，当然也同时包括运行在此进程上的所有任务。通过为每个TaskManager配置单独的slot，就可以将应用在TaskManager上相互隔离开来。TaskManager内部有多线程并行的机制，而且在一台主机上可以部署多个TaskManager，所以Flink在资源配置上非常灵活，在部署应用时可以充分权衡性能和资源的隔离。我们将会在第九章对Flink集群的配置和搭建继续做详细讨论。</p><h3 id="高可用配置"><a href="#高可用配置" class="headerlink" title="高可用配置"></a>高可用配置</h3><p>流式应用程序一般被设计为7 x 24小时运行。所以很重要的一点是：即使出现了进程挂掉的情况，应用仍需要继续保持运行。为了从故障恢复，系统首先需要重启进程、然后重启应用并恢复它的状态。接下来，我们就来了解Flink如何重启失败的进程。</p><p><em>TaskManager故障</em></p><p>如前所述，Flink需要足够数目的slot，来执行一个应用的所有任务。假设一个Flink环境有4个TaskManager，每个提供2个插槽，那么流应用程序执行的最高并行度为8。如果其中一个TaskManager挂掉了，那么可用的slots会降到6。在这种情况下，JobManager会请求ResourceManager提供更多的slots。如果此请求无法满足——例如应用跑在一个standalone集群——那么JobManager在有足够的slots之前，无法重启应用。应用的重启策略决定了JobManager的重启频率，以及两次重启尝试之间的时间间隔。</p><p><em>JobManager故障</em></p><p>比TaskManager故障更严重的问题是JobManager故障。JobManager控制整个流应用程序的执行，并维护执行中的元数据——例如指向已完成检查点的指针。若是对应的JobManager挂掉，则流程序无法继续运行。所以这就导致在Flink应用中，JobManager是单点故障。为了解决这个问题，Flink提供了高可用模式。在原先的JobManager挂掉后，可以将一个作业的状态和元数据迁移到另一个JobManager，并继续执行。</p><p>Flink的高可用模式基于Apache ZooKeeper，我们知道，ZooKeeper是用来管理需要协调和共识的分布式服务的系统。Flink主要利用ZooKeeper来进行领导者（leader）的选举，并把它作为一个高可用和持久化的数据存储。当在高可用模式下运行时，JobManager会将JobGraph以及所有需要的元数据（例如应用程序的jar文件），写入到一个远程的持久化存储系统中。而且，JobManager会将指向存储位置的指针，写入到ZooKeeper的数据存储中。在执行一个应用的过程中，JobManager会接收每个独立任务检查点的状态句柄（也就是存储位置）。当一个检查点完成时（所有任务已经成功地将它们的状态写入到远程存储）， JobManager把状态句柄写入远程存储，并将指向这个远程存储的指针写入ZooKeeper。这样，一个JobManager挂掉之后再恢复，所需要的所有数据信息已经都保存在了远程存储，而ZooKeeper里存有指向此存储位置的指针。图3-3描述了这个设计：</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0303.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0303.png" class="lazyload"></a></p><p>当一个JobManager失败，所有属于这个应用的任务都会自动取消。一个新的JobManager接管工作，会执行以下操作：</p><ul><li>从ZooKeeper请求存储位置（storage location），从远端存储获取JobGraph，Jar文件，以及应用最近一次检查点（checkpoint）的状态句柄（state handles）</li><li>从ResourceManager请求slots，用来继续运行应用</li><li>重启应用，并将所有任务的状态，重设为最近一次已完成的检查点</li></ul><p>如果我们是在容器环境里运行应用（如Kubernetes），故障的JobManager或TaskManager 容器通常会由容器服务自动重启。当运行在YARN或Mesos之上时，JobManager或TaskManager进程会由Flink的保留进程自动触发重启。而在standalone模式下，Flink并未提供重启故障进程的工具。所以，此模式下我们可以增加备用（standby）的 JobManager和TaskManager，用于接管故障的进程。我们将会在“高可用配置”一节中做进一步讨论。</p><h2 id="Flink中的数据传输"><a href="#Flink中的数据传输" class="headerlink" title="Flink中的数据传输"></a>Flink中的数据传输</h2><p>运行中的应用任务，会持续不断地交换数据。TaskManager负责将数据从“发送任务”（sending tasks）传递到“接收任务”（receiving tasks）。TaskManager的网络组件会在缓冲区中收集数据，然后再将其发送，也就是说，数据不是逐条发送的，而是在缓冲区中“攒”成了一批。这种技术是有效利用网络资源和实现高吞吐量的基础，机制类似于网络或磁盘I/O协议中使用的缓冲技术。</p><blockquote><p>通过缓冲区来传递数据，意味着Flink的处理模型是基于微批的。</p></blockquote><p>每个TaskManager都有一个网络缓冲池（默认大小为32KB），用于发送和接收数据。如果发送任务和接收任务运行在不同的TaskManager进程中，那么它们会通过操作系统的网络栈来进行通信。流应用程序需要以管道方式传递数据，所以每对TaskManager之间都需要维护一个永久TCP连接，用来交换数据。在无序连接模式下，每个发送任务都需要能向任何接收任务传递数据。所以我们发现，TaskManager需要为每一个接收任务设置一个专用的网络缓冲区，因为其中的每一个任务都需要接收数据。图3-4展示了这种架构。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0304.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0304.png" class="lazyload"></a></p><p>如图3-4所示，四个发送任务中的每一个都需要至少四个网络缓冲区，用来向每个接收任务发送数据，而每个接收任务也需要至少四个缓冲区来接收数据。需要发送到另一个TaskManager的缓冲数据，会复用同一网络连接。为了实现平滑的管道数据传输，TaskManager必须能够提供足够的缓冲，来同时为所有传出和传入连接提供服务。对于无序或广播连接，每个发送任务都需要为每个接收任务提供一个缓冲；所以，所需缓冲区的数量是相关算子任务数量的平方。Flink网络缓冲区的默认配置足以满足中小型应用；对于更大的应用场景，就需要按照“主内存和网络缓冲区”一节中的叙述调整配置了。</p><p>当发送任务和接收任务在同一个TaskManager进程中运行时，发送任务会将传出的数据序列化，放入字节缓冲区，并在缓冲区填满后将其放入队列。接收任务从队列中提取缓冲数据并对其进行反序列化。因此，在同一个TaskManager上运行的任务，它们之间的数据传输不会导致网络通信。</p><p>Flink采用不同的技术来降低任务之间的通信成本。在下面的部分中，我们会简要讨论基于信任度（Credit）的流控制和任务链。</p><h3 id="基于信任度（credit）的流控制"><a href="#基于信任度（credit）的流控制" class="headerlink" title="基于信任度（credit）的流控制"></a>基于信任度（credit）的流控制</h3><p>通过网络连接来发送每条数据的效率很低，会导致很大的开销。为了充分利用网络连接的带宽，就需要进行缓冲了。在流处理的上下文中，缓冲的一个缺点是会增加延迟，因为数据需要在缓冲区中进行收集，而不是立即发送。</p><p>Flink实现了一个基于信任度的流量控制机制，其工作原理如下。接收任务授予发送任务一些“信任度”（credit），也就是为了接收其数据而保留的网络缓冲区数。当发送者收到一个信任度通知，它就会按照被授予的信任度，发送尽可能多的缓冲数据，并且同时发送目前积压数据的大小——也就是已填满并准备发送的网络缓冲的数量。接收者用保留的缓冲区处理发来的数据，并对发送者传来的积压量进行综合考量，为其所有连接的发送者确定下一个信用度授权的优先级。</p><p>基于信用度的流控制可以减少延迟，因为发送者可以在接收者有足够的资源接受数据时立即发送数据。此外，在数据倾斜的情况下，这样分配网络资源是一种很有效的机制，因为信用度是根据发送者积压数据量的规模授予的。因此，基于信用的流量控制是Flink实现高吞吐量和低延迟的重要组成部分。</p><h3 id="任务链（Task-Chaining）"><a href="#任务链（Task-Chaining）" class="headerlink" title="任务链（Task Chaining）"></a>任务链（Task Chaining）</h3><p>Flink采用了一种称为任务链的优化技术，可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须将两个或多个算子设为相同的并行度，并通过本地转发（local forward）的方式进行连接。图3-5所示的算子管道满足这些要求。它由三个算子组成，这些算子的任务并行度都被设为2，并且通过本地转发方式相连接。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0305.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0305.png" class="lazyload"></a></p><p>图3-6展示了管道以任务链方式运行的过程。算子的函数被融合成了一个单一的任务，由一个线程执行。由函数生成的数据通过一个简单的方法调用移交给下一个函数；这样在函数之间直接传递数据，基本上没有序列化和通信成本。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0306.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0306.png" class="lazyload"></a></p><p>任务链可以显著降低本地任务之间的通信成本，但也有一些场景，在没有链接的情况下运行管道操作是有意义的。例如，如果任务链中某个函数执行的开销巨大，那就可以将一条长的任务链管道断开，或者将一条链断开为两个任务，从而可以将这个开销大的函数调度到不同的槽（slots）中。图3-7显示了在没有任务链的情况下相同管道操作的执行情况。所有函数都由独立的单个任务来评估，每个任务都在专有的线程中运行。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0307.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0307.png" class="lazyload"></a></p><p>任务链在Flink中默认会启用。在“控制任务链”一节中，我们展示了如何禁用应用程序的任务链，以及如何控制各个算子的链接行为。</p><h2 id="事件时间（Event-Time）处理"><a href="#事件时间（Event-Time）处理" class="headerlink" title="事件时间（Event-Time）处理"></a>事件时间（Event-Time）处理</h2><p>在“时间语义”一节，我们重点强调了时间语义在流处理应用中的重要性，并且解释了处理时间（processing time）和事件时间（event time）的不同。处理时间比较好理解，因为它是基于处理器本地时间的；但同时，它会带来比较混乱、不一致、并且不可重现的结果。相比之下，事件时间语义能够产生可重现且一致的结果，这也是许多流处理场景希望解决的一大难题。但是，与处理时间应用程序相比，事件时间应用程序会更复杂，需要额外的配置。另外，支持事件时间的流处理器，也比纯粹在处理时间中运行的系统内部更为复杂。</p><p>Flink为常见的事件时间处理操作提供了直观且易于使用的原语，同时暴露了表达性很强的API，用户可以使用自定义算子实现更高级的事件时间应用程序。很好地理解Flink的内部时间处理，对于实现这样的高级应用程序会有很大帮助，有时也是必需的。上一章介绍了Flink利用两个概念来支持事件时间语义：记录时间戳（timestamps）和水位线（watermarks）。接下来，我们将描述Flink如何在内部实现并处理时间戳和水位线，进而支持具有事件时间语义的流式应用程序。</p><h3 id="时间戳（Timestamps）"><a href="#时间戳（Timestamps）" class="headerlink" title="时间戳（Timestamps）"></a>时间戳（Timestamps）</h3><p>由Flink事件时间流应用程序处理的所有记录都必须伴有时间戳。时间戳将数据与特定时间点相关联，通常就是数据所表示的事件发生的时间点。而只要时间戳大致跟数据流保持一致，基本上随着数据流的前进而增大，应用程序就可以自由选择时间戳的含义。不过正如“时间语义”一节中所讨论的，在现实场景中，时间戳基本上都是乱序的，所以采用“事件时间”而非“处理事件”往往会显得更为重要。</p><p>当Flink以事件时间模式处理数据流时，它会根据数据记录的时间戳来处理基于时间的算子。例如，时间窗口算子根据相关时间戳将数据分配给不同的时间窗口。Flink将时间戳编码为16字节的长整型值，并将其作为元数据附加到数据记录中。它的内置运算符会将这个长整型值解释为一个具有毫秒精度的Unix时间戳，也就是1970-01-01-00:00:00.000以来的毫秒数。当然，如果用户进行了自定义，那么运算符可以有自己的解释，例如，可以将精度调整到微秒。</p><h3 id="水位线-Watermarks"><a href="#水位线-Watermarks" class="headerlink" title="水位线(Watermarks)"></a>水位线(Watermarks)</h3><p>除了时间戳，基于事件时间的Flink应用程序还必须支持水位线（watermark）。在基于事件时间的应用中，水位线用于生成每个任务的当前事件时间。基于时间的算子使用这个“当前事件时间”来触发计算和处理操作。例如，一个时间窗口任务（time-window task）会在任务的事件时间超出窗口的关闭边界时，完成窗口计算，并输出计算结果。</p><p>在Flink中，水位线被实现为一条特殊的数据记录，它里面以长整型值保存了一个时间戳。水位线在带有时间戳的数据流中，跟随着其它数据一起流动，如图3-8所示。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0308.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0308.png" class="lazyload"></a></p><p>水位线有两个基本属性：</p><ul><li>必须单调递增，以确保任务的事件时间时钟在向前推进，而不是在后退。</li><li>它们与数据的时间戳相关。带有时间戳T的水位线表示，所有后续数据的时间戳都应该大于T。</li></ul><p>上面的第二个属性用于处理带有乱序时间戳的数据流，比如图3-8中时间戳3和5的数据。基于时间的算子任务会收集和处理数据（这些数据可能具有乱序的时间戳），并在事件时间时钟到达某个时刻时完成计算。这个时刻就表示数据收集的截止，具有之前时间戳的数据应该都已经到达、不再需要了；而其中的事件时间时钟，正是由当前接收到的水位线来指示的。如果任务再接收到的数据违反了watermark的这一属性，也就是时间戳小于以前接收到的水位线时，它所属的那部分计算可能已经完成了。这种数据被称为延迟数据（late records）。Flink提供了处理延迟数据的不同方式，我们会在“处理延迟数据”一节中讨论。</p><p>水位线还有一个很有趣的特性，它允许应用程序自己来平衡结果的完整性和延迟。如果水位线与数据的时间戳非常接近，那么我们可以得到较低的处理延迟，因为任务在完成计算之前只会短暂地等待更多数据到达。而同时，结果的完整性可能会受到影响，因为相关数据可能因为迟到而被视为“延迟数据”，这样就不会包含在结果中。相反，非常保守的水位线提供了足够的时间去等待所有数据到达，这样会增加处理延迟，但提高了结果的完整性。</p><h3 id="watermark的传递和事件时间"><a href="#watermark的传递和事件时间" class="headerlink" title="watermark的传递和事件时间"></a>watermark的传递和事件时间</h3><p>在本节中，我们将讨论算子如何处理水位线。Flink把watermark作为一条特殊的数据来实现，它也会由算子任务接收和发送。任务会有一个内部的时间服务，它会维护定时器，并在收到watermark时触发。任务可以在计时器服务中注册定时器，以便在将来特定的时间点执行计算。例如，窗口算子为每个活动窗口注册一个定时器，当事件时间超过窗口的结束时间时，该计时器将清除窗口的状态。</p><p>当任务收到watermark时，将执行以下操作：</p><ul><li>任务根据watermark的时间戳更新其内部事件时钟。</li><li>任务的时间服务会将所有过期的计时器标识出来，它们的时间小于当前的事件时间。对于每个过期的计时器，任务调用一个回调函数，该函数可以执行计算并发送结果。</li><li>任务会发出一个带有更新后的事件时间的watermark。</li></ul><blockquote><p>Flink限制通过DataStream API访问时间戳和watermark。函数不能读取或修改数据的时间戳和watermark，但底层的“处理函数”（process functions）除外，它们可以读取当前处理数据的时间戳、请求算子的当前事件时间，还可以注册定时器。通常的函数都不会暴露这些可以设置时间戳、操作任务事件时间时钟、或者发出水位线的API。而基于时间的数据流算子任务则会配置发送出的数据的时间戳，以确保它们能够与已到达的水位线平齐。例如，窗口计算完成后，时间窗口的算子任务会将窗口的结束时间作为时间戳附加到将要发送出的结果数据上，然后再使用触发窗口计算的时间戳发出watermark。</p></blockquote><p>现在，让我们更详细地解释一下任务在接收到新的watermark时，如何继续发送watermark并更新其事件时钟。正如我们在“数据并发和任务并发”中所了解的，Flink将数据流拆分为多个分区，并通过单独的算子任务并行地处理每个分区。每个分区都是一个流，里面包含了带着时间戳的数据和watermark。一个算子与它前置或后续算子的连接方式有多种情况，所以它对应的任务可以从一个或多个“输入分区”接收数据和watermark，同时也可以将数据和watermark发送到一个或多个“输出分区”。接下来，我们将详细描述一个任务如何向多个输出任务发送watermark，以及如何通过接收到的watermark来驱动事件时间时钟前进。</p><p>任务为每个输入分区维护一个分区水位线（watermark）。当从一个分区接收到watermark时，它会比较新接收到的值和当前水位值，然后将相应的分区watermark更新为两者的最大值。然后，任务会比较所有分区watermark的大小，将其事件时钟更新为所有分区watermark的最小值。如果事件时间时钟前进了，任务就将处理所有被触发的定时器操作，并向所有连接的输出分区发送出相应的watermark，最终将新的事件时间广播给所有下游任务。</p><p>图3-9显示了具有四个输入分区和三个输出分区的任务如何接收watermark、更新分区watermark和事件时间时钟，以及向下游发出watermark。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0309.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0309.png" class="lazyload"></a></p><p>具有两个或多个输入流（如Union或CoFlatMap）的算子任务（参见“多流转换”一节）也会以所有分区watermark的最小值作为事件时间时钟。它们并不区分不同输入流的分区watermark，所以两个输入流的数据都是基于相同的事件时间时钟进行处理的。当然我们可以想到，如果应用程序的各个输入流的事件时间不一致，那么这种处理方式可能会导致问题。</p><p>Flink的水位处理和传递算法，确保了算子任务发出的时间戳和watermark是“对齐”的。不过它依赖一个条件，那就是所有分区都会提供不断增长的watermark。一旦一个分区不再推进水位线的上升，或者完全处于空闲状态、不再发送任何数据和watermark，任务的事件时间时钟就将停滞不前，任务的定时器也就无法触发了。对于基于时间的算子来说，它们需要依赖时钟的推进来执行计算和清除状态，这种情况显然就会有问题。如果任务没有定期从所有输入任务接收到新的watermark，那么基于时间的算子的处理延迟和状态空间的大小都会显著增加。</p><p>对于具有两个输入流而且watermark明显不同的算子，也会出现类似的情况。具有两个输入流的任务的事件时间时钟，将会同较慢的那条流的watermark保持一致，而通常较快流的数据或者中间结果会在state中缓冲，直到事件时间时钟达到这条流的watermark，才会允许处理它们。</p><h3 id="时间戳的分配和水位线的产生"><a href="#时间戳的分配和水位线的产生" class="headerlink" title="时间戳的分配和水位线的产生"></a>时间戳的分配和水位线的产生</h3><p>我们已经解释了什么是时间戳和水位线，以及它们是如何由Flink内部处理的；然而我们还没有讨论它们的产生。流应用程序接收到数据流时，通常就会先分配时间戳并生成水位线（watermark）。因为时间戳的选择是由不同的应用程序决定的，而且watermark取决于时间戳和流的特性，所以应用程序必须首先显式地分配时间戳并生成watermark。Flink流应用程序可以通过三种方式分配时间戳和生成watermark：</p><ul><li>在数据源（source）处分配：当数据流被摄入到应用程序中时，可以由“源函数”SourceFunction分配和生成时间戳和watermark。SourceFunction可以产生并发送一个数据流；数据会与相关的时间戳一起发送出去，而watermark可以作为一条特殊数据在任何时间点发出。如果SourceFunction（暂时）不再发出watermark，它可以声明自己处于“空闲”（idle）状态。Flink会在后续算子的水位计算中，把空闲的SourceFunction产生的流分区排除掉。source的这一空闲机制，可以用来解决前面提到的水位不再上升的问题。源函数（Source Function）在“实现自定义源函数”一节中进行了更详细的讨论。</li><li>定期分配：在Flink中，DataStream API提供一个名为AssignerWithPeriodicWatermarks的用户定义函数，它可以从每个数据中提取时间戳，并被定期调用以生成当前watermark。提取出的时间戳被分配给相应的数据，而生成的watermark也会添加到流中。这个函数将在“分配时间戳和生成水位线”一节中讨论。</li><li>间断分配：AssignerWithPunctuatedWatermarks是另一个用户定义的函数，它同样会从每个数据中提取一个时间戳。它可以用于生成特殊输入数据中的watermark。与AssignerWithPeriodicWatermarks相比，此函数可以（但不是必须）从每个记录中提取watermark。我们在“分配时间戳和生成水位线”一节中同样讨论了该函数。</li></ul><p>用户定义的时间戳分配函数并没有严格的限制，通常会放在尽可能靠近source算子的位置，因为当经过一些算子处理后，数据及其时间戳的顺序就更加难以解释了。所以尽管我们可以在流应用程序的中段覆盖已有的时间戳和watermark——Flink通过用户定义的函数提供了这种灵活性，但这显然并不是推荐的做法。</p><h2 id="状态管理"><a href="#状态管理" class="headerlink" title="状态管理"></a>状态管理</h2><p>在第2章中，我们已经知道大多数流应用程序都是有状态的。许多算子会不断地读取和更新状态，例如在窗口中收集的数据、读取输入源的位置，或者像机器学习模型那样的用户定制化的算子状态。 Flink用同样的方式处理所有的状态，无论是内置的还是用户自定义的算子。本节我们将会讨论Flink支持的不同类型的状态，并解释“状态后端”是如何存储和维护状态的。</p><p>一般来说，由一个任务维护，并且用来计算某个结果的所有数据，都属于这个任务的状态。你可以认为状态就是一个本地变量，可以被任务的业务逻辑访问。图3-10显示了任务与其状态之间的交互。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0310.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0310.png" class="lazyload"></a></p><p>任务会接收一些输入数据。在处理数据时，任务可以读取和更新状态，并根据输入数据和状态计算结果。最简单的例子，就是统计接收到多少条数据的任务。当任务收到新数据时，它会访问状态以获取当前的计数，然后让计数递增，更新状态并发送出新的计数。</p><p>应用程序里，读取和写入状态的逻辑一般都很简单直接，而有效可靠的状态管理会复杂一些。这包括如何处理很大的状态——可能会超过内存，并且保证在发生故障时不会丢失任何状态。幸运的是，Flink会帮我们处理这相关的所有问题，包括状态一致性、故障处理以及高效存储和访问，以便开发人员可以专注于应用程序的逻辑。</p><p>在Flink中，状态始终与特定算子相关联。为了使运行时的Flink了解算子的状态，算子需要预先注册其状态。总的说来，有两种类型的状态：算子状态（operator state）和键控状态（keyed state），它们有着不同的范围访问，我们将在下面展开讨论。</p><h3 id="算子状态"><a href="#算子状态" class="headerlink" title="算子状态"></a>算子状态</h3><p>算子状态的作用范围限定为算子任务。这意味着由同一并行任务所处理的所有数据都可以访问到相同的状态，状态对于同一任务而言是共享的。算子状态不能由相同或不同算子的另一个任务访问。图3-11显示了任务如何访问算子状态。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0311.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0311.png" class="lazyload"></a></p><p>Flink为算子状态提供三种基本数据结构：</p><h4 id="列表状态（List-state）"><a href="#列表状态（List-state）" class="headerlink" title="列表状态（List state）"></a>列表状态（List state）</h4><p>将状态表示为一组数据的列表。</p><h4 id="联合列表状态（Union-list-state）"><a href="#联合列表状态（Union-list-state）" class="headerlink" title="联合列表状态（Union list state）"></a>联合列表状态（Union list state）</h4><p>也将状态表示为数据的列表。它与常规列表状态的区别在于，在发生故障时，或者从保存点（savepoint）启动应用程序时如何恢复。我们将在后面继续讨论。</p><h4 id="广播状态（Broadcast-state）"><a href="#广播状态（Broadcast-state）" class="headerlink" title="广播状态（Broadcast state）"></a>广播状态（Broadcast state）</h4><p>如果一个算子有多项任务，而它的每项任务状态又都相同，那么这种特殊情况最适合应用广播状态。在保存检查点和重新调整算子并行度时，会用到这个特性。这两部分内容将在本章后面讨论。</p><h3 id="键控状态（Keyed-State）"><a href="#键控状态（Keyed-State）" class="headerlink" title="键控状态（Keyed State）"></a>键控状态（Keyed State）</h3><p>顾名思义，键控状态是根据输入数据流中定义的键（key）来维护和访问的。Flink为每个键值维护一个状态实例，并将具有相同键的所有数据，都分区到同一个算子任务中，这个任务会维护和处理这个key对应的状态。当任务处理一条数据时，它会自动将状态的访问范围限定为当前数据的key。因此，具有相同key的所有数据都会访问相同的状态。图3-12显示了任务如何与键控状态进行交互。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0312.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0312.png" class="lazyload"></a></p><p>我们可以将键控状态看成是在算子所有并行任务上，对键进行分区（或分片）之后的一个键值映射（key-value map）。 Flink为键控状态提供不同的数据结构，用于确定map中每个key存储的值的类型。我们简单了解一下最常见的键控状态。</p><h4 id="值状态（Value-state）"><a href="#值状态（Value-state）" class="headerlink" title="值状态（Value state）"></a>值状态（Value state）</h4><p>为每个键存储一个任意类型的单个值。复杂数据结构也可以存储为值状态。</p><h4 id="列表状态（List-state）-1"><a href="#列表状态（List-state）-1" class="headerlink" title="列表状态（List state）"></a>列表状态（List state）</h4><p>为每个键存储一个值的列表。列表里的每个数据可以是任意类型。</p><h4 id="映射状态（Map-state）"><a href="#映射状态（Map-state）" class="headerlink" title="映射状态（Map state）"></a>映射状态（Map state）</h4><p>为每个键存储一个键值映射（map）。map的key和value可以是任意类型。</p><p>状态的数据结构可以让Flink实现更有效的状态访问。我们将在“在运行时上下文（RuntimeContext）中声明键控状态”中做进一步讨论。</p><h3 id="状态后端（State-Backends）"><a href="#状态后端（State-Backends）" class="headerlink" title="状态后端（State Backends）"></a>状态后端（State Backends）</h3><p>每传入一条数据，有状态的算子任务都会读取和更新状态。由于有效的状态访问对于处理数据的低延迟至关重要，因此每个并行任务都会在本地维护其状态，以确保快速的状态访问。状态到底是如何被存储、访问以及维护的？这件事由一个可插入的组件决定，这个组件就叫做状态后端（state backend）。状态后端主要负责两件事：本地的状态管理，以及将检查点（checkpoint）状态写入远程存储。</p><p>对于本地状态管理，状态后端会存储所有键控状态，并确保所有的访问都被正确地限定在当前键范围。 Flink提供了默认的状态后端，会将键控状态作为内存中的对象进行管理，将它们存储在JVM堆上。另一种状态后端则会把状态对象进行序列化，并将它们放入RocksDB中，然后写入本地硬盘。第一种方式可以提供非常快速的状态访问，但它受内存大小的限制；而访问RocksDB状态后端存储的状态速度会较慢，但其状态可以增长到非常大。</p><p>状态检查点的写入也非常重要，这是因为Flink是一个分布式系统，而状态只能在本地维护。 TaskManager进程（所有任务在其上运行）可能在任何时间点挂掉。因此，它的本地存储只能被认为是不稳定的。状态后端负责将任务的状态检查点写入远程的持久存储。写入检查点的远程存储可以是分布式文件系统，也可以是数据库。不同的状态后端在状态检查点的写入机制方面有所不同。例如，RocksDB状态后端支持增量的检查点，这对于非常大的状态来说，可以显著减少状态检查点写入的开销。</p><p>我们将在“选择状态后端”一节中更详细地讨论不同的状态后端及其优缺点。</p><h3 id="调整有状态算子的并行度"><a href="#调整有状态算子的并行度" class="headerlink" title="调整有状态算子的并行度"></a>调整有状态算子的并行度</h3><p>流应用程序的一个常见要求是，为了增大或较小输入数据的速率，需要灵活地调整算子的并行度。对于无状态算子而言，并行度的调整没有任何问题，但更改有状态算子的并行度显然就没那么简单了，因为它们的状态需要重新分区并分配给更多或更少的并行任务。 Flink支持四种模式来调整不同类型的状态。</p><p>具有键控状态的算子通过将键重新分区为更少或更多任务来缩放并行度。不过，并行度调整时任务之间会有一些必要的状态转移。为了提高效率，Flink并不会对单独的key做重新分配，而是用所谓的“键组”（key group）把键管理起来。键组是key的分区形式，同时也是Flink为任务分配key的方式。图3-13显示了如何在键组中重新分配键控状态。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0313.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0313.png" class="lazyload"></a></p><p>具有算子列表状态的算子，会通过重新分配列表中的数据项目来进行并行度缩放。从概念上讲，所有并行算子任务的列表项目会被收集起来，并将其均匀地重新分配给更少或更多的任务。如果列表条目少于算子的新并行度，则某些任务将以空状态开始。图3-14显示了算子列表状态的重新分配。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0314.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0314.png" class="lazyload"></a></p><p>具有算子联合列表状态的算子，会通过向每个任务广播状态的完整列表，来进行并行度的缩放。然后，任务可以选择要使用的状态项和要丢弃的状态项。图3-15显示了如何重新分配算子联合列表状态。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0315.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0315.png" class="lazyload"></a></p><p>具有算子广播状态的算子，通过将状态复制到新任务，来增大任务的并行度。这是没问题的，因为广播状态保证了所有任务都具有相同的状态。而对于缩小并行度的情况，我们可以直接取消剩余任务，因为状态是相同的，已经被复制并且不会丢失。图3-16显示了算子广播状态的重新分配。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0316.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0316.png" class="lazyload"></a></p><h2 id="检查点，保存点和状态恢复"><a href="#检查点，保存点和状态恢复" class="headerlink" title="检查点，保存点和状态恢复"></a>检查点，保存点和状态恢复</h2><p>Flink是一个分布式数据处理系统，因此必须有一套机制处理各种故障，比如被杀掉的进程，故障的机器和中断的网络连接。任务都是在本地维护状态的，所以Flink必须确保状态不会丢失，并且在发生故障时能够保持一致。</p><p>在本节中，我们将介绍Flink的检查点（checkpoint）和恢复机制，这保证了“精确一次”（exactly-once）的状态一致性。我们还会讨论Flink独特的保存点（savepoint）功能，这是一个“瑞士军刀”式的工具，可以解决许多操作数据流时面对的问题。</p><h3 id="一致的检查点（Checkpoints）"><a href="#一致的检查点（Checkpoints）" class="headerlink" title="一致的检查点（Checkpoints）"></a>一致的检查点（Checkpoints）</h3><p>Flink的恢复机制的核心，就是应用状态的一致检查点。有状态流应用的一致检查点，其实就是所有任务状态在某个时间点的一份拷贝，而这个时间点应该是所有任务都恰好处理完一个相同的输入数据的时候。这个过程可以通过一致检查点的一个简单算法步骤来解释。这个算法的步骤是：</p><ul><li>暂停所有输入流的摄取，也就是不再接收新数据的输入。</li><li>等待所有正在处理的数据计算完毕，这意味着结束时，所有任务都已经处理了所有输入数据。</li><li>通过将每个任务的状态复制到远程持久存储，来得到一个检查点。所有任务完成拷贝操作后，检查点就完成了。</li><li>恢复所有输入流的摄取。</li></ul><p>需要注意，Flink实现的并不是这种简单的机制。我们将在本节后面介绍Flink更精妙的检查点算法。</p><p>图3-17显示了一个简单应用中的一致检查点。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0317.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0317.png" class="lazyload"></a></p><p>上面的应用程序中具有单一的输入源（source）任务，输入数据就是一组不断增长的数字的流——1,2,3等。数字流被划分为偶数流和奇数流。求和算子（sum）的两个任务会分别实时计算当前所有偶数和奇数的总和。源任务会将其输入流的当前偏移量存储为状态，而求和任务则将当前的总和值存储为状态。在图3-17中，Flink在输入偏移量为5时，将检查点写入了远程存储，当前的总和为6和9。</p><h3 id="从一致检查点中恢复状态"><a href="#从一致检查点中恢复状态" class="headerlink" title="从一致检查点中恢复状态"></a>从一致检查点中恢复状态</h3><p>在执行流应用程序期间，Flink会定期检查状态的一致检查点。如果发生故障，Flink将会使用最近的检查点来一致恢复应用程序的状态，并重新启动处理流程。图3-18显示了恢复过程。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0318.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0318.png" class="lazyload"></a></p><p>应用程序从检查点的恢复分为三步：</p><ul><li>重新启动整个应用程序。</li><li>将所有的有状态任务的状态重置为最近一次的检查点。</li><li>恢复所有任务的处理。</li></ul><p>这种检查点的保存和恢复机制可以为应用程序状态提供“精确一次”（exactly-once）的一致性，因为所有算子都会保存检查点并恢复其所有状态，这样一来所有的输入流就都会被重置到检查点完成时的位置。至于数据源是否可以重置它的输入流，这取决于其实现方式和消费流数据的外部接口。例如，像Apache Kafka这样的事件日志系统可以提供流上之前偏移位置的数据，所以我们可以将源重置到之前的偏移量，重新消费数据。而从套接字（socket）消费数据的流就不能被重置了，因为套接字的数据一旦被消费就会丢弃掉。因此，对于应用程序而言，只有当所有的输入流消费的都是可重置的数据源时，才能确保在“精确一次”的状态一致性下运行。</p><p>从检查点重新启动应用程序后，其内部状态与检查点完成时的状态完全相同。然后它就会开始消费并处理检查点和发生故障之间的所有数据。尽管这意味着Flink会对一些数据处理两次（在故障之前和之后），我们仍然可以说这个机制实现了精确一次的一致性语义，因为所有算子的状态都已被重置，而重置后的状态下还不曾看到这些数据。</p><p>我们必须指出，Flink的检查点保存和恢复机制仅仅可以重置流应用程序的内部状态。对于应用中的一些的输出（sink）算子，在恢复期间，某些结果数据可能会多次发送到下游系统，比如事件日志、文件系统或数据库。对于某些存储系统，Flink提供了具有精确一次输出功能的sink函数，比如，可以在检查点完成时提交发出的记录。另一种适用于许多存储系统的方法是幂等更新。在“应用程序一致性保证”一节中，我们还会详细讨论如何解决应用程序端到端的精确一次一致性问题。</p><h3 id="Flink的检查点算法"><a href="#Flink的检查点算法" class="headerlink" title="Flink的检查点算法"></a>Flink的检查点算法</h3><p>Flink的恢复机制，基于它的一致性检查点。前面我们已经了解了从流应用中创建检查点的简单方法——先暂停应用，保存检查点，然后再恢复应用程序，这种方法很好理解，但它的理念是“停止一切”，这对于即使是中等延迟要求的应用程序而言也是不实用的。所以Flink没有这么简单粗暴，而是基于Chandy-Lamport算法实现了分布式快照的检查点保存。该算法并不会暂停整个应用程序，而是将检查点的保存与数据处理分离，这样就可以实现在其它任务做检查点状态保存状态时，让某些任务继续进行而不受影响。接下来我们将解释此算法的工作原理。</p><p>Flink的检查点算法用到了一种称为“检查点分界线”（checkpoint barrier）的特殊数据形式。与水位线（watermark）类似，检查点分界线由source算子注入到常规的数据流中，它的位置是限定好的，不能超过其他数据，也不能被后面的数据超过。检查点分界线带有检查点ID，用来标识它所属的检查点；这样，这个分界线就将一条流逻辑上分成了两部分。分界线之前到来的数据导致的状态更改，都会被包含在当前分界线所属的检查点中；而基于分界线之后的数据导致的所有更改，就会被包含在之后的检查点中。</p><p>我们用一个简单的流应用程序作为示例，来一步一步解释这个算法。该应用程序有两个源（source）任务，每个任务都消费一个增长的数字流。源任务的输出被划分为两部分：偶数和奇数的流。每个分区由一个任务处理，该任务计算所有收到的数字的总和，并将更新的总和转发给输出（sink）任务。这个应用程序的结构如图3-19所示。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0319.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0319.png" class="lazyload"></a></p><p>JobManager会向每个数据源（source）任务发送一条带有新检查点ID的消息，通过这种方式来启动检查点，如图3-20所示。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0320.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0320.png" class="lazyload"></a></p><p>当source任务收到消息时，它会暂停发出新的数据，在状态后端触发本地状态的检查点保存，并向所有传出的流分区广播带着检查点ID的分界线（barriers）。状态后端在状态检查点完成后会通知任务，而任务会向JobManager确认检查点完成。在发出所有分界线后，source任务就可以继续常规操作，发出新的数据了。通过将分界线注入到输出流中，源函数（source function）定义了检查点在流中所处的位置。图3-21显示了两个源任务将本地状态保存到检查点，并发出检查点分界线之后的流应用程序。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0321.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0321.png" class="lazyload"></a></p><p>源任务发出的检查点分界线（barrier），将被传递给所连接的任务。与水位线（watermark）类似，barrier会被广播到所有连接的并行任务，以确保每个任务从它的每个输入流中都能接收到。当任务收到一个新检查点的barrier时，它会等待这个检查点的所有输入分区的barrier到达。在等待的过程中，任务并不会闲着，而是会继续处理尚未提供barrier的流分区中的数据。对于那些barrier已经到达的分区，如果继续有新的数据到达，它们就不会被立即处理，而是先缓存起来。这个等待所有分界线到达的过程，称为“分界线对齐”（barrier alignment），如图3-22所示。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0322.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0322.png" class="lazyload"></a></p><p>当任务从所有输入分区都收到barrier时，它就会在状态后端启动一个检查点的保存，并继续向所有下游连接的任务广播检查点分界线，如图3-23所示。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0323.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0323.png" class="lazyload"></a></p><p>所有的检查点barrier都发出后，任务就开始处理之前缓冲的数据。在处理并发出所有缓冲数据之后，任务就可以继续正常处理输入流了。图3-24显示了此时的应用程序。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0324.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0324.png" class="lazyload"></a></p><p>最终，检查点分界线会到达输出（sink）任务。当sink任务接收到barrier时，它也会先执行“分界线对齐”，然后将自己的状态保存到检查点，并向JobManager确认已接收到barrier。一旦从应用程序的所有任务收到一个检查点的确认信息，JobManager就会将这个检查点记录为已完成。图3-25显示了检查点算法的最后一步。这样，当发生故障时，我们就可以用已完成的检查点恢复应用程序了。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0325.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0325.png" class="lazyload"></a></p><h3 id="检查点的性能影响"><a href="#检查点的性能影响" class="headerlink" title="检查点的性能影响"></a>检查点的性能影响</h3><p>Flink的检查点算法可以在不停止整个应用程序的情况下，生成一致的分布式检查点。但是，它可能会增加应用程序的处理延迟。Flink对此有一些调整措施，可以在某些场景下显得对性能的影响没那么大。</p><p>当任务将其状态保存到检查点时，它其实处于一个阻塞状态，而此时新的输入会被缓存起来。由于状态可能变得非常大，而且检查点需要通过网络将数据写入远程存储系统，检查点的写入很容易就会花费几秒到几分钟的时间——这对于要求低延迟的应用程序而言，显然是不可接受的。在Flink的设计中，真正负责执行检查点写入的，其实是状态后端。具体怎样复制任务的状态，取决于状态后端的实现方式。例如，文件系统（FileSystem）状态后端和RocksDB状态后端都支持了异步（asynchronous）检查点。触发检查点操作时，状态后端会先创建状态的本地副本。本地拷贝完成后，任务就将继续常规的数据处理，这往往并不会花费太多时间。一个后台线程会将本地快照异步复制到远程存储，并在完成检查点后再回来通知任务。异步检查点的机制，显著减少了任务继续处理数据之前的等待时间。此外，RocksDB状态后端还实现了增量的检查点，这样可以大大减少要传输的数据量。</p><p>为了减少检查点算法对处理延迟的影响，另一种技术是调整分界线对齐的步骤。对于需要非常低的延迟、并且可以容忍“至少一次”（at-least-once）状态保证的应用程序，Flink可以将检查点算法配置为，在等待barrier对齐期间处理所有到达的数据，而不是把barrier已经到达的那些分区的数据缓存起来。当检查点的所有barrier到达，算子任务就会将状态写入检查点——当然，现在的状态中，就可能包括了一些“提前”的更改，这些更改由本该属于下一个检查点的数据到来时触发。如果发生故障，从检查点恢复时，就将再次处理这些数据：这意味着检查点现在提供的是“至少一次”（at-least-once）而不是“精确一次”（exactly-once）的一致性保证。</p><h3 id="保存点（Savepoints）"><a href="#保存点（Savepoints）" class="headerlink" title="保存点（Savepoints）"></a>保存点（Savepoints）</h3><p>Flink的恢复算法是基于状态检查点的。Flink根据可配置的策略，定期保存并自动丢弃检查点。检查点的目的是确保在发生故障时可以重新启动应用程序，所以当应用程序被显式地撤销（cancel）时，检查点会被删除掉。除此之外，应用程序状态的一致性快照还可用于除故障恢复之外的更多功能。</p><p>Flink中一个最有价值，也是最独特的功能是保存点（savepoints）。原则上，创建保存点使用的算法与检查点完全相同，因此保存点可以认为就是具有一些额外元数据的检查点。 Flink不会自动创建保存点，因此用户（或者外部调度程序）必须明确地触发创建操作。同样，Flink也不会自动清理保存点。第10章将会具体介绍如何触发和处理保存点。</p><h4 id="使用保存点"><a href="#使用保存点" class="headerlink" title="使用保存点"></a>使用保存点</h4><p>有了应用程序和与之兼容的保存点，我们就可以从保存点启动应用程序了。这会将应用程序的状态初始化为保存点的状态，并从保存点创建时的状态开始运行应用程序。虽然看起来这种行为似乎与用检查点从故障中恢复应用程序完全相同，但实际上故障恢复只是一种特殊情况，它只是在相同的集群上以相同的配置启动相同的应用程序。而从保存点启动应用程序会更加灵活，这就可以让我们做更多事情了。</p><ul><li>可以从保存点启动不同但兼容的应用程序。这样一来，我们就可以及时修复应用程序中的逻辑bug，并让流式应用的源尽可能多地提供之前发生的事件，然后重新处理，以便修复之前的计算结果。修改后的应用程序还可用于运行A / B测试，或者具有不同业务逻辑的假设场景。这里要注意，应用程序和保存点必须兼容才可以这么做——也就是说，应用程序必须能够加载保存点的状态。</li><li>可以使用不同的并行度来启动相同的应用程序，可以将应用程序的并行度增大或减小。</li><li>可以在不同的集群上启动同样的应用程序。这非常有意义，意味着我们可以将应用程序迁移到较新的Flink版本或不同的集群上去。</li><li>可以使用保存点暂停应用程序，稍后再恢复。这样做的意义在于，可以为更高优先级的应用程序释放集群资源，或者在输入数据不连续生成时释放集群资源。</li><li>还可以将保存点设置为某一版本，并归档（archive）存储应用程序的状态。</li></ul><p>保存点是非常强大的功能，所以许多用户会定期创建保存点以便能够及时退回之前的状态。我们见到的各种场景中，保存点一个最有趣的应用是不断将流应用程序迁移到更便宜的数据中心上去。</p><h4 id="从保存点启动应用程序"><a href="#从保存点启动应用程序" class="headerlink" title="从保存点启动应用程序"></a>从保存点启动应用程序</h4><p>前面提到的保存点的所有用例，都遵循相同的模式。那就是首先创建正在运行的应用程序的保存点，然后在一个新启动的应用程序中用它来恢复状态。之前我们已经知道，保存点的创建和检查点非常相似，而接下来我们就将介绍对于一个从保存点启动的应用程序，Flink如何初始化其状态。</p><p>应用程序由多个算子组成。每个算子可以定义一个或多个键控状态和算子状态。算子由一个或多个算子任务并行执行。因此，一个典型的应用程序会包含多个状态，这些状态分布在多个算子任务中，这些任务可以运行在不同的TaskManager进程上。</p><p>图3-26显示了一个具有三个算子的应用程序，每个算子执行两个算子任务。一个算子（OP-1）具有单一的算子状态（OS-1），而另一个算子（OP-2）具有两个键控状态（KS-1和KS-2）。当保存点创建时，会将所有任务的状态复制到持久化的存储位置。</p><p>保存点中的状态拷贝会以算子标识符（operator ID）和状态名称（state name）组织起来。算子ID和状态名称必须能够将保存点的状态数据，映射到一个正在启动的应用程序的算子状态。从保存点启动应用程序时，Flink会将保存点的数据重新分配给相应的算子任务。</p><blockquote><p>请注意，保存点不包含有关算子任务的信息。这是因为当应用程序以不同的并行度启动时，任务数量可能会更改。</p></blockquote><p>如果我们要从保存点启动一个修改过的应用程序，那么保存点中的状态只能映射到符合标准的应用程序——它里面的算子必须具有相应的ID和状态名称。默认情况下，Flink会自动分配唯一的算子ID。然而，一个算子的ID，是基于它之前算子的ID确定性地生成的。因此，算子的ID会在其前序算子改变时改变，比如，当我们添加了新的或移除掉一个算子时，前序算子ID改变，当前算子ID就会变化。所以对于具有默认算子ID的应用程序而言，如果想在不丢失状态的前提下升级，就会受到极大的限制。因此，我们强烈建议在程序中为算子手动分配唯一ID，而不是依靠Flink的默认分配。我们将在“指定唯一的算子标识符”一节中详细说明如何分配算子标识符。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第三章，Flink运行架构&quot;&gt;&lt;a href=&quot;#第三章，Flink运行架构&quot; class=&quot;headerlink&quot; title=&quot;第三章，Flink运行架构&quot;&gt;&lt;/a&gt;第三章，Flink运行架构&lt;/h1&gt;&lt;h2 id=&quot;系统架构&quot;&gt;&lt;a href=&quot;#系统架构&quot;
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink系列02流处理基础</title>
    <link href="https://masteryang4.github.io/2020/06/27/flink%E7%B3%BB%E5%88%9702%E6%B5%81%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80/"/>
    <id>https://masteryang4.github.io/2020/06/27/flink%E7%B3%BB%E5%88%9702%E6%B5%81%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80/</id>
    <published>2020-06-27T15:24:42.000Z</published>
    <updated>2020-06-27T15:25:52.705Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第二章，流处理基础"><a href="#第二章，流处理基础" class="headerlink" title="第二章，流处理基础"></a>第二章，流处理基础</h1><h2 id="数据流编程简介"><a href="#数据流编程简介" class="headerlink" title="数据流编程简介"></a>数据流编程简介</h2><p>在我们深入研究流处理的基础知识之前，让我们来看看在数据流程编程的背景和使用的术语。</p><h3 id="数据流图-dataflow-graph"><a href="#数据流图-dataflow-graph" class="headerlink" title="数据流图(dataflow graph)"></a>数据流图(dataflow graph)</h3><p>顾名思义，数据流程序描述了数据如何在算子之间流动。数据流程序通常表示为有向图，其中节点称为算子，用来表示计算，边表示数据之间的依赖性。算子是数据流程序的基本功能单元。他们从输入消耗数据，对它们执行计算，并生成数据输出用于进一步处理。一个数据流图必须至少有一个数据源和一个数据接收器。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0201.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0201.png" class="lazyload"></a></p><p>像图2-1中的数据流图被称为逻辑流图，因为它们表示了计算逻辑的高级视图。为了执行一个数据流程序，Flink会将逻辑流图转换为物理数据流图，详细说明程序的执行方式。例如，如果我们使用分布式处理引擎，每个算子在不同的物理机器可能有几个并行的任务运行。图2-2显示了图2-1逻辑图的物理数据流图。而在逻辑数据流图中节点表示算子，在物理数据流图中，节点是任务。“Extract hashtags”和“Count”算子有两个并行算子任务，每个算子任务对输入数据的子集执行计算。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0202.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0202.png" class="lazyload"></a></p><h3 id="数据并行和任务并行"><a href="#数据并行和任务并行" class="headerlink" title="数据并行和任务并行"></a>数据并行和任务并行</h3><p>我们可以以不同方式利用数据流图中的并行性。第一，我们可以对输入数据进行分区，并在数据的子集上并行执行具有相同算子的任务并行。这种类型的并行性被称为数据并行性。数据并行是有用的，因为它允许处理大量数据，并将计算分散到不同的计算节点上。第二，我们可以将不同的算子在相同或不同的数据上并行执行。这种并行性称为任务并行性。使用任务并行性，我们可以更好地利用计算资源。</p><h3 id="数据交换策略"><a href="#数据交换策略" class="headerlink" title="数据交换策略"></a>数据交换策略</h3><p>数据交换策略定义了在物理执行流图中如何将数据分配给任务。数据交换策略可以由执行引擎自动选择，具体取决于算子的语义或我们明确指定的语义。在这里，我们简要回顾一些常见的数据交换策略，如图2-3所示。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0203.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0203.png" class="lazyload"></a></p><ul><li>前向策略将数据从一个任务发送到接收任务。如果两个任务都位于同一台物理计算机上（这通常由任务调度器确保），这种交换策略可以避免网络通信。</li><li>广播策略将所有数据发送到算子的所有的并行任务上面去。因为这种策略会复制数据和涉及网络通信，所以代价相当昂贵。</li><li>基于键控的策略通过Key值(键)对数据进行分区保证具有相同Key的数据将由同一任务处理。在图2-2中，输出“Extract hashtags”算子使用键来分区（hashtag），以便count算子的任务可以正确计算每个#标签的出现次数。</li><li>随机策略统一将数据分配到算子的任务中去，以便均匀地将负载分配到不同的计算任务。</li></ul><h2 id="并行处理流数据"><a href="#并行处理流数据" class="headerlink" title="并行处理流数据"></a>并行处理流数据</h2><p>既然我们熟悉了数据流编程的基础知识，现在是时候看看这些概念如何应用于并行的处理数据流了。但首先，让我们定义术语数据流：数据流是一个可能无限的事件序列。</p><p>数据流中的事件可以表示监控数据，传感器测量数据，信用卡交易数据，气象站观测数据，在线用户交互数据，网络搜索数据等。在本节中，我们将学习如何并行处理无限流，使用数据流编程范式。</p><h3 id="延迟和吞吐量"><a href="#延迟和吞吐量" class="headerlink" title="延迟和吞吐量"></a>延迟和吞吐量</h3><p>流处理程序不同与批处理程序。在评估性能时，要求也有所不同。对于批处理程序，我们通常关心一个作业的总的执行时间，或我们的处理引擎读取输入所需的时间，执行计算，并回写结果。由于流处理程序是连续运行的，输入可能是无界的，所以数据流处理中没有总执行时间的概念。 相反，流处理程序必须尽可能快的提供输入数据的计算结果。我们使用延迟和吞吐量来表征流处理的性能要求。</p><h3 id="延迟"><a href="#延迟" class="headerlink" title="延迟"></a>延迟</h3><p>延迟表示处理事件所需的时间。它是接收事件和看到在输出中处理此事件的效果之间的时间间隔。要直观的理解延迟，考虑去咖啡店买咖啡。当你进入咖啡店时，可能还有其他顾客在里面。因此，你排队等候直到轮到你下订单。收银员收到你的付款并通知准备饮料的咖啡师。一旦你的咖啡准备好了，咖啡师会叫你的名字，你可以到柜台拿你的咖啡。服务延迟是从你进入咖啡店的那一刻起，直到你喝上第一口咖啡之间的时间间隔。</p><p>在数据流中，延迟是以时间为单位测量的，例如毫秒。根据应用程序，我们可能会关心平均延迟，最大延迟或百分位延迟。例如，平均延迟值为10ms意味着处理事件的平均时间在10毫秒内。或者，延迟值为95%，10ms表示95%的事件在10ms内处理完毕。平均值隐藏了处理延迟的真实分布，可能会让人难以发现问题。如果咖啡师在准备卡布奇诺之前用完了牛奶，你必须等到他们从供应室带来一些。虽然你可能会因为这么长时间的延迟而生气，但大多数其他客户仍然会感到高兴。</p><p>确保低延迟对于许多流应用程序来说至关重要，例如欺诈检测，系统警报，网络监控和提供具有严格服务水平协议的服务。低延迟是流处理的关键特性，它实现了我们所谓的实时应用程序。像Apache Flink这样的现代流处理器可以提供低至几毫秒的延迟。相比之下，传统批处理程序延迟通常从几分钟到几个小时不等。在批处理中，首先需要收集事件批次，然后才能处理它们。因此，延迟是受每个批次中最后一个事件的到达时间的限制。所以自然而然取决于批的大小。真正的流处理不会引入这样的人为延迟，因此可以实现真正的低延迟。真的流模型，事件一进入系统就可以得到处理。延迟更密切地反映了在每个事件上必须进行的实际工作。</p><h3 id="吞吐量"><a href="#吞吐量" class="headerlink" title="吞吐量"></a>吞吐量</h3><p>吞吐量是衡量系统处理能力的指标，也就是处理速率。也就是说，吞吐量告诉我们每个时间单位系统可以处理多少事件。重温咖啡店的例子，如果商店营业时间为早上7点至晚上7点。当天为600个客户提供了服务，它的平均吞吐量将是每小时50个客户。虽然我们希望延迟尽可能低，但我们通常也需要吞吐量尽可能高。</p><p>吞吐量以每个时间单位系统所能处理的事件数量或操作数量来衡量。值得注意的是，事件处理速率取决于事件到达的速率，低吞吐量并不一定表示性能不佳。 在流式系统中，我们通常希望确保我们的系统可以处理最大的预期事件到达的速率。也就是说，我们主要的关注点在于确定的峰值吞吐量是多少，当系统处于最大负载时性能怎么样。为了更好地理解峰值吞吐量的概念，让我们考虑一个流处理 程序没有收到任何输入的数据，因此没有消耗任何系统资源。当第一个事件进来时，它会尽可能以最小延迟立即处理。例如，如果你是第一个出现在咖啡店的顾客，在早上开门后，你将立即获得服务。理想情况下，您希望此延迟保持不变 ，并且独立于传入事件的速率。但是，一旦我们达到使系统资源被完全使用的事件传入速率，我们将不得不开始缓冲事件。在咖啡店里 ，午餐后会看到这种情况发生。许多人出现在同一时间，必须排队等候。在此刻，咖啡店系统已达到其峰值吞吐量，进一步增加 事件传入的速率只会导致更糟糕的延迟。如果系统继续以可以处理的速率接收数据，缓冲区可能变为不可用，数据可能会丢失。这种情况是众所周知的 作为背压，有不同的策略来处理它。</p><h3 id="延迟与吞吐量的对比"><a href="#延迟与吞吐量的对比" class="headerlink" title="延迟与吞吐量的对比"></a>延迟与吞吐量的对比</h3><p>此时，应该清楚延迟和吞吐量不是独立指标。如果事件需要在处理流水线中待上很长时间，我们不能轻易确保高吞吐量。同样，如果系统容量很小，事件将被缓冲，而且必须等待才能得到处理。</p><p>让我们重温一下咖啡店的例子来阐明一下延迟和吞吐量如何相互影响。首先，应该清楚存在没有负载时的最佳延迟。也就是说，如果你是咖啡店的唯一客户，会很快得到咖啡。然而，在繁忙时期，客户将不得不排队等待，并且会有延迟增加。另一个影响延迟和吞吐量的因素是处理事件所花费的时间或为每个客户提供服务所花费的时间。想象一下，期间圣诞节假期，咖啡师不得不为每杯咖啡画圣诞老人。这意味着准备一杯咖啡需要的时间会增加，导致每个人花费 更多的时间在等待咖啡师画圣诞老人，从而降低整体吞吐量。</p><p>那么，你可以同时获得低延迟和高吞吐量吗？或者这是一个无望的努力？我们可以降低得到咖啡的延迟 ，方法是：聘请一位更熟练的咖啡师来准备咖啡。在高负载时，这种变化也会增加吞吐量，因为会在相同的时间内为更多的客户提供服务。 实现相同结果的另一种方法是雇用第二个咖啡师来利用并行性。这里的主要想法是降低延迟来增加吞吐量。当然，如果系统可以更快的执行操作，它可以在相同的时间内执行更多操作。 事实上，在流中利用并行性时也会发生这种情况。通过并行处理多个流，在同时处理更多事件的同时降低延迟。</p><h2 id="数据流上的操作"><a href="#数据流上的操作" class="headerlink" title="数据流上的操作"></a>数据流上的操作</h2><p>流处理引擎通常提供一组内置操作：摄取(ingest)，转换(transform)和输出流(output)。这些操作可以 结合到数据流图中来实现逻辑流处理程序。在本节中，我们描述最常见的流处理操作。</p><p>操作可以是无状态的或有状态的。无状态操作不保持任何内部状态。也就是说，事件的处理不依赖于过去看到的任何事件，也没有保留历史。 无状态操作很容易并行化，因为事件可以彼此独立地处理，也独立于事件到达的顺序(和事件到达顺序没有关系)。 而且，在失败的情况下，无状态操作可以是简单的重新启动并从中断处继续处理。相反， 有状态操作可能会维护之前收到的事件的信息。此状态可以通过传入事件更新，也可以用于未来事件的处理逻辑。有状态的流 处理应用程序更难以并行化和以容错的方式来运行，因为状态需要有效的进行分区和在发生故障的情况下可靠地恢复。</p><h3 id="数据摄入和数据吞吐量"><a href="#数据摄入和数据吞吐量" class="headerlink" title="数据摄入和数据吞吐量"></a>数据摄入和数据吞吐量</h3><p>数据摄取和数据出口操作允许流处理程序与外部系统通信。数据摄取是操作从外部源获取原始数据并将其转换为其他格式(ETL)。实现数据提取逻辑的运算符被称为数据源。数据源可以从TCP Socket，文件，Kafka Topic或传感器数据接口中提取数据。数据出口是以适合消费的形式产出到外部系统。执行数据出口的运算符称为数据接收器，包括文件，数据库，消息队列和监控接口。</p><h3 id="转换算子"><a href="#转换算子" class="headerlink" title="转换算子"></a>转换算子</h3><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0204.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0204.png" class="lazyload"></a></p><p>转换算子是单遍处理算子，碰到一个事件处理一个事件。这些操作在使用后会消费一个事件，然后对事件数据做一些转换，产生一个新的输出流。转换逻辑可以集成在 操作符中或由UDF函数提供，如图所示图2-4。程序员编写实现自定义计算逻辑。</p><p>操作符可以接受多个输入流并产生多个输出流。他们还可以通过修改数据流图的结构要么将流分成多个流，要么将流合并为一条流。</p><h3 id="滚动聚合"><a href="#滚动聚合" class="headerlink" title="滚动聚合"></a>滚动聚合</h3><p>滚动聚合是一种聚合，例如sum，minimum和maximum，为每个输入事件不断更新。 聚合操作是有状态的，并将当前状态与传入事件一起计算以产生更新的聚合值。请注意能够有效地将当前状态与事件相结合 产生单个值，聚合函数必须是关联的和可交换的。否则，操作符必须存储完整的流数据历史。图2-5显示了最小滚动 聚合。操作符保持当前的最小值和相应地为每个传入的事件来更新最小值。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0205.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0205.png" class="lazyload"></a></p><h3 id="窗口操作符"><a href="#窗口操作符" class="headerlink" title="窗口操作符"></a>窗口操作符</h3><p>转换和滚动聚合一次处理一个事件产生输出事件并可能更新状态。但是，有些操作必须收集并缓冲数据以计算其结果。 例如，考虑不同流之间的连接或整体聚合这样的操作，例如中值函数。为了在无界流上高效运行这些操作符，我们需要限制 这些操作维护的数据量。在本节中，我们将讨论窗口操作，提供此服务。</p><p>窗口还可以在语义上实现关于流的比较复杂的查询。我们已经看到了滚动聚合的方式，以聚合值编码整个流的历史数据来为每个事件提供低延迟的结果。 但如果我们只对最近的数据感兴趣的话会怎样？考虑给司机提供实时交通信息的应用程序。这个程序可以使他们避免拥挤的路线。在这种场景下，你想知道某个位置在最近几分钟内是否有事故发生。 另一方面，了解所有发生过的事故在这个应用场景下并没有什么卵用。更重要的是，通过将流历史缩减为单一聚合值，我们将丢失这段时间内数据的变化。例如，我们可能想知道每5分钟有多少车辆穿过 某个路口。</p><p>窗口操作不断从无限事件流中创建有限的事件集，好让我们执行有限集的计算。通常会基于数据属性或基于时间的窗口来分配事件。 要正确定义窗口运算符语义，我们需要确定如何给窗口分配事件以及对窗口中的元素进行求值的频率是什么样的。 窗口的行为由一组策略定义。窗口策略决定何时创建新的窗口以及要分配的事件属于哪个窗口，以及何时对窗口中的元素进行求值。 而窗口的求值基于触发条件。一旦触发条件得到满足，窗口的内容将会被发送到求值函数，求值函数会将计算逻辑应用于窗口中的元素。 求值函数可以是sum或minimal或自定义的聚合函数。 求值策略可以根据时间或者数据属性计算(例如，在过去五秒内收到的事件或者最近的一百个事件等等)。 接下来，我们描述常见窗口类型的语义。</p><ul><li>滚动窗口是将事件分配到固定大小的不重叠的窗口中。当通过窗口的结尾时，全部事件被发送到求值函数进行处理。基于计数的滚动窗口定义了在触发求值之前需要收集多少事件。图2-6显示了一个基于计数的翻滚窗口，每四个元素一个窗口。基于时间的滚动窗口定义一个时间间隔，包含在此时间间隔内的事件。图2-7显示了基于时间的滚动窗口，将事件收集到窗口中每10分钟触发一次计算。</li></ul><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0206.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0206.png" class="lazyload"></a></p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0207.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0207.png" class="lazyload"></a></p><ul><li>滑动窗口将事件分配到固定大小的重叠的窗口中去。因此，事件可能属于多个桶。我们通过提供窗口的长度和滑动距离来定义滑动窗口。滑动距离定义了创建新窗口的间隔。基于滑动计数的窗口，图2-8的长度为四个事件，三个为滑动距离。</li></ul><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0208.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0208.png" class="lazyload"></a></p><ul><li>会话窗口在常见的真实场景中很有用，一些场景既不能使用滚动窗口也不能使用滑动窗口。考虑一个分析在线用户行为的应用程序。在应用程序里，我们想把源自同一时期的用户活动或会话事件分组在一起。会话由一系列相邻时间发生的事件组成，接下来有一段时间没有活动。例如，用户在App上浏览一系列的新闻，然后关掉App，那么浏览新闻这段时间的浏览事件就是一个会话。会话窗口事先没有定义窗口的长度，而是取决于数据的实际情况，滚动窗口和滑动窗口无法应用于这个场景。相反，我们需要将同一会话中的事件分配到同一个窗口中去，而不同的会话可能窗口长度不一样。会话窗口会定义一个间隙值来区分不同的会话。间隙值的意思是：用户一段时间内不活动，就认为用户的会话结束了。图2-9显示了一个会话窗口。</li></ul><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0209.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0209.png" class="lazyload"></a></p><p>到目前为止，所有窗口类型都是在整条流上去做窗口操作。但实际上你可能想要将一条流分流成多个逻辑流并定义并行窗口。 例如，如果我们正在接收来自不同传感器的测量结果，那么可能想要在做窗口计算之前按传感器ID对流进行分流操作。 在并行窗口中，每条流都独立于其他流，然后应用了窗口逻辑。图2-10显示了一个基于计数的长度为2的并行滚动窗口，根据事件颜色分流。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0210.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0210.png" class="lazyload"></a></p><p>在流处理中，窗口操作与两个主要概念密切相关：时间语义和状态管理。时间也许是流处理最重要的方面。即使低延迟是流处理的一个有吸引力的特性，它的真正价值不仅仅是快速分析。真实世界的系统，网络和通信渠道远非完美，流数据经常被推迟或无序(乱序)到达。理解如何在这种条件下提供准确和确定的结果是至关重要的。 更重要的是，流处理程序可以按原样处理事件制作的也应该能够处理相同的历史事件方式，从而实现离线分析甚至时间旅行分析。 当然，前提是我们的系统可以保存状态，因为可能有故障发生。到目前为止，我们看到的所有窗口类型在产生结果前都需要保存之前的数据。实际上，如果我们想计算任何指标，即使是简单的计数，我们也需要保存状态。考虑到流处理程序可能会运行几天，几个月甚至几年，我们需要确保状态可以在发生故障的情况下可靠地恢复。 并且即使程序崩溃，我们的系统也能保证计算出准确的结果。本章，我们将在流处理应用可能发生故障的语境下，深入探讨时间和状态的概念。</p><h2 id="时间语义"><a href="#时间语义" class="headerlink" title="时间语义"></a>时间语义</h2><p>在本节中，我们将介绍时间语义，并描述流中不同的时间概念。我们将讨论流处理器在乱序事件流的情况下如何提供准确的计算结果，以及我们如何处理历史事件流，如何在流中进行时间旅行。</p><h3 id="在流处理中一分钟代表什么？"><a href="#在流处理中一分钟代表什么？" class="headerlink" title="在流处理中一分钟代表什么？"></a>在流处理中一分钟代表什么？</h3><p>在处理可能是无限的事件流（包含了连续到达的事件），时间成为流处理程序的核心方面。假设我们想要连续的计算结果，可能每分钟就要计算一次。在我们的流处理程序上下文中，一分钟的意思是什么？</p><p>考虑一个程序需要分析一款移动端的在线游戏的用户所产生的事件流。游戏中的用户分了组，而应用程序将收集每个小组的活动数据，基于小组中的成员多快达到了游戏设定的目标，然后在游戏中提供奖励。例如额外的生命和用户升级。例如，如果一个小组中的所有用户在一分钟之内都弹出了500个泡泡，他们将升一级。Alice是一个勤奋的玩家，她在每天早晨的通勤时间玩游戏。问题在于Alice住在柏林，并且乘地铁去上班。而柏林的地铁手机信号很差。我们设想一个这样的场景，Alice当她的手机连上网时，开始弹泡泡，然后游戏会将数据发送到我们编写的应用程序中，这时地铁突然进入了隧道，她的手机也断网了。Alice还在玩这个游戏，而产生的事件将会缓存在手机中。当地铁离开隧道，Alice的手机又在线了，而手机中缓存的游戏事件将发送到应用程序。我们的应用程序应该如何处理这些数据？在这个场景中一分钟的意思是什么？这个一分钟应该包含Alice离线的那段时间吗？下图展示了这个问题。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0211.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0211.png" class="lazyload"></a></p><p>在线手游是一个简单的场景，展示了应用程序的运算应该取决于事件实际发生的时间，而不是应用程序收到事件的时间。如果我们按照应用程序收到事件的时间来进行处理的话，最糟糕的后果就是，Alice和她的朋友们再也不玩这个游戏了。但是还有很多时间语义非常关键的应用程序，我们需要保证时间语义的正确性。如果我们只考虑我们在一分钟之内收到了多少数据，我们的结果会变化，因为结果取决于网络连接的速度或处理的速度。相反，定义一分钟之内的事件数量，这个一分钟应该是数据本身的时间。</p><p>在Alice的这个例子中，流处理程序可能会碰到两个不同的时间概念：处理时间和事件时间。我们将在接下来的部分，讨论这两个概念。</p><h3 id="处理时间"><a href="#处理时间" class="headerlink" title="处理时间"></a>处理时间</h3><p>处理时间是处理流的应用程序的机器的本地时钟的时间（墙上时钟）。处理时间的窗口包含了一个时间段内来到机器的所有事件。这个时间段指的是机器的墙上时钟。如下图所示，在Alice的这个例子中，处理时间窗口在Alice的手机离线的情况下，时间将会继续行走。但这个处理时间窗口将不会收集Alice的手机离线时产生的事件。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0212.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0212.png" class="lazyload"></a></p><h3 id="事件时间"><a href="#事件时间" class="headerlink" title="事件时间"></a>事件时间</h3><p>事件时间是流中的事件实际发生的时间。事件时间基于流中的事件所包含的时间戳。通常情况下，在事件进入流处理程序前，事件数据就已经包含了时间戳。下图展示了事件时间窗口将会正确的将事件分发到窗口中去。可以如实反应事情是怎么发生的。即使事件可能存在延迟。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0213.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0213.png" class="lazyload"></a></p><p>事件时间使得计算结果的过程不需要依赖处理数据的速度。基于事件时间的操作是可以预测的，而计算结果也是确定的。无论流处理程序处理流数据的速度快或是慢，无论事件到达流处理程序的速度快或是慢，事件时间窗口的计算结果都是一样的。</p><p>可以处理迟到的事件只是我们使用事件时间所克服的一个挑战而已。普遍存在的事件乱序问题可以使用事件时间得到解决。考虑和Alice玩同样游戏的Bob，他恰好和Alice在同一趟地铁上。Alice和Bob虽然玩的游戏一样，但他们的手机信号是不同的运营商提供的。当Alice的手机没信号时，Bob的手机依然有信号，游戏数据可以正常发送出去。</p><p>如果使用事件时间，即使碰到了事件乱序到达的情况，我们也可以保证结果的正确性。还有，当我们在处理可以重播的流数据时，由于时间戳的确定性，我们可以快进过去。也就是说，我们可以重播一条流，然后分析历史数据，就好像流中的事件是实时发生一样。另外，我们可以快进历史数据来使我们的应用程序追上现在的事件，然后应用程序仍然是一个实时处理程序，而且业务逻辑不需要改变。</p><h3 id="水位线（Watermarks）"><a href="#水位线（Watermarks）" class="headerlink" title="水位线（Watermarks）"></a>水位线（Watermarks）</h3><p>在我们对事件时间窗口的讨论中，我们忽略了一个很重要的方面：我们应该怎样去决定何时触发事件时间窗口的计算？也就是说，在我们可以确定一个时间点之前的所有事件都已经到达之前，我们需要等待多久？我们如何知道事件是迟到的？在分布式系统无法准确预测行为的现实条件下，以及外部组件所引发的事件的延迟，以上问题并没有准确的答案。在本小节中，我们将会看到如何使用水位线来设置事件时间窗口的行为。</p><p>水位线是全局进度的度量标准。系统可以确信在一个时间点之后，不会有早于这个时间点发生的事件到来了。本质上，水位线提供了一个逻辑时钟，这个逻辑时钟告诉系统当前的事件时间。当一个运算符接收到含有时间T的水位线时，这个运算符会认为早于时间T的发生的事件已经全部都到达了。对于事件时间窗口和乱序事件的处理，水位线非常重要。运算符一旦接收到水位线，运算符会认为一段时间内发生的所有事件都已经观察到，可以触发针对这段时间内所有事件的计算了。</p><p>水位线提供了一种结果可信度和延时之间的妥协。激进的水位线设置可以保证低延迟，但结果的准确性不够。在这种情况下，迟到的事件有可能晚于水位线到达，我们需要编写一些代码来处理迟到事件。另一方面，如果水位线设置的过于宽松，计算的结果准确性会很高，但可能会增加流处理程序不必要的延时。</p><p>在很多真实世界的场景里面，系统无法获得足够的知识来完美的确定水位线。在手游这个场景中，我们无法得知一个用户离线时间会有多长，他们可能正在穿越一条隧道，可能正在乘飞机，可能永远不会再玩儿了。水位线无论是用户自定义的或者是自动生成的，在一个分布式系统中追踪全局的时间进度都不是很容易。所以仅仅依靠水位线可能并不是一个很好的主意。流处理系统还需要提供一些机制来处理迟到的元素（在水位线之后到达的事件）。根据应用场景，我们可能需要把迟到事件丢弃掉，或者写到日志里，或者使用迟到事件来更新之前已经计算好的结果。</p><h3 id="处理时间-vs-事件时间"><a href="#处理时间-vs-事件时间" class="headerlink" title="处理时间 vs 事件时间"></a>处理时间 vs 事件时间</h3><p>大家可能会有疑问，既然事件时间已经可以解决我们的所有问题，为什么我们还要对比这两个时间概念？真相是，处理时间在很多情况下依然很有用。处理时间窗口将会带来理论上最低的延迟。因为我们不需要考虑迟到事件以及乱序事件，所以一个窗口只需要简单的缓存窗口内的数据即可，一旦机器时间超过指定的处理时间窗口的结束时间，就会触发窗口的计算。所以对于一些处理速度比结果准确性更重要的流处理程序，处理时间就派上用场了。另一个应用场景是，当我们需要在真实的时间场景下，周期性的报告结果时，同时不考虑结果的准确性。一个例子就是一个实时监控的仪表盘，负责显示当事件到达时立即聚合的结果。最后，处理时间窗口可以提供流本身数据的忠实表达，对于一些案例可能是很必要的特性。例如我们可能对观察流和对每分钟事件的计数（检测可能存在的停电状况）很感兴趣。简单的说，处理时间提供了低延迟，同时结果也取决于处理速度，并且也不能保证确定性。另一方面，事件时间保证了结果的确定性，同时还可以使我们能够处理迟到的或者乱序的事件流。</p><h2 id="状态和持久化模型"><a href="#状态和持久化模型" class="headerlink" title="状态和持久化模型"></a>状态和持久化模型</h2><p>我们现在转向另一个对于流处理程序非常重要的话题：状态。在数据处理中，状态是普遍存在的。任何稍微复杂一点的计算，都涉及到状态。为了产生计算结果，一个函数在一段时间内的一定数量的事件上来累加状态（例如，聚合计算或者模式匹配）。有状态的运算符使用输入的事件以及内部保存的状态来计算得到输出。例如，一个滚动聚合运算符需要输出这个运算符所观察到的所有事件的累加和。这个运算符将会在内部保存当前观察到的所有事件的累加和，同时每输入一个事件就更新一次累加和的计算结果。相似的，当一个运算符检测到一个“高温”事件紧接着十分钟以内检测到一个“烟雾”事件时，将会报警。直到运算符观察到一个“烟雾”事件或者十分钟的时间段已经过去，这个运算符需要在内部状态中一直保存着“高温”事件。</p><p>当我们考虑一下使用批处理系统来分析一个无界数据集时，会发现状态的重要性显而易见。在现代流处理器兴起之前，处理无界数据集的一个通常做法是将输入的事件攒成微批，然后交由批处理器来处理。当一个任务结束时，计算结果将被持久化，而所有的运算符状态就丢失了。一旦一个任务在计算下一个微批次的数据时，这个任务是无法访问上一个任务的状态的（都丢掉了）。这个问题通常使用将状态代理到外部系统（例如数据库）的方法来解决。相反，在一个连续不间断运行的流处理任务中，事件的状态是一直存在的，我们可以将状态暴露出来作为编程模型中的一等公民。当然，我们的确可以使用外部系统来管理流的状态，即使这个解决方案会带来额外的延迟。</p><p>由于流处理运算符默认处理的是无界数据流。所以我们必须要注意不要让内部状态无限的增长。为了限制状态的大小，运算符通常情况下会保存一些之前所观察到的事件流的总结或者概要。这个总结可能是一个计数值，一个累加和，或者事件流的采样，窗口的缓存操作，或者是一个自定义的数据结构，这个数据结构用来保存数据流中感兴趣的一些特性。</p><p>我们可以想象的到，支持有状态的运算符可能会碰到一些实现上的挑战：</p><p><em>状态管理</em></p><p>系统需要高效的管理状态，并保证针对状态的并发更新，不会产生竞争条件（race condition）。</p><p><em>状态分区</em></p><p>并行会带来复杂性。因为计算结果同时取决于已经保存的状态和输入的事件流。幸运的是，大多数情况下，我们可以使用Key来对状态进行分区，然后独立的管理每一个分区。例如，当我们处理一组传感器的测量事件流时，我们可以使用分区的运算符状态来针对不同的传感器独立的保存状态。</p><p><em>状态恢复</em></p><p>第三个挑战是有状态的运算符如何保证状态可以恢复，即使出现任务失败的情况，计算也是正确的。</p><p>下一节，我们将讨论任务失败和计算结果的保证。</p><h3 id="任务失败"><a href="#任务失败" class="headerlink" title="任务失败"></a>任务失败</h3><p>流任务中的运算符状态是很宝贵的，也需要抵御任务失败带来的问题。如果在任务失败的情况下，状态丢失的话，在任务恢复以后计算的结果将是不正确的。流任务会连续不断的运行很长时间，而状态可能已经收集了几天甚至几个月。在失败的情况下，重新处理所有的输入并重新生成一个丢失的状态，将会很浪费时间，开销也很大。</p><p>在本章开始时，我们看到如何将流的编程建模成数据流模型。在执行之前，流程序将会被翻译成物理层数据流图，物理层数据流图由连接的并行任务组成，而一个并行任务运行一些运算符逻辑，消费输入流数据，并为其他任务产生输出流数据。真实场景下，可能有数百个这样的任务并行运行在很多的物理机器上。在长时间的运行中，流任务中的任意一个任务在任意时间点都有可能失败。我们如何保证任务的失败能被正确的处理，以使任务能继续的运行下去呢？事实上，我们可能希望我们的流处理器不仅能在任务失败的情况下继续处理数据，还能保证计算结果的正确性以及运算符状态的安全。我们在本小节来讨论这些问题。</p><h4 id="什么是任务失败？"><a href="#什么是任务失败？" class="headerlink" title="什么是任务失败？"></a>什么是任务失败？</h4><p>对于流中的每一个事件，一个处理任务分为以下步骤：（1）接收事件，并将事件存储在本地的缓存中；（2）可能会更新内部状态；（3）产生输出记录。这些步骤都能失败，而系统必须对于在失败的场景下如何处理有清晰的定义。如果任务在第一步就失败了，事件会丢失吗？如果当更新内部状态的时候任务失败，那么内部状态会在任务恢复以后更新吗？在以上这些场景中，输出是确定性的吗？</p><p>在批处理场景下，所有的问题都不是问题。因为我们可以很方便的重新计算。所以不会有事件丢失，状态也可以得到完全恢复。在流的世界里，处理失败不是一个小问题。流系统在失败的情况下需要保证结果的准确性。接下来，我们需要看一下现代流处理系统所提供的一些保障，以及实现这些保障的机制。</p><h4 id="结果的保证"><a href="#结果的保证" class="headerlink" title="结果的保证"></a>结果的保证</h4><p>当我们讨论保证计算的结果时，我们的意思是流处理器的内部状态需要保证一致性。也就是说我们关心的是应用程序的代码在故障恢复以后看到的状态值是什么。要注意保证应用程序状态的一致性并不是保证应用程序的输出结果的一致性。一旦输出结果被持久化，结果的准确性就很难保证了。除非持久化系统支持事务。</p><p><em>AT-MOST-ONCE</em></p><p>当任务故障时，最简单的做法是什么都不干，既不恢复丢失的状态，也不重播丢失的事件。At-most-once语义的含义是最多处理一次事件。换句话说，事件可以被丢弃掉，也没有任何操作来保证结果的准确性。这种类型的保证也叫“没有保证”，因为一个丢弃掉所有事件的系统其实也提供了这样的保障。没有保障听起来是一个糟糕的主意，但如果我们能接受近似的结果，并且希望尽可能低的延迟，那么这样也挺好。</p><p><em>AT-LEAST-ONCE</em></p><p>在大多数的真实应用场景，我们希望不丢失事件。这种类型的保障成为at-least-once，意思是所有的事件都得到了处理，而且一些事件还可能被处理多次。如果结果的正确性仅仅依赖于数据的完整性，那么重复处理是可以接受的。例如，判断一个事件是否在流中出现过，at-least-once这样的保证完全可以正确的实现。在最坏的情况下，我们多次遇到了这个事件。而如果我们要对一个特定的事件进行计数，计算结果就可能是错误的了。</p><p>为了保证在at-least-once语义的保证下，计算结果也能正确。我们还需要另一套系统来从数据源或者缓存中重新播放数据。持久化的事件日志系统将会把所有的事件写入到持久化存储中。所以如果任务发生故障，这些数据可以重新播放。还有一种方法可以获得同等的效果，就是使用结果承认机制。这种方法将会把每一条数据都保存在缓存中，直到数据的处理等到所有的任务的承认。一旦得到所有任务的承认，数据将被丢弃。</p><p><em>EXACTLY-ONCE</em></p><p>恰好处理一次是最严格的保证，也是最难实现的。恰好处理一次语义不仅仅意味着没有事件丢失，还意味着针对每一个数据，内部状态仅仅更新一次。本质上，恰好处理一次语义意味着我们的应用程序可以提供准确的结果，就好像从未发生过故障。</p><p>提供恰好处理一次语义的保证必须有至少处理一次语义的保证才行，同时还需要数据重放机制。另外，流处理器还需要保证内部状态的一致性。也就是说，在故障恢复以后，流处理器应该知道一个事件有没有在状态中更新。事务更新是达到这个目标的一种方法，但可能引入很大的性能问题。Flink使用了一种轻量级快照机制来保证恰好处理一次语义。</p><p><em>端到端恰好处理一次</em></p><p>目前我们看到的一致性保证都是由流处理器实现的，也就是说都是在Flink流处理器内部保证的。而在真实世界中，流处理应用除了流处理器以外还包含了数据源（例如Kafka）和持久化系统。端到端的一致性保证意味着结果的正确性贯穿了整个流处理应用的始终。每一个组件都保证了它自己的一致性。而整个端到端的一致性级别取决于所有组件中一致性最弱的组件。要注意的是，我们可以通过弱一致性来实现更强的一致性语义。例如，当任务的操作具有幂等性时，比如流的最大值或者最小值的计算。在这种场景下，我们可以通过最少处理一次这样的一致性来实现恰好处理一次这样的最高级别的一致性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第二章，流处理基础&quot;&gt;&lt;a href=&quot;#第二章，流处理基础&quot; class=&quot;headerlink&quot; title=&quot;第二章，流处理基础&quot;&gt;&lt;/a&gt;第二章，流处理基础&lt;/h1&gt;&lt;h2 id=&quot;数据流编程简介&quot;&gt;&lt;a href=&quot;#数据流编程简介&quot; class=&quot;he
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink系列01有状态的流式处理简介</title>
    <link href="https://masteryang4.github.io/2020/06/27/flink%E7%B3%BB%E5%88%9701%E6%9C%89%E7%8A%B6%E6%80%81%E7%9A%84%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B/"/>
    <id>https://masteryang4.github.io/2020/06/27/flink%E7%B3%BB%E5%88%9701%E6%9C%89%E7%8A%B6%E6%80%81%E7%9A%84%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B/</id>
    <published>2020-06-27T15:21:57.000Z</published>
    <updated>2020-06-27T15:24:20.800Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第一章，有状态的流式处理简介"><a href="#第一章，有状态的流式处理简介" class="headerlink" title="第一章，有状态的流式处理简介"></a>第一章，有状态的流式处理简介</h1><p>Apache Flink是一个分布式流处理器，具有直观和富有表现力的API，可实现有状态的流处理应用程序。它以容错的方式有效地大规模运行这些应用程序。 Flink于2014年4月加入Apache软件基金会作为孵化项目，并于2015年1月成为顶级项目。从一开始，Flink就拥有一个非常活跃且不断增长的用户和贡献者社区。到目前为止，已有超过五百人为Flink做出贡献，并且它已经发展成为最复杂的开源流处理引擎之一，并得到了广泛采用的证明。 Flink为不同行业和全球的许多公司和企业提供大规模的商业关键应用。</p><p>流处理技术在大大小小的公司中越来越受欢迎，因为它为许多已建立的用例（如数据分析，ETL和事务应用程序）提供了卓越的解决方案，同时也促进了新颖的应用程序，软件架构和商机。接下来我们将讨论，为什么有状态流处理变得如此受欢迎并评估其潜力。我们首先回顾传统的数据应用程序架构并指出它们的局限性。接下来，我们介绍基于状态流处理的应用程序设计 与传统方法相比，它具有许多有趣的特征最后，我们简要讨论开源流处理器的发展，并在本地Flink实例上运行流应用程序。</p><h2 id="传统数据处理架构"><a href="#传统数据处理架构" class="headerlink" title="传统数据处理架构"></a>传统数据处理架构</h2><p>数十年来，数据和数据处理在企业中无处不在。多年来，数据的收集和使用一直在增长，公司已经设计并构建了基础架构来管理数据。大多数企业实施的传统架构区分了两种类型的数据处理：事务处理（OLTP）和分析处理（OLAP）。</p><h3 id="事务处理"><a href="#事务处理" class="headerlink" title="事务处理"></a>事务处理</h3><p>公司将各种应用程序用于日常业务活动，例如企业资源规划（ERP）系统，客户关系管理（CRM）软件和基于Web的应用程序。这些系统通常设计有单独的层，用于数据处理（应用程序本身）和数据存储（事务数据库系统），如图1-1所示。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0101.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0101.png" class="lazyload"></a></p><p>应用程序通常连接到外部服务或直接面向用户，并持续处理传入的事件，如网站上的订单，电子邮件或点击。处理事件时，应用程序将会读取远程数据库的状态，或者通过运行事务来更新它。通常，一个数据库系统可以服务于多个应用程序，它们有时会访问相同的数据库或表。</p><p>当应用程序需要扩展时，这样的设计可能会导致问题。由于多个应用程序可能会同时用到相同的数据表示，或者共享相同的基础设施，因此想要更改表的结构或扩展数据库，就需要仔细的规划和大量的工作。克服紧耦合应用程序的最新方法是微服务设计模式。微服务被设计为小型、完备且独立的应用程序。他们遵循UNIX的理念，即“只做一件事并且把它做好”。通过将几个微服务相互连接来构建更复杂的应用程序，这些微服务仅通过标准化接口（例如RESTful HTTP连接）进行通信。由于微服务严格地彼此分离并且仅通过明确定义的接口进行通信，因此每个微服务都可以用不同技术栈来实现，包括编程语言、类库和数据存储。微服务和所有必需的软件和服务通常捆绑在一起并部署在独立的容器中。图1-2描绘了一种微服务架构。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0102.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0102.png" class="lazyload"></a></p><h3 id="分析处理"><a href="#分析处理" class="headerlink" title="分析处理"></a>分析处理</h3><p>大量数据存储在公司的各种事务数据库系统中，它们可以为公司业务运营提供宝贵的参考意见。例如，分析订单处理系统的数据，可以获得销量随时间的增长曲线；可以识别延迟发货的原因；还可以预测未来的销量以便提前调整库存。但是，事务数据通常分布在多个数据库中，它们往往汇总起来联合分析时更有价值。而且，数据通常需要转换为通用格式。</p><p>所以我们一般不会直接在事务数据库上运行分析查询，而是复制数据到数据仓库。数据仓库是对工作负载进行分析和查询的专用数据存储。为了填充数据仓库，需要将事务数据库系统管理的数据复制过来。将数据复制到数据仓库的过程称为extract-transform-load（ETL）。 ETL过程从事务数据库中提取数据，将其转换为某种通用的结构表示，可能包括验证，值的规范化，编码，重复数据删除（去重）和模式转换，最后将其加载到分析数据库中。 ETL过程可能非常复杂，并且通常需要技术复杂的解决方案来满足性能要求。 ETL过程需要定期运行以保持数据仓库中的数据同步。</p><p>将数据导入数据仓库后，可以查询和分析数据。通常，在数据仓库上执行两类查询。第一种类型是定期报告查询，用于计算与业务相关的统计信息，比如收入、用户增长或者输出的产量。这些指标汇总到报告中，帮助管理层评估业务的整体健康状况。第二种类型是即席查询，旨在提供特定问题的答案并支持关键业务决策，例如收集统计在投放商业广告上的花费，和获取的相应收入，以评估营销活动的有效性。两种查询由批处理方式由数据仓库执行，如图1-3所示。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0103.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0103.png" class="lazyload"></a></p><p>如今，Apache Hadoop生态系统的组件，已经是许多企业IT基础架构中不可或缺的组成部分。现在的做法不是直接将所有数据都插入关系数据库系统，而是将大量数据（如日志文件，社交媒体或Web点击日志）写入Hadoop的分布式文件系统（HDFS）、S3或其他批量数据存储库，如Apache HBase，以较低的成本提供大容量存储容量。驻留在此类存储系统中的数据可以通过SQL-on-Hadoop引擎查询和处理，例如Apache Hive，Apache Drill或Apache Impala。但是，基础结构与传统数据仓库架构基本相同。</p><h2 id="有状态的流式处理"><a href="#有状态的流式处理" class="headerlink" title="有状态的流式处理"></a>有状态的流式处理</h2><p>日常生活中，所有数据都是作为连续的事件流创建的。比如网站或者移动应用中的用户交互动作，订单的提交，服务器日志或传感器测量数据：所有这些都是事件流。实际上，很少有应用场景，能一次性地生成所需要的完整（有限）数据集。实际应用中更多的是无限事件流。有状态的流处理就是用于处理这种无限事件流的应用程序设计模式，在公司的IT基础设施中有广泛的应用场景。在我们讨论其用例之前，我们将简要介绍有状态流处理的工作原理。</p><p>如果我们想要无限处理事件流，并且不愿意繁琐地每收到一个事件就记录一次，那这样的应用程序就需要是有状态的，也就是说能够存储和访问中间数据。当应用程序收到一个新事件时，它可以从状态中读取数据，或者向该状态写入数据，总之可以执行任何计算。原则上讲，我们可以在各种不同的地方存储和访问状态，包括程序变量（内存）、本地文件，还有嵌入式或外部数据库。</p><p>Apache Flink将应用程序状态，存储在内存或者嵌入式数据库中。由于Flink是一个分布式系统，因此需要保护本地状态以防止在应用程序或计算机故障时数据丢失。 Flink通过定期将应用程序状态的一致性检查点（check point）写入远程且持久的存储，来保证这一点。状态、状态一致性和Flink的检查点将在后面的章节中更详细地讨论，但是，现在，图1-4显示了有状态的流式Flink应用程序。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0104.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0104.png" class="lazyload"></a></p><p>有状态的流处理应用程序，通常从事件日志中提取输入事件。事件日志就用来存储和分发事件流。事件被写入持久的仅添加（append-only）日志，这意味着无法更改写入事件的顺序。写入事件日志的流，可以被相同或不同的消费者多次读取。由于日志的仅附加（append-only）属性，事件始终以完全相同的顺序发布给所有消费者。现在已有几种事件日志系统，其中Apache Kafka是最受欢迎的，可以作为开源软件使用，或者是云计算提供商提供的集成服务。</p><p>在Flink上运行的有状态的流处理应用程序，是很有意思的一件事。在这个架构中，事件日志会按顺序保留输入事件，并且可以按确定的顺序重播它们。如果发生故障，Flink将从先前的检查点（check point）恢复其状态，并重置事件日志上的读取位置，这样就可以恢复整个应用。应用程序将重放（并快进）事件日志中的输入事件，直到它到达流的尾部。此技术一般用于从故障中恢复，但也可用于更新应用程序、修复bug或者修复以前发出的结果，另外还可以用于将应用程序迁移到其他群集，或使用不同的应用程序版本执行A / B测试。</p><p>如前所述，有状态的流处理是一种通用且灵活的设计架构，可用于许多不同的场景。在下文中，我们提出了三类通常使用有状态流处理实现的应用程序：（1）事件驱动应用程序，（2）数据管道应用程序，以及（3）数据分析应用程序。</p><p>我们将应用程序分类描述，是为了强调有状态流处理适用于多种业务场景；而实际的应用中，往往会具有以上多种情况的特征。</p><h3 id="事件驱动应用程序（Event-Driven-Applications）"><a href="#事件驱动应用程序（Event-Driven-Applications）" class="headerlink" title="事件驱动应用程序（Event-Driven Applications）"></a>事件驱动应用程序（Event-Driven Applications）</h3><p>事件驱动的应用程序是有状态的流应用程序，它们使用特定的业务逻辑来提取事件流并处理事件。根据业务逻辑，事件驱动的应用程序可以触发诸如发送警报、或电子邮件之类的操作，或者将事件写入向外发送的事件流以供另一个应用程序使用。</p><p>事件驱动应用程序的典型场景包括：</p><ul><li>实时推荐（例如，在客户浏览零售商网站时推荐产品）</li><li>行为模式检测或复杂事件处理（例如，用于信用卡交易中的欺诈检测）</li><li>异常检测（例如，检测侵入计算机网络的尝试</li></ul><p>事件驱动应用程序是微服务的演变。它们通过事件日志而不是REST调用进行通信，并将应用程序数据保存为本地状态，而不是将其写入外部数据存储区（例如关系数据库或键值数据库）。图1-5显示了由事件驱动的流应用程序组成的服务架构。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0105.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0105.png" class="lazyload"></a></p><p>图1-5中的应用程序通过事件日志连接。一个应用程序将其输出发送到事件日志通道（kafka），另一个应用程序使用其他应用程序发出的事件。事件日志通道将发送者和接收者分离，并提供异步、非阻塞的事件传输。每个应用程序都可以是有状态的，并且可以本地管理自己的状态而无需访问外部数据存储。应用程序也可以单独处理和扩展。</p><p>与事务性应用程序或微服务相比，事件驱动的应用程序具有多种优势。与读写远程数据库相比，本地状态访问提供了非常好的性能。扩展性和容错性都由流处理器来保证，并且以事件日志作为输入源，应用程序的整个输入数据可以可靠地存储，并且可以确定性地重放。此外，Flink可以将应用程序的状态重置为先前的保存点（save point），从而可以在不丢失状态的情况下更新或重新扩展应用程序。</p><p>事件驱动的应用程序对运行它们的流处理器有很高的要求，并不是所有流处理器都适合运行事件驱动的应用程序。 API的表现力，以及对状态处理和事件时间支持的程度，决定了可以实现和执行的业务逻辑。这方面取决于流处理器的API，主要看它能提供什么样的状态类型，以及它对事件时间处理的支持程度。此外，精确一次（exactly-once）的状态一致性和扩展应用程序的能力是事件驱动应用程序的基本要求。 Apache Flink符合所有的这些要求，是运行此类应用程序的一个非常好的选择。</p><h3 id="数据管道（Data-Pipelines）"><a href="#数据管道（Data-Pipelines）" class="headerlink" title="数据管道（Data Pipelines）"></a>数据管道（Data Pipelines）</h3><p>当今的IT架构包括许多不同的数据存储，例如关系型数据库和专用数据库系统、事件日志、分布式文件系统，内存中的缓存和搜索索引。所有这些系统都以不同的格式和数据结构存储数据，为其特定的访问模式提供最佳性能。公司通常将相同的数据存储在多个不同的系统中，以提高数据访问的性能。例如，网上商店中提供的产品的信息，可以存储在交易数据库中，同时也存储在缓存（如redis）和搜索索引（如ES）中。由于数据的这种复制，数据存储必须保持同步。</p><p>在不同存储系统中同步数据的传统方法是定期ETL作业。但是，它们不能满足当今许多场景的延迟要求。另一种方法是使用事件日志（event log）来发布更新。更新将写入事件日志并由事件日志分发。日志的消费者获取到更新之后，将更新合并到受影响的数据存储中。根据使用情况，传输的数据可能需要标准化、使用外部数据进行扩展，或者在目标数据存储提取之前进行聚合。</p><p>以较低的延迟，来提取、转换和插入数据是有状态流处理应用程序的另一个常见应用场景。这种类型的应用程序称为数据管道（data pipeline）。数据管道必须能够在短时间内处理大量数据。操作数据管道的流处理器还应具有许多源（source）和接收器（sink）的连接器，以便从各种存储系统读取数据并将数据写入各种存储系统。当然，同样地，Flink完成了所有这些功能。</p><h3 id="流分析"><a href="#流分析" class="headerlink" title="流分析"></a>流分析</h3><p>ETL作业定期将数据导入数据存储区，数据的处理是由即席查询（用户自定义查询）或设定好的通常查询来做的。无论架构是基于数据仓库还是基于Hadoop生态系统的组件，这都是批处理。多年来最好的处理方式就是，定期将数据加载到数据分析系统中，但它给分析管道带了的延迟相当大，而且无法避免。</p><p>根据设定好的时间间隔，可能需要数小时或数天才能将数据点包含在报告中。我们前面已经提到，数据管道可以实现低延迟的ETL，所以在某种程度上，可以通过使用数据管道将数据导入存储区来减少延迟。但是，即使持续不停地进行ETL操作，在用查询来处理事件之前总会有延迟。虽然这种延迟在过去可能是可以接受的，但是今天的应用程序，往往要求必须能够实时收集数据，并立即对其进行操作（例如，在手机游戏中去适应不断变化的条件，或者在电商网站中提供个性化的用户体验）。</p><p>流式分析应用程序不是等待定期触发，而是连续地提取事件流，并且通过纳入最新事件来更新其计算结果，这个过程是低延迟的。这有些类似于数据库中用于更新视图（views）的技术。通常，流应用程序将其结果存储在支持更新的外部数据存储中，例如数据库或键值（key-value）存储。流分析应用程序的实时更新结果可用于驱动监控仪表板（dashboard）应用程序，如图1-6所示。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0106.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0106.png" class="lazyload"></a></p><p>流分析应用程序最大的优势就是，将每个事件纳入到分析结果所需的时间短得多。除此之外，流分析应用程序还有另一个不太明显的优势。传统的分析管道由几个独立的组件组成，例如ETL过程、存储系统、对于基于Hadoop的环境，还包括用于触发任务（jobs）的数据处理和调度程序。相比之下，如果我们运行一个有状态流应用程序，那么流处理器就会负责所有这些处理步骤，包括事件提取、带有状态维护的连续计算以及更新结果。此外，流处理器可以从故障中恢复，并且具有精确一次（exactly-once）的状态一致性保证，还可以调整应用程序的计算资源。像Flink这样的流处理器还支持事件时间（event-time）处理，这可以保证产生正确和确定的结果，并且能够在很短的时间内处理大量数据。</p><p>流分析应用程序通常用于：</p><ul><li>监控手机网络的质量分析</li><li>移动应用中的用户行为</li><li>实时数据的即席分析</li></ul><p>虽然我们不在此处介绍，但Flink还提供对流上的分析SQL查询的支持。</p><h2 id="开源流处理的演进"><a href="#开源流处理的演进" class="headerlink" title="开源流处理的演进"></a>开源流处理的演进</h2><p>数据流处理并不是一项新技术。一些最初的研究原型和商业产品可以追溯到20世纪90年代（1990s）。然而，在很大程度上，过去采用的流处理技术是由成熟的开源流处理器驱动的。如今，分布式开源流处理器在不同行业的许多企业中，处理着核心业务应用，比如电商、社交媒体、电信、游戏和银行等。开源软件是这一趋势的主要驱动力，主要原因有两个：</p><ul><li>开源流处理软件是大家每一个人都可以评估和使用的产品。</li><li>由于许多开源社区的努力，可扩展流处理技术正在迅速成熟和发展</li></ul><p>仅仅一个Apache软件基金会就支持了十几个与流处理相关的项目。新的分布式流处理项目不断进入开源阶段，并不断增加新的特性和功能。开源社区不断改进其项目的功能，并正在推动流处理的技术边界。我们将简要介绍一下过去，看看开源流处理的起源和今天的状态。</p><h3 id="流处理的历史"><a href="#流处理的历史" class="headerlink" title="流处理的历史"></a>流处理的历史</h3><p>第一代分布式开源流处理器（2011）专注于具有毫秒延迟的事件处理，并提供了在发生故障时防止事件丢失的保证。这些系统具有相当低级的API，并且对于流应用程序的准确性和结果的一致性，不提供内置支持，因为结果会取决于到达事件的时间和顺序。另外，即使事件没有丢失，也可能不止一次地处理它们。与批处理器相比，第一代开源流处理器牺牲了结果准确性，用来获得更低的延迟。为了让当时的数据处理系统，可以同时提供快速和准确的结果，人们设计了所谓的lambda架构，如图1-7所示。</p><p><a href="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0107.png" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://confucianzuoyuan.github.io/flink-tutorial/images/spaf_0107.png" class="lazyload"></a></p><p>lambda架构增强了传统的批处理架构，其“快速层”（speed layer）由低延迟的流处理器来支持。数据到达之后由流处理器提取出来，并写入批处理存储。流处理器近乎实时地计算近似结果并将它们写入“快速表”（speed table）。批处理器定期处理批量存储中的数据，将准确的结果写入批处理表，并从速度表中删除相应的不准确结果。应用程序会合并快速表中的近似结果和批处理表中的准确结果，然后消费最终的结果。</p><p>lambda架构现在已经不再是最先进的，但仍在许多地方使用。该体系结构的最初目标是改善原始批处理分析体系结构的高延迟。但是，它有一些明显的缺点。首先，它需要对一个应用程序，做出两个语义上等效的逻辑实现，用于两个独立的、具有不同API的处理系统。其次，流处理器计算的结果只是近似的。第三，lambda架构很难建立和维护。</p><p>通过在第一代基础上进行改进，下一代分布式开源流处理器（2013）提供了更好的故障保证，并确保在发生故障时，每个输入记录仅对结果产生一次影响（exactly -once）。此外，编程API从相当低级的操作符接口演变为高级API。但是，一些改进（例如更高的吞吐量和更好的故障保证）是以将处理延迟从毫秒增加到几秒为代价的。此外，结果仍然取决于到达事件的时间和顺序。</p><p>第三代分布式开源流处理器（2015）解决了结果对到达事件的时间和顺序的依赖性。结合精确一次（exactly-once）的故障语义，这一代系统是第一个具有计算一致性和准确结果的开源流处理器。通过基于实际数据来计算结果（“重演”数据），这些系统还能够以与“实时”数据相同的方式处理历史数据。另一个改进是解决了延迟/吞吐量无法同时保证的问题。先前的流处理器仅能提供高吞吐量或者低延迟（其中之一），而第三代系统能够同时提供这两个特性。这一代的流处理器使得lambda架构过时了。当然，这一代流处理以flink为代表。</p><p>除了目前讨论的特性，例如容错、性能和结果准确性之外，流处理器还不断添加新的操作功能，例如高可用性设置，与资源管理器（如YARN或Kubernetes）的紧密集成，以及能够动态扩展流应用程序。其他功能包括：支持升级应用程序代码，或将作业迁移到其他群集或新版本的流处理器，而不会丢失当前状态。</p><h2 id="Flink-简介"><a href="#Flink-简介" class="headerlink" title="Flink 简介"></a>Flink 简介</h2><p>Apache Flink是第三代分布式流处理器，它拥有极富竞争力的功能。它提供准确的大规模流处理，具有高吞吐量和低延迟。特别的是，以下功能使Flink脱颖而出：</p><ul><li>事件时间（event-time）和处理时间（processing-tme）语义。即使对于无序事件流，事件时间（event-time）语义仍然能提供一致且准确的结果。而处理时间（processing-time）语义可用于具有极低延迟要求的应用程序。</li><li>精确一次（exactly-once）的状态一致性保证。</li><li>每秒处理数百万个事件，毫秒级延迟。 Flink应用程序可以扩展为在数千个核（cores）上运行。</li><li>分层API，具有不同的权衡表现力和易用性。本书介绍了DataStream API和过程函数（process function），为常见的流处理操作提供原语，如窗口和异步操作，以及精确控制状态和时间的接口。本书不讨论Flink的关系API，SQL和LINQ风格的Table API。</li><li>连接到最常用的存储系统，如Apache Kafka，Apache Cassandra，Elasticsearch，JDBC，Kinesis和（分布式）文件系统，如HDFS和S3。</li><li>由于其高可用的设置（无单点故障），以及与Kubernetes，YARN和Apache Mesos的紧密集成，再加上从故障中快速恢复和动态扩展任务的能力，Flink能够以极少的停机时间7*24全天候运行流应用程序。</li><li>能够更新应用程序代码并将作业（jobs）迁移到不同的Flink集群，而不会丢失应用程序的状态。</li><li>详细且可自定义的系统和应用程序指标集合，以提前识别问题并对其做出反应。</li><li>最后但同样重要的是，Flink也是一个成熟的批处理器。</li></ul><p>除了这些功能之外，Flink还是一个非常易于开发的框架，因为它易于使用的API。嵌入式执行模式，可以在单个JVM进程中启动应用程序和整个Flink系统，这种模式一般用于在IDE中运行和调试Flink作业。在开发和测试Flink应用程序时，此功能非常有用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第一章，有状态的流式处理简介&quot;&gt;&lt;a href=&quot;#第一章，有状态的流式处理简介&quot; class=&quot;headerlink&quot; title=&quot;第一章，有状态的流式处理简介&quot;&gt;&lt;/a&gt;第一章，有状态的流式处理简介&lt;/h1&gt;&lt;p&gt;Apache Flink是一个分布式流处理器
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flink" scheme="https://masteryang4.github.io/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>mysql的binlog</title>
    <link href="https://masteryang4.github.io/2020/06/23/mysql%E7%9A%84binlog/"/>
    <id>https://masteryang4.github.io/2020/06/23/mysql%E7%9A%84binlog/</id>
    <published>2020-06-23T11:09:31.000Z</published>
    <updated>2020-06-23T11:11:12.598Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是Binlog"><a href="#什么是Binlog" class="headerlink" title="什么是Binlog"></a>什么是Binlog</h1><p>MySQL的二进制日志可以说是MySQL最重要的日志了，它记录了所有的DDL和DML(除了数据查询语句)语句，以事件形式记录，还包含语句所执行的消耗的时间，MySQL的二进制日志是事务安全型的。</p><p>  一般来说开启二进制日志大概会有1%的性能损耗 。二进制有两个最重要的使用场景: </p><p>  <strong>其一：MySQL Replication在Master端开启binlog，Master把它的二进制日志传递给slaves来达到master-slave数据一致的目的。</strong></p><p>  <strong>其二：自然就是数据恢复了，通过使用MySQLBinlog工具来使恢复数据。</strong></p><p>二进制日志包括两类文件：二进制日志索引文件（文件名后缀为.index）用于记录所有的二进制文件，二进制日志文件（文件名后缀为.00000*）记录数据库所有的DDL和DML(除了数据查询语句)语句事件。</p><h1 id="Binlog的开启"><a href="#Binlog的开启" class="headerlink" title="Binlog的开启"></a>Binlog的开启</h1><p>在MySQL的配置文件(Linux: <code>/etc/my.cnf</code> ,  Windows:<code>\my.ini</code>)下,修改配置在[mysqld] 区块设置/添加</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log-bin&#x3D;mysql-bin</span><br></pre></td></tr></table></figure></div><p>这个表示binlog日志的前缀是mysql-bin，以后生成的日志文件就是 mysql-bin.123456 的文件后面的数字按顺序生成。每次mysql重启或者到达单个文件大小的阈值时，新生一个文件，按顺序编号。</p><h1 id="Binlog的分类设置"><a href="#Binlog的分类设置" class="headerlink" title="Binlog的分类设置"></a>Binlog的分类设置</h1><p>MySQL Binlog的格式，那就是有三种，分别是STATEMENT,MIXED,ROW。</p><p>在配置文件中选择配置</p><blockquote><p>canal无执行引擎，一般row</p></blockquote><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">binlog_format&#x3D;row</span><br></pre></td></tr></table></figure></div><p>区别：</p><ul><li><p>statement</p><p><strong>语句级，binlog会记录每次一执行写操作的语句。</strong></p><p>相对row模式节省空间，但是可能产生不一致性，比如1</p></li></ul><p>update  tt set create_date=<code>now()</code></p><p>  如果用binlog日志进行恢复，由于执行时间不同可能产生的数据就不同。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">优点：节省空间</span><br><span class="line">缺点：有可能造成数据不一致。</span><br></pre></td></tr></table></figure></div><ul><li><p>row</p><p><strong>行级，binlog会记录每次操作后每行记录的变化。</strong></p></li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">优点：保持数据的绝对一致性。因为不管sql是什么，引用了什么函数，他只记录执行后的效果。</span><br><span class="line">缺点：占用较大空间。</span><br></pre></td></tr></table></figure></div><ul><li><p>mixed</p><p>statement的升级版，一定程度上解决了，因为一些情况而造成的statement模式不一致问题</p><p>在某些情况下譬如：</p><p>当函数中包含 UUID() 时；</p><p>包含 AUTO_INCREMENT 字段的表被更新时；</p><p>执行 INSERT DELAYED 语句时；</p><p>用 UDF 时；</p><p>会按照 ROW的方式进行处理</p></li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">优点：节省空间，同时兼顾了一定的一致性。</span><br><span class="line">缺点：还有些极个别情况依旧会造成不一致，</span><br></pre></td></tr></table></figure></div><p>另外statement和mixed对于需要对binlog的监控的情况都不方便。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;什么是Binlog&quot;&gt;&lt;a href=&quot;#什么是Binlog&quot; class=&quot;headerlink&quot; title=&quot;什么是Binlog&quot;&gt;&lt;/a&gt;什么是Binlog&lt;/h1&gt;&lt;p&gt;MySQL的二进制日志可以说是MySQL最重要的日志了，它记录了所有的DDL和DML
      
    
    </summary>
    
    
      <category term="SQL" scheme="https://masteryang4.github.io/categories/SQL/"/>
    
      <category term="MySQL" scheme="https://masteryang4.github.io/categories/SQL/MySQL/"/>
    
    
      <category term="SQL" scheme="https://masteryang4.github.io/tags/SQL/"/>
    
      <category term="MySQL" scheme="https://masteryang4.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>spark系列之spark-core</title>
    <link href="https://masteryang4.github.io/2020/06/19/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark-core/"/>
    <id>https://masteryang4.github.io/2020/06/19/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark-core/</id>
    <published>2020-06-19T03:04:35.000Z</published>
    <updated>2020-06-19T16:45:31.087Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark内核概述"><a href="#Spark内核概述" class="headerlink" title="Spark内核概述"></a>Spark内核概述</h1><p>Spark内核泛指Spark的核心运行机制，包括Spark核心组件的运行机制、Spark任务调度机制、Spark内存管理机制、Spark核心功能的运行原理等，熟练掌握Spark内核原理，能够帮助我们更好地完成Spark代码设计，并能够帮助我们准确锁定项目运行过程中出现的问题的症结所在。</p><h2 id="Spark核心组件"><a href="#Spark核心组件" class="headerlink" title="Spark核心组件"></a>Spark核心组件</h2><p><strong>Driver</strong></p><p>Spark驱动器节点，用于执行Spark任务中的main方法，负责实际代码的执行工作。Driver在Spark作业执行时主要负责：</p><p>1) 将用户程序转化为作业（Job）；</p><p>2) 在Executor之间调度任务（Task）；</p><p>3) 跟踪Executor的执行情况；</p><p>4) 通过UI展示查询运行情况；</p><p><strong>Executor</strong></p><p>Spark Executor节点是负责在Spark作业中运行具体任务，任务彼此之间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个Spark应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。</p><p>Executor有两个核心功能：</p><p>1) 负责运行组成Spark应用的任务，并将结果返回给驱动器（Driver）；</p><p>2) 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</p><h2 id="Spark通用运行流程"><a href="#Spark通用运行流程" class="headerlink" title="Spark通用运行流程"></a>Spark通用运行流程</h2><p><a href="https://pic.downk.cc/item/5eec244614195aa5949543db.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eec244614195aa5949543db.png" class="lazyload"></a></p><p>上图为Spark通用运行流程图，体现了基本的Spark应用程序在部署中的基本提交流程。</p><p>这个流程是按照如下的核心步骤进行工作的：</p><p>1) 任务提交后，都会先启动Driver程序；</p><p>2) 随后Driver向集群管理器注册应用程序；</p><p>3) 之后集群管理器根据此任务的配置文件分配Executor并启动；</p><p>4) Driver开始执行main函数，Spark查询为懒执行，当执行到Action算子时开始反向推算，根据宽依赖进行Stage的划分，随后每一个Stage对应一个Taskset，Taskset中有多个Task，查找可用资源Executor进行调度；</p><p>5) 根据本地化原则，Task会被分发到指定的Executor去执行，在任务执行的过程中，Executor也会不断与Driver进行通信，报告任务运行情况。</p><h1 id="Spark部署模式"><a href="#Spark部署模式" class="headerlink" title="Spark部署模式"></a>Spark部署模式</h1><p>Spark支持多种集群管理器（Cluster Manager），分别为：</p><p>1) Standalone：独立模式，Spark原生的简单集群管理器，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统，使用Standalone可以很方便地搭建一个集群；</p><p>2) Hadoop YARN：统一的资源管理机制，在上面可以运行多套计算框架，如MR、Storm等。根据Driver在集群中的位置不同，分为yarn client和yarn cluster；</p><p>3) Apache Mesos：一个强大的分布式资源管理框架，它允许多种不同的框架部署在其上，包括Yarn。</p><p>4) K8S : 容器式部署环境。</p><p>实际上，除了上述这些通用的集群管理器外，Spark内部也提供了方便用户测试和学习的本地集群部署模式和Windows环境。由于在实际工厂环境下使用的绝大多数的集群管理器是Hadoop YARN，因此我们关注的重点是Hadoop YARN模式下的Spark集群部署。</p><h2 id="Yarn模式运行机制"><a href="#Yarn模式运行机制" class="headerlink" title="Yarn模式运行机制"></a>Yarn模式运行机制</h2><h3 id="YARN-Cluster模式"><a href="#YARN-Cluster模式" class="headerlink" title="YARN Cluster模式"></a>YARN Cluster模式</h3><p>1) 执行脚本提交任务，实际是启动一个SparkSubmit的JVM进程；</p><p>2) SparkSubmit类中的main方法反射调用YarnClusterApplication的main方法；</p><p>3) YarnClusterApplication创建Yarn客户端，然后向Yarn发送执行指令：bin/java ApplicationMaster；</p><p>4) Yarn框架收到指令后会在指定的NM中启动ApplicationMaster；</p><p>5) ApplicationMaster启动Driver线程，执行用户的作业；</p><p>6) AM向RM注册，申请资源；</p><p>7) 获取资源后AM向NM发送指令：bin/java CoarseGrainedExecutorBackend；</p><p>8) CoarseGrainedExecutorBackend进程会接收消息，跟Driver通信，注册已经启动的Executor；然后启动计算对象Executor等待接收任务</p><p>Driver分配任务并监控任务的执行。</p><blockquote><p>注意：SparkSubmit、ApplicationMaster和CoarseGrainedExecutorBackend是独立的进程；Driver是独立的线程；Executor和YarnClusterApplication是对象。</p></blockquote><p><strong>【9步】</strong></p><p><a href="https://pic.downk.cc/item/5eeb18e314195aa59476ee22.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eeb18e314195aa59476ee22.png" class="lazyload"></a></p><h3 id="YARN-Client模式"><a href="#YARN-Client模式" class="headerlink" title="YARN Client模式"></a>YARN Client模式</h3><p>1) 执行脚本提交任务，实际是启动一个SparkSubmit的JVM进程；</p><p>2) SparkSubmit类中的main方法反射调用用户代码的main方法；</p><p>3) 启动Driver线程，执行用户的作业，并创建ScheduleBackend；</p><p>4) YarnClientSchedulerBackend向RM发送指令：bin/java ExecutorLauncher；</p><p>5) Yarn框架收到指令后会在指定的NM中启动ExecutorLauncher（实际上还是调用ApplicationMaster的main方法）；</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExecutorLauncher</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">ApplicationMaster</span>.main(args)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>6) AM向RM注册，申请资源；</p><p>7) 获取资源后AM向NM发送指令：bin/java CoarseGrainedExecutorBackend；</p><p>8) CoarseGrainedExecutorBackend进程会接收消息，跟Driver通信，注册已经启动的Executor；然后启动计算对象Executor等待接收任务</p><p>9) Driver分配任务并监控任务的执行。</p><blockquote><p>注意：SparkSubmit、ApplicationMaster和CoarseGrainedExecutorBackend是独立的进程；Executor和Driver是对象。</p></blockquote><p><a href="https://pic.downk.cc/item/5eecab6c14195aa59434d7ec.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eecab6c14195aa59434d7ec.png" class="lazyload"></a></p><h2 id="Standalone模式运行机制"><a href="#Standalone模式运行机制" class="headerlink" title="Standalone模式运行机制"></a>Standalone模式运行机制</h2><p>Standalone集群有2个重要组成部分，分别是：</p><p>1) Master(RM)：是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责；</p><p>2) Worker(NM)：是一个进程，一个Worker运行在集群中的一台服务器上，主要负责两个职责，一个是用自己的内存存储RDD的某个或某些partition；另一个是启动其他进程和线程（Executor），对RDD上的partition进行并行的处理和计算。</p><h3 id="Standalone-Cluster模式"><a href="#Standalone-Cluster模式" class="headerlink" title="Standalone Cluster模式"></a>Standalone Cluster模式</h3><p>在Standalone Cluster模式下，任务提交后，Master会找到一个Worker启动Driver。Driver启动后向Master注册应用程序，Master根据submit脚本的资源需求找到内部资源至少可以启动一个Executor的所有Worker，然后在这些Worker之间分配Executor，Worker上的Executor启动后会向Driver反向注册，所有的Executor注册完成后，Driver开始执行main函数，之后执行到Action算子时，开始划分Stage，每个Stage生成对应的taskSet，之后将Task分发到各个Executor上执行。</p><h3 id="Standalone-Client模式"><a href="#Standalone-Client模式" class="headerlink" title="Standalone Client模式"></a>Standalone Client模式</h3><p>在Standalone Client模式下，Driver在任务提交的本地机器上运行。Driver启动后向Master注册应用程序，Master根据submit脚本的资源需求找到内部资源至少可以启动一个Executor的所有Worker，然后在这些Worker之间分配Executor，Worker上的Executor启动后会向Driver反向注册，所有的Executor注册完成后，Driver开始执行main函数，之后执行到Action算子时，开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行。</p><h1 id="Spark通讯架构"><a href="#Spark通讯架构" class="headerlink" title="Spark通讯架构"></a>Spark通讯架构</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark中通信框架的发展：</p><ul><li><p>Spark早期版本中采用Akka作为内部通信部件。</p></li><li><p>Spark1.3中引入Netty通信框架，为了解决Shuffle的大数据传输问题使用</p></li><li><p>Spark1.6中Akka和Netty可以配置使用。Netty完全实现了Akka在Spark中的功能。</p></li><li><p>Spark2系列中，Spark抛弃Akka，使用Netty。</p></li></ul><p>RPC通信协议原理图：</p><p><a href="https://i.loli.net/2020/06/19/18RpdelwZthvmG7.png" data-fancybox="group" data-caption="RPC1.png" class="fancybox"><img alt="RPC1.png" title="RPC1.png" data-src="https://i.loli.net/2020/06/19/18RpdelwZthvmG7.png" class="lazyload"></a></p><p>Spark通信终端</p><p>Driver:</p><p><code>class DriverEndpoint extends ThreadSafeRpcEndpoint</code></p><p>Executor</p><p><code>class CoarseGrainedExecutorBackend extends ThreadSafeRpcEndpoint</code></p><p><a href="https://pic.downk.cc/item/5eec269214195aa594979034.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eec269214195aa594979034.png" class="lazyload"></a></p><h2 id="spark通讯架构解析"><a href="#spark通讯架构解析" class="headerlink" title="spark通讯架构解析"></a>spark通讯架构解析</h2><p><a href="https://pic.downk.cc/item/5eec26e514195aa59497cf6b.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eec26e514195aa59497cf6b.png" class="lazyload"></a></p><ul><li><p>RpcEndpoint：RPC通信终端。Spark针对每个节点（Client/Master/Worker）都称之为一个RPC终端，且都实现RpcEndpoint接口，内部根据不同端点的需求，设计不同的消息和不同的业务处理，如果需要发送（询问）则调用Dispatcher。在Spark中，所有的终端都存在生命周期：</p><ul><li>Constructor</li><li>onStart</li><li>receive*</li><li>onStop</li></ul></li><li><p>RpcEnv：RPC上下文环境，每个RPC终端运行时依赖的上下文环境称为RpcEnv；在把当前Spark版本中使用的NettyRpcEnv</p></li><li><p>Dispatcher：消息调度（分发）器，针对于RPC终端需要发送远程消息或者从远程RPC接收到的消息，分发至对应的指令收件箱（发件箱）。如果指令接收方是自己则存入收件箱，如果指令接收方不是自己，则放入发件箱；</p></li><li><p>Inbox：指令消息收件箱。一个本地RpcEndpoint对应一个收件箱，Dispatcher在每次向Inbox存入消息时，都将对应EndpointData加入内部ReceiverQueue中，另外Dispatcher创建时会启动一个单独线程进行轮询ReceiverQueue，进行收件箱消息消费；</p></li><li><p>RpcEndpointRef：RpcEndpointRef是对远程RpcEndpoint的一个引用。当我们需要向一个具体的RpcEndpoint发送消息时，一般我们需要获取到该RpcEndpoint的引用，然后通过该应用发送消息。</p></li><li><p>OutBox：指令消息发件箱。对于当前RpcEndpoint来说，一个目标RpcEndpoint对应一个发件箱，如果向多个目标RpcEndpoint发送信息，则有多个OutBox。当消息放入Outbox后，紧接着通过TransportClient将消息发送出去。消息放入发件箱以及发送过程是在同一个线程中进行；</p></li><li><p>RpcAddress：表示远程的RpcEndpointRef的地址，Host + Port。</p></li><li><p>TransportClient：Netty通信客户端，一个OutBox对应一个TransportClient，TransportClient不断轮询OutBox，根据OutBox消息的receiver信息，请求对应的远程TransportServer；</p></li><li><p>TransportServer：Netty通信服务端，一个RpcEndpoint对应一个TransportServer，接受远程消息后调用Dispatcher分发消息至对应收发件箱；</p></li></ul><h1 id="Spark任务调度机制"><a href="#Spark任务调度机制" class="headerlink" title="Spark任务调度机制"></a>Spark任务调度机制</h1><p>在生产环境下，Spark集群的部署方式一般为YARN-Cluster模式，之后的内核分析内容中我们默认集群的部署方式为YARN-Cluster模式。在上一章中我们讲解了Spark YARN-Cluster模式下的任务提交流程，但是我们并没有具体说明Driver的工作流程， Driver线程主要是初始化SparkContext对象，准备运行所需的上下文，然后一方面保持与ApplicationMaster的RPC连接，通过ApplicationMaster申请资源，另一方面根据用户业务逻辑开始调度任务，将任务下发到已有的空闲Executor上。</p><p>当ResourceManager向ApplicationMaster返回Container资源时，ApplicationMaster就尝试在对应的Container上启动Executor进程，Executor进程起来后，会向Driver反向注册，注册成功后保持与Driver的心跳，同时等待Driver分发任务，当分发的任务执行完毕后，将任务状态上报给Driver。</p><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p>当Driver起来后，Driver则会根据用户程序逻辑准备任务，并根据Executor资源情况逐步分发任务。在详细阐述任务调度前，首先说明下Spark里的几个概念。一个Spark应用程序包括Job、Stage以及Task三个概念：</p><p>1) Job是以Action方法为界，遇到一个Action方法则触发一个Job；</p><p>2) Stage是Job的子集，以RDD宽依赖(即Shuffle)为界，遇到Shuffle做一次划分；</p><p>3) Task是Stage的子集，以并行度(分区数)来衡量，分区数是多少，则有多少个task。</p><blockquote><p>Spark的任务调度总体来说分两路进行，一路是Stage级的调度，一路是Task级的调度。</p></blockquote><p><strong>【重点】总体调度流程</strong>如下图所示</p><p><a href="https://pic.downk.cc/item/5eece22914195aa5947d1eb5.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece22914195aa5947d1eb5.png" class="lazyload"></a></p><p>Spark RDD通过其Transactions操作，形成了RDD血缘（依赖）关系图，即DAG，最后通过Action的调用，触发Job并调度执行，执行过程中会创建两个调度器：DAGScheduler和TaskScheduler。</p><ul><li><p>DAGScheduler负责Stage级的调度，主要是将job切分成若干Stages，并将每个Stage打包成TaskSet交给TaskScheduler调度。</p></li><li><p>TaskScheduler负责Task级的调度，将DAGScheduler给过来的TaskSet按照指定的调度策略分发到Executor上执行，调度过程中SchedulerBackend负责提供可用资源，其中SchedulerBackend有多种实现，分别对接不同的资源管理系统。</p></li></ul><p><a href="https://pic.downk.cc/item/5eece28714195aa5947d9aaa.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece28714195aa5947d9aaa.png" class="lazyload"></a></p><blockquote><p>【扩展】EventQueue：双端（阻塞）队列；</p><p>​                BlockingQueue：阻塞队列</p></blockquote><p><strong>Driver初始化SparkContext过程中，会分别初始化DAGScheduler、TaskScheduler、SchedulerBackend以及HeartbeatReceiver，并启动SchedulerBackend以及HeartbeatReceiver。</strong></p><p>SchedulerBackend通过ApplicationMaster申请资源，并不断从TaskScheduler中拿到合适的Task分发到Executor执行。HeartbeatReceiver负责接收Executor的心跳信息，监控Executor的存活状况，并通知到TaskScheduler。</p><h2 id="Spark-Stage级调度"><a href="#Spark-Stage级调度" class="headerlink" title="Spark Stage级调度"></a>Spark Stage级调度</h2><p>Spark的任务调度是从DAG切割开始，主要是由DAGScheduler来完成。当遇到一个Action操作后就会触发一个Job的计算，并交给DAGScheduler来提交，下图是涉及到Job提交的相关方法调用流程图。</p><p><a href="https://pic.downk.cc/item/5eece2f214195aa5947e2440.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece2f214195aa5947e2440.png" class="lazyload"></a></p><p>1) Job由最终的RDD和Action方法封装而成；</p><p>2) SparkContext将Job交给DAGScheduler提交，它会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages，具体划分策略是，由最终的RDD不断通过依赖回溯判断父依赖是否是宽依赖，即以Shuffle为界，划分Stage，窄依赖的RDD之间被划分到同一个Stage中，可以进行pipeline式的计算。划分的Stages分两类，一类叫做ResultStage，为DAG最下游的Stage，由Action方法决定，另一类叫做ShuffleMapStage，为下游Stage准备数据.，下面看一个简单的例子WordCount。</p><p><a href="https://pic.downk.cc/item/5eece31e14195aa5947e58cd.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece31e14195aa5947e58cd.png" class="lazyload"></a></p><p>Job由saveAsTextFile触发，该Job由RDD-3和saveAsTextFile方法组成，根据RDD之间的依赖关系从RDD-3开始回溯搜索，直到没有依赖的RDD-0，在回溯搜索过程中，RDD-3依赖RDD-2，并且是宽依赖，所以在RDD-2和RDD-3之间划分Stage，RDD-3被划到最后一个Stage，即ResultStage中，RDD-2依赖RDD-1，RDD-1依赖RDD-0，这些依赖都是窄依赖，所以将RDD-0、RDD-1和RDD-2划分到同一个Stage，形成pipeline操作。即ShuffleMapStage中，实际执行的时候，数据记录会一气呵成地执行RDD-0到RDD-2的转化。不难看出，其本质上是一个深度优先搜索（Depth First Search）算法。</p><p>一个Stage是否被提交，需要判断它的父Stage是否执行，只有在父Stage执行完毕才能提交当前Stage，如果一个Stage没有父Stage，那么从该Stage开始提交。Stage提交时会将Task信息（分区信息以及方法等）序列化并被打包成TaskSet交给TaskScheduler，一个Partition对应一个Task，另一方面TaskScheduler会监控Stage的运行状态，只有Executor丢失或者Task由于Fetch失败才需要重新提交失败的Stage以调度运行失败的任务，其他类型的Task失败会在TaskScheduler的调度过程中重试。</p><p>相对来说DAGScheduler做的事情较为简单，仅仅是在Stage层面上划分DAG，提交Stage并监控相关状态信息。TaskScheduler则相对较为复杂，下面详细阐述其细节。</p><h2 id="Spark-Task级调度"><a href="#Spark-Task级调度" class="headerlink" title="Spark Task级调度"></a>Spark Task级调度</h2><p>Spark Task的调度是由TaskScheduler来完成，由前文可知，DAGScheduler将Stage打包到TaskSet交给TaskScheduler，TaskScheduler会将TaskSet封装为TaskSetManager加入到调度队列中，TaskSetManager结构如下图所示。</p><p><a href="https://pic.downk.cc/item/5eece39914195aa5947eff2e.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece39914195aa5947eff2e.png" class="lazyload"></a></p><p><strong>TaskSetManager负责监控管理同一个Stage中的Tasks，TaskScheduler就是以TaskSetManager为单元来调度任务。</strong></p><p>前面也提到，TaskScheduler初始化后会启动SchedulerBackend，它负责跟外界打交道，接收Executor的注册信息，并维护Executor的状态，所以说SchedulerBackend是管“粮食”的，同时它在启动后会定期地去“询问”TaskScheduler有没有任务要运行，也就是说，它会定期地“问”TaskScheduler“我有这么余粮，你要不要啊”，TaskScheduler在SchedulerBackend“问”它的时候，会从调度队列中按照指定的调度策略选择TaskSetManager去调度运行，大致方法调用流程如下图所示：</p><p><a href="https://pic.downk.cc/item/5eece3e214195aa5947f601a.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece3e214195aa5947f601a.png" class="lazyload"></a></p><p>将<strong>TaskSetManager加入rootPool调度池中</strong>之后，调用SchedulerBackend的riviveOffers方法给driverEndpoint发送ReviveOffer消息；driverEndpoint收到ReviveOffer消息后调用makeOffers方法，过滤出活跃状态的Executor（这些Executor都是任务启动时反向注册到Driver的Executor），然后将Executor封装成WorkerOffer对象；准备好计算资源（WorkerOffer）后，taskScheduler基于这些资源调用resourceOffer在Executor上分配task。</p><h3 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h3><p>TaskScheduler支持两种调度策略，<strong>一种是FIFO，也是默认的调度策略</strong>，另一种是FAIR。在TaskScheduler初始化过程中会实例化rootPool，表示树的根节点，是Pool类型。</p><p>1) FIFO调度策略</p><p>如果是采用FIFO调度策略，则直接简单地将TaskSetManager按照先来先到的方式入队，出队时直接拿出最先进队的TaskSetManager，其树结构如下图所示，TaskSetManager保存在一个FIFO队列中。</p><p><a href="https://pic.downk.cc/item/5eece42314195aa5947fbfe5.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece42314195aa5947fbfe5.png" class="lazyload"></a></p><p>2) FAIR调度策略</p><p>FAIR调度策略的树结构如下图所示：</p><p><a href="https://pic.downk.cc/item/5eece44414195aa5947ff312.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece44414195aa5947ff312.png" class="lazyload"></a></p><p>FAIR模式中有一个rootPool和多个子Pool，各个子Pool中存储着所有待分配的TaskSetMagager。</p><p>在FAIR模式中，需要先对子Pool进行排序，再对子Pool里面的TaskSetManager进行排序，因为Pool和TaskSetMagager都继承了Schedulable特质，因此使用相同的排序算法。</p><p>排序过程的比较是基于Fair-share来比较的，<strong>每个要排序的对象包含三个属性: runningTasks值（正在运行的Task数）、minShare值（资源合理利用的值，比如核的利用率）、weight值</strong>，比较时会综合考量runningTasks值，minShare值以及weight值。</p><p>注意，minShare、weight的值均在公平调度配置文件fairscheduler.xml中被指定，调度池在构建阶段会读取此文件的相关配置。</p><ul><li><p>如果A对象的runningTasks大于它的minShare，B对象的runningTasks小于它的minShare，那么B排在A前面；（runningTasks比minShare小的先执行）</p></li><li><p>如果A、B对象的runningTasks都小于它们的minShare，那么就比较runningTasks与minShare的比值（minShare使用率），谁小谁排前面；（minShare使用率低的先执行）</p></li><li><p>如果A、B对象的runningTasks都大于它们的minShare，那么就比较runningTasks与weight的比值（权重使用率），谁小谁排前面。（权重使用率低的先执行）</p></li><li><p>如果上述比较均相等，则比较名字。</p></li></ul><p>整体上来说就是通过minShare和weight这两个参数控制比较过程，可以做到让minShare使用率和权重使用率少（实际运行task比例较少）的先运行。</p><p>FAIR模式排序完成后，所有的TaskSetManager被放入一个ArrayBuffer里，之后依次被取出并发送给Executor执行。</p><p>从调度队列中拿到TaskSetManager后，由于TaskSetManager封装了一个Stage的所有Task，并负责管理调度这些Task，那么接下来的工作就是TaskSetManager按照一定的规则一个个取出Task给TaskScheduler，TaskScheduler再交给SchedulerBackend去发到Executor上执行。</p><h3 id="本地化调度"><a href="#本地化调度" class="headerlink" title="本地化调度"></a>本地化调度</h3><p>WHY？【解决任务发给谁的问题】【其实就是计算和数据的相对位置】</p><blockquote><p>【移动数据不如移动计算】</p></blockquote><p>DAGScheduler切割Job，划分Stage, 通过调用submitStage来提交一个Stage对应的tasks，submitStage会调用submitMissingTasks，submitMissingTasks 确定每个需要计算的 task 的preferredLocations，通过调用getPreferrdeLocations()得到partition 的优先位置，由于一个partition对应一个Task，此partition的优先位置就是task的优先位置，对于要提交到TaskScheduler的TaskSet中的每一个Task，该task优先位置与其对应的partition对应的优先位置一致。</p><p>从调度队列中拿到TaskSetManager后，那么接下来的工作就是TaskSetManager按照一定的规则一个个取出task给TaskScheduler，TaskScheduler再交给SchedulerBackend去发到Executor上执行。前面也提到，TaskSetManager封装了一个Stage的所有Task，并负责管理调度这些Task。</p><p>根据每个Task的优先位置，确定Task的Locality级别，Locality一共有五种，优先级由高到低顺序：</p><table><thead><tr><th>名称</th><th>解析</th></tr></thead><tbody><tr><td>PROCESS_LOCAL</td><td>进程本地化，task和数据在同一个Executor中，性能最好。</td></tr><tr><td>NODE_LOCAL</td><td>节点本地化，task和数据在同一个节点中，但是task和数据不在同一个Executor中，数据需要在进程间进行传输。</td></tr><tr><td>RACK_LOCAL</td><td>机架本地化，task和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输。</td></tr><tr><td>NO_PREF</td><td>对于task来说，从哪里获取都一样，没有好坏之分。</td></tr><tr><td>ANY</td><td>task和数据可以在集群的任何地方，而且不在一个机架中，性能最差。</td></tr></tbody></table><p>在调度执行时，Spark调度总是会尽量让每个task以最高的本地性级别来启动，当一个task以X本地性级别启动，但是该本地性级别对应的所有节点都没有空闲资源而启动失败，此时并不会马上降低本地性级别启动而是在某个时间长度内再次以X本地性级别来启动该task，若超过限时时间则降级启动，去尝试下一个本地性级别，依次类推。</p><p><strong>可以通过调大每个类别的最大容忍延迟时间，在等待阶段对应的Executor可能就会有相应的资源去执行此task，这就在在一定程度上提到了运行性能。</strong></p><h3 id="失败重试与黑名单机制"><a href="#失败重试与黑名单机制" class="headerlink" title="失败重试与黑名单机制"></a>失败重试与黑名单机制</h3><p>除了选择合适的Task调度运行外，还需要监控Task的执行状态，前面也提到，与外部打交道的是SchedulerBackend，Task被提交到Executor启动执行后，Executor会将执行状态上报给SchedulerBackend，SchedulerBackend则告诉TaskScheduler，TaskScheduler找到该Task对应的TaskSetManager，并通知到该TaskSetManager，这样TaskSetManager就知道Task的失败与成功状态，对于失败的Task，会记录它失败的次数，如果失败次数还没有超过最大重试次数，那么就把它放回待调度的Task池子中，否则整个Application失败。</p><p><strong>在记录Task失败次数过程中，会记录它上一次失败所在的Executor Id和Host，这样下次再调度这个Task时，会使用黑名单机制，避免它被调度到上一次失败的节点上，起到一定的容错作用</strong>。黑名单记录Task上一次失败所在的Executor Id和Host，以及其对应的“拉黑”时间，“拉黑”时间是指这段时间内不要再往这个节点上调度这个Task了。</p><h1 id="Spark-shuffle解析"><a href="#Spark-shuffle解析" class="headerlink" title="Spark shuffle解析"></a>Spark shuffle解析</h1><blockquote><p>shuffle的本质就是落盘，关键是如何落盘，是排序后落盘还是不排序落盘</p></blockquote><h2 id="核心要点"><a href="#核心要点" class="headerlink" title="核心要点"></a>核心要点</h2><p><strong>ShuffleMapStage与ResultStage</strong></p><p><a href="https://pic.downk.cc/item/5eece31e14195aa5947e58cd.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece31e14195aa5947e58cd.png" class="lazyload"></a></p><p>在划分stage时，最后一个stage称为finalStage，它本质上是一个ResultStage对象，前面的所有stage被称为ShuffleMapStage。</p><p>ShuffleMapStage的结束伴随着shuffle文件的写磁盘。</p><p>ResultStage基本上对应代码中的action算子，即将一个函数应用在RDD的各个partition的数据集上，意味着一个job的运行结束。</p><h2 id="HashShuffle解析"><a href="#HashShuffle解析" class="headerlink" title="HashShuffle解析"></a>HashShuffle解析</h2><h3 id="未优化的HashShuffle"><a href="#未优化的HashShuffle" class="headerlink" title="未优化的HashShuffle"></a>未优化的HashShuffle</h3><p>这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p><p>如下图中有3个 Reducer，从Task 开始那边各自把自己进行 Hash 计算(分区器：hash/numreduce取模)，分类出3个不同的类别，每个 Task 都分成3种类别的数据，想把不同的数据汇聚然后计算出最终的结果，所以Reducer 会在每个 Task 中把属于自己类别的数据收集过来，汇聚成一个同类别的大集合，每1个 Task 输出3份本地文件，这里有4个 Mapper Tasks，所以总共输出了4个 Tasks x 3个分类文件 = 12个本地小文件。</p><p><a href="https://pic.downk.cc/item/5eec291b14195aa59499bc7c.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eec291b14195aa59499bc7c.png" class="lazyload"></a></p><h3 id="优化的HashShuffle"><a href="#优化的HashShuffle" class="headerlink" title="优化的HashShuffle"></a>优化的HashShuffle</h3><p>优化的HashShuffle过程就是启用合并机制，合并机制就是复用buffer，开启合并机制的配置是spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>这里还是有4个Tasks，数据类别还是分成3种类型，因为Hash算法会根据你的 Key 进行分类，在同一个进程中，无论是有多少过Task，都会把同样的Key放在同一个Buffer里，然后把Buffer中的数据写入以Core数量为单位的本地文件中，(一个Core只有一种类型的Key的数据)，每1个Task所在的进程中，分别写入共同进程中的3份本地文件，这里有4个Mapper Tasks，所以总共输出是 2个Cores x 3个分类文件 = 6个本地小文件。</p><p><a href="https://pic.downk.cc/item/5eec293514195aa59499d446.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eec293514195aa59499d446.png" class="lazyload"></a></p><h2 id="SortShuffle解析"><a href="#SortShuffle解析" class="headerlink" title="SortShuffle解析"></a>SortShuffle解析</h2><h3 id="普通SortShuffle"><a href="#普通SortShuffle" class="headerlink" title="普通SortShuffle"></a>普通SortShuffle</h3><p>在该模式下，数据会先写入一个数据结构，reduceByKey写入Map，一边通过Map局部聚合，一遍写入内存。Join算子写入ArrayList直接写入内存中。然后需要判断是否达到阈值，如果达到就会将内存数据结构的数据写入到磁盘，清空内存数据结构。</p><p>在溢写磁盘前，先根据key进行排序，排序过后的数据，会分批写入到磁盘文件中。默认批次为10000条，数据会以每批一万条写入到磁盘文件。写入磁盘文件通过缓冲区溢写的方式，每次溢写都会产生一个磁盘文件，也就是说一个Task过程会产生多个临时文件。</p><p>最后在每个Task中，将所有的临时文件合并，这就是merge过程，此过程将所有临时文件读取出来，一次写入到最终文件。意味着一个Task的所有数据都在这一个文件中。同时单独写一份索引文件，标识下游各个Task的数据在文件中的索引，start offset和end offset。</p><p><a href="https://pic.downk.cc/item/5eec294714195aa59499f0b0.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eec294714195aa59499f0b0.png" class="lazyload"></a></p><p><a href="https://pic.downk.cc/item/5eece5f014195aa594824296.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece5f014195aa594824296.png" class="lazyload"></a></p><h3 id="bypass-SortShuffle"><a href="#bypass-SortShuffle" class="headerlink" title="bypass SortShuffle"></a>bypass SortShuffle</h3><p>bypass运行机制的触发条件如下：</p><p><strong>1)不是聚合类的shuffle算子，比如reduceByKey。</strong></p><p><strong>2) shuffle reduce task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值，默认为200。</strong></p><p>此时task会为每个reduce端的task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。<strong>最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</strong></p><p><strong>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的</strong>，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p><p><strong>而该机制与普通SortShuffleManager运行机制的不同在于：不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</strong></p><h1 id="Spark内存管理"><a href="#Spark内存管理" class="headerlink" title="Spark内存管理"></a>Spark内存管理</h1><h2 id="堆内和堆外内存规划"><a href="#堆内和堆外内存规划" class="headerlink" title="堆内和堆外内存规划"></a>堆内和堆外内存规划</h2><p>作为一个JVM 进程，Executor 的内存管理建立在JVM的内存管理之上，Spark对 JVM的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。堆内内存受到JVM统一管理，堆外内存是直接向操作系统进行内存的申请和释放。</p><p><a href="https://pic.downk.cc/item/5eece67e14195aa5948335f5.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece67e14195aa5948335f5.png" class="lazyload"></a></p><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>堆内内存的大小，由Spark应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。Executor 内运行的并发任务共享 JVM 堆内内存，这些任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存，而这些任务在执行 Shuffle 时占用的内存被规划为执行（Execution）内存，剩余的部分不做特殊规划，那些Spark内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间。不同的管理模式下，这三部分占用的空间大小各不相同。<br>Spark对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由JVM完成，Spark只能在申请后和释放前记录这些内存，我们来看其具体流程：<br>申请内存流程如下：<br>Spark 在代码中 new 一个对象实例；<br>JVM 从堆内内存分配空间，创建对象并返回对象引用；<br>Spark 保存该对象的引用，记录该对象占用的内存。<br>释放内存流程如下：</p><ol><li>Spark记录该对象释放的内存，删除该对象的引用；</li><li>等待JVM的垃圾回收机制释放该对象占用的堆内内存。<br>我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。<br>对于Spark中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期。此外，在被Spark标记为释放的对象实例，很有可能在实际上并没有被JVM回收，导致实际可用的内存小于Spark记录的可用内存。所以 Spark并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。<br>虽然不能精准控制堆内内存的申请和释放，但 Spark 通过对存储内存和执行内存各自独立的规划管理，可以决定是否要在存储内存里缓存新的 RDD，以及是否为新的任务分配执行内存，在一定程度上可以提升内存的利用率，减少异常的出现。</li></ol><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p><strong>为了进一步优化内存的使用以及提高Shuffle时排序的效率，Spark引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。</strong><br>堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。<br>利用JDK Unsafe API（从Spark 2.0开始，在管理堆外的存储内存时不再基于Tachyon，而是与堆外的执行内存一样，基于 JDK Unsafe API 实现），Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。<strong>堆外内存可以被精确地申请和释放（堆外内存之所以能够被精确的申请和释放，是由于内存的申请和释放不再通过JVM机制，而是直接向操作系统申请，JVM对于内存的清理是无法准确指定时间点的，因此无法实现精确的释放），而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。</strong><br><strong>在默认情况下堆外内存并不启用</strong>，可通过配置<code>spark.memory.offHeap.enabled</code> 参数启用，并由 <code>spark.memory.offHeap.size</code>参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。</p><h2 id="内存空间分配"><a href="#内存空间分配" class="headerlink" title="内存空间分配"></a>内存空间分配</h2><h3 id="静态内存管理"><a href="#静态内存管理" class="headerlink" title="静态内存管理"></a>静态内存管理</h3><p>在Spark最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在Spark应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置，堆内内存的分配如图所示：</p><p><a href="https://pic.downk.cc/item/5eece6fb14195aa59483f7f0.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece6fb14195aa59483f7f0.png" class="lazyload"></a></p><p>可以看到，可用的堆内内存的大小需要按照下列方式计算：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">可用的存储内存 &#x3D; systemMaxMemory * spark.storage.memoryFraction * spark.storage.safety Fraction</span><br><span class="line"></span><br><span class="line">可用的执行内存 &#x3D; systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safety Fraction</span><br></pre></td></tr></table></figure></div><p>其中systemMaxMemory取决于当前JVM堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的memoryFraction 参数和safetyFraction 参数相乘得出。上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 1-safetyFraction 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险（上文提到，对于非序列化对象的内存采样估算会产生误差）。值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。</p><p>Storage内存和Execution内存都有预留空间，目的是防止OOM，因为Spark堆内内存大小的记录是不准确的，需要留出保险区域。</p><p><strong>堆外的空间分配较为简单，只有存储内存和执行内存</strong>，如下图所示。可用的执行内存和存储内存占用的空间大小直接由参数spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域。</p><p><a href="https://pic.downk.cc/item/5eece75d14195aa594849f09.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece75d14195aa594849f09.png" class="lazyload"></a></p><p>静态内存管理机制实现起来较为简单，但如果用户不熟悉Spark的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><h3 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h3><p>Spark1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域，统一内存管理的堆内内存结构如图所示：</p><p><a href="https://pic.downk.cc/item/5eece7a214195aa594850a7e.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece7a214195aa594850a7e.png" class="lazyload"></a></p><p>统一内存管理的堆外内存结构如下图所示：</p><p><a href="https://pic.downk.cc/item/5eece7ca14195aa594854ab4.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece7ca14195aa594854ab4.png" class="lazyload"></a></p><p>其中最重要的优化在于<strong>动态占用机制</strong>，其规则如下：</p><p>1) 设定基本的存储内存和执行内存区域（spark.storage.storageFraction参数），该设定确定了双方各自拥有的空间的范围；</p><p>2) 双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的Block）</p><p><strong>3) 执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间；</strong></p><p><strong>4) 存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle过程中的很多因素，实现起来较为复杂。【为了让execution保证计算准确】</strong></p><p>统一内存管理的动态占用机制如图所示：</p><p>【重点】</p><p><a href="https://pic.downk.cc/item/5eece82214195aa59485e100.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eece82214195aa59485e100.png" class="lazyload"></a></p><p>凭借统一内存管理机制，Spark在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护Spark内存的难度，但并不意味着开发者可以高枕无忧。如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的RDD数据通常都是长期驻留内存的。所以要想充分发挥Spark的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理。</p><h2 id="存储内存管理"><a href="#存储内存管理" class="headerlink" title="存储内存管理"></a>存储内存管理</h2><p>RDD的持久化机制</p><p>RDD的缓存过程</p><p>淘汰与落盘</p><h2 id="执行内存管理"><a href="#执行内存管理" class="headerlink" title="执行内存管理"></a>执行内存管理</h2><p>Shuffle Write</p><p>Shuffle Read</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">任务的划分：</span><br><span class="line">ShuffleMapStage(1) &#x3D;&gt; ShuffleMapTask(N) &#x3D;&gt; shuffle write</span><br><span class="line">                                        &#x3D;&gt; shuffle read</span><br><span class="line">ResultStage(1)     &#x3D;&gt; ResultTask(N)     &#x3D;&gt; shuffle read</span><br><span class="line"></span><br><span class="line">任务的封装：</span><br><span class="line">Task &#x3D;&gt; TaskSet &#x3D;&gt; TaskSetManager &#x3D;&gt; TaskPool</span><br><span class="line"></span><br><span class="line">任务调度器(默认使用FIFO)</span><br><span class="line">FIFO : 先进先出</span><br><span class="line">FAIR : 公平 （runningTasks, minShare, weight）sortWith</span><br><span class="line"></span><br><span class="line">任务本地化级别：</span><br><span class="line">PROCESS_LOCAL : 内存数据的数据处理</span><br><span class="line">NODE_LOCAL    : yarn集群的方式访问HDFS文件</span><br><span class="line">RACK_LOCAL</span><br><span class="line">Any</span><br><span class="line"></span><br><span class="line">任务的执行</span><br><span class="line">Driver &#x3D;&gt; encode(Task) &#x3D;&gt; RPC &#x3D;&gt; ExecutorBackend &#x3D;&gt; decode(Task) &#x3D;&gt; Executor</span><br><span class="line">Executor &#x3D;&gt; ThreadPool &#x3D;&gt; TaskRunner &#x3D;&gt; run &#x3D;&gt; Task.run &#x3D;&gt; XXXTask.runTask</span><br><span class="line"></span><br><span class="line">Shuffle管理器 :</span><br><span class="line">SortShuffleManager</span><br><span class="line"></span><br><span class="line">Shuffle Writer:</span><br><span class="line">1.UnsafeShuffleWriter &#x3D;&gt; SerializedShuffleHandle</span><br><span class="line">2.BypassMergeSortShuffleWriter &#x3D;&gt; BypassMergeSortShuffleHandle</span><br><span class="line"></span><br><span class="line">    没有预聚合功能 &amp; reduce阶段的分区数量 &lt;&#x3D; 阈值（200）</span><br><span class="line"></span><br><span class="line">    有预聚合功能的算子   : reduceByKey combineByKey, aggregateByKey, foldByKey</span><br><span class="line">    没有预聚合功能的算子 : groupByKey sortByKey</span><br><span class="line"></span><br><span class="line">    实现方式类似于HashShuffle</span><br><span class="line"></span><br><span class="line">3.SortShuffleWriter &#x3D;&gt; BaseShuffleHandle</span><br><span class="line"></span><br><span class="line">    写磁盘文件时，首席会按照分区进行排序，然后默认按照key.hashCode排序</span><br><span class="line">    排序时，如果超过内存阈值 ：5m</span><br><span class="line"></span><br><span class="line">预聚合的原理： 在shuffle落盘之前的聚合功能</span><br><span class="line">PartitionedAppendOnlyMap &#x3D;&gt; Hashtable &#x3D;&gt; ( (分区ID，Key)， value )</span><br><span class="line">不支持预聚合</span><br><span class="line">PartitionedPairBuffer &#x3D;&gt; ( (分区ID，Key)， value )</span><br><span class="line"></span><br><span class="line">Spark内存</span><br><span class="line">静态内存管理：</span><br><span class="line">    存储内存：</span><br><span class="line">    val systemMaxMemory &#x3D; conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)</span><br><span class="line">    val memoryFraction &#x3D; conf.getDouble(&quot;spark.storage.memoryFraction&quot;, 0.6)</span><br><span class="line">    val safetyFraction &#x3D; conf.getDouble(&quot;spark.storage.safetyFraction&quot;, 0.9)</span><br><span class="line">    (systemMaxMemory * memoryFraction * safetyFraction).toLong</span><br><span class="line">    执行内存：</span><br><span class="line">    val systemMaxMemory &#x3D; conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)</span><br><span class="line">    val memoryFraction &#x3D; conf.getDouble(&quot;spark.shuffle.memoryFraction&quot;, 0.2)</span><br><span class="line">    val safetyFraction &#x3D; conf.getDouble(&quot;spark.shuffle.safetyFraction&quot;, 0.8)</span><br><span class="line">    (systemMaxMemory * memoryFraction * safetyFraction).toLong</span><br><span class="line">统一内存管理</span><br><span class="line">    存储内存：</span><br><span class="line">     val usableMemory &#x3D; systemMemory - reservedMemory</span><br><span class="line">     val memoryFraction &#x3D; conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6)</span><br><span class="line">     maxMemory &#x3D; (usableMemory * memoryFraction).toLong</span><br><span class="line">    onHeapStorageRegionSize &#x3D;</span><br><span class="line">        (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong,</span><br><span class="line">    执行内存：</span><br><span class="line"></span><br><span class="line">Spark配置：</span><br><span class="line">spark.scheduler.mode : 任务调度器，默认为FIFO，可以改为FAIR</span><br><span class="line">spark.locality.wait: 本地化等待时间，默认3s</span><br><span class="line">spark.shuffle.sort.bypassMergeThreshold : 忽略排序的阈值</span><br><span class="line">spark.local.dir : 本地文件存储路径</span><br><span class="line">spark.shuffle.spill.batchSize : 溢写磁盘的数据量 10000</span><br><span class="line">spark.memory.useLegacyMode : 内存管理兼容模式</span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Spark内核概述&quot;&gt;&lt;a href=&quot;#Spark内核概述&quot; class=&quot;headerlink&quot; title=&quot;Spark内核概述&quot;&gt;&lt;/a&gt;Spark内核概述&lt;/h1&gt;&lt;p&gt;Spark内核泛指Spark的核心运行机制，包括Spark核心组件的运行机制、Spa
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="spark" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"/>
    
    
      <category term="教程" scheme="https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="spark" scheme="https://masteryang4.github.io/tags/spark/"/>
    
      <category term="spark-core" scheme="https://masteryang4.github.io/tags/spark-core/"/>
    
  </entry>
  
  <entry>
    <title>kafka事务</title>
    <link href="https://masteryang4.github.io/2020/06/18/kafka%E4%BA%8B%E5%8A%A1/"/>
    <id>https://masteryang4.github.io/2020/06/18/kafka%E4%BA%8B%E5%8A%A1/</id>
    <published>2020-06-18T14:24:20.000Z</published>
    <updated>2020-06-18T14:25:10.271Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kafka事务"><a href="#kafka事务" class="headerlink" title="kafka事务"></a>kafka事务</h1><p>Kafka从【0.11】版本开始引入了事务支持。</p><p>事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</p><h2 id="Producer事务"><a href="#Producer事务" class="headerlink" title="Producer事务"></a>Producer事务</h2><p>为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并将Producer获得的PID和Transaction ID绑定。这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID。</p><p>为了管理Transaction，Kafka引入了一个新的组件Transaction Coordinator。</p><p>Producer就是通过和Transaction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将事务所有写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</p><h2 id="Consumer事务"><a href="#Consumer事务" class="headerlink" title="Consumer事务"></a>Consumer事务</h2><p>上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其时无法保证Commit的信息被精确消费。</p><p>这是由于Consumer可以通过offset访问任意信息，而且不同的Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;kafka事务&quot;&gt;&lt;a href=&quot;#kafka事务&quot; class=&quot;headerlink&quot; title=&quot;kafka事务&quot;&gt;&lt;/a&gt;kafka事务&lt;/h1&gt;&lt;p&gt;Kafka从【0.11】版本开始引入了事务支持。&lt;/p&gt;
&lt;p&gt;事务可以保证Kafka在Exactl
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="kafka" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/"/>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="kafka" scheme="https://masteryang4.github.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka_exactly_once语义</title>
    <link href="https://masteryang4.github.io/2020/06/18/kafka-exactly-once%E8%AF%AD%E4%B9%89/"/>
    <id>https://masteryang4.github.io/2020/06/18/kafka-exactly-once%E8%AF%AD%E4%B9%89/</id>
    <published>2020-06-18T14:22:35.000Z</published>
    <updated>2020-06-18T14:28:03.087Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Exactly-Once语义"><a href="#Exactly-Once语义" class="headerlink" title="Exactly Once语义"></a>Exactly Once语义</h1><blockquote><p>kafka 每个分区内的 Exactly Once</p></blockquote><p>将服务器的ACK级别设置为<code>-1</code>，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。</p><p>相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once语义。</p><p>At Least Once可以保证数据不丢失，但是不能保证数据不重复；</p><p>相对的，At Least Once可以保证数据不重复，但是不能保证数据不丢失。</p><p>但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义。在0.11版本以前的Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。</p><blockquote><p>【0.11】版本的Kafka，引入了一项重大特性：幂等性。</p></blockquote><p>开启幂等性<code>enable.idempotence=true</code>。</p><p>所谓的<strong>幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条</strong>。幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即：</p><blockquote><p>At Least Once + 幂等性 = Exactly Once</p></blockquote><p>要启用幂等性，只需要将Producer的参数中<code>enable.idompotence</code>设置为<code>true</code>即可。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。</span><br><span class="line">开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。</span><br><span class="line">而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。</span><br></pre></td></tr></table></figure></div><p>但是<strong>PID重启就会变化</strong>，同时不同的Partition也具有不同主键，</p><p>所以幂等性无法保证<strong>跨分区跨会话</strong>的Exactly Once。</p><blockquote><p>保证 kafka 数据无重复</p><p>​    1、幂等性+<code>ack=-1</code>+事务</p><p>​    2、可以在下一级：SparkStreaming、redis 或者 hive 中 dwd 层去重，</p><p>​          去重的手段：分组、按照id开窗只取第一个值；</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Exactly-Once语义&quot;&gt;&lt;a href=&quot;#Exactly-Once语义&quot; class=&quot;headerlink&quot; title=&quot;Exactly Once语义&quot;&gt;&lt;/a&gt;Exactly Once语义&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;kafka 每个分
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="kafka" scheme="https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/"/>
    
    
      <category term="大数据" scheme="https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="kafka" scheme="https://masteryang4.github.io/tags/kafka/"/>
    
  </entry>
  
</feed>
