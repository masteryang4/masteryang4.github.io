<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>spark系列之spark-sql | MasterYangBlog</title><meta name="description" content="spark系列之spark-sql"><meta name="keywords" content="教程,大数据,spark,spark-sql"><meta name="author" content="Yang4"><meta name="copyright" content="Yang4"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="spark系列之spark-sql"><meta name="twitter:description" content="spark系列之spark-sql"><meta name="twitter:image" content="https://pic.downk.cc/item/5ee64771c2a9a83be5f7a357.jpg"><meta property="og:type" content="article"><meta property="og:title" content="spark系列之spark-sql"><meta property="og:url" content="https://masteryang4.github.io/2020/06/18/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark-sql/"><meta property="og:site_name" content="MasterYangBlog"><meta property="og:description" content="spark系列之spark-sql"><meta property="og:image" content="https://pic.downk.cc/item/5ee64771c2a9a83be5f7a357.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://masteryang4.github.io/2020/06/18/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark-sql/"><link rel="prev" title="kafka知识整理" href="https://masteryang4.github.io/2020/06/18/kafka%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/"><link rel="next" title="spark系列之spark基础" href="https://masteryang4.github.io/2020/06/17/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark%E5%9F%BA%E7%A1%80/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://masteryang4.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="MasterYangBlog" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">MasterYangBlog</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/n.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">56</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">44</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">24</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#SparkSQL概述"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">SparkSQL概述</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#简介"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">简介</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Hive与SparkSQL"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">Hive与SparkSQL</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#DataFrame简介"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text">DataFrame简介</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#DataSet简介"><span class="toc_mobile_items-number">1.4.</span> <span class="toc_mobile_items-text">DataSet简介</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#SparkSQL核心编程"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">SparkSQL核心编程</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#DataFrame"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">DataFrame</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#创建df"><span class="toc_mobile_items-number">2.1.1.</span> <span class="toc_mobile_items-text">创建df</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#SQL语法"><span class="toc_mobile_items-number">2.1.2.</span> <span class="toc_mobile_items-text">SQL语法</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#DSL语法"><span class="toc_mobile_items-number">2.1.3.</span> <span class="toc_mobile_items-text">DSL语法</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#RDD转换为DataFrame"><span class="toc_mobile_items-number">2.1.4.</span> <span class="toc_mobile_items-text">RDD转换为DataFrame</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#DataFrame转换为RDD"><span class="toc_mobile_items-number">2.1.5.</span> <span class="toc_mobile_items-text">DataFrame转换为RDD</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#DataSet"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">DataSet</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#创建DataSet"><span class="toc_mobile_items-number">2.2.1.</span> <span class="toc_mobile_items-text">创建DataSet</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#RDD转换为DataSet"><span class="toc_mobile_items-number">2.2.2.</span> <span class="toc_mobile_items-text">RDD转换为DataSet</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#DataSet转换为RDD"><span class="toc_mobile_items-number">2.2.3.</span> <span class="toc_mobile_items-text">DataSet转换为RDD</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#DataFrame和DataSet转换"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">DataFrame和DataSet转换</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#RDD、DataFrame、DataSet三者关系"><span class="toc_mobile_items-number">2.4.</span> <span class="toc_mobile_items-text">RDD、DataFrame、DataSet三者关系</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#三者共性"><span class="toc_mobile_items-number">2.4.1.</span> <span class="toc_mobile_items-text">三者共性</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#三者区别"><span class="toc_mobile_items-number">2.4.2.</span> <span class="toc_mobile_items-text">三者区别</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#三者的互相转换"><span class="toc_mobile_items-number">2.4.3.</span> <span class="toc_mobile_items-text">三者的互相转换</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#IDEA开发SparkSQL"><span class="toc_mobile_items-number">2.5.</span> <span class="toc_mobile_items-text">IDEA开发SparkSQL</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#用户自定义函数"><span class="toc_mobile_items-number">2.6.</span> <span class="toc_mobile_items-text">用户自定义函数</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#UDF"><span class="toc_mobile_items-number">2.6.1.</span> <span class="toc_mobile_items-text">UDF</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#UDAF"><span class="toc_mobile_items-number">2.6.2.</span> <span class="toc_mobile_items-text">UDAF</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数据的加载和保存"><span class="toc_mobile_items-number">2.7.</span> <span class="toc_mobile_items-text">数据的加载和保存</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#通用的加载和保存方式"><span class="toc_mobile_items-number">2.7.1.</span> <span class="toc_mobile_items-text">通用的加载和保存方式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Parquet"><span class="toc_mobile_items-number">2.7.2.</span> <span class="toc_mobile_items-text">Parquet</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#JSON-CSV-MySQL"><span class="toc_mobile_items-number">2.7.3.</span> <span class="toc_mobile_items-text">JSON&#x2F;CSV&#x2F;MySQL</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Hive"><span class="toc_mobile_items-number">2.7.4.</span> <span class="toc_mobile_items-text">Hive</span></a></li></ol></li></ol></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#SparkSQL概述"><span class="toc-number">1.</span> <span class="toc-text">SparkSQL概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#简介"><span class="toc-number">1.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive与SparkSQL"><span class="toc-number">1.2.</span> <span class="toc-text">Hive与SparkSQL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame简介"><span class="toc-number">1.3.</span> <span class="toc-text">DataFrame简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSet简介"><span class="toc-number">1.4.</span> <span class="toc-text">DataSet简介</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SparkSQL核心编程"><span class="toc-number">2.</span> <span class="toc-text">SparkSQL核心编程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame"><span class="toc-number">2.1.</span> <span class="toc-text">DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#创建df"><span class="toc-number">2.1.1.</span> <span class="toc-text">创建df</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SQL语法"><span class="toc-number">2.1.2.</span> <span class="toc-text">SQL语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DSL语法"><span class="toc-number">2.1.3.</span> <span class="toc-text">DSL语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD转换为DataFrame"><span class="toc-number">2.1.4.</span> <span class="toc-text">RDD转换为DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame转换为RDD"><span class="toc-number">2.1.5.</span> <span class="toc-text">DataFrame转换为RDD</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSet"><span class="toc-number">2.2.</span> <span class="toc-text">DataSet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#创建DataSet"><span class="toc-number">2.2.1.</span> <span class="toc-text">创建DataSet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD转换为DataSet"><span class="toc-number">2.2.2.</span> <span class="toc-text">RDD转换为DataSet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataSet转换为RDD"><span class="toc-number">2.2.3.</span> <span class="toc-text">DataSet转换为RDD</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame和DataSet转换"><span class="toc-number">2.3.</span> <span class="toc-text">DataFrame和DataSet转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD、DataFrame、DataSet三者关系"><span class="toc-number">2.4.</span> <span class="toc-text">RDD、DataFrame、DataSet三者关系</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#三者共性"><span class="toc-number">2.4.1.</span> <span class="toc-text">三者共性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#三者区别"><span class="toc-number">2.4.2.</span> <span class="toc-text">三者区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#三者的互相转换"><span class="toc-number">2.4.3.</span> <span class="toc-text">三者的互相转换</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IDEA开发SparkSQL"><span class="toc-number">2.5.</span> <span class="toc-text">IDEA开发SparkSQL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#用户自定义函数"><span class="toc-number">2.6.</span> <span class="toc-text">用户自定义函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#UDF"><span class="toc-number">2.6.1.</span> <span class="toc-text">UDF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UDAF"><span class="toc-number">2.6.2.</span> <span class="toc-text">UDAF</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据的加载和保存"><span class="toc-number">2.7.</span> <span class="toc-text">数据的加载和保存</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#通用的加载和保存方式"><span class="toc-number">2.7.1.</span> <span class="toc-text">通用的加载和保存方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parquet"><span class="toc-number">2.7.2.</span> <span class="toc-text">Parquet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JSON-CSV-MySQL"><span class="toc-number">2.7.3.</span> <span class="toc-text">JSON&#x2F;CSV&#x2F;MySQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive"><span class="toc-number">2.7.4.</span> <span class="toc-text">Hive</span></a></li></ol></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://pic.downk.cc/item/5ee64771c2a9a83be5f7a357.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">spark系列之spark-sql</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-06-18<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-06-18</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/">spark</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="SparkSQL概述"><a href="#SparkSQL概述" class="headerlink" title="SparkSQL概述"></a>SparkSQL概述</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Spark SQL是Spark用于结构化数据(structured data)处理的Spark模块</p>
<h2 id="Hive与SparkSQL"><a href="#Hive与SparkSQL" class="headerlink" title="Hive与SparkSQL"></a>Hive与SparkSQL</h2><p>其中SparkSQL作为Spark生态的一员继续发展，而不再受限于Hive，只是兼容Hive；</p>
<p>而Hive on Spark是一个Hive的发展计划，该计划将Spark作为Hive的底层引擎之一，也就是说，Hive将不再受限于一个引擎，可以采用Map-Reduce、Tez、Spark等引擎。</p>
<p>Spark SQL为了简化RDD的开发，提高开发效率，提供了2个编程抽象，类似Spark Core中的RDD</p>
<ul>
<li><p>DataFrame</p>
</li>
<li><p>DataSet</p>
</li>
</ul>
<h2 id="DataFrame简介"><a href="#DataFrame简介" class="headerlink" title="DataFrame简介"></a>DataFrame简介</h2><p>在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</p>
<p>同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从 API 易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API 要更加友好，门槛更低。</p>
<p><a href="https://pic.downk.cc/item/5eead8dd14195aa594f363b6.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eead8dd14195aa594f363b6.png" class="lazyload"></a></p>
<p>上图直观地体现了DataFrame和RDD的区别。</p>
<p>左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。</p>
<p>DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待</p>
<p>DataFrame也是懒执行的，但性能上比RDD要高，主要原因：<strong>优化的执行计划</strong>，即查询计划通过Spark catalyst optimiser进行优化</p>
<h2 id="DataSet简介"><a href="#DataSet简介" class="headerlink" title="DataSet简介"></a>DataSet简介</h2><p>DataSet是分布式数据集合。DataSet是<strong>Spark 1.6</strong>中添加的一个新抽象，是DataFrame的一个扩展。它提供了RDD的优势（强类型，使用强大的lambda函数的能力）以及Spark SQL优化执行引擎的优点。DataSet也可以使用功能性的转换（操作map，flatMap，filter等等）。</p>
<ul>
<li><p>DataSet是DataFrame API的一个扩展，是SparkSQL最新的数据抽象</p>
</li>
<li><p>用户友好的API风格，既具有类型安全检查也具有DataFrame的查询优化特性；</p>
</li>
<li><p>用样例类来对DataSet中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称；</p>
</li>
<li><p>DataSet是强类型的。比如可以有DataSet[Car]，DataSet[Person]。</p>
</li>
<li><p>DataFrame是DataSet的特列，DataFrame=DataSet[Row] ，所以可以通过as方法将DataFrame转换为DataSet。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息都用Row来表示。获取数据时需要指定顺序</p>
</li>
</ul>
<h1 id="SparkSQL核心编程"><a href="#SparkSQL核心编程" class="headerlink" title="SparkSQL核心编程"></a>SparkSQL核心编程</h1><p>Spark Core中，如果想要执行应用程序，需要首先构建上下文环境对象SparkContext，Spark SQL其实可以理解为对Spark Core的一种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。</p>
<p>SparkSession是Spark最新的SQL查询起始点，SparkSession内部封装了SparkContext，所以计算实际上是由sparkContext完成的。</p>
<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>Spark SQL的DataFrame API 允许我们使用 DataFrame 而不用必须去注册临时表或者生成 SQL 表达式。DataFrame API 既有 transformation操作也有action操作。</p>
<h3 id="创建df"><a href="#创建df" class="headerlink" title="创建df"></a>创建df</h3><p>在Spark SQL中SparkSession是创建DataFrame和执行SQL的入口，创建DataFrame有三种方式：通过Spark的数据源进行创建；从一个存在的RDD进行转换；还可以从Hive Table进行查询返回。</p>
<p>1、 从Spark数据源进行创建</p>
<ul>
<li>读取json文件创建DataFrame</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"data/user.json"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br></pre></td></tr></table></figure></div>

<p>2、从RDD进行转换</p>
<p>3、从Hive Table进行查询返回</p>
<h3 id="SQL语法"><a href="#SQL语法" class="headerlink" title="SQL语法"></a>SQL语法</h3><p>SQL语法风格是指我们查询数据的时候使用SQL语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"data/user.json"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， name: string]</span><br><span class="line">scala&gt; sqlDF.show</span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">20</span>|zhangsan|</span><br><span class="line">| <span class="number">30</span>|     lisi|</span><br><span class="line">| <span class="number">40</span>|   wangwu|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure></div>

<blockquote>
<p>注意：普通临时表是Session范围内的，如果想应用范围内有效，可以使用全局临时表。</p>
<p>使用全局临时表时需要全路径访问，如：global_temp.people</p>
</blockquote>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createGlobalTempView(<span class="string">"people"</span>)</span><br><span class="line">scala&gt; spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">20</span>|zhangsan|</span><br><span class="line">| <span class="number">30</span>|     lisi|</span><br><span class="line">| <span class="number">40</span>|   wangwu|</span><br><span class="line">+---+--------+</span><br><span class="line"></span><br><span class="line">scala&gt; spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">20</span>|zhangsan|</span><br><span class="line">| <span class="number">30</span>|     lisi|</span><br><span class="line">| <span class="number">40</span>|   wangwu|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure></div>

<h3 id="DSL语法"><a href="#DSL语法" class="headerlink" title="DSL语法"></a>DSL语法</h3><p>DataFrame提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"data/user.json"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: <span class="type">Long</span> (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- username: string (nullable = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="string">"username"</span>).show()</span><br><span class="line">+--------+</span><br><span class="line">|username|</span><br><span class="line">+--------+</span><br><span class="line">|zhangsan|</span><br><span class="line">|     lisi|</span><br><span class="line">|   wangwu|</span><br><span class="line">+--------+</span><br><span class="line"></span><br><span class="line">scala&gt; df.select($<span class="string">"username"</span>,$<span class="string">"age"</span> + <span class="number">1</span>).show</span><br><span class="line">scala&gt; df.select(<span class="symbol">'username</span>, <span class="symbol">'age</span> + <span class="number">1</span>).show()</span><br><span class="line">scala&gt; df.select(<span class="symbol">'username</span>, <span class="symbol">'age</span> + <span class="number">1</span> as <span class="string">"newage"</span>).show()</span><br><span class="line"></span><br><span class="line">+--------+---------+</span><br><span class="line">|username|(age + <span class="number">1</span>)|</span><br><span class="line">+--------+---------+</span><br><span class="line">|zhangsan|        <span class="number">21</span>|</span><br><span class="line">|    lisi|        <span class="number">31</span>|</span><br><span class="line">| wangwu|         <span class="number">41</span>|</span><br><span class="line">+--------+---------+</span><br><span class="line"></span><br><span class="line">scala&gt; df.filter($<span class="string">"age"</span>&gt;<span class="number">30</span>).show</span><br><span class="line">+---+---------+</span><br><span class="line">|age| username|</span><br><span class="line">+---+---------+</span><br><span class="line">| <span class="number">40</span>|    wangwu|</span><br><span class="line">+---+---------+</span><br><span class="line"></span><br><span class="line">scala&gt; df.groupBy(<span class="string">"age"</span>).count.show</span><br><span class="line">+---+-----+</span><br><span class="line">|age|count|</span><br><span class="line">+---+-----+</span><br><span class="line">| <span class="number">20</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">30</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">40</span>|    <span class="number">1</span>|</span><br><span class="line">+---+-----+</span><br></pre></td></tr></table></figure></div>

<h3 id="RDD转换为DataFrame"><a href="#RDD转换为DataFrame" class="headerlink" title="RDD转换为DataFrame"></a>RDD转换为DataFrame</h3><p>在IDEA中开发程序时，如果需要RDD与DF或者DS之间互相操作，那么需要引入 import spark.implicits._</p>
<p>这里的spark不是Scala中的包名，而是创建的sparkSession对象的变量名称，所以必须先创建SparkSession对象再导入。这里的spark对象不能使用var声明，因为Scala只支持val修饰的对象的引入。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> idRDD = sc.textFile(<span class="string">"data/id.txt"</span>)</span><br><span class="line">scala&gt; idRDD.toDF(<span class="string">"id"</span>).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|</span><br><span class="line">|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">4</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure></div>

<blockquote>
<p>实际开发中，一般通过样例类将RDD转换为DataFrame</p>
</blockquote>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">User</span></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">sc</span>.<span class="title">makeRDD</span>(<span class="params"><span class="type">List</span>(("zhangsan",30</span>), (<span class="params">"lisi",40</span>))).<span class="title">map</span>(<span class="params">t=&gt;<span class="type">User</span>(t._1, t._2</span>)).<span class="title">toDF</span>.<span class="title">show</span></span></span><br><span class="line"><span class="class"><span class="title">+--------+---+</span></span></span><br><span class="line"><span class="class"><span class="title">|</span>     <span class="title">name|age|</span></span></span><br><span class="line"><span class="class"><span class="title">+--------+---+</span></span></span><br><span class="line"><span class="class"><span class="title">|zhangsan|</span> 30<span class="title">|</span></span></span><br><span class="line"><span class="class"><span class="title">|</span>    <span class="title">lisi|</span> 40<span class="title">|</span></span></span><br><span class="line"><span class="class"><span class="title">+--------+---+</span></span></span><br></pre></td></tr></table></figure></div>

<h3 id="DataFrame转换为RDD"><a href="#DataFrame转换为RDD" class="headerlink" title="DataFrame转换为RDD"></a>DataFrame转换为RDD</h3><p>DataFrame其实就是对RDD的封装，所以可以直接获取内部的RDD</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = sc.makeRDD(<span class="type">List</span>((<span class="string">"zhangsan"</span>,<span class="number">30</span>), (<span class="string">"lisi"</span>,<span class="number">40</span>))).map(t=&gt;<span class="type">User</span>(t._1, t._2)).toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = df.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">46</span>] at rdd at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> array = rdd.collect</span><br><span class="line">array: <span class="type">Array</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">Array</span>([zhangsan,<span class="number">30</span>], [lisi,<span class="number">40</span>])</span><br></pre></td></tr></table></figure></div>

<p>注意：此时得到的RDD存储类型为Row</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; array(<span class="number">0</span>)</span><br><span class="line">res28: org.apache.spark.sql.<span class="type">Row</span> = [zhangsan,<span class="number">30</span>]</span><br><span class="line">scala&gt; array(<span class="number">0</span>)(<span class="number">0</span>)</span><br><span class="line">res29: <span class="type">Any</span> = zhangsan</span><br><span class="line">scala&gt; array(<span class="number">0</span>).getAs[<span class="type">String</span>](<span class="string">"name"</span>)</span><br><span class="line">res30: <span class="type">String</span> = zhangsan</span><br></pre></td></tr></table></figure></div>

<h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>DataSet是具有强类型的数据集合，需要提供对应的类型信息。</p>
<h3 id="创建DataSet"><a href="#创建DataSet" class="headerlink" title="创建DataSet"></a>创建DataSet</h3><p>1、使用样例类序列创建DataSet</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"zhangsan"</span>,<span class="number">2</span>)).toDS()</span><br><span class="line"></span><br><span class="line">caseClassDS: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: <span class="type">Long</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; caseClassDS.show</span><br><span class="line">+---------+---+</span><br><span class="line">|     name|age|</span><br><span class="line">+---------+---+</span><br><span class="line">| zhangsan|  <span class="number">2</span>|</span><br><span class="line">+---------+---+</span><br></pre></td></tr></table></figure></div>

<p>2、使用基本类型的序列创建DataSet</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Int</span>] = [value: int]</span><br><span class="line"></span><br><span class="line">scala&gt; ds.show</span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">2</span>|</span><br><span class="line">|    <span class="number">3</span>|</span><br><span class="line">|    <span class="number">4</span>|</span><br><span class="line">|    <span class="number">5</span>|</span><br><span class="line">+-----+</span><br></pre></td></tr></table></figure></div>

<blockquote>
<p>注意：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet</p>
</blockquote>
<h3 id="RDD转换为DataSet"><a href="#RDD转换为DataSet" class="headerlink" title="RDD转换为DataSet"></a>RDD转换为DataSet</h3><p>SparkSQL能够自动将包含有case类的RDD转换成DataSet，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seq或者Array等复杂的结构。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">User</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">sc</span>.<span class="title">makeRDD</span>(<span class="params"><span class="type">List</span>(("zhangsan",30</span>), (<span class="params">"lisi",49</span>))).<span class="title">map</span>(<span class="params">t=&gt;<span class="type">User</span>(t._1, t._2</span>)).<span class="title">toDS</span></span></span><br><span class="line"><span class="class"><span class="title">res11</span></span>: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br></pre></td></tr></table></figure></div>

<h3 id="DataSet转换为RDD"><a href="#DataSet转换为RDD" class="headerlink" title="DataSet转换为RDD"></a>DataSet转换为RDD</h3><p>DataSet其实也是对RDD的封装，所以可以直接获取内部的RDD</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">User</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">sc</span>.<span class="title">makeRDD</span>(<span class="params"><span class="type">List</span>(("zhangsan",30</span>), (<span class="params">"lisi",49</span>))).<span class="title">map</span>(<span class="params">t=&gt;<span class="type">User</span>(t._1, t._2</span>)).<span class="title">toDS</span></span></span><br><span class="line"><span class="class"><span class="title">res11</span></span>: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = res11.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">User</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">51</span>] at rdd at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">User</span>] = <span class="type">Array</span>(<span class="type">User</span>(zhangsan,<span class="number">30</span>), <span class="type">User</span>(lisi,<span class="number">49</span>))</span><br></pre></td></tr></table></figure></div>

<h2 id="DataFrame和DataSet转换"><a href="#DataFrame和DataSet转换" class="headerlink" title="DataFrame和DataSet转换"></a>DataFrame和DataSet转换</h2><p>DataFrame其实是DataSet的特例，所以它们之间是可以互相转换的。</p>
<ul>
<li>DataFrame转换为DataSet</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">User</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">df</span> </span>= sc.makeRDD(<span class="type">List</span>((<span class="string">"zhangsan"</span>,<span class="number">30</span>), (<span class="string">"lisi"</span>,<span class="number">49</span>))).toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">User</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br></pre></td></tr></table></figure></div>

<ul>
<li>DataSet转换为DataFrame</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">User</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = ds.toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br></pre></td></tr></table></figure></div>

<h2 id="RDD、DataFrame、DataSet三者关系"><a href="#RDD、DataFrame、DataSet三者关系" class="headerlink" title="RDD、DataFrame、DataSet三者关系"></a>RDD、DataFrame、DataSet三者关系</h2><p>在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看：</p>
<ul>
<li><p>Spark1.0 =&gt; RDD </p>
</li>
<li><p>Spark1.3 =&gt; DataFrame</p>
</li>
<li><p>Spark1.6 =&gt; Dataset</p>
</li>
</ul>
<p>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的Spark版本中，DataSet有可能会逐步取代RDD和DataFrame成为唯一的API接口。</p>
<h3 id="三者共性"><a href="#三者共性" class="headerlink" title="三者共性"></a>三者共性</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RDD、DataFrame、DataSet全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利;</span><br><span class="line">三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算;</span><br><span class="line">三者有许多共同的函数，如filter，排序等;</span><br><span class="line">在对DataFrame和Dataset进行操作许多操作都需要这个包:import spark.implicits._（在创建好SparkSession对象后尽量直接导入）</span><br><span class="line">三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</span><br><span class="line">三者都有partition的概念</span><br><span class="line">DataFrame和DataSet均可使用模式匹配获取各个字段的值和类型</span><br></pre></td></tr></table></figure></div>

<h3 id="三者区别"><a href="#三者区别" class="headerlink" title="三者区别"></a>三者区别</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1)RDD</span><br><span class="line">    RDD一般和spark mlib同时使用</span><br><span class="line">    RDD不支持sparksql操作</span><br><span class="line">2)DataFrame</span><br><span class="line">    与RDD和Dataset不同，DataFrame每一行的类型固定为Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</span><br><span class="line">    DataFrame与DataSet一般不与 spark mlib 同时使用</span><br><span class="line">    DataFrame与DataSet均支持 SparkSQL 的操作，比如select，groupby之类，还能注册临时表&#x2F;视窗，进行 sql 语句操作</span><br><span class="line">    DataFrame与DataSet支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)</span><br><span class="line">3)DataSet</span><br><span class="line">    Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。 DataFrame其实就是DataSet的一个特例  type DataFrame &#x3D; Dataset[Row]</span><br><span class="line">    DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息</span><br></pre></td></tr></table></figure></div>

<h3 id="三者的互相转换"><a href="#三者的互相转换" class="headerlink" title="三者的互相转换"></a>三者的互相转换</h3><p><a href="https://pic.downk.cc/item/5eeae20e14195aa594fe779a.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://pic.downk.cc/item/5eeae20e14195aa594fe779a.png" class="lazyload"></a></p>
<h2 id="IDEA开发SparkSQL"><a href="#IDEA开发SparkSQL" class="headerlink" title="IDEA开发SparkSQL"></a>IDEA开发SparkSQL</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSQL01_Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建上下文环境配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkSQL01_Demo"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkSession对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line">    <span class="comment">//RDD=&gt;DataFrame=&gt;DataSet转换需要引入隐式转换规则，否则无法转换</span></span><br><span class="line">    <span class="comment">//spark不是包名，是上下文环境对象名</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取json文件 创建DataFrame  &#123;"username": "lisi","age": 18&#125;</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"D:\\dev\\workspace\\spark-bak\\spark-bak-00\\input\\test.json"</span>)</span><br><span class="line">    <span class="comment">//df.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//SQL风格语法</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">    <span class="comment">//spark.sql("select avg(age) from user").show</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//DSL风格语法</span></span><br><span class="line">    <span class="comment">//df.select("username","age").show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****RDD=&gt;DataFrame=&gt;DataSet*****</span></span><br><span class="line">    <span class="comment">//RDD</span></span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)] = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="number">1</span>,<span class="string">"qiaofeng"</span>,<span class="number">30</span>),(<span class="number">2</span>,<span class="string">"xuzhu"</span>,<span class="number">28</span>),(<span class="number">3</span>,<span class="string">"duanyu"</span>,<span class="number">20</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df1: <span class="type">DataFrame</span> = rdd1.toDF(<span class="string">"id"</span>,<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">    <span class="comment">//df1.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//DateSet</span></span><br><span class="line">    <span class="keyword">val</span> ds1: <span class="type">Dataset</span>[<span class="type">User</span>] = df1.as[<span class="type">User</span>]</span><br><span class="line">    <span class="comment">//ds1.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****DataSet=&gt;DataFrame=&gt;RDD*****</span></span><br><span class="line">    <span class="comment">//DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df2: <span class="type">DataFrame</span> = ds1.toDF()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//RDD  返回的RDD类型为Row，里面提供的getXXX方法可以获取字段值，类似jdbc处理结果集，但是索引从0开始</span></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Row</span>] = df2.rdd</span><br><span class="line">    <span class="comment">//rdd2.foreach(a=&gt;println(a.getString(1)))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****RDD=&gt;DataSet*****</span></span><br><span class="line">    rdd1.map&#123;</span><br><span class="line">      <span class="keyword">case</span> (id,name,age)=&gt;<span class="type">User</span>(id,name,age)</span><br><span class="line">    &#125;.toDS()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****DataSet=&gt;=&gt;RDD*****</span></span><br><span class="line">    ds1.rdd</span><br><span class="line"></span><br><span class="line">    <span class="comment">//释放资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">id:<span class="type">Int</span>,name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure></div>

<h2 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h2><p>用户可以通过spark.udf功能添加自定义函数，实现自定义功能。</p>
<h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><p>创建DataFrame</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"data/user.json"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br></pre></td></tr></table></figure></div>

<p>注册UDF</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.udf.register(<span class="string">"addName"</span>,(x:<span class="type">String</span>)=&gt; <span class="string">"Name:"</span>+x)</span><br><span class="line">res9: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br></pre></td></tr></table></figure></div>

<p>创建临时表</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br></pre></td></tr></table></figure></div>

<p>应用UDF</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"Select addName(name),age from people"</span>).show()</span><br></pre></td></tr></table></figure></div>

<h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><p>强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数。</p>
<p><strong>需求：实现求平均工资</strong></p>
<p>1、RDD实现</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"app"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> res: (<span class="type">Int</span>, <span class="type">Int</span>) = sc.makeRDD(<span class="type">List</span>((<span class="string">"zhangsan"</span>, <span class="number">20</span>), (<span class="string">"lisi"</span>, <span class="number">30</span>), (<span class="string">"wangw"</span>, <span class="number">40</span>))).map &#123;</span><br><span class="line">  <span class="keyword">case</span> (name, age) =&gt; &#123;</span><br><span class="line">    (age, <span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;.reduce &#123;</span><br><span class="line">  (t1, t2) =&gt; &#123;</span><br><span class="line">    (t1._1 + t2._1, t1._2 + t2._2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">println(res._1/res._2)</span><br><span class="line"><span class="comment">// 关闭连接</span></span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure></div>

<p>2、累加器实现</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAC</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">Int</span>,<span class="type">Int</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> sum:<span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">var</span> count:<span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">return</span> sum ==<span class="number">0</span> &amp;&amp; count == <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">Int</span>, <span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> newMyAc = <span class="keyword">new</span> <span class="type">MyAC</span></span><br><span class="line">    newMyAc.sum = <span class="keyword">this</span>.sum</span><br><span class="line">    newMyAc.count = <span class="keyword">this</span>.count</span><br><span class="line">    newMyAc</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    sum =<span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    sum += v</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">Int</span>, <span class="type">Int</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    other <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> o:<span class="type">MyAC</span>=&gt;&#123;</span><br><span class="line">        sum += o.sum</span><br><span class="line">        count += o.count</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">case</span> _=&gt;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: <span class="type">Int</span> = sum/count</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>3、实现方式 - UDAF - 弱类型</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">定义类继承UserDefinedAggregateFunction，并重写其中方法</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAveragUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 聚合函数输入参数的数据类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"age"</span>,<span class="type">IntegerType</span>)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 聚合函数缓冲区中值的数据类型(age,count)</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"sum"</span>,<span class="type">LongType</span>),<span class="type">StructField</span>(<span class="string">"count"</span>,<span class="type">LongType</span>)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 函数返回值的数据类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 稳定性：对于相同的输入是否一直返回相同的输出。</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 函数缓冲区初始化</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 存年龄的总和</span></span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line">    <span class="comment">// 存年龄的个数</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 更新缓冲区中的数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>,input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getInt(<span class="number">0</span>)</span><br><span class="line">      buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并缓冲区</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>,buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算最终结果</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">。。。</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建聚合函数</span></span><br><span class="line"><span class="keyword">var</span> myAverage = <span class="keyword">new</span> <span class="type">MyAveragUDAF</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//在spark中注册聚合函数</span></span><br><span class="line">spark.udf.register(<span class="string">"avgAge"</span>,myAverage)</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"select avgAge(age) from user"</span>).show()</span><br></pre></td></tr></table></figure></div>

<p>4、实现方式 - UDAF - 强类型</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//输入数据类型</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User01</span>(<span class="params">username:<span class="type">String</span>,age:<span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//缓存类型</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">AgeBuffer</span>(<span class="params">var sum:<span class="type">Long</span>,var count:<span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">/**</span></span></span><br><span class="line"><span class="class">  <span class="title">*</span> <span class="title">定义类继承org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">expressions</span>.<span class="title">Aggregator</span></span></span><br><span class="line"><span class="class">  <span class="title">*</span> <span class="title">重写类中的方法</span></span></span><br><span class="line"><span class="class">  <span class="title">*/</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">MyAveragUDAF1</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">User01</span>,<span class="type">AgeBuffer</span>,<span class="type">Double</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    <span class="type">AgeBuffer</span>(<span class="number">0</span>L,<span class="number">0</span>L)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">AgeBuffer</span>, a: <span class="type">User01</span>): <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    b.sum = b.sum + a.age</span><br><span class="line">    b.count = b.count + <span class="number">1</span></span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">AgeBuffer</span>, b2: <span class="type">AgeBuffer</span>): <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    b1.sum = b1.sum + b2.sum</span><br><span class="line">    b1.count = b1.count + b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">AgeBuffer</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    buff.sum.toDouble/buff.count</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//DataSet默认额编解码器，用于序列化，固定写法</span></span><br><span class="line">  <span class="comment">//自定义类型就是produce   自带类型根据类型选择</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">AgeBuffer</span>] = &#123;</span><br><span class="line">    <span class="type">Encoders</span>.product</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">。。。</span><br><span class="line"></span><br><span class="line"><span class="comment">//封装为DataSet</span></span><br><span class="line"><span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User01</span>] = df.as[<span class="type">User01</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建聚合函数</span></span><br><span class="line"><span class="keyword">var</span> myAgeUdaf1 = <span class="keyword">new</span> <span class="type">MyAveragUDAF1</span></span><br><span class="line"><span class="comment">//将聚合函数转换为查询的列</span></span><br><span class="line"><span class="keyword">val</span> col: <span class="type">TypedColumn</span>[<span class="type">User01</span>, <span class="type">Double</span>] = myAgeUdaf1.toColumn</span><br><span class="line"></span><br><span class="line"><span class="comment">//查询</span></span><br><span class="line">ds.select(col).show()</span><br></pre></td></tr></table></figure></div>

<h2 id="数据的加载和保存"><a href="#数据的加载和保存" class="headerlink" title="数据的加载和保存"></a>数据的加载和保存</h2><h3 id="通用的加载和保存方式"><a href="#通用的加载和保存方式" class="headerlink" title="通用的加载和保存方式"></a>通用的加载和保存方式</h3><p>SparkSQL提供了通用的保存数据和数据加载的方式。这里的通用指的是使用相同的API，根据不同的参数读取和保存不同格式的数据，<strong>SparkSQL默认读取和保存的文件格式为parquet</strong></p>
<p>1) 加载数据</p>
<p><code>spark.read.load</code>是加载数据的通用方法</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.format(<span class="string">"…"</span>)[.option(<span class="string">"…"</span>)].load(<span class="string">"…"</span>)</span><br></pre></td></tr></table></figure></div>

<p>1)保存数据</p>
<p><code>df.write.save</code> 是保存数据的通用方法</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;df.write.format(<span class="string">"…"</span>)[.option(<span class="string">"…"</span>)].save(<span class="string">"…"</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h3><p><strong>Spark SQL的默认数据源为Parquet格式。</strong></p>
<p><strong>Parquet是一种能够有效存储嵌套数据的列式存储格式。</strong></p>
<p>数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作，不需要使用format。修改配置项spark.sql.sources.default，可修改默认数据源格式。</p>
<p>加载数据</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.load(<span class="string">"/opt/module/spark-local/examples/src/main/resources/users.parquet"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br></pre></td></tr></table></figure></div>

<p>保存数据</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> df = spark.read.json(<span class="string">"/opt/module/data/input/people.json"</span>)</span><br><span class="line"><span class="comment">//保存为parquet格式</span></span><br><span class="line">scala&gt; df.write.mode(<span class="string">"append"</span>).save(<span class="string">"/opt/module/data/output"</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="JSON-CSV-MySQL"><a href="#JSON-CSV-MySQL" class="headerlink" title="JSON/CSV/MySQL"></a>JSON/CSV/MySQL</h3><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><p>Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL编译时可以包含 Hive 支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的一点是，如果要在 Spark SQL 中包含Hive 的库，并不需要事先安装 Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。</p>
<p>若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好 Hive，Spark SQL 也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p>
<p>spark-shell默认是Hive支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）。</p>
<p>1、内嵌的Hive</p>
<p>如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可.</p>
<p>Hive 的元数据存储在 derby 中, 仓库地址:$SPARK_HOME/spark-warehouse</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line">。。。</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"create table aa(id int)"</span>)</span><br><span class="line"></span><br><span class="line">。。。</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|       aa|      <span class="literal">false</span>|</span><br><span class="line">+--------+---------+-----------+</span><br></pre></td></tr></table></figure></div>

<p>向表加载本地数据</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"load data local inpath 'input/ids.txt' into table aa"</span>)</span><br><span class="line"></span><br><span class="line">。。。</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"select * from aa"</span>).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|</span><br><span class="line">|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">4</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure></div>

<p>在实际使用中, 几乎没有任何人会使用内置的 Hive</p>
<p>2、外部的Hive</p>
<p>如果想连接外部已经部署好的Hive，需要通过以下几个步骤：</p>
<ul>
<li><p>Spark要接管Hive需要把hive-site.xml拷贝到conf/目录下</p>
</li>
<li><p>把Mysql的驱动copy到jars/目录下</p>
</li>
<li><p>如果访问不到hdfs，则需要把core-site.xml和hdfs-site.xml拷贝到conf/目录下</p>
</li>
</ul>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line"><span class="number">20</span>/<span class="number">04</span>/<span class="number">25</span> <span class="number">22</span>:<span class="number">05</span>:<span class="number">14</span> <span class="type">WARN</span> <span class="type">ObjectStore</span>: <span class="type">Failed</span> to get database global_temp, returning <span class="type">NoSuchObjectException</span></span><br><span class="line">+--------+--------------------+-----------+</span><br><span class="line">|database|           tableName|isTemporary|</span><br><span class="line">+--------+--------------------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|                 emp|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|hive_hbase_emp_table|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>| relevance_hbase_emp|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|          staff_hive|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|                 ttt|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|   user_visit_action|      <span class="literal">false</span>|</span><br><span class="line">+--------+--------------------+-----------+</span><br></pre></td></tr></table></figure></div>

<p>3、运行 Spark SQL CLI</p>
<p>Spark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。在Spark目录下执行如下命令启动Spark SQL CLI，直接执行SQL语句，类似一Hive窗口</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-sql</span><br></pre></td></tr></table></figure></div>

<p>4、代码操作Hive</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>

<p>将hive-site.xml文件拷贝到项目的resources目录中，代码实现</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建SparkSession</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .master(<span class="string">"local[*]"</span>)</span><br><span class="line">  .appName(<span class="string">"sql"</span>)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure></div>

<blockquote>
<p>注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址: config(“spark.sql.warehouse.dir”, “hdfs://linux1:9000/user/hive/warehouse”)</p>
</blockquote>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yang4</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://masteryang4.github.io/2020/06/18/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark-sql/">https://masteryang4.github.io/2020/06/18/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark-sql/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://masteryang4.github.io">MasterYangBlog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%95%99%E7%A8%8B/">教程    </a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据    </a><a class="post-meta__tags" href="/tags/spark/">spark    </a><a class="post-meta__tags" href="/tags/spark-sql/">spark-sql    </a></div><div class="post_share"><div class="social-share" data-image="https://pic.downk.cc/item/5ee64771c2a9a83be5f7a357.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://pic.downk.cc/item/5ea1a251c2a9a83be535b287.png" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://pic.downk.cc/item/5ea1a33ac2a9a83be536f9bc.png" alt="支付宝"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/06/18/kafka%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/"><img class="prev_cover lazyload" data-src="https://pic.downk.cc/item/5eeb0c4714195aa594556a5b.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>kafka知识整理</span></div></a></div><div class="next-post pull_right"><a href="/2020/06/17/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark%E5%9F%BA%E7%A1%80/"><img class="next_cover lazyload" data-src="https://pic.downk.cc/item/5ee64771c2a9a83be5f7a357.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>spark系列之spark基础</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/06/19/spark系列之spark-core/" title="spark系列之spark-core"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5ee64771c2a9a83be5f7a357.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-06-19</div><div class="relatedPosts_title">spark系列之spark-core</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/17/spark系列之spark-streaming/" title="spark系列之spark-streaming"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5ee64771c2a9a83be5f7a357.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-06-17</div><div class="relatedPosts_title">spark系列之spark-streaming</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/17/spark系列之spark基础/" title="spark系列之spark基础"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5ee64771c2a9a83be5f7a357.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-06-17</div><div class="relatedPosts_title">spark系列之spark基础</div></div></a></div><div class="relatedPosts_item"><a href="/2020/05/12/精-Redis总结与思考/" title="[精]Redis总结与思考"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5edd02ccc2a9a83be5db9710.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-12</div><div class="relatedPosts_title">[精]Redis总结与思考</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/27/flink系列05Flink-DataStream-API/" title="flink系列05Flink DataStream API"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5ef7647614195aa59476f65a.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-06-27</div><div class="relatedPosts_title">flink系列05Flink DataStream API</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/27/flink系列04第一个Flink程序/" title="flink系列04第一个Flink程序"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5ef7647614195aa59476f65a.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-06-27</div><div class="relatedPosts_title">flink系列04第一个Flink程序</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'IeiQD5I6g4Doamc68SctmEnW-gzGzoHsz',
  appKey:'ORWhRoGUBY02RR9DMa5OSIow',
  placeholder:'评论一下~（支持Markdown格式）',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'zh-cn',
  recordIP: true
});</script></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By Yang4</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/fireworks.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>