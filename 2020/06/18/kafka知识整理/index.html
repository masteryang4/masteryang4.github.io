<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>kafka知识整理 | MasterYangBlog</title><meta name="description" content="kafka知识整理"><meta name="keywords" content="教程,大数据,kafka"><meta name="author" content="Yang4"><meta name="copyright" content="Yang4"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="kafka知识整理"><meta name="twitter:description" content="kafka知识整理"><meta name="twitter:image" content="https://pic.downk.cc/item/5eeb0c4714195aa594556a5b.png"><meta property="og:type" content="article"><meta property="og:title" content="kafka知识整理"><meta property="og:url" content="https://masteryang4.github.io/2020/06/18/kafka%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/"><meta property="og:site_name" content="MasterYangBlog"><meta property="og:description" content="kafka知识整理"><meta property="og:image" content="https://pic.downk.cc/item/5eeb0c4714195aa594556a5b.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://masteryang4.github.io/2020/06/18/kafka%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/"><link rel="prev" title="MySQL练习题" href="https://masteryang4.github.io/2020/06/18/MySQL%E7%BB%83%E4%B9%A0%E9%A2%98/"><link rel="next" title="spark系列之spark-sql" href="https://masteryang4.github.io/2020/06/18/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark-sql/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://masteryang4.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="MasterYangBlog" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">MasterYangBlog</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/n.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">55</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">44</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">24</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#kafka"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">kafka</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka的定义"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">kafka的定义</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#消息队列有什么好处"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">消息队列有什么好处</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#消费队列的两种模式"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text">消费队列的两种模式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka中的相关概念"><span class="toc_mobile_items-number">1.4.</span> <span class="toc_mobile_items-text">kafka中的相关概念</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka配置文件"><span class="toc_mobile_items-number">1.5.</span> <span class="toc_mobile_items-text">kafka配置文件</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka分布式的broker-id配置"><span class="toc_mobile_items-number">1.6.</span> <span class="toc_mobile_items-text">kafka分布式的broker.id配置</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka的群起脚本"><span class="toc_mobile_items-number">1.7.</span> <span class="toc_mobile_items-text">kafka的群起脚本</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka的命令行操作命令"><span class="toc_mobile_items-number">1.8.</span> <span class="toc_mobile_items-text">kafka的命令行操作命令</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka工作流程"><span class="toc_mobile_items-number">1.9.</span> <span class="toc_mobile_items-text">kafka工作流程</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka生产者的分区分配策略"><span class="toc_mobile_items-number">1.10.</span> <span class="toc_mobile_items-text">kafka生产者的分区分配策略</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka如何保证数据可靠性"><span class="toc_mobile_items-number">1.11.</span> <span class="toc_mobile_items-text">kafka如何保证数据可靠性</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#都有哪些副本数据同步策略-优缺点是什么"><span class="toc_mobile_items-number">1.12.</span> <span class="toc_mobile_items-text">都有哪些副本数据同步策略 优缺点是什么</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka的副本同步策略是什么-这个策略会出现什么问题"><span class="toc_mobile_items-number">1.13.</span> <span class="toc_mobile_items-text">kafka的副本同步策略是什么 这个策略会出现什么问题</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka中的ISR是什么"><span class="toc_mobile_items-number">1.14.</span> <span class="toc_mobile_items-text">kafka中的ISR是什么</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka中的ack应答机制是什么"><span class="toc_mobile_items-number">1.15.</span> <span class="toc_mobile_items-text">kafka中的ack应答机制是什么</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka如何进行故障处理"><span class="toc_mobile_items-number">1.16.</span> <span class="toc_mobile_items-text">kafka如何进行故障处理</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka消费者的消费方式"><span class="toc_mobile_items-number">1.17.</span> <span class="toc_mobile_items-text">kafka消费者的消费方式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka消费者的分区分配策略"><span class="toc_mobile_items-number">1.18.</span> <span class="toc_mobile_items-text">kafka消费者的分区分配策略</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka消费者如何维护offset"><span class="toc_mobile_items-number">1.19.</span> <span class="toc_mobile_items-text">kafka消费者如何维护offset</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka中的消费者组是什么"><span class="toc_mobile_items-number">1.20.</span> <span class="toc_mobile_items-text">kafka中的消费者组是什么</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka为什么能够高效读写数据"><span class="toc_mobile_items-number">1.21.</span> <span class="toc_mobile_items-text">kafka为什么能够高效读写数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka的零拷贝技术如何实现"><span class="toc_mobile_items-number">1.22.</span> <span class="toc_mobile_items-text">kafka的零拷贝技术如何实现</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#zk在kafka中的作用"><span class="toc_mobile_items-number">1.23.</span> <span class="toc_mobile_items-text">zk在kafka中的作用</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka的消息发送流程是什么样的"><span class="toc_mobile_items-number">1.24.</span> <span class="toc_mobile_items-text">kafka的消息发送流程是什么样的</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#如何使用kafka-API-实现异步消息发送"><span class="toc_mobile_items-number">1.25.</span> <span class="toc_mobile_items-text">如何使用kafka API 实现异步消息发送</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#准备知识"><span class="toc_mobile_items-number">1.25.1.</span> <span class="toc_mobile_items-text">准备知识</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#不带回调的API"><span class="toc_mobile_items-number">1.25.2.</span> <span class="toc_mobile_items-text">不带回调的API</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#带回调的API"><span class="toc_mobile_items-number">1.25.3.</span> <span class="toc_mobile_items-text">带回调的API</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#kafka-API中没有写producer-close-为什么读不到数据-也没有回调方法"><span class="toc_mobile_items-number">1.25.4.</span> <span class="toc_mobile_items-text">kafka API中没有写producer.close()为什么读不到数据 也没有回调方法</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#如何使用kafka-API-实现同步消息发送"><span class="toc_mobile_items-number">1.26.</span> <span class="toc_mobile_items-text">如何使用kafka API 实现同步消息发送</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka的分区器怎么写-如何自定义分区器"><span class="toc_mobile_items-number">1.27.</span> <span class="toc_mobile_items-text">kafka的分区器怎么写 如何自定义分区器</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka的消费者需要注意的主要问题是什么"><span class="toc_mobile_items-number">1.28.</span> <span class="toc_mobile_items-text">kafka的消费者需要注意的主要问题是什么</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#如何使用kafka-API-实现消息接收-消费者"><span class="toc_mobile_items-number">1.29.</span> <span class="toc_mobile_items-text">如何使用kafka API 实现消息接收(消费者)</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#准备知识-1"><span class="toc_mobile_items-number">1.29.1.</span> <span class="toc_mobile_items-text">准备知识</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#自动提交offset"><span class="toc_mobile_items-number">1.29.2.</span> <span class="toc_mobile_items-text">自动提交offset</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#重置offset"><span class="toc_mobile_items-number">1.29.3.</span> <span class="toc_mobile_items-text">重置offset</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#手动提交offset的两种方式"><span class="toc_mobile_items-number">1.29.4.</span> <span class="toc_mobile_items-text">手动提交offset的两种方式</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka中重复消费数据和漏消费数据的情况"><span class="toc_mobile_items-number">1.30.</span> <span class="toc_mobile_items-text">kafka中重复消费数据和漏消费数据的情况</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka-API-如何实现自定义存储offset"><span class="toc_mobile_items-number">1.31.</span> <span class="toc_mobile_items-text">kafka API 如何实现自定义存储offset</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka中的拦截器是如何实现的-原理是什么"><span class="toc_mobile_items-number">1.32.</span> <span class="toc_mobile_items-text">kafka中的拦截器是如何实现的 原理是什么</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#请实现一个kafka的拦截器"><span class="toc_mobile_items-number">1.33.</span> <span class="toc_mobile_items-text">请实现一个kafka的拦截器</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#flume如何对接kafka"><span class="toc_mobile_items-number">1.34.</span> <span class="toc_mobile_items-text">flume如何对接kafka</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#实现flume中不同的event发往kafka中不同的topic"><span class="toc_mobile_items-number">1.35.</span> <span class="toc_mobile_items-text">实现flume中不同的event发往kafka中不同的topic</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#如何监控kafka"><span class="toc_mobile_items-number">1.36.</span> <span class="toc_mobile_items-text">如何监控kafka</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#kafka面试题总结"><span class="toc_mobile_items-number">1.37.</span> <span class="toc_mobile_items-text">kafka面试题总结</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#1-Kafka中的ISR、OSR、AR又代表什么？"><span class="toc_mobile_items-number">1.37.1.</span> <span class="toc_mobile_items-text">1.Kafka中的ISR、OSR、AR又代表什么？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#2-Kafka中的HW、LEO等分别代表什么？"><span class="toc_mobile_items-number">1.37.2.</span> <span class="toc_mobile_items-text">2.Kafka中的HW、LEO等分别代表什么？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#3-Kafka中是怎么体现消息顺序性的？"><span class="toc_mobile_items-number">1.37.3.</span> <span class="toc_mobile_items-text">3.Kafka中是怎么体现消息顺序性的？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#4-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？"><span class="toc_mobile_items-number">1.37.4.</span> <span class="toc_mobile_items-text">4.Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#5-Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？"><span class="toc_mobile_items-number">1.37.5.</span> <span class="toc_mobile_items-text">5.Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#6-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？"><span class="toc_mobile_items-number">1.37.6.</span> <span class="toc_mobile_items-text">6.“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#7-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1？"><span class="toc_mobile_items-number">1.37.7.</span> <span class="toc_mobile_items-text">7.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#8-有哪些情形会造成重复消费？"><span class="toc_mobile_items-number">1.37.8.</span> <span class="toc_mobile_items-text">8.有哪些情形会造成重复消费？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#9-有哪些情景会造成消息漏消费？"><span class="toc_mobile_items-number">1.37.9.</span> <span class="toc_mobile_items-text">9.有哪些情景会造成消息漏消费？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？"><span class="toc_mobile_items-number">1.37.10.</span> <span class="toc_mobile_items-text">10.当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#11-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"><span class="toc_mobile_items-number">1.37.11.</span> <span class="toc_mobile_items-text">11.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#12-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"><span class="toc_mobile_items-number">1.37.12.</span> <span class="toc_mobile_items-text">12.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#13-Kafka有内部的topic吗？如果有是什么？有什么所用？"><span class="toc_mobile_items-number">1.37.13.</span> <span class="toc_mobile_items-text">13.Kafka有内部的topic吗？如果有是什么？有什么所用？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#14-Kafka分区分配的概念？"><span class="toc_mobile_items-number">1.37.14.</span> <span class="toc_mobile_items-text">14.Kafka分区分配的概念？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#15-简述Kafka的日志目录结构？"><span class="toc_mobile_items-number">1.37.15.</span> <span class="toc_mobile_items-text">15.简述Kafka的日志目录结构？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#16-如果我指定了一个offset，Kafka-Controller怎么查找到对应的消息？"><span class="toc_mobile_items-number">1.37.16.</span> <span class="toc_mobile_items-text">16.如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#17-聊一聊Kafka-Controller的作用？"><span class="toc_mobile_items-number">1.37.17.</span> <span class="toc_mobile_items-text">17.聊一聊Kafka Controller的作用？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#18-Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？"><span class="toc_mobile_items-number">1.37.18.</span> <span class="toc_mobile_items-text">18.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#19-失效副本是指什么？有那些应对措施？"><span class="toc_mobile_items-number">1.37.19.</span> <span class="toc_mobile_items-text">19.失效副本是指什么？有那些应对措施？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#20-Kafka的那些设计让它有如此高的性能？"><span class="toc_mobile_items-number">1.37.20.</span> <span class="toc_mobile_items-text">20.Kafka的那些设计让它有如此高的性能？</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#其他kafka相关面试题搜集-一"><span class="toc_mobile_items-number">1.38.</span> <span class="toc_mobile_items-text">其他kafka相关面试题搜集(一)</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#1、请说明什么是Apache-Kafka"><span class="toc_mobile_items-number">1.38.1.</span> <span class="toc_mobile_items-text">1、请说明什么是Apache Kafka?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#2、说说Kafka的使用场景？"><span class="toc_mobile_items-number">1.38.2.</span> <span class="toc_mobile_items-text">2、说说Kafka的使用场景？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#3、使用Kafka有什么优点和缺点？"><span class="toc_mobile_items-number">1.38.3.</span> <span class="toc_mobile_items-text">3、使用Kafka有什么优点和缺点？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#4、为什么说Kafka性能很好，体现在哪里？"><span class="toc_mobile_items-number">1.38.4.</span> <span class="toc_mobile_items-text">4、为什么说Kafka性能很好，体现在哪里？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#5、请说明什么是传统的消息传递方法"><span class="toc_mobile_items-number">1.38.5.</span> <span class="toc_mobile_items-text">5、请说明什么是传统的消息传递方法?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#6、请说明Kafka相对传统技术有什么优势"><span class="toc_mobile_items-number">1.38.6.</span> <span class="toc_mobile_items-text">6、请说明Kafka相对传统技术有什么优势?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#7、解释Kafka的Zookeeper是什么-我们可以在没有Zookeeper的情况下使用Kafka吗"><span class="toc_mobile_items-number">1.38.7.</span> <span class="toc_mobile_items-text">7、解释Kafka的Zookeeper是什么?我们可以在没有Zookeeper的情况下使用Kafka吗?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#8、解释Kafka的用户如何消费信息"><span class="toc_mobile_items-number">1.38.8.</span> <span class="toc_mobile_items-text">8、解释Kafka的用户如何消费信息?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#9、解释如何提高远程用户的吞吐量"><span class="toc_mobile_items-number">1.38.9.</span> <span class="toc_mobile_items-text">9、解释如何提高远程用户的吞吐量?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10、解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息"><span class="toc_mobile_items-number">1.38.10.</span> <span class="toc_mobile_items-text">10、解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#11、解释如何减少ISR中的扰动-broker什么时候离开ISR"><span class="toc_mobile_items-number">1.38.11.</span> <span class="toc_mobile_items-text">11、解释如何减少ISR中的扰动?broker什么时候离开ISR?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#12、Kafka为什么需要复制"><span class="toc_mobile_items-number">1.38.12.</span> <span class="toc_mobile_items-text">12、Kafka为什么需要复制?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#13、如果副本在ISR中停留了很长时间表明什么"><span class="toc_mobile_items-number">1.38.13.</span> <span class="toc_mobile_items-text">13、如果副本在ISR中停留了很长时间表明什么?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#14、请说明如果首选的副本不在ISR中会发生什么"><span class="toc_mobile_items-number">1.38.14.</span> <span class="toc_mobile_items-text">14、请说明如果首选的副本不在ISR中会发生什么?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#15、有可能在生产后发生消息偏移吗"><span class="toc_mobile_items-number">1.38.15.</span> <span class="toc_mobile_items-text">15、有可能在生产后发生消息偏移吗?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#16、Kafka的设计时什么样的呢？"><span class="toc_mobile_items-number">1.38.16.</span> <span class="toc_mobile_items-text">16、Kafka的设计时什么样的呢？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#17、数据传输的事务定义有哪三种？"><span class="toc_mobile_items-number">1.38.17.</span> <span class="toc_mobile_items-text">17、数据传输的事务定义有哪三种？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#18、Kafka判断一个节点是否还活着有那两个条件？"><span class="toc_mobile_items-number">1.38.18.</span> <span class="toc_mobile_items-text">18、Kafka判断一个节点是否还活着有那两个条件？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#19、producer是否直接将数据发送到broker的leader-主节点-？"><span class="toc_mobile_items-number">1.38.19.</span> <span class="toc_mobile_items-text">19、producer是否直接将数据发送到broker的leader(主节点)？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#20、Kafa-consumer是否可以消费指定分区消息？"><span class="toc_mobile_items-number">1.38.20.</span> <span class="toc_mobile_items-text">20、Kafa consumer是否可以消费指定分区消息？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#21、Kafka消息是采用Pull模式，还是Push模式？"><span class="toc_mobile_items-number">1.38.21.</span> <span class="toc_mobile_items-text">21、Kafka消息是采用Pull模式，还是Push模式？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#22、Kafka存储在硬盘上的消息格式是什么？"><span class="toc_mobile_items-number">1.38.22.</span> <span class="toc_mobile_items-text">22、Kafka存储在硬盘上的消息格式是什么？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#23、Kafka高效文件存储设计特点："><span class="toc_mobile_items-number">1.38.23.</span> <span class="toc_mobile_items-text">23、Kafka高效文件存储设计特点：</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#24、Kafka-与传统消息系统之间有三个关键区别"><span class="toc_mobile_items-number">1.38.24.</span> <span class="toc_mobile_items-text">24、Kafka 与传统消息系统之间有三个关键区别</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#25、Kafka创建Topic时如何将分区放置到不同的Broker中"><span class="toc_mobile_items-number">1.38.25.</span> <span class="toc_mobile_items-text">25、Kafka创建Topic时如何将分区放置到不同的Broker中</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#26、Kafka新建的分区会在哪个目录下创建"><span class="toc_mobile_items-number">1.38.26.</span> <span class="toc_mobile_items-text">26、Kafka新建的分区会在哪个目录下创建</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#27、partition的数据如何保存到硬盘"><span class="toc_mobile_items-number">1.38.27.</span> <span class="toc_mobile_items-text">27、partition的数据如何保存到硬盘</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#28、kafka的ack机制"><span class="toc_mobile_items-number">1.38.28.</span> <span class="toc_mobile_items-text">28、kafka的ack机制</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#29、Kafka的消费者如何消费数据"><span class="toc_mobile_items-number">1.38.29.</span> <span class="toc_mobile_items-text">29、Kafka的消费者如何消费数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#30、消费者负载均衡策略"><span class="toc_mobile_items-number">1.38.30.</span> <span class="toc_mobile_items-text">30、消费者负载均衡策略</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#31、kafka消息数据是否有序？"><span class="toc_mobile_items-number">1.38.31.</span> <span class="toc_mobile_items-text">31、kafka消息数据是否有序？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#32、kafaka生产数据时数据的分组策略-生产者决定数据产生到集群的哪个partition中"><span class="toc_mobile_items-number">1.38.32.</span> <span class="toc_mobile_items-text">32、kafaka生产数据时数据的分组策略,生产者决定数据产生到集群的哪个partition中</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#33、kafka-consumer-什么情况会触发再平衡reblance"><span class="toc_mobile_items-number">1.38.33.</span> <span class="toc_mobile_items-text">33、kafka consumer 什么情况会触发再平衡reblance?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#34、描述下kafka-consumer-再平衡步骤"><span class="toc_mobile_items-number">1.38.34.</span> <span class="toc_mobile_items-text">34、描述下kafka consumer 再平衡步骤?</span></a></li></ol></li></ol></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka"><span class="toc-number">1.</span> <span class="toc-text">kafka</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka的定义"><span class="toc-number">1.1.</span> <span class="toc-text">kafka的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#消息队列有什么好处"><span class="toc-number">1.2.</span> <span class="toc-text">消息队列有什么好处</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#消费队列的两种模式"><span class="toc-number">1.3.</span> <span class="toc-text">消费队列的两种模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka中的相关概念"><span class="toc-number">1.4.</span> <span class="toc-text">kafka中的相关概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka配置文件"><span class="toc-number">1.5.</span> <span class="toc-text">kafka配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka分布式的broker-id配置"><span class="toc-number">1.6.</span> <span class="toc-text">kafka分布式的broker.id配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka的群起脚本"><span class="toc-number">1.7.</span> <span class="toc-text">kafka的群起脚本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka的命令行操作命令"><span class="toc-number">1.8.</span> <span class="toc-text">kafka的命令行操作命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka工作流程"><span class="toc-number">1.9.</span> <span class="toc-text">kafka工作流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka生产者的分区分配策略"><span class="toc-number">1.10.</span> <span class="toc-text">kafka生产者的分区分配策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka如何保证数据可靠性"><span class="toc-number">1.11.</span> <span class="toc-text">kafka如何保证数据可靠性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#都有哪些副本数据同步策略-优缺点是什么"><span class="toc-number">1.12.</span> <span class="toc-text">都有哪些副本数据同步策略 优缺点是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka的副本同步策略是什么-这个策略会出现什么问题"><span class="toc-number">1.13.</span> <span class="toc-text">kafka的副本同步策略是什么 这个策略会出现什么问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka中的ISR是什么"><span class="toc-number">1.14.</span> <span class="toc-text">kafka中的ISR是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka中的ack应答机制是什么"><span class="toc-number">1.15.</span> <span class="toc-text">kafka中的ack应答机制是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka如何进行故障处理"><span class="toc-number">1.16.</span> <span class="toc-text">kafka如何进行故障处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka消费者的消费方式"><span class="toc-number">1.17.</span> <span class="toc-text">kafka消费者的消费方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka消费者的分区分配策略"><span class="toc-number">1.18.</span> <span class="toc-text">kafka消费者的分区分配策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka消费者如何维护offset"><span class="toc-number">1.19.</span> <span class="toc-text">kafka消费者如何维护offset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka中的消费者组是什么"><span class="toc-number">1.20.</span> <span class="toc-text">kafka中的消费者组是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka为什么能够高效读写数据"><span class="toc-number">1.21.</span> <span class="toc-text">kafka为什么能够高效读写数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka的零拷贝技术如何实现"><span class="toc-number">1.22.</span> <span class="toc-text">kafka的零拷贝技术如何实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zk在kafka中的作用"><span class="toc-number">1.23.</span> <span class="toc-text">zk在kafka中的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka的消息发送流程是什么样的"><span class="toc-number">1.24.</span> <span class="toc-text">kafka的消息发送流程是什么样的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何使用kafka-API-实现异步消息发送"><span class="toc-number">1.25.</span> <span class="toc-text">如何使用kafka API 实现异步消息发送</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#准备知识"><span class="toc-number">1.25.1.</span> <span class="toc-text">准备知识</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#不带回调的API"><span class="toc-number">1.25.2.</span> <span class="toc-text">不带回调的API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#带回调的API"><span class="toc-number">1.25.3.</span> <span class="toc-text">带回调的API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kafka-API中没有写producer-close-为什么读不到数据-也没有回调方法"><span class="toc-number">1.25.4.</span> <span class="toc-text">kafka API中没有写producer.close()为什么读不到数据 也没有回调方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何使用kafka-API-实现同步消息发送"><span class="toc-number">1.26.</span> <span class="toc-text">如何使用kafka API 实现同步消息发送</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka的分区器怎么写-如何自定义分区器"><span class="toc-number">1.27.</span> <span class="toc-text">kafka的分区器怎么写 如何自定义分区器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka的消费者需要注意的主要问题是什么"><span class="toc-number">1.28.</span> <span class="toc-text">kafka的消费者需要注意的主要问题是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何使用kafka-API-实现消息接收-消费者"><span class="toc-number">1.29.</span> <span class="toc-text">如何使用kafka API 实现消息接收(消费者)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#准备知识-1"><span class="toc-number">1.29.1.</span> <span class="toc-text">准备知识</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#自动提交offset"><span class="toc-number">1.29.2.</span> <span class="toc-text">自动提交offset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#重置offset"><span class="toc-number">1.29.3.</span> <span class="toc-text">重置offset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#手动提交offset的两种方式"><span class="toc-number">1.29.4.</span> <span class="toc-text">手动提交offset的两种方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka中重复消费数据和漏消费数据的情况"><span class="toc-number">1.30.</span> <span class="toc-text">kafka中重复消费数据和漏消费数据的情况</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka-API-如何实现自定义存储offset"><span class="toc-number">1.31.</span> <span class="toc-text">kafka API 如何实现自定义存储offset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka中的拦截器是如何实现的-原理是什么"><span class="toc-number">1.32.</span> <span class="toc-text">kafka中的拦截器是如何实现的 原理是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#请实现一个kafka的拦截器"><span class="toc-number">1.33.</span> <span class="toc-text">请实现一个kafka的拦截器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#flume如何对接kafka"><span class="toc-number">1.34.</span> <span class="toc-text">flume如何对接kafka</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实现flume中不同的event发往kafka中不同的topic"><span class="toc-number">1.35.</span> <span class="toc-text">实现flume中不同的event发往kafka中不同的topic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何监控kafka"><span class="toc-number">1.36.</span> <span class="toc-text">如何监控kafka</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka面试题总结"><span class="toc-number">1.37.</span> <span class="toc-text">kafka面试题总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Kafka中的ISR、OSR、AR又代表什么？"><span class="toc-number">1.37.1.</span> <span class="toc-text">1.Kafka中的ISR、OSR、AR又代表什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Kafka中的HW、LEO等分别代表什么？"><span class="toc-number">1.37.2.</span> <span class="toc-text">2.Kafka中的HW、LEO等分别代表什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Kafka中是怎么体现消息顺序性的？"><span class="toc-number">1.37.3.</span> <span class="toc-text">3.Kafka中是怎么体现消息顺序性的？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？"><span class="toc-number">1.37.4.</span> <span class="toc-text">4.Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？"><span class="toc-number">1.37.5.</span> <span class="toc-text">5.Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？"><span class="toc-number">1.37.6.</span> <span class="toc-text">6.“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1？"><span class="toc-number">1.37.7.</span> <span class="toc-text">7.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-有哪些情形会造成重复消费？"><span class="toc-number">1.37.8.</span> <span class="toc-text">8.有哪些情形会造成重复消费？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-有哪些情景会造成消息漏消费？"><span class="toc-number">1.37.9.</span> <span class="toc-text">9.有哪些情景会造成消息漏消费？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？"><span class="toc-number">1.37.10.</span> <span class="toc-text">10.当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"><span class="toc-number">1.37.11.</span> <span class="toc-text">11.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"><span class="toc-number">1.37.12.</span> <span class="toc-text">12.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-Kafka有内部的topic吗？如果有是什么？有什么所用？"><span class="toc-number">1.37.13.</span> <span class="toc-text">13.Kafka有内部的topic吗？如果有是什么？有什么所用？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-Kafka分区分配的概念？"><span class="toc-number">1.37.14.</span> <span class="toc-text">14.Kafka分区分配的概念？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#15-简述Kafka的日志目录结构？"><span class="toc-number">1.37.15.</span> <span class="toc-text">15.简述Kafka的日志目录结构？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#16-如果我指定了一个offset，Kafka-Controller怎么查找到对应的消息？"><span class="toc-number">1.37.16.</span> <span class="toc-text">16.如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#17-聊一聊Kafka-Controller的作用？"><span class="toc-number">1.37.17.</span> <span class="toc-text">17.聊一聊Kafka Controller的作用？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#18-Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？"><span class="toc-number">1.37.18.</span> <span class="toc-text">18.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#19-失效副本是指什么？有那些应对措施？"><span class="toc-number">1.37.19.</span> <span class="toc-text">19.失效副本是指什么？有那些应对措施？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#20-Kafka的那些设计让它有如此高的性能？"><span class="toc-number">1.37.20.</span> <span class="toc-text">20.Kafka的那些设计让它有如此高的性能？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#其他kafka相关面试题搜集-一"><span class="toc-number">1.38.</span> <span class="toc-text">其他kafka相关面试题搜集(一)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1、请说明什么是Apache-Kafka"><span class="toc-number">1.38.1.</span> <span class="toc-text">1、请说明什么是Apache Kafka?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2、说说Kafka的使用场景？"><span class="toc-number">1.38.2.</span> <span class="toc-text">2、说说Kafka的使用场景？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3、使用Kafka有什么优点和缺点？"><span class="toc-number">1.38.3.</span> <span class="toc-text">3、使用Kafka有什么优点和缺点？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4、为什么说Kafka性能很好，体现在哪里？"><span class="toc-number">1.38.4.</span> <span class="toc-text">4、为什么说Kafka性能很好，体现在哪里？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5、请说明什么是传统的消息传递方法"><span class="toc-number">1.38.5.</span> <span class="toc-text">5、请说明什么是传统的消息传递方法?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6、请说明Kafka相对传统技术有什么优势"><span class="toc-number">1.38.6.</span> <span class="toc-text">6、请说明Kafka相对传统技术有什么优势?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7、解释Kafka的Zookeeper是什么-我们可以在没有Zookeeper的情况下使用Kafka吗"><span class="toc-number">1.38.7.</span> <span class="toc-text">7、解释Kafka的Zookeeper是什么?我们可以在没有Zookeeper的情况下使用Kafka吗?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8、解释Kafka的用户如何消费信息"><span class="toc-number">1.38.8.</span> <span class="toc-text">8、解释Kafka的用户如何消费信息?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9、解释如何提高远程用户的吞吐量"><span class="toc-number">1.38.9.</span> <span class="toc-text">9、解释如何提高远程用户的吞吐量?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10、解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息"><span class="toc-number">1.38.10.</span> <span class="toc-text">10、解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11、解释如何减少ISR中的扰动-broker什么时候离开ISR"><span class="toc-number">1.38.11.</span> <span class="toc-text">11、解释如何减少ISR中的扰动?broker什么时候离开ISR?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12、Kafka为什么需要复制"><span class="toc-number">1.38.12.</span> <span class="toc-text">12、Kafka为什么需要复制?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13、如果副本在ISR中停留了很长时间表明什么"><span class="toc-number">1.38.13.</span> <span class="toc-text">13、如果副本在ISR中停留了很长时间表明什么?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14、请说明如果首选的副本不在ISR中会发生什么"><span class="toc-number">1.38.14.</span> <span class="toc-text">14、请说明如果首选的副本不在ISR中会发生什么?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#15、有可能在生产后发生消息偏移吗"><span class="toc-number">1.38.15.</span> <span class="toc-text">15、有可能在生产后发生消息偏移吗?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#16、Kafka的设计时什么样的呢？"><span class="toc-number">1.38.16.</span> <span class="toc-text">16、Kafka的设计时什么样的呢？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#17、数据传输的事务定义有哪三种？"><span class="toc-number">1.38.17.</span> <span class="toc-text">17、数据传输的事务定义有哪三种？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#18、Kafka判断一个节点是否还活着有那两个条件？"><span class="toc-number">1.38.18.</span> <span class="toc-text">18、Kafka判断一个节点是否还活着有那两个条件？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#19、producer是否直接将数据发送到broker的leader-主节点-？"><span class="toc-number">1.38.19.</span> <span class="toc-text">19、producer是否直接将数据发送到broker的leader(主节点)？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#20、Kafa-consumer是否可以消费指定分区消息？"><span class="toc-number">1.38.20.</span> <span class="toc-text">20、Kafa consumer是否可以消费指定分区消息？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#21、Kafka消息是采用Pull模式，还是Push模式？"><span class="toc-number">1.38.21.</span> <span class="toc-text">21、Kafka消息是采用Pull模式，还是Push模式？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#22、Kafka存储在硬盘上的消息格式是什么？"><span class="toc-number">1.38.22.</span> <span class="toc-text">22、Kafka存储在硬盘上的消息格式是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#23、Kafka高效文件存储设计特点："><span class="toc-number">1.38.23.</span> <span class="toc-text">23、Kafka高效文件存储设计特点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#24、Kafka-与传统消息系统之间有三个关键区别"><span class="toc-number">1.38.24.</span> <span class="toc-text">24、Kafka 与传统消息系统之间有三个关键区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#25、Kafka创建Topic时如何将分区放置到不同的Broker中"><span class="toc-number">1.38.25.</span> <span class="toc-text">25、Kafka创建Topic时如何将分区放置到不同的Broker中</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#26、Kafka新建的分区会在哪个目录下创建"><span class="toc-number">1.38.26.</span> <span class="toc-text">26、Kafka新建的分区会在哪个目录下创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#27、partition的数据如何保存到硬盘"><span class="toc-number">1.38.27.</span> <span class="toc-text">27、partition的数据如何保存到硬盘</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#28、kafka的ack机制"><span class="toc-number">1.38.28.</span> <span class="toc-text">28、kafka的ack机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#29、Kafka的消费者如何消费数据"><span class="toc-number">1.38.29.</span> <span class="toc-text">29、Kafka的消费者如何消费数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#30、消费者负载均衡策略"><span class="toc-number">1.38.30.</span> <span class="toc-text">30、消费者负载均衡策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#31、kafka消息数据是否有序？"><span class="toc-number">1.38.31.</span> <span class="toc-text">31、kafka消息数据是否有序？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#32、kafaka生产数据时数据的分组策略-生产者决定数据产生到集群的哪个partition中"><span class="toc-number">1.38.32.</span> <span class="toc-text">32、kafaka生产数据时数据的分组策略,生产者决定数据产生到集群的哪个partition中</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#33、kafka-consumer-什么情况会触发再平衡reblance"><span class="toc-number">1.38.33.</span> <span class="toc-text">33、kafka consumer 什么情况会触发再平衡reblance?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#34、描述下kafka-consumer-再平衡步骤"><span class="toc-number">1.38.34.</span> <span class="toc-text">34、描述下kafka consumer 再平衡步骤?</span></a></li></ol></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://pic.downk.cc/item/5eeb0c4714195aa594556a5b.png)"><div id="post-info"><div id="post-title"><div class="posttitle">kafka知识整理</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-06-18<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-06-18</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/">kafka</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h2><blockquote>
<p>本文转载自： <a href="https://chenhefei.github.io/2020/04/01/Kafka/Kafka-learning/" target="_blank" rel="noopener">https://chenhefei.github.io/2020/04/01/Kafka/Kafka-learning/</a> </p>
</blockquote>
<h3 id="kafka的定义"><a href="#kafka的定义" class="headerlink" title="kafka的定义"></a>kafka的定义</h3><p>Kafka是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。</p>
<h3 id="消息队列有什么好处"><a href="#消息队列有什么好处" class="headerlink" title="消息队列有什么好处"></a>消息队列有什么好处</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1）解耦</span><br><span class="line">允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</span><br><span class="line">2）可恢复性</span><br><span class="line">系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</span><br><span class="line">3）缓冲</span><br><span class="line">有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</span><br><span class="line">4）灵活性 &amp; 峰值处理能力</span><br><span class="line">在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</span><br><span class="line">5）异步通信</span><br><span class="line">很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</span><br></pre></td></tr></table></figure></div>

<h3 id="消费队列的两种模式"><a href="#消费队列的两种模式" class="headerlink" title="消费队列的两种模式"></a>消费队列的两种模式</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">（1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）</span><br><span class="line">消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。</span><br><span class="line">消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。</span><br><span class="line">（2）发布&#x2F;订阅模式（一对多，消费者消费数据之后不会清除消息）</span><br><span class="line">消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka中的相关概念"><a href="#kafka中的相关概念" class="headerlink" title="kafka中的相关概念"></a>kafka中的相关概念</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1）Producer ：消息生产者，就是向kafka broker发消息的客户端；</span><br><span class="line">2）Consumer ：消息消费者，向kafka broker取消息的客户端；</span><br><span class="line">3）Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</span><br><span class="line">4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</span><br><span class="line">5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；</span><br><span class="line">6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</span><br><span class="line">7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。</span><br><span class="line">8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</span><br><span class="line">9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka配置文件"><a href="#kafka配置文件" class="headerlink" title="kafka配置文件"></a>kafka配置文件</h3><p>位置</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ys@hadoop102 kafka]$ cd config&#x2F;</span><br><span class="line">[ys@hadoop102 config]$ vi server.properties</span><br></pre></td></tr></table></figure></div>

<p>内容</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">properties</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#broker的全局唯一编号，不能重复</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment">#删除topic功能使能</span></span><br><span class="line"><span class="meta">delete.topic.enable</span>=<span class="string">true</span></span><br><span class="line"><span class="comment">#处理网络请求的线程数量</span></span><br><span class="line"><span class="meta">num.network.threads</span>=<span class="string">3</span></span><br><span class="line"><span class="comment">#用来处理磁盘IO的线程数量</span></span><br><span class="line"><span class="meta">num.io.threads</span>=<span class="string">8</span></span><br><span class="line"><span class="comment">#发送套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.send.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#接收套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.receive.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#请求套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.request.max.bytes</span>=<span class="string">104857600</span></span><br><span class="line"><span class="comment">#kafka运行日志存放的路径</span></span><br><span class="line"><span class="meta">log.dirs</span>=<span class="string">/opt/module/kafka/logs</span></span><br><span class="line"><span class="comment">#topic在当前broker上的分区个数</span></span><br><span class="line"><span class="meta">num.partitions</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#用来恢复和清理data下数据的线程数量</span></span><br><span class="line"><span class="meta">num.recovery.threads.per.data.dir</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#segment文件保留的最长时间，超时将被删除</span></span><br><span class="line"><span class="meta">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment">#配置连接Zookeeper集群地址</span></span><br><span class="line"><span class="meta">zookeeper.connect</span>=<span class="string">hadoop102:2181,hadoop103:2181,hadoop104:2181</span></span><br></pre></td></tr></table></figure></div>

<h3 id="kafka分布式的broker-id配置"><a href="#kafka分布式的broker-id配置" class="headerlink" title="kafka分布式的broker.id配置"></a>kafka分布式的broker.id配置</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plain修改配置文件&#x2F;opt&#x2F;module&#x2F;kafka&#x2F;config&#x2F;server.properties中的broker.id&#x3D;1、broker.id&#x3D;2注：broker.id不得重复</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka的群起脚本"><a href="#kafka的群起脚本" class="headerlink" title="kafka的群起脚本"></a>kafka的群起脚本</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">echo "========== $i ==========" </span><br><span class="line">ssh $i '/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties'</span><br><span class="line">done</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka的命令行操作命令"><a href="#kafka的命令行操作命令" class="headerlink" title="kafka的命令行操作命令"></a>kafka的命令行操作命令</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">启动</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line"></span><br><span class="line">查看当前服务器中的所有topic</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --list</span><br><span class="line"></span><br><span class="line">创建topic</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first</span><br><span class="line">选项说明：</span><br><span class="line">--topic 定义topic名</span><br><span class="line">--replication-factor  定义副本数</span><br><span class="line">--partitions  定义分区数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">删除topic</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first</span><br><span class="line">需要server.properties中设置delete.topic.enable&#x3D;true否则只是标记删除。</span><br><span class="line"></span><br><span class="line">发送消息</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-console-producer.sh --broker-list hadoop102:9092 --topic first</span><br><span class="line">&gt;hello world</span><br><span class="line">&gt;atguigu  atguigu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">消费消息</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \</span><br><span class="line">--zookeeper hadoop102:2181 --topic first</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \</span><br><span class="line">--zookeeper hadoop102:2181 --topic first --consumer.config config&#x2F;consumer.properties 指定消费者的配置文件(可将多个消费者放置在一个组内)</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server hadoop102:9092 --topic first</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server hadoop102:9092 --from-beginning --topic first</span><br><span class="line">注 : --from-beginning：会把主题中以往所有的数据都读取出来。</span><br><span class="line"></span><br><span class="line">查看某个Topic的详情</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first</span><br><span class="line"></span><br><span class="line">修改分区数</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka工作流程"><a href="#kafka工作流程" class="headerlink" title="kafka工作流程"></a>kafka工作流程</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</span><br><span class="line"></span><br><span class="line">topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。</span><br><span class="line"></span><br><span class="line">由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。</span><br><span class="line">如下</span><br><span class="line">00000000000000000000.index</span><br><span class="line">00000000000000000000.log</span><br><span class="line">00000000000000170410.index</span><br><span class="line">00000000000000170410.log</span><br><span class="line">00000000000000239430.index</span><br><span class="line">00000000000000239430.log</span><br><span class="line"></span><br><span class="line">index和log文件以当前segment的第一条消息的offset命名。</span><br><span class="line"></span><br><span class="line">“.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址。</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka生产者的分区分配策略"><a href="#kafka生产者的分区分配策略" class="headerlink" title="kafka生产者的分区分配策略"></a>kafka生产者的分区分配策略</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1）分区的原因</span><br><span class="line">（1）方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</span><br><span class="line">（2）可以提高并发，因为可以以Partition为单位读写了。</span><br><span class="line"></span><br><span class="line">2）分区的原则</span><br><span class="line">我们需要将producer发送的数据封装成一个ProducerRecord对象。</span><br><span class="line">（1）指明 partition 的情况下，直接将指明的值直接作为 partiton 值；</span><br><span class="line">（2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；</span><br><span class="line">（3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka如何保证数据可靠性"><a href="#kafka如何保证数据可靠性" class="headerlink" title="kafka如何保证数据可靠性"></a>kafka如何保证数据可靠性</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">为保证producer发送的数据能可靠的发送到指定的topic，</span><br><span class="line">topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），</span><br><span class="line">如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。</span><br></pre></td></tr></table></figure></div>

<h3 id="都有哪些副本数据同步策略-优缺点是什么"><a href="#都有哪些副本数据同步策略-优缺点是什么" class="headerlink" title="都有哪些副本数据同步策略 优缺点是什么"></a>都有哪些副本数据同步策略 优缺点是什么</h3><table>
<thead>
<tr>
<th><strong>方案</strong></th>
<th><strong>优点</strong></th>
<th><strong>缺点</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>半数以上完成同步，就发送ack</strong></td>
<td>延迟低</td>
<td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td>
</tr>
<tr>
<td><strong>全部完成同步，才发送ack</strong></td>
<td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td>
<td>延迟高</td>
</tr>
</tbody></table>
<h3 id="kafka的副本同步策略是什么-这个策略会出现什么问题"><a href="#kafka的副本同步策略是什么-这个策略会出现什么问题" class="headerlink" title="kafka的副本同步策略是什么 这个策略会出现什么问题"></a>kafka的副本同步策略是什么 这个策略会出现什么问题</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Kafka选择了第二种方案，原因如下：</span><br><span class="line">1.同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</span><br><span class="line">2.虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</span><br><span class="line"></span><br><span class="line">采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka中的ISR是什么"><a href="#kafka中的ISR是什么" class="headerlink" title="kafka中的ISR是什么"></a>kafka中的ISR是什么</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。</span><br><span class="line"></span><br><span class="line">当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。</span><br><span class="line"></span><br><span class="line">如果follower长时间未向leader同步数据，则该follower将被踢出ISR，</span><br><span class="line"></span><br><span class="line">该时间阈值由replica.lag.time.max.ms参数设定。</span><br><span class="line"></span><br><span class="line">Leader发生故障之后，就会从ISR中选举新的leader。</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka中的ack应答机制是什么"><a href="#kafka中的ack应答机制是什么" class="headerlink" title="kafka中的ack应答机制是什么"></a>kafka中的ack应答机制是什么</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。</span><br><span class="line">所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">acks参数配置：</span><br><span class="line">acks：</span><br><span class="line">0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；</span><br><span class="line">1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；</span><br><span class="line">-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka如何进行故障处理"><a href="#kafka如何进行故障处理" class="headerlink" title="kafka如何进行故障处理"></a>kafka如何进行故障处理</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LEO：指的是每个副本最大的offset；</span><br><span class="line">HW：指的是消费者能见到的最大的offset，ISR队列中最小的LEO。</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）follower故障follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。</span><br><span class="line">（2）leader故障</span><br><span class="line">leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</span><br><span class="line">注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka消费者的消费方式"><a href="#kafka消费者的消费方式" class="headerlink" title="kafka消费者的消费方式"></a>kafka消费者的消费方式</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">consumer采用pull（拉）模式从broker中读取数据。</span><br><span class="line">push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</span><br><span class="line">pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka消费者的分区分配策略"><a href="#kafka消费者的分区分配策略" class="headerlink" title="kafka消费者的分区分配策略"></a>kafka消费者的分区分配策略</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。</span><br><span class="line">Kafka有两种分配策略，一是RoundRobin，一是Range。</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka消费者如何维护offset"><a href="#kafka消费者如何维护offset" class="headerlink" title="kafka消费者如何维护offset"></a>kafka消费者如何维护offset</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</span><br><span class="line"></span><br><span class="line">Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets。</span><br><span class="line"></span><br><span class="line">1）修改配置文件consumer.properties</span><br><span class="line">exclude.internal.topics&#x3D;false</span><br><span class="line">2）读取offset</span><br><span class="line">0.11.0.0之前版本:</span><br><span class="line">bin&#x2F;kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter &quot;kafka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter&quot; --consumer.config config&#x2F;consumer.properties --from-beginning</span><br><span class="line">0.11.0.0之后版本(含):</span><br><span class="line">bin&#x2F;kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --consumer.config config&#x2F;consumer.properties --from-beginning</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka中的消费者组是什么"><a href="#kafka中的消费者组是什么" class="headerlink" title="kafka中的消费者组是什么"></a>kafka中的消费者组是什么</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">配置config&#x2F;consumer.properties文件中的group.id</span><br><span class="line">然后在启动消费者时候使用同一个配置文件 就可以让消费者在一个组内</span><br><span class="line"></span><br><span class="line">同一个消费者组中的消费者，同一时刻只能有一个消费者消费。</span><br><span class="line"></span><br><span class="line">如果消费者组中的消费者多于当前的分区数 会有警告提醒</span><br><span class="line">No broker partitions consumed by consumer thread ...</span><br><span class="line"></span><br><span class="line">如果停止了所有的消费者 那么offset会维护在我们选择的地方(zk中或者是本地) </span><br><span class="line">再次启动消费者会根据选择的GTP(group topic partition所维护的offset位置进行继续消费)</span><br><span class="line"></span><br><span class="line">下图为zk中维护的信息</span><br></pre></td></tr></table></figure></div>

<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522214846.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522214846.png" class="lazyload"></a></p>
<h3 id="kafka为什么能够高效读写数据"><a href="#kafka为什么能够高效读写数据" class="headerlink" title="kafka为什么能够高效读写数据"></a>kafka为什么能够高效读写数据</h3><ul>
<li>分布式框架</li>
<li>分区</li>
<li>顺序写磁盘<ul>
<li>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</li>
</ul>
</li>
<li>零复制技术<ul>
<li><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522214804.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522214804.png" class="lazyload"></a></li>
</ul>
</li>
</ul>
<h3 id="kafka的零拷贝技术如何实现"><a href="#kafka的零拷贝技术如何实现" class="headerlink" title="kafka的零拷贝技术如何实现"></a>kafka的零拷贝技术如何实现</h3><p>kafka中的消费者在读取服务端的数据时，需要将服务端的磁盘文件通过网络发送到消费者进程，网络发送需要经过几种网络节点。如下图所示：</p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522215536.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522215536.png" class="lazyload"></a></p>
<p>传统的读取文件数据并发送到网络的步骤如下：<br>（1）操作系统将数据从磁盘文件中读取到内核空间的页面缓存；<br>（2）应用程序将数据从内核空间读入用户空间缓冲区；<br>（3）应用程序将读到数据写回内核空间并放入socket缓冲区；<br>（4）操作系统将数据从socket缓冲区复制到网卡接口，此时数据才能通过网络发送。</p>
<p>通常情况下，Kafka的消息会有多个订阅者，生产者发布的消息会被不同的消费者多次消费，为了优化这个流程，Kafka使用了“零拷贝技术”，如下图所示：</p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522215604.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522215604.png" class="lazyload"></a></p>
<p>“零拷贝技术”只用将磁盘文件的数据复制到页面缓存中一次，然后将数据从页面缓存直接发送到网络中（发送给不同的订阅者时，都可以使用同一个页面缓存），避免了重复复制操作。</p>
<p>如果有10个消费者，传统方式下，数据复制次数为4*10=40次，而使用“零拷贝技术”只需要1+10=11次，一次为从磁盘复制到页面缓存，10次表示10个消费者各自读取一次页面缓存。</p>
<hr>
<p>传统的文件拷贝通常需要从用户态去转到核心态，经过read buffer，然后再返回到用户态的应用层buffer，然后再从用户态把数据拷贝到核心态的socket buffer，然后发送到网卡。</p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522220145.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522220145.png" class="lazyload"></a></p>
<p>传统的数据传输需要多次的用户态和核心态之间的切换，而且还要把数据复制多次，最终才打到网卡。</p>
<p>如果减少了用户态与核心态之间的切换，是不是就会更快了呢？</p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522220205.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522220205.png" class="lazyload"></a></p>
<p>此时我们会发现用户态“空空如也”。数据没有来到用户态，而是直接在核心态就进行了传输，但这样依然还是有多次复制。首先数据被读取到read buffer中，然后发到socket buffer，最后才发到网卡。虽然减少了用户态和核心态的切换，但依然存在多次数据复制。</p>
<p>如果可以进一步减少数据复制的次数，甚至没有数据复制是不是就会做到最快呢？</p>
<p><strong>DMA</strong></p>
<p>别急，这里我们先介绍一个新的武器:DMA。</p>
<p>DMA，全称叫Direct Memory Access，一种可让某些硬件子系统去直接访问系统主内存，而不用依赖CPU的计算机系统的功能。听着是不是很厉害，跳过CPU，直接访问主内存。传统的内存访问都需要通过CPU的调度来完成。如下图：</p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522220517.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522220517.png" class="lazyload"></a></p>
<p>而DMA，则可以绕过CPU，硬件自己去直接访问系统主内存。如下图：</p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522220536.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522220536.png" class="lazyload"></a></p>
<p>很多硬件都支持DMA，这其中就包括网卡。</p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522220558.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522220558.png" class="lazyload"></a></p>
<p><strong>零拷贝</strong></p>
<p>回到本文中的文件传输，有了DMA后，就可以实现绝对的零拷贝了，因为网卡是直接去访问系统主内存的。如下图：</p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522220616.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522220616.png" class="lazyload"></a></p>
<p><strong>Java的零拷贝实现</strong></p>
<p>在Java中的零拷贝实现是在FileChannel中，其中有个方法transferTo(position,fsize,src)。</p>
<p>传统的文件传输是通过java.io.DataOutputStream，java.io.FileInputStream来实现的，然后通过while循环来读取input，然后写入到output中。</p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522220646.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522220646.png" class="lazyload"></a></p>
<p>零拷贝则是通过java.nio.channels.FileChannel中的transferTo方法来实现的。transferTo方法底层是基于操作系统的sendfile这个system call来实现的（不再需要拷贝到用户态了），sendfile负责把数据从某个fd（file descriptor）传输到另一个fd。</p>
<p>sendfile：</p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522220717.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522220717.png" class="lazyload"></a></p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522220822.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522220822.png" class="lazyload"></a></p>
<p>Java的transferTo：</p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522220850.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522220850.png" class="lazyload"></a></p>
<p><strong>传统方式与零拷贝性能对比</strong></p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522220906.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522220906.png" class="lazyload"></a></p>
<p>可以看出速度快出至少三倍多。Kafka在文件传输的过程中正是使用了零拷贝技术对文件进行拷贝。建议以后多用FileChannel的transferTo吧。</p>
<p><strong>总结</strong></p>
<ul>
<li>传统的文件传输有多次用户态和内核态之间的切换，而且文件在多个buffer之间要复制多次最终才被发送到网卡。</li>
<li>DMA是一种硬件直接访问系统主内存的技术。</li>
<li>多种硬件都已使用了DMA技术，其中就包括网卡（NIC）。</li>
<li>DMA技术让CPU得到解放，让CPU可以不用一直守着来完成文件传输。</li>
<li>零拷贝技术减少了用户态与内核态之间的切换，让拷贝次数降到最低，从而实现高性能。</li>
<li>Kafka使用零拷贝技术来进行文件的传输。</li>
</ul>
<h3 id="zk在kafka中的作用"><a href="#zk在kafka中的作用" class="headerlink" title="zk在kafka中的作用"></a>zk在kafka中的作用</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。</span><br><span class="line">Controller的管理工作都是依赖于Zookeeper的。</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">每个broker都会在zk进行注册</span><br><span class="line">然后KafkaController会实时监听zk中的&#x2F;brokers&#x2F;ids下的节点情况[0,1,2]</span><br><span class="line">如果broker0宕机 ids中的节点会实时变化为[1,2]</span><br><span class="line">KafkaController会更新topic中的leader和isr队列</span><br><span class="line">KafkaController会获取当前可用的isr并从中选出新的leader</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka的消息发送流程是什么样的"><a href="#kafka的消息发送流程是什么样的" class="headerlink" title="kafka的消息发送流程是什么样的"></a>kafka的消息发送流程是什么样的</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Kafka的Producer发送消息采用的是异步发送的方式。</span><br><span class="line">在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，</span><br><span class="line">以及一个线程共享变量——RecordAccumulator(这个里面有分区)</span><br><span class="line"></span><br><span class="line">main线程将消息发送给RecordAccumulator，</span><br><span class="line">Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</span><br><span class="line"></span><br><span class="line">注意这里面 先走拦截器 再走序列化器 再走分区器</span><br><span class="line">达到batch.size大小或者是linger.ms时间就发到RecordAccumulator中</span><br><span class="line">sender线程去拉取</span><br></pre></td></tr></table></figure></div>

<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522223103.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522223103.png" class="lazyload"></a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">相关参数：</span><br><span class="line">batch.size：只有数据积累到batch.size之后，sender才会发送数据。(默认16kb)</span><br><span class="line">linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。</span><br></pre></td></tr></table></figure></div>

<h3 id="如何使用kafka-API-实现异步消息发送"><a href="#如何使用kafka-API-实现异步消息发送" class="headerlink" title="如何使用kafka API 实现异步消息发送"></a>如何使用kafka API 实现异步消息发送</h3><h4 id="准备知识"><a href="#准备知识" class="headerlink" title="准备知识"></a>准备知识</h4><p>需要用到的类：</p>
<p><strong>KafkaProducer</strong>：需要创建一个生产者对象，用来发送数据</p>
<p><strong>ProducerConfig</strong>：获取所需的一系列配置参数</p>
<p><strong>ProducerRecord</strong>：每条数据都要封装成一个ProducerRecord对象</p>
<p>几个比较重要的配置项</p>
<p>//kafka集群，broker-list<br>props.put(“bootstrap.servers”, “hadoop102:9092”);</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>); <span class="comment">//重试次数 props.put("retries", 1); //批次大小 props.put("batch.size", 16384); //等待时间 props.put("linger.ms", 1); //RecordAccumulator缓冲区大小 props.put("buffer.memory", 33554432); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li>kafka集群位置</li>
<li>批次大小</li>
<li>批次等待时间</li>
<li>重试次数</li>
<li>缓冲区大小</li>
<li>序列化器(<code>org\apache\kafka\common\serialization\Serializer.java</code>)</li>
</ul>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200523030013.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200523030013.png" class="lazyload"></a></p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522224718.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522224718.png" class="lazyload"></a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">properties</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">org\apache\kafka\clients\producer\ProducerConfig.java</span></span><br><span class="line"></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String BOOTSTRAP_SERVERS_CONFIG = "bootstrap.servers";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METADATA_MAX_AGE_CONFIG = "metadata.max.age.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String BATCH_SIZE_CONFIG = "batch.size";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String ACKS_CONFIG = "acks";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String LINGER_MS_CONFIG = "linger.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String CLIENT_ID_CONFIG = "client.id";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String SEND_BUFFER_CONFIG = "send.buffer.bytes";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RECEIVE_BUFFER_CONFIG = "receive.buffer.bytes";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String MAX_REQUEST_SIZE_CONFIG = "max.request.size";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RECONNECT_BACKOFF_MS_CONFIG = "reconnect.backoff.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RECONNECT_BACKOFF_MAX_MS_CONFIG = "reconnect.backoff.max.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String MAX_BLOCK_MS_CONFIG = "max.block.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String BUFFER_MEMORY_CONFIG = "buffer.memory";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RETRY_BACKOFF_MS_CONFIG = "retry.backoff.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String COMPRESSION_TYPE_CONFIG = "compression.type";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRICS_SAMPLE_WINDOW_MS_CONFIG = "metrics.sample.window.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRICS_NUM_SAMPLES_CONFIG = "metrics.num.samples";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRICS_RECORDING_LEVEL_CONFIG = "metrics.recording.level";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRIC_REPORTER_CLASSES_CONFIG = "metric.reporters";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION = "max.in.flight.requests.per.connection";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RETRIES_CONFIG = "retries";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String KEY_SERIALIZER_CLASS_CONFIG = "key.serializer";</span></span><br><span class="line"></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String VALUE_SERIALIZER_CLASS_CONFIG = "value.serializer";</span></span><br><span class="line"></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String CONNECTIONS_MAX_IDLE_MS_CONFIG = "connections.max.idle.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String PARTITIONER_CLASS_CONFIG = "partitioner.class";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String REQUEST_TIMEOUT_MS_CONFIG = "request.timeout.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String INTERCEPTOR_CLASSES_CONFIG = "interceptor.classes";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String ENABLE_IDEMPOTENCE_CONFIG = "enable.idempotence";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String TRANSACTION_TIMEOUT_CONFIG = "transaction.timeout.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String TRANSACTIONAL_ID_CONFIG = "transactional.id";</span></span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">properties</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">org\apache\kafka\clients\consumer\ConsumerConfig.java</span></span><br><span class="line"></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String GROUP_ID_CONFIG = "group.id";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String MAX_POLL_RECORDS_CONFIG = "max.poll.records";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String MAX_POLL_INTERVAL_MS_CONFIG = "max.poll.interval.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String SESSION_TIMEOUT_MS_CONFIG = "session.timeout.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String HEARTBEAT_INTERVAL_MS_CONFIG = "heartbeat.interval.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String BOOTSTRAP_SERVERS_CONFIG = "bootstrap.servers";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String AUTO_COMMIT_INTERVAL_MS_CONFIG = "auto.commit.interval.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String PARTITION_ASSIGNMENT_STRATEGY_CONFIG = "partition.assignment.strategy";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String AUTO_OFFSET_RESET_CONFIG = "auto.offset.reset";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String FETCH_MIN_BYTES_CONFIG = "fetch.min.bytes";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String FETCH_MAX_BYTES_CONFIG = "fetch.max.bytes";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final int DEFAULT_FETCH_MAX_BYTES = 52428800;</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String FETCH_MAX_WAIT_MS_CONFIG = "fetch.max.wait.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METADATA_MAX_AGE_CONFIG = "metadata.max.age.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String MAX_PARTITION_FETCH_BYTES_CONFIG = "max.partition.fetch.bytes";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final int DEFAULT_MAX_PARTITION_FETCH_BYTES = 1048576;</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String SEND_BUFFER_CONFIG = "send.buffer.bytes";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RECEIVE_BUFFER_CONFIG = "receive.buffer.bytes";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String CLIENT_ID_CONFIG = "client.id";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RECONNECT_BACKOFF_MS_CONFIG = "reconnect.backoff.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RECONNECT_BACKOFF_MAX_MS_CONFIG = "reconnect.backoff.max.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RETRY_BACKOFF_MS_CONFIG = "retry.backoff.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRICS_SAMPLE_WINDOW_MS_CONFIG = "metrics.sample.window.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRICS_NUM_SAMPLES_CONFIG = "metrics.num.samples";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRICS_RECORDING_LEVEL_CONFIG = "metrics.recording.level";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRIC_REPORTER_CLASSES_CONFIG = "metric.reporters";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String CHECK_CRCS_CONFIG = "check.crcs";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String KEY_DESERIALIZER_CLASS_CONFIG = "key.deserializer";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String VALUE_DESERIALIZER_CLASS_CONFIG = "value.deserializer";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String CONNECTIONS_MAX_IDLE_MS_CONFIG = "connections.max.idle.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String REQUEST_TIMEOUT_MS_CONFIG = "request.timeout.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String INTERCEPTOR_CLASSES_CONFIG = "interceptor.classes";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String EXCLUDE_INTERNAL_TOPICS_CONFIG = "exclude.internal.topics";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final boolean DEFAULT_EXCLUDE_INTERNAL_TOPICS = true;</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String ISOLATION_LEVEL_CONFIG = "isolation.level";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String DEFAULT_ISOLATION_LEVEL;</span></span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">properties</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">org\apache\kafka\clients\CommonClientConfigs.java</span></span><br><span class="line"></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String BOOTSTRAP_SERVERS_CONFIG = "bootstrap.servers";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METADATA_MAX_AGE_CONFIG = "metadata.max.age.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String SEND_BUFFER_CONFIG = "send.buffer.bytes";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RECEIVE_BUFFER_CONFIG = "receive.buffer.bytes";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String CLIENT_ID_CONFIG = "client.id";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RECONNECT_BACKOFF_MS_CONFIG = "reconnect.backoff.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RECONNECT_BACKOFF_MAX_MS_CONFIG = "reconnect.backoff.max.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String RETRY_BACKOFF_MS_CONFIG = "retry.backoff.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRICS_SAMPLE_WINDOW_MS_CONFIG = "metrics.sample.window.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRICS_NUM_SAMPLES_CONFIG = "metrics.num.samples";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRICS_RECORDING_LEVEL_CONFIG = "metrics.recording.level";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String METRIC_REPORTER_CLASSES_CONFIG = "metric.reporters";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String SECURITY_PROTOCOL_CONFIG = "security.protocol";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String DEFAULT_SECURITY_PROTOCOL = "PLAINTEXT";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String CONNECTIONS_MAX_IDLE_MS_CONFIG = "connections.max.idle.ms";</span></span><br><span class="line"><span class="attr">public</span> <span class="string">static final String REQUEST_TIMEOUT_MS_CONFIG = "request.timeout.ms";</span></span><br></pre></td></tr></table></figure></div>

<h4 id="不带回调的API"><a href="#不带回调的API" class="headerlink" title="不带回调的API"></a>不带回调的API</h4><p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200523005738.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200523005738.png" class="lazyload"></a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 整个配置中的key可以使用ProducerConfig中定义的常量</span></span><br><span class="line">        <span class="comment">//kafka集群，broker-list</span></span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>);</span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//重试次数</span></span><br><span class="line">        props.put(<span class="string">"retries"</span>, <span class="number">1</span>); </span><br><span class="line"></span><br><span class="line">        <span class="comment">//批次大小</span></span><br><span class="line">        props.put(<span class="string">"batch.size"</span>, <span class="number">16384</span>); </span><br><span class="line"></span><br><span class="line">        <span class="comment">//等待时间</span></span><br><span class="line">        props.put(<span class="string">"linger.ms"</span>, <span class="number">1</span>); </span><br><span class="line"></span><br><span class="line">        <span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line">        props.put(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"first"</span>, Integer.toString(i), Integer.toString(i)));</span><br><span class="line">            <span class="comment">// 轮循 这个会用到分区器 </span></span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord(<span class="string">"second"</span>,<span class="string">"value++&gt;"</span>+i));</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 根据给的key进行hash 然后放在不同的分区 这个会使用到分区器</span></span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord(<span class="string">"second"</span>,<span class="string">"key"</span>+i,<span class="string">"value==&gt;"</span>+i));</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 具体指定了分区号 就不再使用到key 这个不会用到分区器</span></span><br><span class="line">            <span class="keyword">if</span>(i&lt;<span class="number">5</span>)&#123;</span><br><span class="line">                producer.send(<span class="keyword">new</span> ProducerRecord(<span class="string">"second"</span>,<span class="string">"key"</span>+i,<span class="string">"value**&gt;"</span>+i));</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                producer.send(<span class="keyword">new</span> ProducerRecord(<span class="string">"second"</span>,<span class="string">"key"</span>+i,<span class="string">"value^^&gt;"</span>+i));</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">               </span><br><span class="line"></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 分区器源码解读</span></span><br><span class="line">org\apache\kafka\clients\producer\Partitioner.java</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Configurable</span>, <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//唯一实现类</span></span><br><span class="line">org\apache\kafka\clients\producer\internals\DefaultPartitioner.java</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DefaultPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 传进来的是topic key 还有序列化后的key value 序列化后的value</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">        <span class="comment">//如果key是空的 后面的逻辑用了自增然后对分区取余 其实就是轮循</span></span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> nextValue = nextValue(topic);</span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">                <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 如果key不是空的 将keyBytes传进去然后做hash murmur2是一种哈希算法</span></span><br><span class="line">            <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="带回调的API"><a href="#带回调的API" class="headerlink" title="带回调的API"></a>带回调的API</h4><p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200523005738.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200523005738.png" class="lazyload"></a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">跟上面不同的就是在使用send方法时候 带上一个回调函数</span><br><span class="line">    <span class="comment">// 回调方法:当前消息发出后 不管是消息成功发送还是发送失败 都会执行该回调方法</span></span><br><span class="line">    <span class="comment">// metadata 当前消息的元数据</span></span><br><span class="line">    <span class="comment">// metadata能拿到当前分区的各种数据 如下图所示</span></span><br><span class="line">    <span class="comment">// 偏移量 分区 主题 时间戳 等等</span></span><br><span class="line">    <span class="comment">// exception 当消息发送失败 会返回该异常</span></span><br></pre></td></tr></table></figure></div>

<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200523010150.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200523010150.png" class="lazyload"></a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org\apache\kafka\clients\producer\Callback.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Callback</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 这是一个接口 里面有一个方法</span></span><br><span class="line"></span><br><span class="line">它有两个实现类</span><br></pre></td></tr></table></figure></div>

<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200523010746.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200523010746.png" class="lazyload"></a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 示例 带回调的API</span></span><br><span class="line"><span class="keyword">package</span> com.atguigu.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Future;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCallBackProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1. 创建配置对象</span></span><br><span class="line">        Properties props  = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">//kafka集群的位置</span></span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"hadoop102:9092"</span>);</span><br><span class="line">        <span class="comment">//ack级别</span></span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG,<span class="string">"all"</span>);</span><br><span class="line">        <span class="comment">//重试次数</span></span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG,<span class="number">3</span>);</span><br><span class="line">        <span class="comment">//批次大小</span></span><br><span class="line">        props.put(ProducerConfig.BATCH_SIZE_CONFIG,<span class="number">16384</span>);</span><br><span class="line">        <span class="comment">//等待时间</span></span><br><span class="line">        props.put(ProducerConfig.LINGER_MS_CONFIG,<span class="number">1</span>);</span><br><span class="line">        <span class="comment">//缓冲区大小</span></span><br><span class="line">        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG,<span class="number">33554432</span>);</span><br><span class="line">        <span class="comment">//k v 序列化器</span></span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 创建生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String,String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.生产数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10000</span> ; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"second"</span>, <span class="string">"atguigu@@@@@"</span> + i), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                <span class="comment">/**</span></span><br><span class="line"><span class="comment">                 * 回调方法: 当前的消息发送出去以后，会执行回调方法。</span></span><br><span class="line"><span class="comment">                 * <span class="doctag">@param</span> metadata  当前消息的元数据信息。</span></span><br><span class="line"><span class="comment">                 * <span class="doctag">@param</span> exception 当发送失败，会返回异常。</span></span><br><span class="line"><span class="comment">                 */</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="comment">//发送成功</span></span><br><span class="line">                        System.out.println(metadata.topic() + <span class="string">" -- "</span> + metadata.partition() + <span class="string">" -- "</span> + metadata.offset());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       <span class="comment">// TimeUnit.MILLISECONDS.sleep(100);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭</span></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="kafka-API中没有写producer-close-为什么读不到数据-也没有回调方法"><a href="#kafka-API中没有写producer-close-为什么读不到数据-也没有回调方法" class="headerlink" title="kafka API中没有写producer.close()为什么读不到数据 也没有回调方法"></a>kafka API中没有写producer.close()为什么读不到数据 也没有回调方法</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这是因为异步发送消息的原因</span><br><span class="line"></span><br><span class="line">main线程在发送完数据之后就结束了 这个时间小于了批次拉取设置的时间1ms </span><br><span class="line"></span><br><span class="line">sender线程去拉取数据的同时需要执行main线程中的回调方法 </span><br><span class="line">但是现在main线程已经关闭 所以无法执行回调方法</span><br><span class="line"></span><br><span class="line">如果我们不写close方法 而是让main线程休眠100ms 这时sender就能在这个时间内拉取到数据并执行回调方法</span><br><span class="line"></span><br><span class="line">所以close方法肯定会等待sender线程拉取数据完成后再进行关闭</span><br><span class="line">具体实现可以看close()方法的源码 如下</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org\apache\kafka\clients\producer\KafkaProducer.java</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Close this producer. This method blocks until all previously sent requests complete.</span></span><br><span class="line"><span class="comment">     * This method is equivalent to &lt;code&gt;close(Long.MAX_VALUE, TimeUnit.MILLISECONDS)&lt;/code&gt;.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * &lt;strong&gt;If close() is called from &#123;<span class="doctag">@link</span> Callback&#125;, a warning message will be logged and close(0, TimeUnit.MILLISECONDS)</span></span><br><span class="line"><span class="comment">     * will be called instead. We do this because the sender thread would otherwise try to join itself and</span></span><br><span class="line"><span class="comment">     * block forever.&lt;/strong&gt;</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptException If the thread is interrupted while blocked</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">//关闭此生产者。 此方法一直阻塞所有以前发送的请求完成。 此方法等效于close(Long.MAX_VALUE, TimeUnit.MILLISECONDS) 如果关闭（）被从调用Callback ，警告消息将被记录并关闭（0，TimeUnit.MILLISECONDS）将被代替调用。 我们这样做是因为发件人线程否则将尝试加入自己和永远阻塞。</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        close(Long.MAX_VALUE, TimeUnit.MILLISECONDS);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * This method waits up to &lt;code&gt;timeout&lt;/code&gt; for the producer to complete the sending of all incomplete requests.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * If the producer is unable to complete all requests before the timeout expires, this method will fail</span></span><br><span class="line"><span class="comment">     * any unsent and unacknowledged records immediately.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * If invoked from within a &#123;<span class="doctag">@link</span> Callback&#125; this method will not block and will be equivalent to</span></span><br><span class="line"><span class="comment">     * &lt;code&gt;close(0, TimeUnit.MILLISECONDS)&lt;/code&gt;. This is done since no further sending will happen while</span></span><br><span class="line"><span class="comment">     * blocking the I/O thread of the producer.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> timeout The maximum time to wait for producer to complete any pending requests. The value should be</span></span><br><span class="line"><span class="comment">     *                non-negative. Specifying a timeout of zero means do not wait for pending send requests to complete.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> timeUnit The time unit for the &lt;code&gt;timeout&lt;/code&gt;</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptException If the thread is interrupted while blocked</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IllegalArgumentException If the &lt;code&gt;timeout&lt;/code&gt; is negative.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">// 这种方法最多等待timeout的生产者完成所有未完成的请求的发送。</span></span><br><span class="line"><span class="comment">// 如果生产者是无法完成所有请求超时到期之前，此方法将立即失败任何未发送和未确认的记录。</span></span><br><span class="line"><span class="comment">// 如果从内调用Callback此方法不会阻止和将等效于close(0, TimeUnit.MILLISECONDS) 这样做是因为同时阻断生产者的I/O线程没有进一步的发送会发生</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(<span class="keyword">long</span> timeout, TimeUnit timeUnit)</span> </span>&#123;</span><br><span class="line">        close(timeout, timeUnit, <span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(<span class="keyword">long</span> timeout, TimeUnit timeUnit, <span class="keyword">boolean</span> swallowException)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (timeout &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"The timeout cannot be negative."</span>);</span><br><span class="line"></span><br><span class="line">        log.info(<span class="string">"Closing the Kafka producer with timeoutMillis = &#123;&#125; ms."</span>, timeUnit.toMillis(timeout));</span><br><span class="line">        <span class="comment">// this will keep track of the first encountered exception</span></span><br><span class="line">        AtomicReference&lt;Throwable&gt; firstException = <span class="keyword">new</span> AtomicReference&lt;&gt;();</span><br><span class="line">        <span class="keyword">boolean</span> invokedFromCallback = Thread.currentThread() == <span class="keyword">this</span>.ioThread;</span><br><span class="line">        <span class="keyword">if</span> (timeout &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (invokedFromCallback) &#123;</span><br><span class="line">                log.warn(<span class="string">"Overriding close timeout &#123;&#125; ms to 0 ms in order to prevent useless blocking due to self-join. "</span> +</span><br><span class="line">                        <span class="string">"This means you have incorrectly invoked close with a non-zero timeout from the producer call-back."</span>, timeout);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// Try to close gracefully.</span></span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>.sender != <span class="keyword">null</span>)</span><br><span class="line">                    <span class="keyword">this</span>.sender.initiateClose();</span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>.ioThread != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        <span class="keyword">this</span>.ioThread.join(timeUnit.toMillis(timeout));</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (InterruptedException t) &#123;</span><br><span class="line">                        firstException.compareAndSet(<span class="keyword">null</span>, t);</span><br><span class="line">                        log.error(<span class="string">"Interrupted while joining ioThread"</span>, t);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.sender != <span class="keyword">null</span> &amp;&amp; <span class="keyword">this</span>.ioThread != <span class="keyword">null</span> &amp;&amp; <span class="keyword">this</span>.ioThread.isAlive()) &#123;</span><br><span class="line">            log.info(<span class="string">"Proceeding to force close the producer since pending requests could not be completed "</span> +</span><br><span class="line">                    <span class="string">"within timeout &#123;&#125; ms."</span>, timeout);</span><br><span class="line">            <span class="keyword">this</span>.sender.forceClose();</span><br><span class="line">            <span class="comment">// Only join the sender thread when not calling from callback.</span></span><br><span class="line">            <span class="comment">// 仅当不从回调调用时才加入发送者线程。</span></span><br><span class="line">            <span class="keyword">if</span> (!invokedFromCallback) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="keyword">this</span>.ioThread.join();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    firstException.compareAndSet(<span class="keyword">null</span>, e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ClientUtils.closeQuietly(interceptors, <span class="string">"producer interceptors"</span>, firstException);</span><br><span class="line">        ClientUtils.closeQuietly(metrics, <span class="string">"producer metrics"</span>, firstException);</span><br><span class="line">        ClientUtils.closeQuietly(keySerializer, <span class="string">"producer keySerializer"</span>, firstException);</span><br><span class="line">        ClientUtils.closeQuietly(valueSerializer, <span class="string">"producer valueSerializer"</span>, firstException);</span><br><span class="line">        ClientUtils.closeQuietly(partitioner, <span class="string">"producer partitioner"</span>, firstException);</span><br><span class="line">        AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId);</span><br><span class="line">        log.debug(<span class="string">"The Kafka producer has closed."</span>);</span><br><span class="line">        <span class="keyword">if</span> (firstException.get() != <span class="keyword">null</span> &amp;&amp; !swallowException)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Failed to close kafka producer"</span>, firstException.get());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="如何使用kafka-API-实现同步消息发送"><a href="#如何使用kafka-API-实现同步消息发送" class="headerlink" title="如何使用kafka API 实现同步消息发送"></a>如何使用kafka API 实现同步消息发送</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。</span><br><span class="line">由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需在调用Future对象的get方法即可。</span><br><span class="line"></span><br><span class="line">区别就在于在send方法处拿到返回值future</span><br><span class="line">然后调用future中的get方法</span><br><span class="line">调用此方法就会阻塞当前线程 一直等到结果返回</span><br><span class="line"></span><br><span class="line">java\util\concurrent\Future.java</span><br></pre></td></tr></table></figure></div>

<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200523014826.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200523014826.png" class="lazyload"></a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Future;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCallBackProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1. 创建配置对象</span></span><br><span class="line">        Properties props  = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">//kafka集群的位置</span></span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"hadoop102:9092"</span>);</span><br><span class="line">        <span class="comment">//ack级别</span></span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG,<span class="string">"all"</span>);</span><br><span class="line">        <span class="comment">//重试次数</span></span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG,<span class="number">3</span>);</span><br><span class="line">        <span class="comment">//批次大小</span></span><br><span class="line">        props.put(ProducerConfig.BATCH_SIZE_CONFIG,<span class="number">16384</span>);</span><br><span class="line">        <span class="comment">//等待时间</span></span><br><span class="line">        props.put(ProducerConfig.LINGER_MS_CONFIG,<span class="number">1</span>);</span><br><span class="line">        <span class="comment">//缓冲区大小</span></span><br><span class="line">        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG,<span class="number">33554432</span>);</span><br><span class="line">        <span class="comment">//k v 序列化器</span></span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 创建生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String,String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.生产数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10000</span> ; i++) &#123;</span><br><span class="line">            Future&lt;RecordMetadata&gt; future = producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"second"</span>, <span class="string">"atguigu@@@@@"</span> + i), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                <span class="comment">/**</span></span><br><span class="line"><span class="comment">                 * 回调方法: 当前的消息发送出去以后，会执行回调方法。</span></span><br><span class="line"><span class="comment">                 * <span class="doctag">@param</span> metadata  当前消息的元数据信息。</span></span><br><span class="line"><span class="comment">                 * <span class="doctag">@param</span> exception 当发送失败，会返回异常。</span></span><br><span class="line"><span class="comment">                 */</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="comment">//发送成功</span></span><br><span class="line">                        System.out.println(metadata.topic() + <span class="string">" -- "</span> + metadata.partition() + <span class="string">" -- "</span> + metadata.offset());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="comment">// 发送一个之后阻塞线程等待返回结果才继续发送下一个</span></span><br><span class="line">            <span class="comment">// 阻塞等待 ， 同步发送</span></span><br><span class="line">            <span class="comment">// 此时会发现结果严格按照发送的顺序</span></span><br><span class="line">            RecordMetadata recordMetadata = future.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭</span></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka的分区器怎么写-如何自定义分区器"><a href="#kafka的分区器怎么写-如何自定义分区器" class="headerlink" title="kafka的分区器怎么写 如何自定义分区器"></a>kafka的分区器怎么写 如何自定义分区器</h3><ul>
<li>继承<code>Partitioner</code></li>
<li>重写三个方法<code>configure() partition() close()</code></li>
</ul>
<blockquote>
<p>可以根据传进的key分区 也可根据value分区</p>
<p>在定义好自己的分区器之后 还要再配置中添加分区器的全类名 否则会走默认的分区器</p>
</blockquote>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">系统默认分区器</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DefaultPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Compute the partition for the given record.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic The topic name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key The key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyBytes serialized key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value The value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueBytes serialized value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> nextValue = nextValue(topic);</span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">                <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">nextValue</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">        AtomicInteger counter = topicCounterMap.get(topic);</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == counter) &#123;</span><br><span class="line">            counter = <span class="keyword">new</span> AtomicInteger(ThreadLocalRandom.current().nextInt());</span><br><span class="line">            AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">            <span class="keyword">if</span> (currentCounter != <span class="keyword">null</span>) &#123;</span><br><span class="line">                counter = currentCounter;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> counter.getAndIncrement();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 简单实现一个分区器</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (key == <span class="keyword">null</span>) &#123; <span class="comment">// key为空 到0号分区</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// key不为空 到1号分区</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 如果要使用自己定义的分区器 要在配置中指定分区器并传入分区器的全类名</span></span><br><span class="line">props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG,<span class="string">"com.atguigu.kafka.partitioner.MyPartitioner"</span>);</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka的消费者需要注意的主要问题是什么"><a href="#kafka的消费者需要注意的主要问题是什么" class="headerlink" title="kafka的消费者需要注意的主要问题是什么"></a>kafka的消费者需要注意的主要问题是什么</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。</span><br><span class="line"></span><br><span class="line">由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</span><br><span class="line"></span><br><span class="line">所以offset的维护是Consumer消费数据是必须考虑的问题。</span><br></pre></td></tr></table></figure></div>

<h3 id="如何使用kafka-API-实现消息接收-消费者"><a href="#如何使用kafka-API-实现消息接收-消费者" class="headerlink" title="如何使用kafka API 实现消息接收(消费者)"></a>如何使用kafka API 实现消息接收(消费者)</h3><h4 id="准备知识-1"><a href="#准备知识-1" class="headerlink" title="准备知识"></a>准备知识</h4><p>需要用到的类：</p>
<p><strong>KafkaConsumer</strong>：需要创建一个消费者对象，用来消费数据</p>
<p><strong>ConsumerConfig</strong>：获取所需的一系列配置参数</p>
<p><strong>ConsuemrRecord</strong>：每条数据都要封装成一个ConsumerRecord对象</p>
<p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。</p>
<p>自动提交offset的相关参数：</p>
<p><strong>enable.auto.commit</strong>：是否开启自动提交offset功能</p>
<p><strong>auto.commit.interval.ms</strong>：自动提交offset的时间间隔</p>
<hr>
<p>几个比较重要的配置项</p>
<ul>
<li>自动提交offset功能</li>
<li>自动提交时间间隔</li>
<li>消费者组</li>
<li>反序列化器(对应生产者端的序列化<code>org\apache\kafka\common\serialization\Deserializer.java</code>)</li>
</ul>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200523025729.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200523025729.png" class="lazyload"></a></p>
<h4 id="自动提交offset"><a href="#自动提交offset" class="headerlink" title="自动提交offset"></a>自动提交offset</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> fun.hoffee.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费者</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyConsumer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 创建配置对象</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">//指定kafka集群的位置</span></span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"hadoop102:9092"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启自动提交offset</span></span><br><span class="line">        <span class="comment">//props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,true);</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//自动提交offset的间隔</span></span><br><span class="line">        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定消费者组</span></span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kv的反序列化器</span></span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 订阅主题</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">"first"</span>, <span class="string">"second"</span>, <span class="string">"third"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.println(record.topic() + <span class="string">" -- "</span> + record.partition() + <span class="string">" -- "</span> + record.offset() + <span class="string">" -- "</span> + record.key() + <span class="string">" -- "</span> + record.value());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 此时创建的是新组 不能消费到之前的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果想要消费之前的数据 需要重置offset</span></span><br><span class="line"><span class="comment">// 由auto.offset.rest参数(ConsumerConfig中的AUTO_OFFSET_RESET_CONFIG = "auto.offset.reset";)控制  默认值为latest</span></span><br><span class="line"><span class="comment">// 可以配置为 earliest | latest | none</span></span><br><span class="line">---</span><br><span class="line"><span class="comment">// 文档说明如下 :</span></span><br><span class="line"><span class="comment">// What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted): </span></span><br><span class="line"><span class="comment">// earliest: automatically reset the offset to the earliest offset </span></span><br><span class="line"><span class="comment">// latest: automatically reset the offset to the latest offset </span></span><br><span class="line"><span class="comment">// none: throw exception to the consumer if no previous offset is found for the consumer's group </span></span><br><span class="line"><span class="comment">// anything else: throw exception to the consumer.</span></span><br><span class="line"><span class="comment">// 当Kafka中没有初始偏移量或服务器上不再存在当前偏移量时（例如，因为该数据已被删除），该怎么办：</span></span><br><span class="line"><span class="comment">// 最早：自动将偏移量重置为最早的偏移量 </span></span><br><span class="line"><span class="comment">// 最新：自动将偏移量重置为最新偏移量 </span></span><br><span class="line"><span class="comment">// 无：如果未找到消费者组的先前偏移量，则向消费者抛出异常 </span></span><br><span class="line"><span class="comment">// 其他：向消费者抛出异常</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 人话: 如果这个是一个新的组 或者是 这个组拿了一个kafka中不存在的偏移量去消费数据时候 kafka就会自动帮忙重置offset 如果配置过这个参数 就按这个参数配置的来 如果没有配置过 默认重置为latest</span></span><br></pre></td></tr></table></figure></div>

<h4 id="重置offset"><a href="#重置offset" class="headerlink" title="重置offset"></a>重置offset</h4><blockquote>
<p>具体说明见上一节代码末尾</p>
</blockquote>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费者</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyConsumer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 创建配置对象</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">//指定kafka集群的位置</span></span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"hadoop102:9092"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启自动提交offset</span></span><br><span class="line">        <span class="comment">//props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,true);</span></span><br><span class="line">        <span class="comment">//关闭自动提交offset</span></span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">false</span>);</span><br><span class="line">        <span class="comment">//自动提交offset的间隔</span></span><br><span class="line">        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//重置offset :   earliest(最早)   latest(最后)</span></span><br><span class="line">        <span class="comment">//满足两个条件: </span></span><br><span class="line">        <span class="comment">// 1. 当前的消费者组在kafka没有消费过所订阅的主题   </span></span><br><span class="line">        <span class="comment">// 2.当前消费者组使用的offset在kafka集群中已经被删除</span></span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class="string">"earliest"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定消费者组</span></span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">"atguigu111"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kv的反序列化器</span></span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,<span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,<span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String,String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 订阅主题</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">"first"</span>,<span class="string">"second"</span>,<span class="string">"third"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            <span class="comment">// 此处是拉取数据方法 poll中传递的参数是超时时间 当主题中没有数据时候 等待超时时间之后再进行拉取数据</span></span><br><span class="line">            <span class="comment">// 假如某一次没有消费到数据 会等待响应的时间之后再进行拉取 单位是ms</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records  = consumer.poll(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.println(record.topic() + <span class="string">" -- "</span> + record.partition() + <span class="string">" -- "</span> + record.offset() +<span class="string">" -- "</span> +</span><br><span class="line">                    record.key() +<span class="string">" -- "</span> + record.value());</span><br><span class="line">       		&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="手动提交offset的两种方式"><a href="#手动提交offset的两种方式" class="headerlink" title="手动提交offset的两种方式"></a>手动提交offset的两种方式</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">虽然自动提交offset十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。</span><br><span class="line"></span><br><span class="line">手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。</span><br><span class="line"></span><br><span class="line">两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；</span><br><span class="line"></span><br><span class="line">不同点是，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">由于同步提交offset有失败重试机制，故更加可靠</span><br><span class="line"></span><br><span class="line">虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交offset的方式。</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">如果关闭了提交offset 在一直没有关闭consumer的情况下 consumer能正常消费数据 </span><br><span class="line">因为consumer从kafka中拿到offset后会一直将offset维护在内存中</span><br><span class="line"></span><br><span class="line">但是一旦关闭 因为没有向kafka提交过offset 则offset还是之前的</span><br><span class="line">那么这段时间生产的数据将被重复消费</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费者</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyConsumer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 创建配置对象</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">//指定kafka集群的位置</span></span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"hadoop102:9092"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启自动提交offset</span></span><br><span class="line">        <span class="comment">//props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,true);</span></span><br><span class="line">        <span class="comment">//关闭自动提交offset</span></span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">false</span>);</span><br><span class="line">        <span class="comment">//自动提交offset的间隔</span></span><br><span class="line">        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//重置offset :   earliest(最早)   latest(最后)</span></span><br><span class="line">        <span class="comment">//满足两个条件: 1. 当前的消费者组在kafka没有消费过所订阅的主题   2.当前消费者组使用的offset在kafka集群中已经被删除</span></span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class="string">"earliest"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定消费者组</span></span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">"atguigu111"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kv的反序列化器</span></span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,<span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,<span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String,String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 订阅主题</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">"first"</span>,<span class="string">"second"</span>,<span class="string">"third"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            <span class="comment">// 此处是拉取数据方法 poll中传递的参数是超时时间 当主题中没有数据时候 等待超时时间之后再进行拉取数据</span></span><br><span class="line">            <span class="comment">// 假如某一次没有消费到数据 会等待响应的时间之后再进行拉取 单位是ms</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records  = consumer.poll(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.println(record.topic() + <span class="string">" -- "</span> + record.partition() + <span class="string">" -- "</span> + record.offset() +<span class="string">" -- "</span> +</span><br><span class="line">                    record.key() +<span class="string">" -- "</span> + record.value());</span><br><span class="line">        &#125;</span><br><span class="line">            <span class="comment">//手动提交offset</span></span><br><span class="line">            <span class="comment">//同步提交 代码会阻塞 直到提交offset成功 才开始消费下一条数据</span></span><br><span class="line">            consumer.commitSync();  <span class="comment">//阻塞</span></span><br><span class="line">            <span class="comment">//异步提交 会触发提交offset的操作 但是会继续消费数据 不管offset是否提交成功</span></span><br><span class="line">            <span class="comment">//consumer.commitAsync();</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka中重复消费数据和漏消费数据的情况"><a href="#kafka中重复消费数据和漏消费数据的情况" class="headerlink" title="kafka中重复消费数据和漏消费数据的情况"></a>kafka中重复消费数据和漏消费数据的情况</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。</span><br><span class="line"></span><br><span class="line">先提交offset后消费，有可能造成数据的漏消费；</span><br><span class="line"></span><br><span class="line">而先消费后提交offset，有可能会造成数据的重复消费。</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这是offset的提交  和 消费数据 这两件事之间的先后顺序问题</span><br><span class="line"></span><br><span class="line">例1 : </span><br><span class="line">消费者poll进100条数据 但是在消费到第60条时候宕机 但是offset已经提交 这时候 offset超前</span><br><span class="line">则后40条出现漏消费</span><br><span class="line"></span><br><span class="line">例2 :</span><br><span class="line">消费者poll进100条数据 但是offset在提交时候失败 但此时是先消费后提交offset的情况 这时候 offset滞后</span><br><span class="line">则这100条数据在下次启动时候会被重复消费</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如何解决这个问题?</span><br><span class="line"></span><br><span class="line">将两件事情绑定在一起 如果失败则同时失败 如果成功则同时成功</span><br><span class="line">不允许出现一个失败一个成功的情况</span><br><span class="line"></span><br><span class="line">将两件事绑定为事务</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka-API-如何实现自定义存储offset"><a href="#kafka-API-如何实现自定义存储offset" class="headerlink" title="kafka API 如何实现自定义存储offset"></a>kafka API 如何实现自定义存储offset</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Kafka 0.9版本之前，offset存储在zookeeper，0.9版本及之后，默认将offset存储在Kafka的一个内置的topic中。除此之外，Kafka还可以选择自定义存储offset。</span><br><span class="line"></span><br><span class="line">offset的维护是相当繁琐的，因为需要考虑到消费者的Rebalace。</span><br><span class="line"></span><br><span class="line">当有新的消费者加入消费者组、已有的消费者推出消费者组或者所订阅的主题的分区发生变化，就会触发到分区的重新分配，重新分配的过程叫做Rebalance。</span><br><span class="line"></span><br><span class="line">消费者发生Rebalance之后，每个消费者消费的分区就会发生变化。因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区最近提交的offset位置继续消费。</span><br><span class="line"></span><br><span class="line">要实现自定义存储offset，需要借助ConsumerRebalanceListener，以下为示例代码，其中提交和获取offset的方法，需要根据所选的offset存储系统自行实现。</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Map&lt;TopicPartition, Long&gt; currentOffset = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 创建配置信息</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// Kafka集群</span></span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>); </span><br><span class="line"></span><br><span class="line">		<span class="comment">// 消费者组，只要group.id相同，就属于同一个消费者组</span></span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>); </span><br><span class="line"></span><br><span class="line">		<span class="comment">// 关闭自动提交offset</span></span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Key和Value的反序列化类</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建一个消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费者订阅主题 在订阅时候创建一个ConsumerRebalanceListener的对象实时监听</span></span><br><span class="line">    	<span class="comment">// 并重写两个方法onPartitionsRevoked 和 onPartitionsAssigned</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">"first"</span>), <span class="keyword">new</span> ConsumerRebalanceListener() &#123;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//该方法会在Rebalance之前调用</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">                commitOffset(currentOffset);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//该方法会在Rebalance之后调用</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">                currentOffset.clear();</span><br><span class="line">                <span class="keyword">for</span> (TopicPartition partition : partitions) &#123;</span><br><span class="line">                    consumer.seek(partition, getOffset(partition));</span><br><span class="line">                    <span class="comment">//定位到最近提交的offset位置继续消费</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="comment">//消费者拉取数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line">                currentOffset.put(<span class="keyword">new</span> TopicPartition(record.topic(), record.partition()), record.offset());</span><br><span class="line">            &#125;</span><br><span class="line">            commitOffset(currentOffset);<span class="comment">//异步提交</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取某分区的最新offset</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getOffset</span><span class="params">(TopicPartition partition)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;<span class="comment">// 这里是伪代码 需要根据具体存储的系统来实现</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//提交该消费者所有分区的offset</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">commitOffset</span><span class="params">(Map&lt;TopicPartition, Long&gt; currentOffset)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 这里是伪代码 需要根据具体存储的系统来实现</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="kafka中的拦截器是如何实现的-原理是什么"><a href="#kafka中的拦截器是如何实现的-原理是什么" class="headerlink" title="kafka中的拦截器是如何实现的 原理是什么"></a>kafka中的拦截器是如何实现的 原理是什么</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。</span><br><span class="line">对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括：</span><br><span class="line">（1）configure(configs)</span><br><span class="line">获取配置信息和初始化数据时调用。</span><br><span class="line"></span><br><span class="line">（2）onSend(ProducerRecord)：</span><br><span class="line">该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算。</span><br><span class="line"></span><br><span class="line">（3）onAcknowledgement(RecordMetadata, Exception)：</span><br><span class="line">该方法会在消息从RecordAccumulator成功发送到Kafka Broker之后，或者在发送过程中失败时调用。并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率。</span><br><span class="line"></span><br><span class="line">（4）close：</span><br><span class="line">关闭interceptor，主要用于执行一些资源清理工作</span><br><span class="line">如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。</span><br></pre></td></tr></table></figure></div>

<h3 id="请实现一个kafka的拦截器"><a href="#请实现一个kafka的拦截器" class="headerlink" title="请实现一个kafka的拦截器"></a>请实现一个kafka的拦截器</h3><p><strong>需求：</strong></p>
<p>实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。</p>
<p><strong>分析:</strong></p>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200523043416.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200523043416.png" class="lazyload"></a></p>
<p><strong>时间拦截器</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> fun.hoffee.kafka.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 在所有的消息内容前面加上时间戳</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//获取当前消息的value</span></span><br><span class="line">        String value = record.value();</span><br><span class="line">        value = System.currentTimeMillis() + <span class="string">" -- "</span> + value;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//构造一个producerRecord</span></span><br><span class="line">        ProducerRecord&lt;String, String&gt; resultRecord =</span><br><span class="line">                <span class="keyword">new</span> ProducerRecord&lt;&gt;(record.topic(), record.partition(), record.key(), value);</span><br><span class="line">        <span class="keyword">return</span> resultRecord;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>计数拦截器</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> fun.hoffee.kafka.interceptor;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 统计发送成功或失败的消息个数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Integer success = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Integer fail = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 相当于原路返回没有做处理</span></span><br><span class="line">        <span class="keyword">return</span> record;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">            success++;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            fail++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 整个拦截器走完之后 调用该方法</span></span><br><span class="line">        System.out.println(<span class="string">"Success : "</span> + success);</span><br><span class="line">        System.out.println(<span class="string">"Fail :"</span> + fail);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>在生产者的配置文件中配置拦截器(可设置多个 设置为一个list)</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> fun.hoffee.kafka.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InterceptorProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 创建配置对象</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">//指定kafka集群的位置，broker-list</span></span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"hadoop102:9092"</span>);</span><br><span class="line">        <span class="comment">//指定ack的应答级别  0  1  -1(all)</span></span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG, <span class="string">"all"</span>);</span><br><span class="line">        <span class="comment">//重试次数</span></span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG, <span class="number">5</span>);</span><br><span class="line">        <span class="comment">//批次大小</span></span><br><span class="line">        props.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);  <span class="comment">// 16kb</span></span><br><span class="line">        <span class="comment">//等待时间</span></span><br><span class="line">        props.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line">        <span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line">        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);  <span class="comment">// 32M</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kv的序列化器</span></span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定拦截器</span></span><br><span class="line">        <span class="comment">// "A list of classes to use as interceptors. Implementing the &lt;code&gt;ProducerInterceptor&lt;/code&gt; interface allows you to intercept (and possibly mutate) the records received by the producer before they are published to the Kafka cluster. By default, there are no interceptors.";</span></span><br><span class="line">        List&lt;String&gt; interceptors = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        interceptors.add(<span class="string">"com.atguigu.kafka.interceptor.TimeInterceptor"</span>);</span><br><span class="line">        interceptors.add(<span class="string">"com.atguigu.kafka.interceptor.CountInterceptor"</span>);</span><br><span class="line">        props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 创建生产者对象</span></span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 生产数据</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord(<span class="string">"second"</span>, <span class="string">"shangguigu==&gt;"</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 关闭</span></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="flume如何对接kafka"><a href="#flume如何对接kafka" class="headerlink" title="flume如何对接kafka"></a>flume如何对接kafka</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用kafkasink</span><br><span class="line"></span><br><span class="line">此时kafkasink相当于kafka的生产者 它可以根据消息的标记发送给kafka中不同的topic</span><br></pre></td></tr></table></figure></div>

<p>flume官网关于kafka sink的介绍如下</p>
<p>这是一个Flume Sink实现，可以将数据发布到 <a href="http://kafka.apache.org/" target="_blank" rel="noopener">Kafka</a>主题。目标之一是将Flume与Kafka集成在一起，以便基于拉式的处理系统可以处理来自各种Flume来源的数据。目前，该版本支持Kafka 0.9.x系列发行版。</p>
<p>此版本的Flume不再支持Kafka的旧版本（0.8.x）。</p>
<p>必需的属性以粗体标记。</p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><strong>type</strong></td>
<td>–</td>
<td>Must be set to <code>org.apache.flume.sink.kafka.KafkaSink</code></td>
</tr>
<tr>
<td><strong>kafka.bootstrap.servers</strong></td>
<td>–</td>
<td>List of brokers Kafka-Sink will connect to, to get the list of topic partitions This can be a partial list of brokers, but we recommend at least two for HA. The format is comma separated list of hostname:port Kafka-Sink将连接到的代理列表，以获取主题分区列表。这可以是部分代理列表，但是对于HA，我们建议至少两个。格式是用逗号分隔的主机名：端口列表</td>
</tr>
<tr>
<td>kafka.topic</td>
<td>default-flume-topic</td>
<td>The topic in Kafka to which the messages will be published. If this parameter is configured, messages will be published to this topic. If the event header contains a “topic” field, the event will be published to that topic overriding the topic configured here. Kafka中将发布消息的主题。如果配置了此参数，则消息将发布到该主题。如果事件标题包含“主题”字段，则事件将发布到该主题，并覆盖此处配置的主题。</td>
</tr>
<tr>
<td>flumeBatchSize</td>
<td>100</td>
<td>How many messages to process in one batch. Larger batches improve throughput while adding latency. 一批中要处理多少条消息。较大的批次可提高吞吐量，同时增加延迟。</td>
</tr>
<tr>
<td>kafka.producer.acks</td>
<td>1</td>
<td>How many replicas must acknowledge a message before its considered successfully written. Accepted values are 0 (Never wait for acknowledgement), 1 (wait for leader only), -1 (wait for all replicas) Set this to -1 to avoid data loss in some cases of leader failure. 在成功考虑一条消息之前，有多少个副本必须确认一条消息。接受的值为0（永远不等待确认），1（仅等待领导者），-1（等待所有副本）将其设置为-1，以避免在某些领导者失败的情况下丢失数据。</td>
</tr>
<tr>
<td>useFlumeEventFormat</td>
<td>false</td>
<td>By default events are put as bytes onto the Kafka topic directly from the event body. Set to true to store events as the Flume Avro binary format. Used in conjunction with the same property on the KafkaSource or with the parseAsFlumeEvent property on the Kafka Channel this will preserve any Flume headers for the producing side. 默认情况下，事件直接从事件主体作为字节放入Kafka主题。设置为true可将事件存储为Flume Avro二进制格式。与KafkaSource上的相同属性或Kafka Channel上的parseAsFlumeEvent属性结合使用，将为生产方保留任何Flume标头。</td>
</tr>
<tr>
<td>defaultPartitionId</td>
<td>–</td>
<td>Specifies a Kafka partition ID (integer) for all events in this channel to be sent to, unless overriden by <code>partitionIdHeader</code>. By default, if this property is not set, events will be distributed by the Kafka Producer’s partitioner - including by <code>key</code> if specified (or by a partitioner specified by <code>kafka.partitioner.class</code>).</td>
</tr>
<tr>
<td>partitionIdHeader</td>
<td>–</td>
<td>When set, the sink will take the value of the field named using the value of this property from the event header and send the message to the specified partition of the topic. If the value represents an invalid partition, an EventDeliveryException will be thrown. If the header value is present then this setting overrides <code>defaultPartitionId</code>.</td>
</tr>
<tr>
<td>kafka.producer.security.protocol</td>
<td>PLAINTEXT</td>
<td>Set to SASL_PLAINTEXT, SASL_SSL or SSL if writing to Kafka using some level of security. See below for additional info on secure setup.</td>
</tr>
<tr>
<td><em>more producer security props</em></td>
<td></td>
<td>If using SASL_PLAINTEXT, SASL_SSL or SSL refer to <a href="http://kafka.apache.org/documentation.html#security" target="_blank" rel="noopener">Kafka security</a> for additional properties that need to be set on producer.</td>
</tr>
<tr>
<td>Other Kafka Producer Properties</td>
<td>–</td>
<td>These properties are used to configure the Kafka Producer. Any producer property supported by Kafka can be used. The only requirement is to prepend the property name with the prefix <code>kafka.producer</code>. For example: kafka.producer.linger.ms</td>
</tr>
</tbody></table>
<p>The Kafka sink also provides defaults for the key.serializer(org.apache.kafka.common.serialization.StringSerializer) and value.serializer(org.apache.kafka.common.serialization.ByteArraySerializer). Modification of these parameters is not recommended.</p>
<p>An example configuration of a Kafka sink is given below. Properties starting with the prefix <code>kafka.producer</code> the Kafka producer. The properties that are passed when creating the Kafka producer are not limited to the properties given in this example. Also it is possible to include your custom properties here and access them inside the preprocessor through the Flume Context object passed in as a method argument.</p>
<p>示例配置如下:</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line">a1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.kafka.topic &#x3D; mytopic &#x2F;&#x2F; 指定写入topic</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers &#x3D; localhost:9092 &#x2F;&#x2F; kafka位置</span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize &#x3D; 20</span><br><span class="line">a1.sinks.k1.kafka.producer.acks &#x3D; 1</span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms &#x3D; 1</span><br><span class="line">a1.sinks.ki.kafka.producer.compression.type &#x3D; snappy</span><br></pre></td></tr></table></figure></div>

<h3 id="实现flume中不同的event发往kafka中不同的topic"><a href="#实现flume中不同的event发往kafka中不同的topic" class="headerlink" title="实现flume中不同的event发往kafka中不同的topic"></a>实现flume中不同的event发往kafka中不同的topic</h3><p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200523053711.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200523053711.png" class="lazyload"></a></p>
<h3 id="如何监控kafka"><a href="#如何监控kafka" class="headerlink" title="如何监控kafka"></a>如何监控kafka</h3><h3 id="kafka面试题总结"><a href="#kafka面试题总结" class="headerlink" title="kafka面试题总结"></a>kafka面试题总结</h3><h4 id="1-Kafka中的ISR、OSR、AR又代表什么？"><a href="#1-Kafka中的ISR、OSR、AR又代表什么？" class="headerlink" title="1.Kafka中的ISR、OSR、AR又代表什么？"></a>1.Kafka中的ISR、OSR、AR又代表什么？</h4><blockquote>
<p>ISR：与leader保持同步的follower集合<br>AR：分区的所有副本</p>
</blockquote>
<h4 id="2-Kafka中的HW、LEO等分别代表什么？"><a href="#2-Kafka中的HW、LEO等分别代表什么？" class="headerlink" title="2.Kafka中的HW、LEO等分别代表什么？"></a>2.Kafka中的HW、LEO等分别代表什么？</h4><blockquote>
<p>LEO：没个副本的最后条消息的offset<br>HW：一个分区中所有副本最小的offset 控制整个分区中哪些数据能够暴露给消费者</p>
</blockquote>
<h4 id="3-Kafka中是怎么体现消息顺序性的？"><a href="#3-Kafka中是怎么体现消息顺序性的？" class="headerlink" title="3.Kafka中是怎么体现消息顺序性的？"></a>3.Kafka中是怎么体现消息顺序性的？</h4><blockquote>
<p>每个分区内，每条消息都有一个offset，故只能保证分区内有序。</p>
</blockquote>
<h4 id="4-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？"><a href="#4-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？" class="headerlink" title="4.Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？"></a>4.Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</h4><blockquote>
<p>拦截器 -&gt; 序列化器 -&gt; 分区器</p>
</blockquote>
<h4 id="5-Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？"><a href="#5-Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？" class="headerlink" title="5.Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？"></a>5.Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？</h4><p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200522223103.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200522223103.png" class="lazyload"></a></p>
<h4 id="6-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？"><a href="#6-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？" class="headerlink" title="6.“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？"></a>6.“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？</h4><blockquote>
<p>正确</p>
</blockquote>
<h4 id="7-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1？"><a href="#7-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1？" class="headerlink" title="7.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？"></a>7.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？</h4><blockquote>
<p>offset+1 记录下次消费的数据的offset</p>
</blockquote>
<h4 id="8-有哪些情形会造成重复消费？"><a href="#8-有哪些情形会造成重复消费？" class="headerlink" title="8.有哪些情形会造成重复消费？"></a>8.有哪些情形会造成重复消费？</h4><p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200523163301.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200523163301.png" class="lazyload"></a></p>
<h4 id="9-有哪些情景会造成消息漏消费？"><a href="#9-有哪些情景会造成消息漏消费？" class="headerlink" title="9.有哪些情景会造成消息漏消费？"></a>9.有哪些情景会造成消息漏消费？</h4><blockquote>
<p>先提交offset，后消费，有可能造成数据的重复</p>
</blockquote>
<h4 id="10-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？"><a href="#10-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？" class="headerlink" title="10.当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？"></a>10.当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？</h4><blockquote>
<p>1）会在zookeeper中的/brokers/topics节点下创建一个新的topic节点，如：/brokers/topics/first</p>
<p>2）触发Controller的监听程序</p>
<p>3）kafka Controller 负责topic的创建工作，并更新metadata cache</p>
</blockquote>
<h4 id="11-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"><a href="#11-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？" class="headerlink" title="11.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"></a>11.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</h4><blockquote>
<p>可以增加</p>
<p>bin/kafka-topics.sh –zookeeper localhost:2181/kafka –alter –topic topic-config –partitions 3</p>
</blockquote>
<h4 id="12-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"><a href="#12-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？" class="headerlink" title="12.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"></a>12.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</h4><blockquote>
<p>不可以减少，现有的分区数据难以处理。</p>
</blockquote>
<h4 id="13-Kafka有内部的topic吗？如果有是什么？有什么所用？"><a href="#13-Kafka有内部的topic吗？如果有是什么？有什么所用？" class="headerlink" title="13.Kafka有内部的topic吗？如果有是什么？有什么所用？"></a>13.Kafka有内部的topic吗？如果有是什么？有什么所用？</h4><blockquote>
<p>__consumer_offsets, 共有50个分区 保存消费者offset</p>
</blockquote>
<h4 id="14-Kafka分区分配的概念？"><a href="#14-Kafka分区分配的概念？" class="headerlink" title="14.Kafka分区分配的概念？"></a>14.Kafka分区分配的概念？</h4><blockquote>
<p>一个topic多个分区，一个消费者组多个消费者，故需要将分区分配个消费者(roundrobin、range)</p>
</blockquote>
<h4 id="15-简述Kafka的日志目录结构？"><a href="#15-简述Kafka的日志目录结构？" class="headerlink" title="15.简述Kafka的日志目录结构？"></a>15.简述Kafka的日志目录结构？</h4><blockquote>
<p>每个分区对应一个文件夹，文件夹的命名为topic-0，topic-1，内部为.log和.index文件</p>
</blockquote>
<h4 id="16-如果我指定了一个offset，Kafka-Controller怎么查找到对应的消息？"><a href="#16-如果我指定了一个offset，Kafka-Controller怎么查找到对应的消息？" class="headerlink" title="16.如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？"></a>16.如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？</h4><blockquote>
<p>先通过offset比对log文件的名字 确定好后 再找到对应的index文件中offset对应的消息索引位置</p>
<p>最后在log文件中找到相应的消息</p>
</blockquote>
<p><a href="https://gitee.com/hoffeechen/image/raw/master/img/20200523163707.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://gitee.com/hoffeechen/image/raw/master/img/20200523163707.png" class="lazyload"></a></p>
<h4 id="17-聊一聊Kafka-Controller的作用？"><a href="#17-聊一聊Kafka-Controller的作用？" class="headerlink" title="17.聊一聊Kafka Controller的作用？"></a>17.聊一聊Kafka Controller的作用？</h4><blockquote>
<p>负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。</p>
</blockquote>
<h4 id="18-Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？"><a href="#18-Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？" class="headerlink" title="18.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？"></a>18.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？</h4><blockquote>
<p>partition leader（ISR），由Controller负责</p>
<p>Controller（先到先得）</p>
</blockquote>
<h4 id="19-失效副本是指什么？有那些应对措施？"><a href="#19-失效副本是指什么？有那些应对措施？" class="headerlink" title="19.失效副本是指什么？有那些应对措施？"></a>19.失效副本是指什么？有那些应对措施？</h4><blockquote>
<p>不能及时与leader同步，暂时踢出ISR，等其追上leader之后再重新加入</p>
</blockquote>
<h4 id="20-Kafka的那些设计让它有如此高的性能？"><a href="#20-Kafka的那些设计让它有如此高的性能？" class="headerlink" title="20.Kafka的那些设计让它有如此高的性能？"></a>20.Kafka的那些设计让它有如此高的性能？</h4><blockquote>
<p>分区，顺序写磁盘，0-copy</p>
</blockquote>
<h3 id="其他kafka相关面试题搜集-一"><a href="#其他kafka相关面试题搜集-一" class="headerlink" title="其他kafka相关面试题搜集(一)"></a>其他kafka相关面试题搜集(一)</h3><h4 id="1、请说明什么是Apache-Kafka"><a href="#1、请说明什么是Apache-Kafka" class="headerlink" title="1、请说明什么是Apache Kafka?"></a>1、请说明什么是Apache Kafka?</h4><blockquote>
<p>Apache Kafka是由Apache开发的一种发布订阅消息系统，它是一个分布式的、分区的和可复制的提交日志服务。</p>
</blockquote>
<h4 id="2、说说Kafka的使用场景？"><a href="#2、说说Kafka的使用场景？" class="headerlink" title="2、说说Kafka的使用场景？"></a>2、说说Kafka的使用场景？</h4><blockquote>
<p>①异步处理<br>②应用解耦<br>③流量削峰<br>④日志处理<br>⑤消息通讯等。</p>
</blockquote>
<h4 id="3、使用Kafka有什么优点和缺点？"><a href="#3、使用Kafka有什么优点和缺点？" class="headerlink" title="3、使用Kafka有什么优点和缺点？"></a>3、使用Kafka有什么优点和缺点？</h4><blockquote>
<p>优点：<br>①支持跨数据中心的消息复制；<br>②单机吞吐量：十万级，最大的优点，就是吞吐量高;<br>③topic数量都吞吐量的影响：topic从几十个到几百个的时候，吞吐量会大幅度下降。所以在同等机器下，kafka尽量保证topic数量不要过多。如果要支撑大规模topic，需要增加更多的机器资源;<br>④时效性：ms级;<br>⑤可用性：非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用;<br>⑥消息可靠性：经过参数优化配置，消息可以做到0丢失;<br>⑦功能支持：功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用。</p>
</blockquote>
<blockquote>
<p>缺点：<br>①由于是批量发送，数据并非真正的实时； 仅支持统一分区内消息有序，无法实现全局消息有序；<br>②有可能消息重复消费；<br>③依赖zookeeper进行元数据管理，等等。</p>
</blockquote>
<h4 id="4、为什么说Kafka性能很好，体现在哪里？"><a href="#4、为什么说Kafka性能很好，体现在哪里？" class="headerlink" title="4、为什么说Kafka性能很好，体现在哪里？"></a>4、为什么说Kafka性能很好，体现在哪里？</h4><blockquote>
<p>①顺序读写<br>②零拷贝<br>③分区<br>④批量发送<br>⑤数据压缩</p>
</blockquote>
<h4 id="5、请说明什么是传统的消息传递方法"><a href="#5、请说明什么是传统的消息传递方法" class="headerlink" title="5、请说明什么是传统的消息传递方法?"></a>5、请说明什么是传统的消息传递方法?</h4><blockquote>
<p>传统的消息传递方法包括两种：<br>排队：在队列中，一组用户可以从服务器中读取消息，每条消息都发送给其中一个人。<br>发布-订阅：在这个模型中，消息被广播给所有的用户。</p>
</blockquote>
<h4 id="6、请说明Kafka相对传统技术有什么优势"><a href="#6、请说明Kafka相对传统技术有什么优势" class="headerlink" title="6、请说明Kafka相对传统技术有什么优势?"></a>6、请说明Kafka相对传统技术有什么优势?</h4><blockquote>
<p>①快速:单一的Kafka代理可以处理成千上万的客户端，每秒处理数兆字节的读写操作。<br>②可伸缩:在一组机器上对数据进行分区<br>③和简化，以支持更大的数据<br>④持久:消息是持久性的，并在集群中进<br>⑤行复制，以防止数据丢失。<br>⑥设计:它提供了容错保证和持久性</p>
</blockquote>
<h4 id="7、解释Kafka的Zookeeper是什么-我们可以在没有Zookeeper的情况下使用Kafka吗"><a href="#7、解释Kafka的Zookeeper是什么-我们可以在没有Zookeeper的情况下使用Kafka吗" class="headerlink" title="7、解释Kafka的Zookeeper是什么?我们可以在没有Zookeeper的情况下使用Kafka吗?"></a>7、解释Kafka的Zookeeper是什么?我们可以在没有Zookeeper的情况下使用Kafka吗?</h4><blockquote>
<p>Zookeeper是一个开放源码的、高性能的协调服务，它用于Kafka的分布式应用。<br>不，不可能越过Zookeeper，直接联系Kafka broker。一旦Zookeeper停止工作，它就不能服务客户端请求。<br>Zookeeper主要用于在集群中不同节点之间进行通信<br>在Kafka中，它被用于提交偏移量，因此如果节点在任何情况下都失败了，它都可以从之前提交的偏移量中获取<br>除此之外，它还执行其他活动，如: leader检测、分布式同步、配置管理、识别新节点何时离开或连接、集群、节点实时状态等等。</p>
</blockquote>
<h4 id="8、解释Kafka的用户如何消费信息"><a href="#8、解释Kafka的用户如何消费信息" class="headerlink" title="8、解释Kafka的用户如何消费信息?"></a>8、解释Kafka的用户如何消费信息?</h4><blockquote>
<p>在Kafka中传递消息是通过使用sendfile API完成的。它支持将字节从套接口转移到磁盘，通过内核空间保存副本，并在内核用户之间调用内核。</p>
</blockquote>
<h4 id="9、解释如何提高远程用户的吞吐量"><a href="#9、解释如何提高远程用户的吞吐量" class="headerlink" title="9、解释如何提高远程用户的吞吐量?"></a>9、解释如何提高远程用户的吞吐量?</h4><blockquote>
<p>如果用户位于与broker不同的数据中心，则可能需要调优套接口缓冲区大小，以对长网络延迟进行摊销。</p>
</blockquote>
<h4 id="10、解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息"><a href="#10、解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息" class="headerlink" title="10、解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息?"></a>10、解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息?</h4><blockquote>
<p>在数据中，为了精确地获得Kafka的消息，你必须遵循两件事:</p>
<p>在数据消耗期间避免重复，在数据生产过程中避免重复。</p>
<p>这里有两种方法，可以在数据生成时准确地获得一个语义:</p>
<p>每个分区使用一个单独的写入器，每当你发现一个网络错误，检查该分区中的最后一条消息，以查看您的最后一次写入是否成功</p>
<p>在消息中包含一个主键(UUID或其他)，并在用户中进行反复制</p>
</blockquote>
<h4 id="11、解释如何减少ISR中的扰动-broker什么时候离开ISR"><a href="#11、解释如何减少ISR中的扰动-broker什么时候离开ISR" class="headerlink" title="11、解释如何减少ISR中的扰动?broker什么时候离开ISR?"></a>11、解释如何减少ISR中的扰动?broker什么时候离开ISR?</h4><blockquote>
<p>ISR是一组与leaders完全同步的消息副本，也就是说ISR中包含了所有提交的消息。ISR应该总是包含所有的副本，直到出现真正的故障。如果一个副本从leader中脱离出来，将会从ISR中删除。</p>
</blockquote>
<h4 id="12、Kafka为什么需要复制"><a href="#12、Kafka为什么需要复制" class="headerlink" title="12、Kafka为什么需要复制?"></a>12、Kafka为什么需要复制?</h4><blockquote>
<p>Kafka的信息复制确保了任何已发布的消息不会丢失，并且可以在机器错误、程序错误或更常见些的软件升级中使用。</p>
</blockquote>
<h4 id="13、如果副本在ISR中停留了很长时间表明什么"><a href="#13、如果副本在ISR中停留了很长时间表明什么" class="headerlink" title="13、如果副本在ISR中停留了很长时间表明什么?"></a>13、如果副本在ISR中停留了很长时间表明什么?</h4><blockquote>
<p>如果一个副本在ISR中保留了很长一段时间，那么它就表明，跟踪器无法像在leader收集数据那样快速地获取数据。</p>
</blockquote>
<h4 id="14、请说明如果首选的副本不在ISR中会发生什么"><a href="#14、请说明如果首选的副本不在ISR中会发生什么" class="headerlink" title="14、请说明如果首选的副本不在ISR中会发生什么?"></a>14、请说明如果首选的副本不在ISR中会发生什么?</h4><blockquote>
<p>如果首选的副本不在ISR中，控制器将无法将leadership转移到首选的副本。</p>
</blockquote>
<h4 id="15、有可能在生产后发生消息偏移吗"><a href="#15、有可能在生产后发生消息偏移吗" class="headerlink" title="15、有可能在生产后发生消息偏移吗?"></a>15、有可能在生产后发生消息偏移吗?</h4><blockquote>
<p>在大多数队列系统中，作为生产者的类无法做到这一点，它的作用是触发并忘记消息。broker将完成剩下的工作，比如使用id进行适当的元数据处理、偏移量等。</p>
<p>作为消息的用户，你可以从Kafka broker中获得补偿。如果你注视SimpleConsumer类，你会注意到它会获取包括偏移量作为列表的MultiFetchResponse对象。此外，当你对Kafka消息进行迭代时，你会拥有包括偏移量和消息发送的MessageAndOffset对象。</p>
</blockquote>
<h4 id="16、Kafka的设计时什么样的呢？"><a href="#16、Kafka的设计时什么样的呢？" class="headerlink" title="16、Kafka的设计时什么样的呢？"></a>16、Kafka的设计时什么样的呢？</h4><blockquote>
<p>Kafka将消息以topic为单位进行归纳</p>
<p>将向Kafka topic发布消息的程序成为producers. 将订阅了topics并消费消息的程序成为consumer.</p>
<p>Kafka以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个broker.</p>
<p>producers通过网络将消息发送到Kafka集群，集群向消费者提供消息</p>
</blockquote>
<h4 id="17、数据传输的事务定义有哪三种？"><a href="#17、数据传输的事务定义有哪三种？" class="headerlink" title="17、数据传输的事务定义有哪三种？"></a>17、数据传输的事务定义有哪三种？</h4><blockquote>
<p>（1）最多一次:<br>消息不会被重复发送，最多被传输一次，但也有可能一次不传输<br>（2）最少一次: 消息不会被漏发送，最少被传输一次，但也有可能被重复传输.<br>（3）精确的一次（Exactly once）: 不会漏传输也不会重复传输,每个消息都传输被一次而且仅仅被传输一次，这是大家所期望的</p>
</blockquote>
<h4 id="18、Kafka判断一个节点是否还活着有那两个条件？"><a href="#18、Kafka判断一个节点是否还活着有那两个条件？" class="headerlink" title="18、Kafka判断一个节点是否还活着有那两个条件？"></a>18、Kafka判断一个节点是否还活着有那两个条件？</h4><blockquote>
<p>（1）节点必须可以维护和ZooKeeper的连接，Zookeeper通过心跳机制检查每个节点的连接<br>（2）如果节点是个follower,他必须能及时的同步leader的写操作，延时不能太久</p>
</blockquote>
<h4 id="19、producer是否直接将数据发送到broker的leader-主节点-？"><a href="#19、producer是否直接将数据发送到broker的leader-主节点-？" class="headerlink" title="19、producer是否直接将数据发送到broker的leader(主节点)？"></a>19、producer是否直接将数据发送到broker的leader(主节点)？</h4><blockquote>
<p>producer直接将数据发送到broker的leader(主节点)，不需要在多个节点进行分发，为了帮助producer做到这点，所有的Kafka节点都可以及时的告知:哪些节点是活动的，目标topic目标分区的leader在哪。这样producer就可以直接将消息发送到目的地了。</p>
</blockquote>
<h4 id="20、Kafa-consumer是否可以消费指定分区消息？"><a href="#20、Kafa-consumer是否可以消费指定分区消息？" class="headerlink" title="20、Kafa consumer是否可以消费指定分区消息？"></a>20、Kafa consumer是否可以消费指定分区消息？</h4><blockquote>
<p>Kafa consumer消费消息时，向broker发出”fetch”请求去消费特定分区的消息，consumer指定消息在日志中的偏移量（offset），就可以消费从这个位置开始的消息，customer拥有了offset的控制权，可以向后回滚去重新消费之前的消息，这是很有意义的</p>
</blockquote>
<h4 id="21、Kafka消息是采用Pull模式，还是Push模式？"><a href="#21、Kafka消息是采用Pull模式，还是Push模式？" class="headerlink" title="21、Kafka消息是采用Pull模式，还是Push模式？"></a>21、Kafka消息是采用Pull模式，还是Push模式？</h4><blockquote>
<p>Kafka最初考虑的问题是，customer应该从brokes拉取消息还是brokers将消息推送到consumer，也就是pull还push。在这方面，Kafka遵循了一种大部分消息系统共同的传统的设计：producer将消息推送到broker，consumer从broker拉取消息一些消息系统比如Scribe和Apache Flume采用了push模式，将消息推送到下游的consumer。这样做有好处也有坏处：由broker决定消息推送的速率，对于不同消费速率的consumer就不太好处理了。消息系统都致力于让consumer以最大的速率最快速的消费消息，但不幸的是，push模式下，当broker推送的速率远大于consumer消费的速率时，consumer恐怕就要崩溃了。最终Kafka还是选取了传统的pull模式</p>
<p>Pull模式的另外一个好处是consumer可以自主决定是否批量的从broker拉取数据。Push模式必须在不知道下游consumer消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。如果为了避免consumer崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。Pull模式下，consumer就可以根据自己的消费能力去决定这些策略</p>
<p>Pull有个缺点是，如果broker没有可供消费的消息，将导致consumer不断在循环中轮询，直到新消息到t达。为了避免这点，Kafka有个参数可以让consumer阻塞知道新消息到达(当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发</p>
</blockquote>
<h4 id="22、Kafka存储在硬盘上的消息格式是什么？"><a href="#22、Kafka存储在硬盘上的消息格式是什么？" class="headerlink" title="22、Kafka存储在硬盘上的消息格式是什么？"></a>22、Kafka存储在硬盘上的消息格式是什么？</h4><blockquote>
<p>消息由一个固定长度的头部和可变长度的字节数组组成。头部包含了一个版本号和CRC32校验码。<br>消息长度: 4 bytes (value: 1+4+n)<br>版本号: 1 byte<br>CRC校验码: 4 bytes<br>具体的消息: n bytes</p>
</blockquote>
<h4 id="23、Kafka高效文件存储设计特点："><a href="#23、Kafka高效文件存储设计特点：" class="headerlink" title="23、Kafka高效文件存储设计特点："></a>23、Kafka高效文件存储设计特点：</h4><blockquote>
<p>(1).Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。<br>(2).通过索引信息可以快速定位message和确定response的最大大小。<br>(3).通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。<br>(4).通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。</p>
</blockquote>
<h4 id="24、Kafka-与传统消息系统之间有三个关键区别"><a href="#24、Kafka-与传统消息系统之间有三个关键区别" class="headerlink" title="24、Kafka 与传统消息系统之间有三个关键区别"></a>24、Kafka 与传统消息系统之间有三个关键区别</h4><blockquote>
<p>(1).Kafka 持久化日志，这些日志可以被重复读取和无限期保留<br>(2).Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据提升容错能力和高可用性<br>(3).Kafka 支持实时的流式处理</p>
</blockquote>
<h4 id="25、Kafka创建Topic时如何将分区放置到不同的Broker中"><a href="#25、Kafka创建Topic时如何将分区放置到不同的Broker中" class="headerlink" title="25、Kafka创建Topic时如何将分区放置到不同的Broker中"></a>25、Kafka创建Topic时如何将分区放置到不同的Broker中</h4><blockquote>
<p>副本因子不能大于 Broker 的个数；<br>第一个分区（编号为0）的第一个副本放置位置是随机从 brokerList 选择的；<br>其他分区的第一个副本放置位置相对于第0个分区依次往后移。也就是如果我们有5个 Broker，5个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个 Broker 上，依次类推；<br>剩余的副本相对于第一个副本放置位置其实是由 nextReplicaShift 决定的，而这个数也是随机产生的</p>
</blockquote>
<h4 id="26、Kafka新建的分区会在哪个目录下创建"><a href="#26、Kafka新建的分区会在哪个目录下创建" class="headerlink" title="26、Kafka新建的分区会在哪个目录下创建"></a>26、Kafka新建的分区会在哪个目录下创建</h4><blockquote>
<p>在启动 Kafka 集群之前，我们需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录，这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘上用于提高读写性能。 当然我们也可以配置 log.dir 参数，含义一样。只需要设置其中一个即可。 如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个目录下创建文件夹用于存放数据。 但是如果 log.dirs 参数配置了多个目录，那么 Kafka 会在哪个文件夹中创建分区目录呢？答案是：Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic名+分区ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录！也就是说，如果你给 log.dirs 参数新增了一个新的磁盘，新的分区目录肯定是先在这个新的磁盘上创建直到这个新的磁盘目录拥有的分区目录不是最少为止。</p>
</blockquote>
<h4 id="27、partition的数据如何保存到硬盘"><a href="#27、partition的数据如何保存到硬盘" class="headerlink" title="27、partition的数据如何保存到硬盘"></a>27、partition的数据如何保存到硬盘</h4><blockquote>
<p>topic中的多个partition以文件夹的形式保存到broker，每个分区序号从0递增， 且消息有序 Partition文件下有多个segment（xxx.index，xxx.log） segment 文件里的 大小和配置文件大小一致可以根据要求修改 默认为1g 如果大小大于1g时，会滚动一个新的segment并且以上一个segment最后一条消息的偏移量命名</p>
</blockquote>
<h4 id="28、kafka的ack机制"><a href="#28、kafka的ack机制" class="headerlink" title="28、kafka的ack机制"></a>28、kafka的ack机制</h4><blockquote>
<p>request.required.acks有三个值 0 1 -1<br>0:生产者不会等待broker的ack，这个延迟最低但是存储的保证最弱当server挂掉的时候就会丢数据<br>1：服务端会等待ack值 leader副本确认接收到消息后发送ack但是如果leader挂掉后他不确保是否复制完成新leader也会导致数据丢失<br>-1：同样在1的基础上 服务端会等所有的follower的副本受到数据后才会受到leader发出的ack，这样数据不会丢失</p>
</blockquote>
<h4 id="29、Kafka的消费者如何消费数据"><a href="#29、Kafka的消费者如何消费数据" class="headerlink" title="29、Kafka的消费者如何消费数据"></a>29、Kafka的消费者如何消费数据</h4><blockquote>
<p>消费者每次消费数据的时候，消费者都会记录消费的物理偏移量（offset）的位置 等到下次消费时，他会接着上次位置继续消费。同时也可以按照指定的offset进行重新消费。</p>
</blockquote>
<h4 id="30、消费者负载均衡策略"><a href="#30、消费者负载均衡策略" class="headerlink" title="30、消费者负载均衡策略"></a>30、消费者负载均衡策略</h4><blockquote>
<p>结合consumer的加入和退出进行再平衡策略。</p>
</blockquote>
<h4 id="31、kafka消息数据是否有序？"><a href="#31、kafka消息数据是否有序？" class="headerlink" title="31、kafka消息数据是否有序？"></a>31、kafka消息数据是否有序？</h4><blockquote>
<p>消费者组里某具体分区是有序的，所以要保证有序只能建一个分区，但是实际这样会存在性能问题，具体业务具体分析后确认。</p>
</blockquote>
<h4 id="32、kafaka生产数据时数据的分组策略-生产者决定数据产生到集群的哪个partition中"><a href="#32、kafaka生产数据时数据的分组策略-生产者决定数据产生到集群的哪个partition中" class="headerlink" title="32、kafaka生产数据时数据的分组策略,生产者决定数据产生到集群的哪个partition中"></a>32、kafaka生产数据时数据的分组策略,生产者决定数据产生到集群的哪个partition中</h4><blockquote>
<p>每一条消息都是以（key，value）格式 Key是由生产者发送数据传入 所以生产者（key）决定了数据产生到集群的哪个partition</p>
</blockquote>
<h4 id="33、kafka-consumer-什么情况会触发再平衡reblance"><a href="#33、kafka-consumer-什么情况会触发再平衡reblance" class="headerlink" title="33、kafka consumer 什么情况会触发再平衡reblance?"></a>33、kafka consumer 什么情况会触发再平衡reblance?</h4><blockquote>
<p>①一旦消费者加入或退出消费组，导致消费组成员列表发生变化，消费组中的所有消费者都要执行再平衡。<br>②订阅主题分区发生变化，所有消费者也都要再平衡。</p>
</blockquote>
<h4 id="34、描述下kafka-consumer-再平衡步骤"><a href="#34、描述下kafka-consumer-再平衡步骤" class="headerlink" title="34、描述下kafka consumer 再平衡步骤?"></a>34、描述下kafka consumer 再平衡步骤?</h4><blockquote>
<p>①关闭数据拉取线程，清空队列和消息流，提交偏移量；<br>②释放分区所有权，删除zk中分区和消费者的所有者关系；<br>③将所有分区重新分配给每个消费者，每个消费者都会分到不同分区；<br>④将分区对应的消费者所有关系写入ZK，记录分区的所有权信息；<br>⑤重启消费者拉取线程管理器，管理每个分区的拉取线程。</p>
</blockquote>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yang4</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://masteryang4.github.io/2020/06/18/kafka%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/">https://masteryang4.github.io/2020/06/18/kafka%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://masteryang4.github.io">MasterYangBlog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%95%99%E7%A8%8B/">教程    </a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据    </a><a class="post-meta__tags" href="/tags/kafka/">kafka    </a></div><div class="post_share"><div class="social-share" data-image="https://pic.downk.cc/item/5eeb0c4714195aa594556a5b.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://pic.downk.cc/item/5ea1a251c2a9a83be535b287.png" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://pic.downk.cc/item/5ea1a33ac2a9a83be536f9bc.png" alt="支付宝"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/06/18/MySQL%E7%BB%83%E4%B9%A0%E9%A2%98/"><img class="prev_cover lazyload" data-src="https://pic.downk.cc/item/5eeb147114195aa5946b0444.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>MySQL练习题</span></div></a></div><div class="next-post pull_right"><a href="/2020/06/18/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark-sql/"><img class="next_cover lazyload" data-src="https://pic.downk.cc/item/5ee64771c2a9a83be5f7a357.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>spark系列之spark-sql</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/06/19/spark系列之spark-core/" title="spark系列之spark-core"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5ee64771c2a9a83be5f7a357.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-06-19</div><div class="relatedPosts_title">spark系列之spark-core</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/17/spark系列之spark-streaming/" title="spark系列之spark-streaming"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5ee64771c2a9a83be5f7a357.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-06-17</div><div class="relatedPosts_title">spark系列之spark-streaming</div></div></a></div><div class="relatedPosts_item"><a href="/2020/05/12/精-Redis总结与思考/" title="[精]Redis总结与思考"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5edd02ccc2a9a83be5db9710.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-12</div><div class="relatedPosts_title">[精]Redis总结与思考</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/27/flink系列05Flink-DataStream-API/" title="flink系列05Flink DataStream API"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5ef7647614195aa59476f65a.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-06-27</div><div class="relatedPosts_title">flink系列05Flink DataStream API</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/27/flink系列04第一个Flink程序/" title="flink系列04第一个Flink程序"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5ef7647614195aa59476f65a.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-06-27</div><div class="relatedPosts_title">flink系列04第一个Flink程序</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/27/flink系列03Flink运行架构/" title="flink系列03Flink运行架构"><img class="relatedPosts_cover lazyload"data-src="https://pic.downk.cc/item/5ef7647614195aa59476f65a.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-06-27</div><div class="relatedPosts_title">flink系列03Flink运行架构</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'IeiQD5I6g4Doamc68SctmEnW-gzGzoHsz',
  appKey:'ORWhRoGUBY02RR9DMa5OSIow',
  placeholder:'评论一下~（支持Markdown格式）',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'zh-cn',
  recordIP: true
});</script></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By Yang4</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/fireworks.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>