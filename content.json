{"meta":{"title":"MasterYangBlog","subtitle":"","description":"","author":"Yang4","url":"https://masteryang4.github.io","root":"/"},"pages":[{"title":"About me","date":"2020-04-15T16:13:50.000Z","updated":"2020-06-14T15:13:27.703Z","comments":true,"path":"about/index.html","permalink":"https://masteryang4.github.io/about/index.html","excerpt":"","text":"json123456789&#123; \"Name\": \"Yang Sen\", \"School\": \"WHU\" \"Sex\": \" ♂ \", \"Address\": \"Shanghai.China\", \"Github\": \"https://github.com/masteryang4\", \"Blog\": \"www.yangsen94.top\", \"E-mail\": \"2692474773@qq.com\"&#125; 技术栈： Java，Python，Scala，大数据，Web，机器学习&amp;深度学习技术等 欢迎互相交♂流♂学习~"},{"title":"文章分类","date":"2020-04-13T13:58:22.000Z","updated":"2020-04-15T15:54:13.404Z","comments":true,"path":"categories/index.html","permalink":"https://masteryang4.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-04-13T13:58:35.000Z","updated":"2020-06-17T14:04:23.097Z","comments":true,"path":"link/index.html","permalink":"https://masteryang4.github.io/link/index.html","excerpt":"","text":"Codesheep程序羊： https://www.codesheep.cn/ 廖雪峰的官方网站： https://www.liaoxuefeng.com/ 牛客网： https://www.nowcoder.com/ 左耳朵耗子（酷壳）： https://coolshell.cn/ 敖丙三太子： https://github.com/AobingJava 知乎： https://www.zhihu.com/ 王道计算机考研： http://cskaoyan.com/forum.php"},{"title":"留言板","date":"2020-04-19T12:00:02.000Z","updated":"2020-04-19T12:10:25.372Z","comments":true,"path":"messageboard/index.html","permalink":"https://masteryang4.github.io/messageboard/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-04-13T13:57:16.000Z","updated":"2020-04-15T15:58:24.136Z","comments":true,"path":"tags/index.html","permalink":"https://masteryang4.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"kafka知识整理","slug":"kafka知识整理","date":"2020-06-18T06:33:51.000Z","updated":"2020-06-18T06:41:13.096Z","comments":true,"path":"2020/06/18/kafka知识整理/","link":"","permalink":"https://masteryang4.github.io/2020/06/18/kafka%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/","excerpt":"","text":"kafka 本文转载自： https://chenhefei.github.io/2020/04/01/Kafka/Kafka-learning/ kafka的定义Kafka是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 消息队列有什么好处Code123456789101）解耦允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。2）可恢复性系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。3）缓冲有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。4）灵活性 &amp; 峰值处理能力在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。5）异步通信很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 消费队列的两种模式Code12345（1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。（2）发布&#x2F;订阅模式（一对多，消费者消费数据之后不会清除消息）消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。 kafka中的相关概念Code1234567891）Producer ：消息生产者，就是向kafka broker发消息的客户端；2）Consumer ：消息消费者，向kafka broker取消息的客户端；3）Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。 kafka配置文件位置 Code12[ys@hadoop102 kafka]$ cd config&#x2F;[ys@hadoop102 config]$ vi server.properties 内容 properties123456789101112131415161718192021222324#broker的全局唯一编号，不能重复broker.id=0#删除topic功能使能delete.topic.enable=true#处理网络请求的线程数量num.network.threads=3#用来处理磁盘IO的线程数量num.io.threads=8#发送套接字的缓冲区大小socket.send.buffer.bytes=102400#接收套接字的缓冲区大小socket.receive.buffer.bytes=102400#请求套接字的缓冲区大小socket.request.max.bytes=104857600#kafka运行日志存放的路径log.dirs=/opt/module/kafka/logs#topic在当前broker上的分区个数num.partitions=1#用来恢复和清理data下数据的线程数量num.recovery.threads.per.data.dir=1#segment文件保留的最长时间，超时将被删除log.retention.hours=168#配置连接Zookeeper集群地址zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181 kafka分布式的broker.id配置Code1plain修改配置文件&#x2F;opt&#x2F;module&#x2F;kafka&#x2F;config&#x2F;server.properties中的broker.id&#x3D;1、broker.id&#x3D;2注：broker.id不得重复 kafka的群起脚本shell12345for i in hadoop102 hadoop103 hadoop104doecho \"========== $i ==========\" ssh $i '/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties'done kafka的命令行操作命令Code12345678910111213141516171819202122232425262728293031323334353637383940启动[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties查看当前服务器中的所有topic[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --list创建topic[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first选项说明：--topic 定义topic名--replication-factor 定义副本数--partitions 定义分区数删除topic[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first需要server.properties中设置delete.topic.enable&#x3D;true否则只是标记删除。发送消息[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-console-producer.sh --broker-list hadoop102:9092 --topic first&gt;hello world&gt;atguigu atguigu消费消息[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \\--zookeeper hadoop102:2181 --topic first[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \\--zookeeper hadoop102:2181 --topic first --consumer.config config&#x2F;consumer.properties 指定消费者的配置文件(可将多个消费者放置在一个组内)[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \\--bootstrap-server hadoop102:9092 --topic first[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \\--bootstrap-server hadoop102:9092 --from-beginning --topic first注 : --from-beginning：会把主题中以往所有的数据都读取出来。查看某个Topic的详情[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first修改分区数[atguigu@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6 kafka工作流程Code12345678910111213141516Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。如下00000000000000000000.index00000000000000000000.log00000000000000170410.index00000000000000170410.log00000000000000239430.index00000000000000239430.logindex和log文件以当前segment的第一条消息的offset命名。“.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址。 kafka生产者的分区分配策略Code1234567891）分区的原因（1）方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；（2）可以提高并发，因为可以以Partition为单位读写了。2）分区的原则我们需要将producer发送的数据封装成一个ProducerRecord对象。（1）指明 partition 的情况下，直接将指明的值直接作为 partiton 值；（2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；（3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 kafka如何保证数据可靠性Code123为保证producer发送的数据能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。 都有哪些副本数据同步策略 优缺点是什么 方案 优点 缺点 半数以上完成同步，就发送ack 延迟低 选举新的leader时，容忍n台节点的故障，需要2n+1个副本 全部完成同步，才发送ack 选举新的leader时，容忍n台节点的故障，需要n+1个副本 延迟高 kafka的副本同步策略是什么 这个策略会出现什么问题Code12345Kafka选择了第二种方案，原因如下：1.同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。2.虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？ kafka中的ISR是什么Code123456789Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader。 kafka中的ack应答机制是什么Code12对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。 Code12345acks参数配置：acks：0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。 kafka如何进行故障处理Code12LEO：指的是每个副本最大的offset；HW：指的是消费者能见到的最大的offset，ISR队列中最小的LEO。 Code1234（1）follower故障follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。（2）leader故障leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 kafka消费者的消费方式Code123consumer采用pull（拉）模式从broker中读取数据。push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。 kafka消费者的分区分配策略Code12一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。Kafka有两种分配策略，一是RoundRobin，一是Range。 kafka消费者如何维护offsetCode1234567891011由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets。1）修改配置文件consumer.propertiesexclude.internal.topics&#x3D;false2）读取offset0.11.0.0之前版本:bin&#x2F;kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter &quot;kafka.coordinator.GroupMetadataManager\\$OffsetsMessageFormatter&quot; --consumer.config config&#x2F;consumer.properties --from-beginning0.11.0.0之后版本(含):bin&#x2F;kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter &quot;kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter&quot; --consumer.config config&#x2F;consumer.properties --from-beginning kafka中的消费者组是什么Code123456789101112配置config&#x2F;consumer.properties文件中的group.id然后在启动消费者时候使用同一个配置文件 就可以让消费者在一个组内同一个消费者组中的消费者，同一时刻只能有一个消费者消费。如果消费者组中的消费者多于当前的分区数 会有警告提醒No broker partitions consumed by consumer thread ...如果停止了所有的消费者 那么offset会维护在我们选择的地方(zk中或者是本地) 再次启动消费者会根据选择的GTP(group topic partition所维护的offset位置进行继续消费)下图为zk中维护的信息 kafka为什么能够高效读写数据 分布式框架 分区 顺序写磁盘 Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。 零复制技术 kafka的零拷贝技术如何实现kafka中的消费者在读取服务端的数据时，需要将服务端的磁盘文件通过网络发送到消费者进程，网络发送需要经过几种网络节点。如下图所示： 传统的读取文件数据并发送到网络的步骤如下：（1）操作系统将数据从磁盘文件中读取到内核空间的页面缓存；（2）应用程序将数据从内核空间读入用户空间缓冲区；（3）应用程序将读到数据写回内核空间并放入socket缓冲区；（4）操作系统将数据从socket缓冲区复制到网卡接口，此时数据才能通过网络发送。 通常情况下，Kafka的消息会有多个订阅者，生产者发布的消息会被不同的消费者多次消费，为了优化这个流程，Kafka使用了“零拷贝技术”，如下图所示： “零拷贝技术”只用将磁盘文件的数据复制到页面缓存中一次，然后将数据从页面缓存直接发送到网络中（发送给不同的订阅者时，都可以使用同一个页面缓存），避免了重复复制操作。 如果有10个消费者，传统方式下，数据复制次数为4*10=40次，而使用“零拷贝技术”只需要1+10=11次，一次为从磁盘复制到页面缓存，10次表示10个消费者各自读取一次页面缓存。 传统的文件拷贝通常需要从用户态去转到核心态，经过read buffer，然后再返回到用户态的应用层buffer，然后再从用户态把数据拷贝到核心态的socket buffer，然后发送到网卡。 传统的数据传输需要多次的用户态和核心态之间的切换，而且还要把数据复制多次，最终才打到网卡。 如果减少了用户态与核心态之间的切换，是不是就会更快了呢？ 此时我们会发现用户态“空空如也”。数据没有来到用户态，而是直接在核心态就进行了传输，但这样依然还是有多次复制。首先数据被读取到read buffer中，然后发到socket buffer，最后才发到网卡。虽然减少了用户态和核心态的切换，但依然存在多次数据复制。 如果可以进一步减少数据复制的次数，甚至没有数据复制是不是就会做到最快呢？ DMA 别急，这里我们先介绍一个新的武器:DMA。 DMA，全称叫Direct Memory Access，一种可让某些硬件子系统去直接访问系统主内存，而不用依赖CPU的计算机系统的功能。听着是不是很厉害，跳过CPU，直接访问主内存。传统的内存访问都需要通过CPU的调度来完成。如下图： 而DMA，则可以绕过CPU，硬件自己去直接访问系统主内存。如下图： 很多硬件都支持DMA，这其中就包括网卡。 零拷贝 回到本文中的文件传输，有了DMA后，就可以实现绝对的零拷贝了，因为网卡是直接去访问系统主内存的。如下图： Java的零拷贝实现 在Java中的零拷贝实现是在FileChannel中，其中有个方法transferTo(position,fsize,src)。 传统的文件传输是通过java.io.DataOutputStream，java.io.FileInputStream来实现的，然后通过while循环来读取input，然后写入到output中。 零拷贝则是通过java.nio.channels.FileChannel中的transferTo方法来实现的。transferTo方法底层是基于操作系统的sendfile这个system call来实现的（不再需要拷贝到用户态了），sendfile负责把数据从某个fd（file descriptor）传输到另一个fd。 sendfile： Java的transferTo： 传统方式与零拷贝性能对比 可以看出速度快出至少三倍多。Kafka在文件传输的过程中正是使用了零拷贝技术对文件进行拷贝。建议以后多用FileChannel的transferTo吧。 总结 传统的文件传输有多次用户态和内核态之间的切换，而且文件在多个buffer之间要复制多次最终才被发送到网卡。 DMA是一种硬件直接访问系统主内存的技术。 多种硬件都已使用了DMA技术，其中就包括网卡（NIC）。 DMA技术让CPU得到解放，让CPU可以不用一直守着来完成文件传输。 零拷贝技术减少了用户态与内核态之间的切换，让拷贝次数降到最低，从而实现高性能。 Kafka使用零拷贝技术来进行文件的传输。 zk在kafka中的作用Code12Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。Controller的管理工作都是依赖于Zookeeper的。 Code12345每个broker都会在zk进行注册然后KafkaController会实时监听zk中的&#x2F;brokers&#x2F;ids下的节点情况[0,1,2]如果broker0宕机 ids中的节点会实时变化为[1,2]KafkaController会更新topic中的leader和isr队列KafkaController会获取当前可用的isr并从中选出新的leader kafka的消息发送流程是什么样的Code12345678910Kafka的Producer发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量——RecordAccumulator(这个里面有分区)main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。注意这里面 先走拦截器 再走序列化器 再走分区器达到batch.size大小或者是linger.ms时间就发到RecordAccumulator中sender线程去拉取 Code123相关参数：batch.size：只有数据积累到batch.size之后，sender才会发送数据。(默认16kb)linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。 如何使用kafka API 实现异步消息发送准备知识需要用到的类： KafkaProducer：需要创建一个生产者对象，用来发送数据 ProducerConfig：获取所需的一系列配置参数 ProducerRecord：每条数据都要封装成一个ProducerRecord对象 几个比较重要的配置项 //kafka集群，broker-listprops.put(“bootstrap.servers”, “hadoop102:9092”); java1props.put(\"acks\", \"all\"); //重试次数 props.put(\"retries\", 1); //批次大小 props.put(\"batch.size\", 16384); //等待时间 props.put(\"linger.ms\", 1); //RecordAccumulator缓冲区大小 props.put(\"buffer.memory\", 33554432); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); kafka集群位置 批次大小 批次等待时间 重试次数 缓冲区大小 序列化器(org\\apache\\kafka\\common\\serialization\\Serializer.java) properties12345678910111213141516171819202122232425262728293031323334org\\apache\\kafka\\clients\\producer\\ProducerConfig.javapublic static final String BOOTSTRAP_SERVERS_CONFIG = \"bootstrap.servers\";public static final String METADATA_MAX_AGE_CONFIG = \"metadata.max.age.ms\";public static final String BATCH_SIZE_CONFIG = \"batch.size\";public static final String ACKS_CONFIG = \"acks\";public static final String LINGER_MS_CONFIG = \"linger.ms\";public static final String CLIENT_ID_CONFIG = \"client.id\";public static final String SEND_BUFFER_CONFIG = \"send.buffer.bytes\";public static final String RECEIVE_BUFFER_CONFIG = \"receive.buffer.bytes\";public static final String MAX_REQUEST_SIZE_CONFIG = \"max.request.size\";public static final String RECONNECT_BACKOFF_MS_CONFIG = \"reconnect.backoff.ms\";public static final String RECONNECT_BACKOFF_MAX_MS_CONFIG = \"reconnect.backoff.max.ms\";public static final String MAX_BLOCK_MS_CONFIG = \"max.block.ms\";public static final String BUFFER_MEMORY_CONFIG = \"buffer.memory\";public static final String RETRY_BACKOFF_MS_CONFIG = \"retry.backoff.ms\";public static final String COMPRESSION_TYPE_CONFIG = \"compression.type\";public static final String METRICS_SAMPLE_WINDOW_MS_CONFIG = \"metrics.sample.window.ms\";public static final String METRICS_NUM_SAMPLES_CONFIG = \"metrics.num.samples\";public static final String METRICS_RECORDING_LEVEL_CONFIG = \"metrics.recording.level\";public static final String METRIC_REPORTER_CLASSES_CONFIG = \"metric.reporters\";public static final String MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION = \"max.in.flight.requests.per.connection\";public static final String RETRIES_CONFIG = \"retries\";public static final String KEY_SERIALIZER_CLASS_CONFIG = \"key.serializer\";public static final String VALUE_SERIALIZER_CLASS_CONFIG = \"value.serializer\";public static final String CONNECTIONS_MAX_IDLE_MS_CONFIG = \"connections.max.idle.ms\";public static final String PARTITIONER_CLASS_CONFIG = \"partitioner.class\";public static final String REQUEST_TIMEOUT_MS_CONFIG = \"request.timeout.ms\";public static final String INTERCEPTOR_CLASSES_CONFIG = \"interceptor.classes\";public static final String ENABLE_IDEMPOTENCE_CONFIG = \"enable.idempotence\";public static final String TRANSACTION_TIMEOUT_CONFIG = \"transaction.timeout.ms\";public static final String TRANSACTIONAL_ID_CONFIG = \"transactional.id\"; properties123456789101112131415161718192021222324252627282930313233343536373839org\\apache\\kafka\\clients\\consumer\\ConsumerConfig.javapublic static final String GROUP_ID_CONFIG = \"group.id\";public static final String MAX_POLL_RECORDS_CONFIG = \"max.poll.records\";public static final String MAX_POLL_INTERVAL_MS_CONFIG = \"max.poll.interval.ms\";public static final String SESSION_TIMEOUT_MS_CONFIG = \"session.timeout.ms\";public static final String HEARTBEAT_INTERVAL_MS_CONFIG = \"heartbeat.interval.ms\";public static final String BOOTSTRAP_SERVERS_CONFIG = \"bootstrap.servers\";public static final String ENABLE_AUTO_COMMIT_CONFIG = \"enable.auto.commit\";public static final String AUTO_COMMIT_INTERVAL_MS_CONFIG = \"auto.commit.interval.ms\";public static final String PARTITION_ASSIGNMENT_STRATEGY_CONFIG = \"partition.assignment.strategy\";public static final String AUTO_OFFSET_RESET_CONFIG = \"auto.offset.reset\";public static final String FETCH_MIN_BYTES_CONFIG = \"fetch.min.bytes\";public static final String FETCH_MAX_BYTES_CONFIG = \"fetch.max.bytes\";public static final int DEFAULT_FETCH_MAX_BYTES = 52428800;public static final String FETCH_MAX_WAIT_MS_CONFIG = \"fetch.max.wait.ms\";public static final String METADATA_MAX_AGE_CONFIG = \"metadata.max.age.ms\";public static final String MAX_PARTITION_FETCH_BYTES_CONFIG = \"max.partition.fetch.bytes\";public static final int DEFAULT_MAX_PARTITION_FETCH_BYTES = 1048576;public static final String SEND_BUFFER_CONFIG = \"send.buffer.bytes\";public static final String RECEIVE_BUFFER_CONFIG = \"receive.buffer.bytes\";public static final String CLIENT_ID_CONFIG = \"client.id\";public static final String RECONNECT_BACKOFF_MS_CONFIG = \"reconnect.backoff.ms\";public static final String RECONNECT_BACKOFF_MAX_MS_CONFIG = \"reconnect.backoff.max.ms\";public static final String RETRY_BACKOFF_MS_CONFIG = \"retry.backoff.ms\";public static final String METRICS_SAMPLE_WINDOW_MS_CONFIG = \"metrics.sample.window.ms\";public static final String METRICS_NUM_SAMPLES_CONFIG = \"metrics.num.samples\";public static final String METRICS_RECORDING_LEVEL_CONFIG = \"metrics.recording.level\";public static final String METRIC_REPORTER_CLASSES_CONFIG = \"metric.reporters\";public static final String CHECK_CRCS_CONFIG = \"check.crcs\";public static final String KEY_DESERIALIZER_CLASS_CONFIG = \"key.deserializer\";public static final String VALUE_DESERIALIZER_CLASS_CONFIG = \"value.deserializer\";public static final String CONNECTIONS_MAX_IDLE_MS_CONFIG = \"connections.max.idle.ms\";public static final String REQUEST_TIMEOUT_MS_CONFIG = \"request.timeout.ms\";public static final String INTERCEPTOR_CLASSES_CONFIG = \"interceptor.classes\";public static final String EXCLUDE_INTERNAL_TOPICS_CONFIG = \"exclude.internal.topics\";public static final boolean DEFAULT_EXCLUDE_INTERNAL_TOPICS = true;public static final String ISOLATION_LEVEL_CONFIG = \"isolation.level\";public static final String DEFAULT_ISOLATION_LEVEL; properties123456789101112131415161718org\\apache\\kafka\\clients\\CommonClientConfigs.javapublic static final String BOOTSTRAP_SERVERS_CONFIG = \"bootstrap.servers\";public static final String METADATA_MAX_AGE_CONFIG = \"metadata.max.age.ms\";public static final String SEND_BUFFER_CONFIG = \"send.buffer.bytes\";public static final String RECEIVE_BUFFER_CONFIG = \"receive.buffer.bytes\";public static final String CLIENT_ID_CONFIG = \"client.id\";public static final String RECONNECT_BACKOFF_MS_CONFIG = \"reconnect.backoff.ms\";public static final String RECONNECT_BACKOFF_MAX_MS_CONFIG = \"reconnect.backoff.max.ms\";public static final String RETRY_BACKOFF_MS_CONFIG = \"retry.backoff.ms\";public static final String METRICS_SAMPLE_WINDOW_MS_CONFIG = \"metrics.sample.window.ms\";public static final String METRICS_NUM_SAMPLES_CONFIG = \"metrics.num.samples\";public static final String METRICS_RECORDING_LEVEL_CONFIG = \"metrics.recording.level\";public static final String METRIC_REPORTER_CLASSES_CONFIG = \"metric.reporters\";public static final String SECURITY_PROTOCOL_CONFIG = \"security.protocol\";public static final String DEFAULT_SECURITY_PROTOCOL = \"PLAINTEXT\";public static final String CONNECTIONS_MAX_IDLE_MS_CONFIG = \"connections.max.idle.ms\";public static final String REQUEST_TIMEOUT_MS_CONFIG = \"request.timeout.ms\"; 不带回调的API java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.atguigu.kafka;import org.apache.kafka.clients.producer.*;import java.util.Properties;import java.util.concurrent.ExecutionException;public class CustomProducer &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; Properties props = new Properties(); // 整个配置中的key可以使用ProducerConfig中定义的常量 //kafka集群，broker-list props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"acks\", \"all\"); //重试次数 props.put(\"retries\", 1); //批次大小 props.put(\"batch.size\", 16384); //等待时间 props.put(\"linger.ms\", 1); //RecordAccumulator缓冲区大小 props.put(\"buffer.memory\", 33554432); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) &#123; producer.send(new ProducerRecord&lt;String, String&gt;(\"first\", Integer.toString(i), Integer.toString(i))); // 轮循 这个会用到分区器 producer.send(new ProducerRecord(\"second\",\"value++&gt;\"+i)); // 根据给的key进行hash 然后放在不同的分区 这个会使用到分区器 producer.send(new ProducerRecord(\"second\",\"key\"+i,\"value==&gt;\"+i)); // 具体指定了分区号 就不再使用到key 这个不会用到分区器 if(i&lt;5)&#123; producer.send(new ProducerRecord(\"second\",\"key\"+i,\"value**&gt;\"+i)); &#125;else&#123; producer.send(new ProducerRecord(\"second\",\"key\"+i,\"value^^&gt;\"+i)); &#125; &#125; producer.close(); &#125;&#125; java1234567891011121314151617181920212223242526272829303132// 分区器源码解读org\\apache\\kafka\\clients\\producer\\Partitioner.javapublic interface Partitioner extends Configurable, Closeable &#123; public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); public void close();&#125;//唯一实现类org\\apache\\kafka\\clients\\producer\\internals\\DefaultPartitioner.javapublic class DefaultPartitioner implements Partitioner &#123; // 传进来的是topic key 还有序列化后的key value 序列化后的value public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); //如果key是空的 后面的逻辑用了自增然后对分区取余 其实就是轮循 if (keyBytes == null) &#123; int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // 如果key不是空的 将keyBytes传进去然后做hash murmur2是一种哈希算法 // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; &#125;&#125; 带回调的API java123456跟上面不同的就是在使用send方法时候 带上一个回调函数 // 回调方法:当前消息发出后 不管是消息成功发送还是发送失败 都会执行该回调方法 // metadata 当前消息的元数据 // metadata能拿到当前分区的各种数据 如下图所示 // 偏移量 分区 主题 时间戳 等等 // exception 当消息发送失败 会返回该异常 java1234567// org\\apache\\kafka\\clients\\producer\\Callback.javapublic interface Callback &#123; public void onCompletion(RecordMetadata metadata, Exception exception);&#125;// 这是一个接口 里面有一个方法它有两个实现类 java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 示例 带回调的APIpackage com.atguigu.kafka.producer;import org.apache.kafka.clients.producer.*;import java.util.Properties;import java.util.concurrent.Future;import java.util.concurrent.TimeUnit;public class MyCallBackProducer &#123; public static void main(String[] args) throws Exception &#123; //1. 创建配置对象 Properties props = new Properties(); //kafka集群的位置 props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,\"hadoop102:9092\"); //ack级别 props.put(ProducerConfig.ACKS_CONFIG,\"all\"); //重试次数 props.put(ProducerConfig.RETRIES_CONFIG,3); //批次大小 props.put(ProducerConfig.BATCH_SIZE_CONFIG,16384); //等待时间 props.put(ProducerConfig.LINGER_MS_CONFIG,1); //缓冲区大小 props.put(ProducerConfig.BUFFER_MEMORY_CONFIG,33554432); //k v 序列化器 props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\"); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\"); //2. 创建生产者对象 KafkaProducer&lt;String,String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); //3.生产数据 for (int i = 0; i &lt; 10000 ; i++) &#123; producer.send(new ProducerRecord&lt;&gt;(\"second\", \"atguigu@@@@@\" + i), new Callback() &#123; /** * 回调方法: 当前的消息发送出去以后，会执行回调方法。 * @param metadata 当前消息的元数据信息。 * @param exception 当发送失败，会返回异常。 */ @Override public void onCompletion(RecordMetadata metadata, Exception exception) &#123; if (exception == null) &#123; //发送成功 System.out.println(metadata.topic() + \" -- \" + metadata.partition() + \" -- \" + metadata.offset()); &#125; &#125; &#125;); &#125; // TimeUnit.MILLISECONDS.sleep(100); //关闭 producer.close(); &#125;&#125; kafka API中没有写producer.close()为什么读不到数据 也没有回调方法Code1234567891011这是因为异步发送消息的原因main线程在发送完数据之后就结束了 这个时间小于了批次拉取设置的时间1ms sender线程去拉取数据的同时需要执行main线程中的回调方法 但是现在main线程已经关闭 所以无法执行回调方法如果我们不写close方法 而是让main线程休眠100ms 这时sender就能在这个时间内拉取到数据并执行回调方法所以close方法肯定会等待sender线程拉取数据完成后再进行关闭具体实现可以看close()方法的源码 如下 java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// org\\apache\\kafka\\clients\\producer\\KafkaProducer.java /** * Close this producer. This method blocks until all previously sent requests complete. * This method is equivalent to &lt;code&gt;close(Long.MAX_VALUE, TimeUnit.MILLISECONDS)&lt;/code&gt;. * &lt;p&gt; * &lt;strong&gt;If close() is called from &#123;@link Callback&#125;, a warning message will be logged and close(0, TimeUnit.MILLISECONDS) * will be called instead. We do this because the sender thread would otherwise try to join itself and * block forever.&lt;/strong&gt; * &lt;p&gt; * * @throws InterruptException If the thread is interrupted while blocked *///关闭此生产者。 此方法一直阻塞所有以前发送的请求完成。 此方法等效于close(Long.MAX_VALUE, TimeUnit.MILLISECONDS) 如果关闭（）被从调用Callback ，警告消息将被记录并关闭（0，TimeUnit.MILLISECONDS）将被代替调用。 我们这样做是因为发件人线程否则将尝试加入自己和永远阻塞。 @Override public void close() &#123; close(Long.MAX_VALUE, TimeUnit.MILLISECONDS); &#125; /** * This method waits up to &lt;code&gt;timeout&lt;/code&gt; for the producer to complete the sending of all incomplete requests. * &lt;p&gt; * If the producer is unable to complete all requests before the timeout expires, this method will fail * any unsent and unacknowledged records immediately. * &lt;p&gt; * If invoked from within a &#123;@link Callback&#125; this method will not block and will be equivalent to * &lt;code&gt;close(0, TimeUnit.MILLISECONDS)&lt;/code&gt;. This is done since no further sending will happen while * blocking the I/O thread of the producer. * * @param timeout The maximum time to wait for producer to complete any pending requests. The value should be * non-negative. Specifying a timeout of zero means do not wait for pending send requests to complete. * @param timeUnit The time unit for the &lt;code&gt;timeout&lt;/code&gt; * @throws InterruptException If the thread is interrupted while blocked * @throws IllegalArgumentException If the &lt;code&gt;timeout&lt;/code&gt; is negative. */// 这种方法最多等待timeout的生产者完成所有未完成的请求的发送。// 如果生产者是无法完成所有请求超时到期之前，此方法将立即失败任何未发送和未确认的记录。// 如果从内调用Callback此方法不会阻止和将等效于close(0, TimeUnit.MILLISECONDS) 这样做是因为同时阻断生产者的I/O线程没有进一步的发送会发生 @Override public void close(long timeout, TimeUnit timeUnit) &#123; close(timeout, timeUnit, false); &#125; private void close(long timeout, TimeUnit timeUnit, boolean swallowException) &#123; if (timeout &lt; 0) throw new IllegalArgumentException(\"The timeout cannot be negative.\"); log.info(\"Closing the Kafka producer with timeoutMillis = &#123;&#125; ms.\", timeUnit.toMillis(timeout)); // this will keep track of the first encountered exception AtomicReference&lt;Throwable&gt; firstException = new AtomicReference&lt;&gt;(); boolean invokedFromCallback = Thread.currentThread() == this.ioThread; if (timeout &gt; 0) &#123; if (invokedFromCallback) &#123; log.warn(\"Overriding close timeout &#123;&#125; ms to 0 ms in order to prevent useless blocking due to self-join. \" + \"This means you have incorrectly invoked close with a non-zero timeout from the producer call-back.\", timeout); &#125; else &#123; // Try to close gracefully. if (this.sender != null) this.sender.initiateClose(); if (this.ioThread != null) &#123; try &#123; this.ioThread.join(timeUnit.toMillis(timeout)); &#125; catch (InterruptedException t) &#123; firstException.compareAndSet(null, t); log.error(\"Interrupted while joining ioThread\", t); &#125; &#125; &#125; &#125; if (this.sender != null &amp;&amp; this.ioThread != null &amp;&amp; this.ioThread.isAlive()) &#123; log.info(\"Proceeding to force close the producer since pending requests could not be completed \" + \"within timeout &#123;&#125; ms.\", timeout); this.sender.forceClose(); // Only join the sender thread when not calling from callback. // 仅当不从回调调用时才加入发送者线程。 if (!invokedFromCallback) &#123; try &#123; this.ioThread.join(); &#125; catch (InterruptedException e) &#123; firstException.compareAndSet(null, e); &#125; &#125; &#125; ClientUtils.closeQuietly(interceptors, \"producer interceptors\", firstException); ClientUtils.closeQuietly(metrics, \"producer metrics\", firstException); ClientUtils.closeQuietly(keySerializer, \"producer keySerializer\", firstException); ClientUtils.closeQuietly(valueSerializer, \"producer valueSerializer\", firstException); ClientUtils.closeQuietly(partitioner, \"producer partitioner\", firstException); AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId); log.debug(\"The Kafka producer has closed.\"); if (firstException.get() != null &amp;&amp; !swallowException) throw new KafkaException(\"Failed to close kafka producer\", firstException.get()); &#125; 如何使用kafka API 实现同步消息发送Code12345678同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需在调用Future对象的get方法即可。区别就在于在send方法处拿到返回值future然后调用future中的get方法调用此方法就会阻塞当前线程 一直等到结果返回java\\util\\concurrent\\Future.java java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.atguigu.kafka.producer;import org.apache.kafka.clients.producer.*;import java.util.Properties;import java.util.concurrent.Future;import java.util.concurrent.TimeUnit;public class MyCallBackProducer &#123; public static void main(String[] args) throws Exception &#123; //1. 创建配置对象 Properties props = new Properties(); //kafka集群的位置 props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,\"hadoop102:9092\"); //ack级别 props.put(ProducerConfig.ACKS_CONFIG,\"all\"); //重试次数 props.put(ProducerConfig.RETRIES_CONFIG,3); //批次大小 props.put(ProducerConfig.BATCH_SIZE_CONFIG,16384); //等待时间 props.put(ProducerConfig.LINGER_MS_CONFIG,1); //缓冲区大小 props.put(ProducerConfig.BUFFER_MEMORY_CONFIG,33554432); //k v 序列化器 props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\"); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\"); //2. 创建生产者对象 KafkaProducer&lt;String,String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); //3.生产数据 for (int i = 0; i &lt; 10000 ; i++) &#123; Future&lt;RecordMetadata&gt; future = producer.send(new ProducerRecord&lt;&gt;(\"second\", \"atguigu@@@@@\" + i), new Callback() &#123; /** * 回调方法: 当前的消息发送出去以后，会执行回调方法。 * @param metadata 当前消息的元数据信息。 * @param exception 当发送失败，会返回异常。 */ @Override public void onCompletion(RecordMetadata metadata, Exception exception) &#123; if (exception == null) &#123; //发送成功 System.out.println(metadata.topic() + \" -- \" + metadata.partition() + \" -- \" + metadata.offset()); &#125; &#125; &#125;); // 发送一个之后阻塞线程等待返回结果才继续发送下一个 // 阻塞等待 ， 同步发送 // 此时会发现结果严格按照发送的顺序 RecordMetadata recordMetadata = future.get(); &#125; //关闭 producer.close(); &#125;&#125; kafka的分区器怎么写 如何自定义分区器 继承Partitioner 重写三个方法configure() partition() close() 可以根据传进的key分区 也可根据value分区 在定义好自己的分区器之后 还要再配置中添加分区器的全类名 否则会走默认的分区器 java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051系统默认分区器public class DefaultPartitioner implements Partitioner &#123; private final ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = new ConcurrentHashMap&lt;&gt;(); public void configure(Map&lt;String, ?&gt; configs) &#123;&#125; /** * Compute the partition for the given record. * * @param topic The topic name * @param key The key to partition on (or null if no key) * @param keyBytes serialized key to partition on (or null if no key) * @param value The value to partition on or null * @param valueBytes serialized value to partition on or null * @param cluster The current cluster metadata */ public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; &#125; private int nextValue(String topic) &#123; AtomicInteger counter = topicCounterMap.get(topic); if (null == counter) &#123; counter = new AtomicInteger(ThreadLocalRandom.current().nextInt()); AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter); if (currentCounter != null) &#123; counter = currentCounter; &#125; &#125; return counter.getAndIncrement(); &#125; public void close() &#123;&#125;&#125; java1234567891011121314151617181920// 简单实现一个分区器public class MyPartitioner implements Partitioner &#123; public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; if (key == null) &#123; // key为空 到0号分区 return 0; &#125; else &#123; // key不为空 到1号分区 return 1; &#125; &#125; public void close() &#123; &#125; public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125;// 如果要使用自己定义的分区器 要在配置中指定分区器并传入分区器的全类名props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG,\"com.atguigu.kafka.partitioner.MyPartitioner\"); kafka的消费者需要注意的主要问题是什么Code12345Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。所以offset的维护是Consumer消费数据是必须考虑的问题。 如何使用kafka API 实现消息接收(消费者)准备知识需要用到的类： KafkaConsumer：需要创建一个消费者对象，用来消费数据 ConsumerConfig：获取所需的一系列配置参数 ConsuemrRecord：每条数据都要封装成一个ConsumerRecord对象 为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。 自动提交offset的相关参数： enable.auto.commit：是否开启自动提交offset功能 auto.commit.interval.ms：自动提交offset的时间间隔 几个比较重要的配置项 自动提交offset功能 自动提交时间间隔 消费者组 反序列化器(对应生产者端的序列化org\\apache\\kafka\\common\\serialization\\Deserializer.java) 自动提交offsetjava123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package fun.hoffee.kafka.consumer;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;/** * 消费者 */public class MyConsumer &#123; public static void main(String[] args) &#123; //1. 创建配置对象 Properties props = new Properties(); //指定kafka集群的位置 props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"hadoop102:9092\"); //开启自动提交offset //props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,true); //自动提交offset的间隔 props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 1); //指定消费者组 props.put(ConsumerConfig.GROUP_ID_CONFIG, \"atguigu\"); //指定kv的反序列化器 props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\"); //2. 创建消费者对象 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); //3. 订阅主题 consumer.subscribe(Arrays.asList(\"first\", \"second\", \"third\")); //4. 消费数据 while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record.topic() + \" -- \" + record.partition() + \" -- \" + record.offset() + \" -- \" + record.key() + \" -- \" + record.value()); &#125; &#125; &#125;&#125;// 此时创建的是新组 不能消费到之前的数据// 如果想要消费之前的数据 需要重置offset// 由auto.offset.rest参数(ConsumerConfig中的AUTO_OFFSET_RESET_CONFIG = \"auto.offset.reset\";)控制 默认值为latest// 可以配置为 earliest | latest | none---// 文档说明如下 :// What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted): // earliest: automatically reset the offset to the earliest offset // latest: automatically reset the offset to the latest offset // none: throw exception to the consumer if no previous offset is found for the consumer's group // anything else: throw exception to the consumer.// 当Kafka中没有初始偏移量或服务器上不再存在当前偏移量时（例如，因为该数据已被删除），该怎么办：// 最早：自动将偏移量重置为最早的偏移量 // 最新：自动将偏移量重置为最新偏移量 // 无：如果未找到消费者组的先前偏移量，则向消费者抛出异常 // 其他：向消费者抛出异常// 人话: 如果这个是一个新的组 或者是 这个组拿了一个kafka中不存在的偏移量去消费数据时候 kafka就会自动帮忙重置offset 如果配置过这个参数 就按这个参数配置的来 如果没有配置过 默认重置为latest 重置offset 具体说明见上一节代码末尾 java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.atguigu.kafka.consumer;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;/** * 消费者 */public class MyConsumer &#123; public static void main(String[] args) &#123; //1. 创建配置对象 Properties props = new Properties(); //指定kafka集群的位置 props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,\"hadoop102:9092\"); //开启自动提交offset //props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,true); //关闭自动提交offset props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,false); //自动提交offset的间隔 props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,1); //重置offset : earliest(最早) latest(最后) //满足两个条件: // 1. 当前的消费者组在kafka没有消费过所订阅的主题 // 2.当前消费者组使用的offset在kafka集群中已经被删除 props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,\"earliest\"); //指定消费者组 props.put(ConsumerConfig.GROUP_ID_CONFIG,\"atguigu111\"); //指定kv的反序列化器 props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringDeserializer\"); //2. 创建消费者对象 KafkaConsumer&lt;String,String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); //3. 订阅主题 consumer.subscribe(Arrays.asList(\"first\",\"second\",\"third\")); //4. 消费数据 while(true)&#123; // 此处是拉取数据方法 poll中传递的参数是超时时间 当主题中没有数据时候 等待超时时间之后再进行拉取数据 // 假如某一次没有消费到数据 会等待响应的时间之后再进行拉取 单位是ms ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record.topic() + \" -- \" + record.partition() + \" -- \" + record.offset() +\" -- \" + record.key() +\" -- \" + record.value()); &#125; &#125; &#125;&#125; 手动提交offset的两种方式Code123456789101112虽然自动提交offset十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。由于同步提交offset有失败重试机制，故更加可靠虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交offset的方式。 Code12345如果关闭了提交offset 在一直没有关闭consumer的情况下 consumer能正常消费数据 因为consumer从kafka中拿到offset后会一直将offset维护在内存中但是一旦关闭 因为没有向kafka提交过offset 则offset还是之前的那么这段时间生产的数据将被重复消费 java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.atguigu.kafka.consumer;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;/** * 消费者 */public class MyConsumer &#123; public static void main(String[] args) &#123; //1. 创建配置对象 Properties props = new Properties(); //指定kafka集群的位置 props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,\"hadoop102:9092\"); //开启自动提交offset //props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,true); //关闭自动提交offset props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,false); //自动提交offset的间隔 props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,1); //重置offset : earliest(最早) latest(最后) //满足两个条件: 1. 当前的消费者组在kafka没有消费过所订阅的主题 2.当前消费者组使用的offset在kafka集群中已经被删除 props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,\"earliest\"); //指定消费者组 props.put(ConsumerConfig.GROUP_ID_CONFIG,\"atguigu111\"); //指定kv的反序列化器 props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringDeserializer\"); //2. 创建消费者对象 KafkaConsumer&lt;String,String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); //3. 订阅主题 consumer.subscribe(Arrays.asList(\"first\",\"second\",\"third\")); //4. 消费数据 while(true)&#123; // 此处是拉取数据方法 poll中传递的参数是超时时间 当主题中没有数据时候 等待超时时间之后再进行拉取数据 // 假如某一次没有消费到数据 会等待响应的时间之后再进行拉取 单位是ms ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record.topic() + \" -- \" + record.partition() + \" -- \" + record.offset() +\" -- \" + record.key() +\" -- \" + record.value()); &#125; //手动提交offset //同步提交 代码会阻塞 直到提交offset成功 才开始消费下一条数据 consumer.commitSync(); //阻塞 //异步提交 会触发提交offset的操作 但是会继续消费数据 不管offset是否提交成功 //consumer.commitAsync(); &#125; &#125;&#125; kafka中重复消费数据和漏消费数据的情况Code12345无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费。 Code123456789这是offset的提交 和 消费数据 这两件事之间的先后顺序问题例1 : 消费者poll进100条数据 但是在消费到第60条时候宕机 但是offset已经提交 这时候 offset超前则后40条出现漏消费例2 :消费者poll进100条数据 但是offset在提交时候失败 但此时是先消费后提交offset的情况 这时候 offset滞后则这100条数据在下次启动时候会被重复消费 Code123456如何解决这个问题?将两件事情绑定在一起 如果失败则同时失败 如果成功则同时成功不允许出现一个失败一个成功的情况将两件事绑定为事务 kafka API 如何实现自定义存储offsetCode123456789Kafka 0.9版本之前，offset存储在zookeeper，0.9版本及之后，默认将offset存储在Kafka的一个内置的topic中。除此之外，Kafka还可以选择自定义存储offset。offset的维护是相当繁琐的，因为需要考虑到消费者的Rebalace。当有新的消费者加入消费者组、已有的消费者推出消费者组或者所订阅的主题的分区发生变化，就会触发到分区的重新分配，重新分配的过程叫做Rebalance。消费者发生Rebalance之后，每个消费者消费的分区就会发生变化。因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区最近提交的offset位置继续消费。要实现自定义存储offset，需要借助ConsumerRebalanceListener，以下为示例代码，其中提交和获取offset的方法，需要根据所选的offset存储系统自行实现。 java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package com.atguigu.kafka.consumer;import org.apache.kafka.clients.consumer.*;import org.apache.kafka.common.TopicPartition;import java.util.*;public class CustomConsumer &#123; private static Map&lt;TopicPartition, Long&gt; currentOffset = new HashMap&lt;&gt;();public static void main(String[] args) &#123; // 创建配置信息 Properties props = new Properties(); // Kafka集群 props.put(\"bootstrap.servers\", \"hadoop102:9092\"); // 消费者组，只要group.id相同，就属于同一个消费者组 props.put(\"group.id\", \"test\"); // 关闭自动提交offset props.put(\"enable.auto.commit\", \"false\"); // Key和Value的反序列化类 props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); // 创建一个消费者 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); // 消费者订阅主题 在订阅时候创建一个ConsumerRebalanceListener的对象实时监听 // 并重写两个方法onPartitionsRevoked 和 onPartitionsAssigned consumer.subscribe(Arrays.asList(\"first\"), new ConsumerRebalanceListener() &#123; //该方法会在Rebalance之前调用 @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; commitOffset(currentOffset); &#125; //该方法会在Rebalance之后调用 @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; currentOffset.clear(); for (TopicPartition partition : partitions) &#123; consumer.seek(partition, getOffset(partition)); //定位到最近提交的offset位置继续消费 &#125; &#125; &#125;); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); //消费者拉取数据 for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value()); currentOffset.put(new TopicPartition(record.topic(), record.partition()), record.offset()); &#125; commitOffset(currentOffset);//异步提交 &#125; &#125; //获取某分区的最新offset private static long getOffset(TopicPartition partition) &#123; return 0;// 这里是伪代码 需要根据具体存储的系统来实现 &#125; //提交该消费者所有分区的offset private static void commitOffset(Map&lt;TopicPartition, Long&gt; currentOffset) &#123; // 这里是伪代码 需要根据具体存储的系统来实现 &#125;&#125; kafka中的拦截器是如何实现的 原理是什么Code1234567891011121314Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括：（1）configure(configs)获取配置信息和初始化数据时调用。（2）onSend(ProducerRecord)：该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算。（3）onAcknowledgement(RecordMetadata, Exception)：该方法会在消息从RecordAccumulator成功发送到Kafka Broker之后，或者在发送过程中失败时调用。并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率。（4）close：关闭interceptor，主要用于执行一些资源清理工作如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。 请实现一个kafka的拦截器需求： 实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。 分析: 时间拦截器 java12345678910111213141516171819202122232425262728293031323334353637383940package fun.hoffee.kafka.interceptor;import org.apache.kafka.clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;import java.util.Map;/** * 在所有的消息内容前面加上时间戳 */public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; &#123; @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123; //获取当前消息的value String value = record.value(); value = System.currentTimeMillis() + \" -- \" + value; //构造一个producerRecord ProducerRecord&lt;String, String&gt; resultRecord = new ProducerRecord&lt;&gt;(record.topic(), record.partition(), record.key(), value); return resultRecord; &#125; @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123; &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; 计数拦截器 java123456789101112131415161718192021222324252627282930313233343536373839404142434445package fun.hoffee.kafka.interceptor;import org.apache.kafka.clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;import java.util.Map;/** * 统计发送成功或失败的消息个数 */public class CountInterceptor implements ProducerInterceptor&lt;String, String&gt; &#123; private Integer success = 0; private Integer fail = 0; @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123; // 相当于原路返回没有做处理 return record; &#125; @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123; if (exception == null) &#123; success++; &#125; else &#123; fail++; &#125; &#125; @Override public void close() &#123; // 整个拦截器走完之后 调用该方法 System.out.println(\"Success : \" + success); System.out.println(\"Fail :\" + fail); &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; 在生产者的配置文件中配置拦截器(可设置多个 设置为一个list) java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package fun.hoffee.kafka.interceptor;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.ArrayList;import java.util.List;import java.util.Properties;public class InterceptorProducer &#123; public static void main(String[] args) &#123; //1. 创建配置对象 Properties props = new Properties(); //指定kafka集群的位置，broker-list props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"hadoop102:9092\"); //指定ack的应答级别 0 1 -1(all) props.put(ProducerConfig.ACKS_CONFIG, \"all\"); //重试次数 props.put(ProducerConfig.RETRIES_CONFIG, 5); //批次大小 props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384); // 16kb //等待时间 props.put(ProducerConfig.LINGER_MS_CONFIG, 1); //RecordAccumulator缓冲区大小 props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432); // 32M //指定kv的序列化器 props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\"); //指定拦截器 // \"A list of classes to use as interceptors. Implementing the &lt;code&gt;ProducerInterceptor&lt;/code&gt; interface allows you to intercept (and possibly mutate) the records received by the producer before they are published to the Kafka cluster. By default, there are no interceptors.\"; List&lt;String&gt; interceptors = new ArrayList&lt;&gt;(); interceptors.add(\"com.atguigu.kafka.interceptor.TimeInterceptor\"); interceptors.add(\"com.atguigu.kafka.interceptor.CountInterceptor\"); props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); //2. 创建生产者对象 KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); //3. 生产数据 for (int i = 0; i &lt; 10; i++) &#123; producer.send(new ProducerRecord(\"second\", \"shangguigu==&gt;\" + i)); &#125; //4. 关闭 producer.close(); &#125;&#125; flume如何对接kafkaCode123使用kafkasink此时kafkasink相当于kafka的生产者 它可以根据消息的标记发送给kafka中不同的topic flume官网关于kafka sink的介绍如下 这是一个Flume Sink实现，可以将数据发布到 Kafka主题。目标之一是将Flume与Kafka集成在一起，以便基于拉式的处理系统可以处理来自各种Flume来源的数据。目前，该版本支持Kafka 0.9.x系列发行版。 此版本的Flume不再支持Kafka的旧版本（0.8.x）。 必需的属性以粗体标记。 Property Name Default Description type – Must be set to org.apache.flume.sink.kafka.KafkaSink kafka.bootstrap.servers – List of brokers Kafka-Sink will connect to, to get the list of topic partitions This can be a partial list of brokers, but we recommend at least two for HA. The format is comma separated list of hostname:port Kafka-Sink将连接到的代理列表，以获取主题分区列表。这可以是部分代理列表，但是对于HA，我们建议至少两个。格式是用逗号分隔的主机名：端口列表 kafka.topic default-flume-topic The topic in Kafka to which the messages will be published. If this parameter is configured, messages will be published to this topic. If the event header contains a “topic” field, the event will be published to that topic overriding the topic configured here. Kafka中将发布消息的主题。如果配置了此参数，则消息将发布到该主题。如果事件标题包含“主题”字段，则事件将发布到该主题，并覆盖此处配置的主题。 flumeBatchSize 100 How many messages to process in one batch. Larger batches improve throughput while adding latency. 一批中要处理多少条消息。较大的批次可提高吞吐量，同时增加延迟。 kafka.producer.acks 1 How many replicas must acknowledge a message before its considered successfully written. Accepted values are 0 (Never wait for acknowledgement), 1 (wait for leader only), -1 (wait for all replicas) Set this to -1 to avoid data loss in some cases of leader failure. 在成功考虑一条消息之前，有多少个副本必须确认一条消息。接受的值为0（永远不等待确认），1（仅等待领导者），-1（等待所有副本）将其设置为-1，以避免在某些领导者失败的情况下丢失数据。 useFlumeEventFormat false By default events are put as bytes onto the Kafka topic directly from the event body. Set to true to store events as the Flume Avro binary format. Used in conjunction with the same property on the KafkaSource or with the parseAsFlumeEvent property on the Kafka Channel this will preserve any Flume headers for the producing side. 默认情况下，事件直接从事件主体作为字节放入Kafka主题。设置为true可将事件存储为Flume Avro二进制格式。与KafkaSource上的相同属性或Kafka Channel上的parseAsFlumeEvent属性结合使用，将为生产方保留任何Flume标头。 defaultPartitionId – Specifies a Kafka partition ID (integer) for all events in this channel to be sent to, unless overriden by partitionIdHeader. By default, if this property is not set, events will be distributed by the Kafka Producer’s partitioner - including by key if specified (or by a partitioner specified by kafka.partitioner.class). partitionIdHeader – When set, the sink will take the value of the field named using the value of this property from the event header and send the message to the specified partition of the topic. If the value represents an invalid partition, an EventDeliveryException will be thrown. If the header value is present then this setting overrides defaultPartitionId. kafka.producer.security.protocol PLAINTEXT Set to SASL_PLAINTEXT, SASL_SSL or SSL if writing to Kafka using some level of security. See below for additional info on secure setup. more producer security props If using SASL_PLAINTEXT, SASL_SSL or SSL refer to Kafka security for additional properties that need to be set on producer. Other Kafka Producer Properties – These properties are used to configure the Kafka Producer. Any producer property supported by Kafka can be used. The only requirement is to prepend the property name with the prefix kafka.producer. For example: kafka.producer.linger.ms The Kafka sink also provides defaults for the key.serializer(org.apache.kafka.common.serialization.StringSerializer) and value.serializer(org.apache.kafka.common.serialization.ByteArraySerializer). Modification of these parameters is not recommended. An example configuration of a Kafka sink is given below. Properties starting with the prefix kafka.producer the Kafka producer. The properties that are passed when creating the Kafka producer are not limited to the properties given in this example. Also it is possible to include your custom properties here and access them inside the preprocessor through the Flume Context object passed in as a method argument. 示例配置如下: Code12345678a1.sinks.k1.channel &#x3D; c1a1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic &#x3D; mytopic &#x2F;&#x2F; 指定写入topica1.sinks.k1.kafka.bootstrap.servers &#x3D; localhost:9092 &#x2F;&#x2F; kafka位置a1.sinks.k1.kafka.flumeBatchSize &#x3D; 20a1.sinks.k1.kafka.producer.acks &#x3D; 1a1.sinks.k1.kafka.producer.linger.ms &#x3D; 1a1.sinks.ki.kafka.producer.compression.type &#x3D; snappy 实现flume中不同的event发往kafka中不同的topic 如何监控kafkakafka面试题总结1.Kafka中的ISR、OSR、AR又代表什么？ ISR：与leader保持同步的follower集合AR：分区的所有副本 2.Kafka中的HW、LEO等分别代表什么？ LEO：没个副本的最后条消息的offsetHW：一个分区中所有副本最小的offset 控制整个分区中哪些数据能够暴露给消费者 3.Kafka中是怎么体现消息顺序性的？ 每个分区内，每条消息都有一个offset，故只能保证分区内有序。 4.Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？ 拦截器 -&gt; 序列化器 -&gt; 分区器 5.Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？ 6.“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？ 正确 7.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？ offset+1 记录下次消费的数据的offset 8.有哪些情形会造成重复消费？ 9.有哪些情景会造成消息漏消费？ 先提交offset，后消费，有可能造成数据的重复 10.当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？ 1）会在zookeeper中的/brokers/topics节点下创建一个新的topic节点，如：/brokers/topics/first 2）触发Controller的监听程序 3）kafka Controller 负责topic的创建工作，并更新metadata cache 11.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？ 可以增加 bin/kafka-topics.sh –zookeeper localhost:2181/kafka –alter –topic topic-config –partitions 3 12.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？ 不可以减少，现有的分区数据难以处理。 13.Kafka有内部的topic吗？如果有是什么？有什么所用？ __consumer_offsets, 共有50个分区 保存消费者offset 14.Kafka分区分配的概念？ 一个topic多个分区，一个消费者组多个消费者，故需要将分区分配个消费者(roundrobin、range) 15.简述Kafka的日志目录结构？ 每个分区对应一个文件夹，文件夹的命名为topic-0，topic-1，内部为.log和.index文件 16.如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？ 先通过offset比对log文件的名字 确定好后 再找到对应的index文件中offset对应的消息索引位置 最后在log文件中找到相应的消息 17.聊一聊Kafka Controller的作用？ 负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。 18.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？ partition leader（ISR），由Controller负责 Controller（先到先得） 19.失效副本是指什么？有那些应对措施？ 不能及时与leader同步，暂时踢出ISR，等其追上leader之后再重新加入 20.Kafka的那些设计让它有如此高的性能？ 分区，顺序写磁盘，0-copy 其他kafka相关面试题搜集(一)1、请说明什么是Apache Kafka? Apache Kafka是由Apache开发的一种发布订阅消息系统，它是一个分布式的、分区的和可复制的提交日志服务。 2、说说Kafka的使用场景？ ①异步处理②应用解耦③流量削峰④日志处理⑤消息通讯等。 3、使用Kafka有什么优点和缺点？ 优点：①支持跨数据中心的消息复制；②单机吞吐量：十万级，最大的优点，就是吞吐量高;③topic数量都吞吐量的影响：topic从几十个到几百个的时候，吞吐量会大幅度下降。所以在同等机器下，kafka尽量保证topic数量不要过多。如果要支撑大规模topic，需要增加更多的机器资源;④时效性：ms级;⑤可用性：非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用;⑥消息可靠性：经过参数优化配置，消息可以做到0丢失;⑦功能支持：功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用。 缺点：①由于是批量发送，数据并非真正的实时； 仅支持统一分区内消息有序，无法实现全局消息有序；②有可能消息重复消费；③依赖zookeeper进行元数据管理，等等。 4、为什么说Kafka性能很好，体现在哪里？ ①顺序读写②零拷贝③分区④批量发送⑤数据压缩 5、请说明什么是传统的消息传递方法? 传统的消息传递方法包括两种：排队：在队列中，一组用户可以从服务器中读取消息，每条消息都发送给其中一个人。发布-订阅：在这个模型中，消息被广播给所有的用户。 6、请说明Kafka相对传统技术有什么优势? ①快速:单一的Kafka代理可以处理成千上万的客户端，每秒处理数兆字节的读写操作。②可伸缩:在一组机器上对数据进行分区③和简化，以支持更大的数据④持久:消息是持久性的，并在集群中进⑤行复制，以防止数据丢失。⑥设计:它提供了容错保证和持久性 7、解释Kafka的Zookeeper是什么?我们可以在没有Zookeeper的情况下使用Kafka吗? Zookeeper是一个开放源码的、高性能的协调服务，它用于Kafka的分布式应用。不，不可能越过Zookeeper，直接联系Kafka broker。一旦Zookeeper停止工作，它就不能服务客户端请求。Zookeeper主要用于在集群中不同节点之间进行通信在Kafka中，它被用于提交偏移量，因此如果节点在任何情况下都失败了，它都可以从之前提交的偏移量中获取除此之外，它还执行其他活动，如: leader检测、分布式同步、配置管理、识别新节点何时离开或连接、集群、节点实时状态等等。 8、解释Kafka的用户如何消费信息? 在Kafka中传递消息是通过使用sendfile API完成的。它支持将字节从套接口转移到磁盘，通过内核空间保存副本，并在内核用户之间调用内核。 9、解释如何提高远程用户的吞吐量? 如果用户位于与broker不同的数据中心，则可能需要调优套接口缓冲区大小，以对长网络延迟进行摊销。 10、解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息? 在数据中，为了精确地获得Kafka的消息，你必须遵循两件事: 在数据消耗期间避免重复，在数据生产过程中避免重复。 这里有两种方法，可以在数据生成时准确地获得一个语义: 每个分区使用一个单独的写入器，每当你发现一个网络错误，检查该分区中的最后一条消息，以查看您的最后一次写入是否成功 在消息中包含一个主键(UUID或其他)，并在用户中进行反复制 11、解释如何减少ISR中的扰动?broker什么时候离开ISR? ISR是一组与leaders完全同步的消息副本，也就是说ISR中包含了所有提交的消息。ISR应该总是包含所有的副本，直到出现真正的故障。如果一个副本从leader中脱离出来，将会从ISR中删除。 12、Kafka为什么需要复制? Kafka的信息复制确保了任何已发布的消息不会丢失，并且可以在机器错误、程序错误或更常见些的软件升级中使用。 13、如果副本在ISR中停留了很长时间表明什么? 如果一个副本在ISR中保留了很长一段时间，那么它就表明，跟踪器无法像在leader收集数据那样快速地获取数据。 14、请说明如果首选的副本不在ISR中会发生什么? 如果首选的副本不在ISR中，控制器将无法将leadership转移到首选的副本。 15、有可能在生产后发生消息偏移吗? 在大多数队列系统中，作为生产者的类无法做到这一点，它的作用是触发并忘记消息。broker将完成剩下的工作，比如使用id进行适当的元数据处理、偏移量等。 作为消息的用户，你可以从Kafka broker中获得补偿。如果你注视SimpleConsumer类，你会注意到它会获取包括偏移量作为列表的MultiFetchResponse对象。此外，当你对Kafka消息进行迭代时，你会拥有包括偏移量和消息发送的MessageAndOffset对象。 16、Kafka的设计时什么样的呢？ Kafka将消息以topic为单位进行归纳 将向Kafka topic发布消息的程序成为producers. 将订阅了topics并消费消息的程序成为consumer. Kafka以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个broker. producers通过网络将消息发送到Kafka集群，集群向消费者提供消息 17、数据传输的事务定义有哪三种？ （1）最多一次:消息不会被重复发送，最多被传输一次，但也有可能一次不传输（2）最少一次: 消息不会被漏发送，最少被传输一次，但也有可能被重复传输.（3）精确的一次（Exactly once）: 不会漏传输也不会重复传输,每个消息都传输被一次而且仅仅被传输一次，这是大家所期望的 18、Kafka判断一个节点是否还活着有那两个条件？ （1）节点必须可以维护和ZooKeeper的连接，Zookeeper通过心跳机制检查每个节点的连接（2）如果节点是个follower,他必须能及时的同步leader的写操作，延时不能太久 19、producer是否直接将数据发送到broker的leader(主节点)？ producer直接将数据发送到broker的leader(主节点)，不需要在多个节点进行分发，为了帮助producer做到这点，所有的Kafka节点都可以及时的告知:哪些节点是活动的，目标topic目标分区的leader在哪。这样producer就可以直接将消息发送到目的地了。 20、Kafa consumer是否可以消费指定分区消息？ Kafa consumer消费消息时，向broker发出”fetch”请求去消费特定分区的消息，consumer指定消息在日志中的偏移量（offset），就可以消费从这个位置开始的消息，customer拥有了offset的控制权，可以向后回滚去重新消费之前的消息，这是很有意义的 21、Kafka消息是采用Pull模式，还是Push模式？ Kafka最初考虑的问题是，customer应该从brokes拉取消息还是brokers将消息推送到consumer，也就是pull还push。在这方面，Kafka遵循了一种大部分消息系统共同的传统的设计：producer将消息推送到broker，consumer从broker拉取消息一些消息系统比如Scribe和Apache Flume采用了push模式，将消息推送到下游的consumer。这样做有好处也有坏处：由broker决定消息推送的速率，对于不同消费速率的consumer就不太好处理了。消息系统都致力于让consumer以最大的速率最快速的消费消息，但不幸的是，push模式下，当broker推送的速率远大于consumer消费的速率时，consumer恐怕就要崩溃了。最终Kafka还是选取了传统的pull模式 Pull模式的另外一个好处是consumer可以自主决定是否批量的从broker拉取数据。Push模式必须在不知道下游consumer消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。如果为了避免consumer崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。Pull模式下，consumer就可以根据自己的消费能力去决定这些策略 Pull有个缺点是，如果broker没有可供消费的消息，将导致consumer不断在循环中轮询，直到新消息到t达。为了避免这点，Kafka有个参数可以让consumer阻塞知道新消息到达(当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发 22、Kafka存储在硬盘上的消息格式是什么？ 消息由一个固定长度的头部和可变长度的字节数组组成。头部包含了一个版本号和CRC32校验码。消息长度: 4 bytes (value: 1+4+n)版本号: 1 byteCRC校验码: 4 bytes具体的消息: n bytes 23、Kafka高效文件存储设计特点： (1).Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。(2).通过索引信息可以快速定位message和确定response的最大大小。(3).通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。(4).通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 24、Kafka 与传统消息系统之间有三个关键区别 (1).Kafka 持久化日志，这些日志可以被重复读取和无限期保留(2).Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据提升容错能力和高可用性(3).Kafka 支持实时的流式处理 25、Kafka创建Topic时如何将分区放置到不同的Broker中 副本因子不能大于 Broker 的个数；第一个分区（编号为0）的第一个副本放置位置是随机从 brokerList 选择的；其他分区的第一个副本放置位置相对于第0个分区依次往后移。也就是如果我们有5个 Broker，5个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个 Broker 上，依次类推；剩余的副本相对于第一个副本放置位置其实是由 nextReplicaShift 决定的，而这个数也是随机产生的 26、Kafka新建的分区会在哪个目录下创建 在启动 Kafka 集群之前，我们需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录，这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘上用于提高读写性能。 当然我们也可以配置 log.dir 参数，含义一样。只需要设置其中一个即可。 如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个目录下创建文件夹用于存放数据。 但是如果 log.dirs 参数配置了多个目录，那么 Kafka 会在哪个文件夹中创建分区目录呢？答案是：Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic名+分区ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录！也就是说，如果你给 log.dirs 参数新增了一个新的磁盘，新的分区目录肯定是先在这个新的磁盘上创建直到这个新的磁盘目录拥有的分区目录不是最少为止。 27、partition的数据如何保存到硬盘 topic中的多个partition以文件夹的形式保存到broker，每个分区序号从0递增， 且消息有序 Partition文件下有多个segment（xxx.index，xxx.log） segment 文件里的 大小和配置文件大小一致可以根据要求修改 默认为1g 如果大小大于1g时，会滚动一个新的segment并且以上一个segment最后一条消息的偏移量命名 28、kafka的ack机制 request.required.acks有三个值 0 1 -10:生产者不会等待broker的ack，这个延迟最低但是存储的保证最弱当server挂掉的时候就会丢数据1：服务端会等待ack值 leader副本确认接收到消息后发送ack但是如果leader挂掉后他不确保是否复制完成新leader也会导致数据丢失-1：同样在1的基础上 服务端会等所有的follower的副本受到数据后才会受到leader发出的ack，这样数据不会丢失 29、Kafka的消费者如何消费数据 消费者每次消费数据的时候，消费者都会记录消费的物理偏移量（offset）的位置 等到下次消费时，他会接着上次位置继续消费。同时也可以按照指定的offset进行重新消费。 30、消费者负载均衡策略 结合consumer的加入和退出进行再平衡策略。 31、kafka消息数据是否有序？ 消费者组里某具体分区是有序的，所以要保证有序只能建一个分区，但是实际这样会存在性能问题，具体业务具体分析后确认。 32、kafaka生产数据时数据的分组策略,生产者决定数据产生到集群的哪个partition中 每一条消息都是以（key，value）格式 Key是由生产者发送数据传入 所以生产者（key）决定了数据产生到集群的哪个partition 33、kafka consumer 什么情况会触发再平衡reblance? ①一旦消费者加入或退出消费组，导致消费组成员列表发生变化，消费组中的所有消费者都要执行再平衡。②订阅主题分区发生变化，所有消费者也都要再平衡。 34、描述下kafka consumer 再平衡步骤? ①关闭数据拉取线程，清空队列和消息流，提交偏移量；②释放分区所有权，删除zk中分区和消费者的所有者关系；③将所有分区重新分配给每个消费者，每个消费者都会分到不同分区；④将分区对应的消费者所有关系写入ZK，记录分区的所有权信息；⑤重启消费者拉取线程管理器，管理每个分区的拉取线程。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"kafka","slug":"大数据/kafka","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"kafka","slug":"kafka","permalink":"https://masteryang4.github.io/tags/kafka/"}]},{"title":"spark系列之spark-sql","slug":"spark系列之spark-sql","date":"2020-06-18T04:25:47.000Z","updated":"2020-06-18T05:11:28.375Z","comments":true,"path":"2020/06/18/spark系列之spark-sql/","link":"","permalink":"https://masteryang4.github.io/2020/06/18/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark-sql/","excerpt":"","text":"SparkSQL概述简介Spark SQL是Spark用于结构化数据(structured data)处理的Spark模块 Hive与SparkSQL其中SparkSQL作为Spark生态的一员继续发展，而不再受限于Hive，只是兼容Hive； 而Hive on Spark是一个Hive的发展计划，该计划将Spark作为Hive的底层引擎之一，也就是说，Hive将不再受限于一个引擎，可以采用Map-Reduce、Tez、Spark等引擎。 Spark SQL为了简化RDD的开发，提高开发效率，提供了2个编程抽象，类似Spark Core中的RDD DataFrame DataSet DataFrame简介在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。 同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从 API 易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API 要更加友好，门槛更低。 上图直观地体现了DataFrame和RDD的区别。 左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。 DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待 DataFrame也是懒执行的，但性能上比RDD要高，主要原因：优化的执行计划，即查询计划通过Spark catalyst optimiser进行优化 DataSet简介DataSet是分布式数据集合。DataSet是Spark 1.6中添加的一个新抽象，是DataFrame的一个扩展。它提供了RDD的优势（强类型，使用强大的lambda函数的能力）以及Spark SQL优化执行引擎的优点。DataSet也可以使用功能性的转换（操作map，flatMap，filter等等）。 DataSet是DataFrame API的一个扩展，是SparkSQL最新的数据抽象 用户友好的API风格，既具有类型安全检查也具有DataFrame的查询优化特性； 用样例类来对DataSet中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称； DataSet是强类型的。比如可以有DataSet[Car]，DataSet[Person]。 DataFrame是DataSet的特列，DataFrame=DataSet[Row] ，所以可以通过as方法将DataFrame转换为DataSet。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息都用Row来表示。获取数据时需要指定顺序 SparkSQL核心编程Spark Core中，如果想要执行应用程序，需要首先构建上下文环境对象SparkContext，Spark SQL其实可以理解为对Spark Core的一种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。 SparkSession是Spark最新的SQL查询起始点，SparkSession内部封装了SparkContext，所以计算实际上是由sparkContext完成的。 DataFrameSpark SQL的DataFrame API 允许我们使用 DataFrame 而不用必须去注册临时表或者生成 SQL 表达式。DataFrame API 既有 transformation操作也有action操作。 创建df在Spark SQL中SparkSession是创建DataFrame和执行SQL的入口，创建DataFrame有三种方式：通过Spark的数据源进行创建；从一个存在的RDD进行转换；还可以从Hive Table进行查询返回。 1、 从Spark数据源进行创建 读取json文件创建DataFrame scala12scala&gt; val df = spark.read.json(\"data/user.json\")df: org.apache.spark.sql.DataFrame = [age: bigint， username: string] 2、从RDD进行转换 3、从Hive Table进行查询返回 SQL语法SQL语法风格是指我们查询数据的时候使用SQL语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助 scala12345678910111213scala&gt; val df = spark.read.json(\"data/user.json\")df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]scala&gt; df.createOrReplaceTempView(\"people\")scala&gt; val sqlDF = spark.sql(\"SELECT * FROM people\")sqlDF: org.apache.spark.sql.DataFrame = [age: bigint， name: string]scala&gt; sqlDF.show+---+--------+|age|username|+---+--------+| 20|zhangsan|| 30| lisi|| 40| wangwu|+---+--------+ 注意：普通临时表是Session范围内的，如果想应用范围内有效，可以使用全局临时表。 使用全局临时表时需要全路径访问，如：global_temp.people scala123456789101112131415161718scala&gt; df.createGlobalTempView(\"people\")scala&gt; spark.sql(\"SELECT * FROM global_temp.people\").show()+---+--------+|age|username|+---+--------+| 20|zhangsan|| 30| lisi|| 40| wangwu|+---+--------+scala&gt; spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()+---+--------+|age|username|+---+--------+| 20|zhangsan|| 30| lisi|| 40| wangwu|+---+--------+ DSL语法DataFrame提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了 scala1234567891011121314151617181920212223242526272829303132333435363738394041424344scala&gt; val df = spark.read.json(\"data/user.json\")df: org.apache.spark.sql.DataFrame = [age: bigint， name: string]scala&gt; df.printSchemaroot |-- age: Long (nullable = true) |-- username: string (nullable = true)scala&gt; df.select(\"username\").show()+--------+|username|+--------+|zhangsan|| lisi|| wangwu|+--------+scala&gt; df.select($\"username\",$\"age\" + 1).showscala&gt; df.select('username, 'age + 1).show()scala&gt; df.select('username, 'age + 1 as \"newage\").show()+--------+---------+|username|(age + 1)|+--------+---------+|zhangsan| 21|| lisi| 31|| wangwu| 41|+--------+---------+scala&gt; df.filter($\"age\"&gt;30).show+---+---------+|age| username|+---+---------+| 40| wangwu|+---+---------+scala&gt; df.groupBy(\"age\").count.show+---+-----+|age|count|+---+-----+| 20| 1|| 30| 1|| 40| 1|+---+-----+ RDD转换为DataFrame在IDEA中开发程序时，如果需要RDD与DF或者DS之间互相操作，那么需要引入 import spark.implicits._ 这里的spark不是Scala中的包名，而是创建的sparkSession对象的变量名称，所以必须先创建SparkSession对象再导入。这里的spark对象不能使用var声明，因为Scala只支持val修饰的对象的引入。 scala12345678910scala&gt; val idRDD = sc.textFile(\"data/id.txt\")scala&gt; idRDD.toDF(\"id\").show+---+| id|+---+| 1|| 2|| 3|| 4|+---+ 实际开发中，一般通过样例类将RDD转换为DataFrame scala123456789scala&gt; case class User(name:String, age:Int)defined class Userscala&gt; sc.makeRDD(List((\"zhangsan\",30), (\"lisi\",40))).map(t=&gt;User(t._1, t._2)).toDF.show+--------+---+| name|age|+--------+---+|zhangsan| 30|| lisi| 40|+--------+---+ DataFrame转换为RDDDataFrame其实就是对RDD的封装，所以可以直接获取内部的RDD scala12345678scala&gt; val df = sc.makeRDD(List((\"zhangsan\",30), (\"lisi\",40))).map(t=&gt;User(t._1, t._2)).toDFdf: org.apache.spark.sql.DataFrame = [name: string, age: int]scala&gt; val rdd = df.rddrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[46] at rdd at &lt;console&gt;:25scala&gt; val array = rdd.collectarray: Array[org.apache.spark.sql.Row] = Array([zhangsan,30], [lisi,40]) 注意：此时得到的RDD存储类型为Row scala123456scala&gt; array(0)res28: org.apache.spark.sql.Row = [zhangsan,30]scala&gt; array(0)(0)res29: Any = zhangsanscala&gt; array(0).getAs[String](\"name\")res30: String = zhangsan DataSetDataSet是具有强类型的数据集合，需要提供对应的类型信息。 创建DataSet1、使用样例类序列创建DataSet scala12345678910111213scala&gt; case class Person(name: String, age: Long)defined class Personscala&gt; val caseClassDS = Seq(Person(\"zhangsan\",2)).toDS()caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: Long]scala&gt; caseClassDS.show+---------+---+| name|age|+---------+---+| zhangsan| 2|+---------+---+ 2、使用基本类型的序列创建DataSet scala12345678910111213scala&gt; val ds = Seq(1,2,3,4,5).toDSds: org.apache.spark.sql.Dataset[Int] = [value: int]scala&gt; ds.show+-----+|value|+-----+| 1|| 2|| 3|| 4|| 5|+-----+ 注意：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet RDD转换为DataSetSparkSQL能够自动将包含有case类的RDD转换成DataSet，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seq或者Array等复杂的结构。 scala12345scala&gt; case class User(name:String, age:Int)defined class Userscala&gt; sc.makeRDD(List((\"zhangsan\",30), (\"lisi\",49))).map(t=&gt;User(t._1, t._2)).toDSres11: org.apache.spark.sql.Dataset[User] = [name: string, age: int] DataSet转换为RDDDataSet其实也是对RDD的封装，所以可以直接获取内部的RDD scala1234567891011scala&gt; case class User(name:String, age:Int)defined class Userscala&gt; sc.makeRDD(List((\"zhangsan\",30), (\"lisi\",49))).map(t=&gt;User(t._1, t._2)).toDSres11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]scala&gt; val rdd = res11.rddrdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[51] at rdd at &lt;console&gt;:25scala&gt; rdd.collectres12: Array[User] = Array(User(zhangsan,30), User(lisi,49)) DataFrame和DataSet转换DataFrame其实是DataSet的特例，所以它们之间是可以互相转换的。 DataFrame转换为DataSet scala12345678scala&gt; case class User(name:String, age:Int)defined class Userscala&gt; val df = sc.makeRDD(List((\"zhangsan\",30), (\"lisi\",49))).toDF(\"name\",\"age\")df: org.apache.spark.sql.DataFrame = [name: string, age: int]scala&gt; val ds = df.as[User]ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int] DataSet转换为DataFrame scala12345scala&gt; val ds = df.as[User]ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]scala&gt; val df = ds.toDFdf: org.apache.spark.sql.DataFrame = [name: string, age: int] RDD、DataFrame、DataSet三者关系在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看： Spark1.0 =&gt; RDD Spark1.3 =&gt; DataFrame Spark1.6 =&gt; Dataset 如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的Spark版本中，DataSet有可能会逐步取代RDD和DataFrame成为唯一的API接口。 三者共性Code1234567RDD、DataFrame、DataSet全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利;三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算;三者有许多共同的函数，如filter，排序等;在对DataFrame和Dataset进行操作许多操作都需要这个包:import spark.implicits._（在创建好SparkSession对象后尽量直接导入）三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出三者都有partition的概念DataFrame和DataSet均可使用模式匹配获取各个字段的值和类型 三者区别Code12345678910111)RDD RDD一般和spark mlib同时使用 RDD不支持sparksql操作2)DataFrame 与RDD和Dataset不同，DataFrame每一行的类型固定为Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值 DataFrame与DataSet一般不与 spark mlib 同时使用 DataFrame与DataSet均支持 SparkSQL 的操作，比如select，groupby之类，还能注册临时表&#x2F;视窗，进行 sql 语句操作 DataFrame与DataSet支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)3)DataSet Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。 DataFrame其实就是DataSet的一个特例 type DataFrame &#x3D; Dataset[Row] DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息 三者的互相转换 IDEA开发SparkSQLxml12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt;&lt;/dependency&gt; scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455object SparkSQL01_Demo &#123; def main(args: Array[String]): Unit = &#123; //创建上下文环境配置对象 val conf: SparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"SparkSQL01_Demo\") //创建SparkSession对象 val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate() //RDD=&gt;DataFrame=&gt;DataSet转换需要引入隐式转换规则，否则无法转换 //spark不是包名，是上下文环境对象名 import spark.implicits._ //读取json文件 创建DataFrame &#123;\"username\": \"lisi\",\"age\": 18&#125; val df: DataFrame = spark.read.json(\"D:\\\\dev\\\\workspace\\\\spark-bak\\\\spark-bak-00\\\\input\\\\test.json\") //df.show() //SQL风格语法 df.createOrReplaceTempView(\"user\") //spark.sql(\"select avg(age) from user\").show //DSL风格语法 //df.select(\"username\",\"age\").show() //*****RDD=&gt;DataFrame=&gt;DataSet***** //RDD val rdd1: RDD[(Int, String, Int)] = spark.sparkContext.makeRDD(List((1,\"qiaofeng\",30),(2,\"xuzhu\",28),(3,\"duanyu\",20))) //DataFrame val df1: DataFrame = rdd1.toDF(\"id\",\"name\",\"age\") //df1.show() //DateSet val ds1: Dataset[User] = df1.as[User] //ds1.show() //*****DataSet=&gt;DataFrame=&gt;RDD***** //DataFrame val df2: DataFrame = ds1.toDF() //RDD 返回的RDD类型为Row，里面提供的getXXX方法可以获取字段值，类似jdbc处理结果集，但是索引从0开始 val rdd2: RDD[Row] = df2.rdd //rdd2.foreach(a=&gt;println(a.getString(1))) //*****RDD=&gt;DataSet***** rdd1.map&#123; case (id,name,age)=&gt;User(id,name,age) &#125;.toDS() //*****DataSet=&gt;=&gt;RDD***** ds1.rdd //释放资源 spark.stop() &#125;&#125;case class User(id:Int,name:String,age:Int) 用户自定义函数用户可以通过spark.udf功能添加自定义函数，实现自定义功能。 UDF创建DataFrame scala12scala&gt; val df = spark.read.json(\"data/user.json\")df: org.apache.spark.sql.DataFrame = [age: bigint， username: string] 注册UDF scala12scala&gt; spark.udf.register(\"addName\",(x:String)=&gt; \"Name:\"+x)res9: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType))) 创建临时表 scala1scala&gt; df.createOrReplaceTempView(\"people\") 应用UDF scala1scala&gt; spark.sql(\"Select addName(name),age from people\").show() UDAF强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数。 需求：实现求平均工资 1、RDD实现 scala1234567891011121314val conf: SparkConf = new SparkConf().setAppName(\"app\").setMaster(\"local[*]\")val sc: SparkContext = new SparkContext(conf)val res: (Int, Int) = sc.makeRDD(List((\"zhangsan\", 20), (\"lisi\", 30), (\"wangw\", 40))).map &#123; case (name, age) =&gt; &#123; (age, 1) &#125;&#125;.reduce &#123; (t1, t2) =&gt; &#123; (t1._1 + t2._1, t1._2 + t2._2) &#125;&#125;println(res._1/res._2)// 关闭连接sc.stop() 2、累加器实现 scala12345678910111213141516171819202122232425262728293031323334353637class MyAC extends AccumulatorV2[Int,Int]&#123; var sum:Int = 0 var count:Int = 0 override def isZero: Boolean = &#123; return sum ==0 &amp;&amp; count == 0 &#125; override def copy(): AccumulatorV2[Int, Int] = &#123; val newMyAc = new MyAC newMyAc.sum = this.sum newMyAc.count = this.count newMyAc &#125; override def reset(): Unit = &#123; sum =0 count = 0 &#125; override def add(v: Int): Unit = &#123; sum += v count += 1 &#125; override def merge(other: AccumulatorV2[Int, Int]): Unit = &#123; other match &#123; case o:MyAC=&gt;&#123; sum += o.sum count += o.count &#125; case _=&gt; &#125; &#125; override def value: Int = sum/count&#125; 3、实现方式 - UDAF - 弱类型 scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/*定义类继承UserDefinedAggregateFunction，并重写其中方法*/class MyAveragUDAF extends UserDefinedAggregateFunction &#123; // 聚合函数输入参数的数据类型 def inputSchema: StructType = StructType(Array(StructField(\"age\",IntegerType))) // 聚合函数缓冲区中值的数据类型(age,count) def bufferSchema: StructType = &#123; StructType(Array(StructField(\"sum\",LongType),StructField(\"count\",LongType))) &#125; // 函数返回值的数据类型 def dataType: DataType = DoubleType // 稳定性：对于相同的输入是否一直返回相同的输出。 def deterministic: Boolean = true // 函数缓冲区初始化 def initialize(buffer: MutableAggregationBuffer): Unit = &#123; // 存年龄的总和 buffer(0) = 0L // 存年龄的个数 buffer(1) = 0L &#125; // 更新缓冲区中的数据 def update(buffer: MutableAggregationBuffer,input: Row): Unit = &#123; if (!input.isNullAt(0)) &#123; buffer(0) = buffer.getLong(0) + input.getInt(0) buffer(1) = buffer.getLong(1) + 1 &#125; &#125; // 合并缓冲区 def merge(buffer1: MutableAggregationBuffer,buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0) buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1) &#125; // 计算最终结果 def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)&#125;。。。//创建聚合函数var myAverage = new MyAveragUDAF//在spark中注册聚合函数spark.udf.register(\"avgAge\",myAverage)spark.sql(\"select avgAge(age) from user\").show() 4、实现方式 - UDAF - 强类型 scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//输入数据类型case class User01(username:String,age:Long)//缓存类型case class AgeBuffer(var sum:Long,var count:Long)/** * 定义类继承org.apache.spark.sql.expressions.Aggregator * 重写类中的方法 */class MyAveragUDAF1 extends Aggregator[User01,AgeBuffer,Double]&#123; override def zero: AgeBuffer = &#123; AgeBuffer(0L,0L) &#125; override def reduce(b: AgeBuffer, a: User01): AgeBuffer = &#123; b.sum = b.sum + a.age b.count = b.count + 1 b &#125; override def merge(b1: AgeBuffer, b2: AgeBuffer): AgeBuffer = &#123; b1.sum = b1.sum + b2.sum b1.count = b1.count + b2.count b1 &#125; override def finish(buff: AgeBuffer): Double = &#123; buff.sum.toDouble/buff.count &#125; //DataSet默认额编解码器，用于序列化，固定写法 //自定义类型就是produce 自带类型根据类型选择 override def bufferEncoder: Encoder[AgeBuffer] = &#123; Encoders.product &#125; override def outputEncoder: Encoder[Double] = &#123; Encoders.scalaDouble &#125;&#125;。。。//封装为DataSetval ds: Dataset[User01] = df.as[User01]//创建聚合函数var myAgeUdaf1 = new MyAveragUDAF1//将聚合函数转换为查询的列val col: TypedColumn[User01, Double] = myAgeUdaf1.toColumn//查询ds.select(col).show() 数据的加载和保存通用的加载和保存方式SparkSQL提供了通用的保存数据和数据加载的方式。这里的通用指的是使用相同的API，根据不同的参数读取和保存不同格式的数据，SparkSQL默认读取和保存的文件格式为parquet 1) 加载数据 spark.read.load是加载数据的通用方法 scala1scala&gt; spark.read.format(\"…\")[.option(\"…\")].load(\"…\") 1)保存数据 df.write.save 是保存数据的通用方法 scala1scala&gt;df.write.format(\"…\")[.option(\"…\")].save(\"…\") ParquetSpark SQL的默认数据源为Parquet格式。 Parquet是一种能够有效存储嵌套数据的列式存储格式。 数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作，不需要使用format。修改配置项spark.sql.sources.default，可修改默认数据源格式。 加载数据 scala123scala&gt; val df = spark.read.load(\"/opt/module/spark-local/examples/src/main/resources/users.parquet\")scala&gt; df.show 保存数据 scala123scala&gt; var df = spark.read.json(\"/opt/module/data/input/people.json\")//保存为parquet格式scala&gt; df.write.mode(\"append\").save(\"/opt/module/data/output\") JSON/CSV/MySQLHiveApache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL编译时可以包含 Hive 支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的一点是，如果要在 Spark SQL 中包含Hive 的库，并不需要事先安装 Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。 若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好 Hive，Spark SQL 也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。 spark-shell默认是Hive支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）。 1、内嵌的Hive 如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可. Hive 的元数据存储在 derby 中, 仓库地址:$SPARK_HOME/spark-warehouse scala1234567891011121314151617scala&gt; spark.sql(\"show tables\").show。。。+--------+---------+-----------+|database|tableName|isTemporary|+--------+---------+-----------++--------+---------+-----------+scala&gt; spark.sql(\"create table aa(id int)\")。。。scala&gt; spark.sql(\"show tables\").show+--------+---------+-----------+|database|tableName|isTemporary|+--------+---------+-----------+| default| aa| false|+--------+---------+-----------+ 向表加载本地数据 scala12345678910111213scala&gt; spark.sql(\"load data local inpath 'input/ids.txt' into table aa\")。。。scala&gt; spark.sql(\"select * from aa\").show+---+| id|+---+| 1|| 2|| 3|| 4|+---+ 在实际使用中, 几乎没有任何人会使用内置的 Hive 2、外部的Hive 如果想连接外部已经部署好的Hive，需要通过以下几个步骤： Spark要接管Hive需要把hive-site.xml拷贝到conf/目录下 把Mysql的驱动copy到jars/目录下 如果访问不到hdfs，则需要把core-site.xml和hdfs-site.xml拷贝到conf/目录下 scala123456789101112scala&gt; spark.sql(\"show tables\").show20/04/25 22:05:14 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException+--------+--------------------+-----------+|database| tableName|isTemporary|+--------+--------------------+-----------+| default| emp| false|| default|hive_hbase_emp_table| false|| default| relevance_hbase_emp| false|| default| staff_hive| false|| default| ttt| false|| default| user_visit_action| false|+--------+--------------------+-----------+ 3、运行 Spark SQL CLI Spark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。在Spark目录下执行如下命令启动Spark SQL CLI，直接执行SQL语句，类似一Hive窗口 shell1bin/spark-sql 4、代码操作Hive xml1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt;&lt;/dependency&gt; 将hive-site.xml文件拷贝到项目的resources目录中，代码实现 scala1234567//创建SparkSessionval spark: SparkSession = SparkSession .builder() .enableHiveSupport() .master(\"local[*]\") .appName(\"sql\") .getOrCreate() 注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址: config(“spark.sql.warehouse.dir”, “hdfs://linux1:9000/user/hive/warehouse”)","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"},{"name":"spark-sql","slug":"spark-sql","permalink":"https://masteryang4.github.io/tags/spark-sql/"}]},{"title":"spark系列之spark基础","slug":"spark系列之spark基础","date":"2020-06-17T15:44:55.000Z","updated":"2020-06-17T17:07:52.754Z","comments":true,"path":"2020/06/17/spark系列之spark基础/","link":"","permalink":"https://masteryang4.github.io/2020/06/17/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark%E5%9F%BA%E7%A1%80/","excerpt":"","text":"概述 Spark Core Spark Core中提供了Spark最基础与最核心的功能，Spark其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib都是在Spark Core的基础上进行扩展的 Spark SQL Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言（HQL）来查询数据。 Spark Streaming Spark Streaming是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。 Spark MLlib MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。 Spark GraphX GraphX是Spark面向图计算提供的框架与算法库。 入门xml1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;!-- 该插件用于将Scala代码编译成class文件 --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;!-- 声明绑定到maven的compile阶段 --&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; WordCount scala1234567891011121314151617181920212223242526// 创建Spark运行配置对象val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"WordCount\")// 创建Spark上下文环境对象（连接对象）val sc : SparkContext = new SparkContext(sparkConf)// 读取文件数据val fileRDD: RDD[String] = sc.textFile(\"input/word.txt\")// 将文件中的数据进行分词val wordRDD: RDD[String] = fileRDD.flatMap( _.split(\" \") )// 转换数据结构 word =&gt; (word, 1)val word2OneRDD: RDD[(String, Int)] = wordRDD.map((_,1))// 将转换结构后的数据按照相同的单词进行分组聚合val word2CountRDD: RDD[(String, Int)] = word2OneRDD.reduceByKey(_+_)// 将数据聚合结果采集到内存中val word2Count: Array[(String, Int)] = word2CountRDD.collect()// 打印结果word2Count.foreach(println)//关闭Spark连接sc.stop() spark运行环境local所谓的Local模式，就是不需要其他任何节点资源就可以在本地执行Spark代码的环境，一般用于教学，调试，演示等， standalonelocal本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用Spark自身节点运行的集群模式，也就是我们所谓的独立部署（Standalone）模式。Spark的Standalone模式体现了经典的master-slave模式。 集群规划: Linux1 Linux2 Linux3 Spark Worker Master Worker Worker yarn独立部署（Standalone）模式由Spark自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的Yarn环境下Spark是如何工作的（其实是因为在国内工作中，Yarn使用的非常多）。 k8s &amp; MesosMesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核,在Twitter得到广泛使用,管理着Twitter超过30,0000台服务器上的应用部署，但是在国内，依然使用着传统的Hadoop大数据框架，所以国内使用Mesos框架的并不多，但是原理其实都差不多。 windows一般教学演示使用。 对比 模式 Spark安装机器数 需启动的进程 所属者 应用场景 Local 1 无 Spark 测试 Standalone 3 Master及Worker Spark 单独部署 Yarn 1 Yarn及HDFS Hadoop 混合部署 端口号 Spark查看当前Spark-shell运行任务情况端口号：4040（计算） Spark Master内部通信服务端口号：7077 Standalone模式下，Spark Master Web端口号：8080（资源） Spark历史服务器端口号：18080 Hadoop YARN任务运行情况查看端口号：8088 spark运行架构核心组件Spark框架有两个核心组件： 1、Driver Spark驱动器节点，用于执行Spark任务中的main方法，负责实际代码的执行工作。Driver在Spark作业执行时主要负责： 将用户程序转化为作业（job） 在Executor之间调度任务(task) 跟踪Executor的执行情况 通过UI展示查询运行情况 实际上，我们无法准确地描述Driver的定义，因为在整个的编程过程中没有看到任何有关Driver的字眼。所以简单理解，所谓的Driver就是驱使整个应用运行起来的程序，也称之为Driver类。 2、Executor Spark Executor是集群中工作节点（Worker）中的一个JVM进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。 Executor有两个核心功能： 负责运行组成Spark应用的任务，并将结果返回给驱动器进程 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。 3、ApplicationMaster Hadoop用户向YARN集群提交应用程序时,提交程序中应该包含ApplicationMaster，用于向资源调度器申请执行任务的资源容器Container，运行用户自己的程序任务job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。 说的简单点就是，RM（资源）和Driver（计算）之间的解耦合靠的就是ApplicationMaster。 提交流程Spark应用程序提交到Yarn环境中执行的时候，一般会有两种部署执行的方式：Client和Cluster。 两种模式，主要区别在于：Driver程序的运行节点。 Yarn Client模式 Client模式将用于监控和调度的Driver模块在客户端执行，而不是Yarn中，所以一般用于测试。 Driver在任务提交的本地机器上运行 Driver启动后会和ResourceManager通讯申请启动ApplicationMaster ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，负责向ResourceManager申请Executor内存 ResourceManager接到ApplicationMaster的资源申请后会分配container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程 Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数 之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将task分发到各个Executor上执行。 Yarn Cluster模式 Cluster模式将用于监控和调度的Driver模块启动在Yarn集群资源中执行。一般应用于实际生产环境。 在YARN Cluster模式下，任务提交后会和ResourceManager通讯申请启动ApplicationMaster， 随后ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver。 Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配container，然后在合适的NodeManager上启动Executor进程 Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数， 之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将task分发到各个Executor上执行。 spark核心编程Spark计算框架为了能够对数据进行高并发和高吞吐的处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是： RDD : 弹性分布式数据集 累加器：分布式共享只写变量 广播变量：分布式共享只读变量 RDDRDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。 数据集：RDD封装了计算逻辑，并不保存数据 RDD并行度与分区默认情况下，Spark可以切分任务，并将任务发送给Executor节点并行计算，而这个并行计算的任务数量我们称之为并行度。这个数量可以在构建RDD时指定。 scala12345678910111213val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark\")val sparkContext = new SparkContext(sparkConf)val dataRDD: RDD[Int] = sparkContext.makeRDD( List(1,2,3,4), 4)val fileRDD: RDD[String] = sparkContext.textFile( \"input\", 2)fileRDD.collect().foreach(println)sparkContext.stop() 读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark源码如下 scala1234567def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = &#123; (0 until numSlices).iterator.map &#123; i =&gt; val start = ((i * length) / numSlices).toInt val end = (((i + 1) * length) / numSlices).toInt (start, end) &#125;&#125; 读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异，具体Spark源码如下 scala1234567891011121314151617181920212223242526272829303132public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException &#123; long totalSize = 0; // compute total size for (FileStatus file: files) &#123; // check we have valid files if (file.isDirectory()) &#123; throw new IOException(\"Not a file: \"+ file.getPath()); &#125; totalSize += file.getLen(); &#125; long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits); long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input. FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize); ... for (FileStatus file: files) &#123; ... if (isSplitable(fs, path)) &#123; long blockSize = file.getBlockSize(); long splitSize = computeSplitSize(goalSize, minSize, blockSize); ... &#125; protected long computeSplitSize(long goalSize, long minSize, long blockSize) &#123; return Math.max(minSize, Math.min(goalSize, blockSize)); &#125; RDD创建1、从集合（内存）中创建RDD parallelize和makeRDD scala123456789101112val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark\")val sparkContext = new SparkContext(sparkConf)val rdd1 = sparkContext.parallelize( List(1,2,3,4))val rdd2 = sparkContext.makeRDD( List(1,2,3,4))rdd1.collect().foreach(println)rdd2.collect().foreach(println)sparkContext.stop() makeRDD方法其实就是parallelize方法 scala12345def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; parallelize(seq, numSlices)&#125; 2、从外部存储（文件）创建RDD 由外部存储系统的数据集创建RDD包括：本地的文件系统，所有Hadoop支持的数据集，比如HDFS、HBase等。 scala123456val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark\")val sparkContext = new SparkContext(sparkConf)val fileRDD: RDD[String] = sparkContext.textFile(\"input\")fileRDD.collect().foreach(println)sparkContext.stop() 3、从其他RDD创建 主要是通过一个RDD运算完后，再产生新的RDD。 4、直接创建RDD（new） 使用new的方式直接构造RDD 转换算子RDD整体上分为Value类型、双Value类型和Key-Value类型 value类型map mapPartitions mapPartitionsWithIndex flatMap glom 将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变 groupBy filter 将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃 sample 根据指定的规则从数据集中抽取数据 distinct coalesce 根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率 repartition 小问题：coalesce和repartition区别？ repartition算子其实底层调用的就是coalesce算子，只不过固定使用了shuffle的操作,可以让数据更均衡一下，可以有效防止数据倾斜问题。 如果缩减分区，一般就采用coalesce，如果想扩大分区，就采用repartition sortBy pipe 管道，针对每个分区，都调用一次shell脚本，返回输出的RDD。 双Value类型intersection 对源RDD和参数RDD求交集后返回一个新的RDD union 对源RDD和参数RDD求并集后返回一个新的RDD subtract 以一个RDD元素为主，去除两个RDD中重复元素，将其他元素保留下来。求差集 zip 将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的Key为第1个RDD中的元素，Value为第2个RDD中的元素。 Key - Value类型partitionBy 将数据按照指定Partitioner重新进行分区。Spark默认的分区器是HashPartitioner reduceByKey groupByKey reduceByKey和groupByKey的区别？ 两个算子没有使用上的区别。所以使用的时候需要根据应用场景来选择。 从性能上考虑，reduceByKey存在预聚合功能，这样，在shuffle的过程中，落盘的数据量会变少，所以读写磁盘的速度会变快。性能更高 aggregateByKey 将数据根据不同的规则进行分区内计算和分区间计算 dataRDD1.aggregateByKey(0)(_+_,_+_) foldByKey 当分区内计算规则和分区间计算规则相同时，aggregateByKey就可以简化为foldByKey dataRDD1.foldByKey(0)(_+_) combineByKey 最通用的对key-value型rdd进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致。 reduceByKey、foldByKey、aggregateByKey、combineByKey的区别？ 从源码的角度来讲，四个算子的底层逻辑是相同的。 aggregateByKey的算子会将初始值和第一个value使用分区内的计算规则进行计算 foldByKey的算子的分区内和分区间的计算规则相同，并且初始值和第一个value使用的规则相同 combineByKey第一个参数就是对第一个value进行处理，所以无需初始值。 reduceByKey不会对第一个value进行处理，分区内和分区间计算规则相同 上面的四个算子都支持预聚合功能。所以shuffle性能比较高 上面的四个算子都可以实现WordCount sortByKey join leftOuterJoin cogroup 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD 行动算子reduce 聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数据 collect 在驱动程序中，以数组Array的形式返回数据集的所有元素 count first take takeOrdered aggregate 分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合 fold 折叠操作，aggregate的简化版操作 countByKey save相关算子 scala12345678// 保存成Text文件rdd.saveAsTextFile(\"output\")// 序列化成对象保存到文件rdd.saveAsObjectFile(\"output1\")// 保存成Sequencefile文件rdd.map((_,1)).saveAsSequenceFile(\"output2\") foreach 分布式遍历RDD中的每一个元素，调用指定函数 RDD序列化1) 闭包检查 从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行。那么在scala的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。 2) Kryo序列化框架 参考地址: https://github.com/EsotericSoftware/kryo Java的序列化能够序列化任何的类。但是比较重，序列化后，对象的提交也比较大。 Spark出于性能的考虑，Spark2.0开始支持另外一种Kryo序列化机制。Kryo速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化。 RDD依赖关系1、RDD血缘关系 RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。 scala12345678910111213141516val fileRDD: RDD[String] = sc.textFile(\"input/1.txt\")println(fileRDD.toDebugString)println(\"----------------------\")val wordRDD: RDD[String] = fileRDD.flatMap(_.split(\" \"))println(wordRDD.toDebugString)println(\"----------------------\")val mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))println(mapRDD.toDebugString)println(\"----------------------\")val resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_+_)println(resultRDD.toDebugString)resultRDD.collect() 2、RDD依赖关系 这里所谓的依赖关系，其实就是RDD之间的关系 scala123456789101112131415161718val sc: SparkContext = new SparkContext(conf)val fileRDD: RDD[String] = sc.textFile(\"input/1.txt\")println(fileRDD.dependencies)println(\"----------------------\")val wordRDD: RDD[String] = fileRDD.flatMap(_.split(\" \"))println(wordRDD.dependencies)println(\"----------------------\")val mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))println(mapRDD.dependencies)println(\"----------------------\")val resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_+_)println(resultRDD.dependencies)resultRDD.collect() 3、RDD窄依赖 窄依赖表示每一个父RDD的Partition最多被子RDD的一个Partition使用，窄依赖我们形象的比喻为独生子女。 4、RDD宽依赖 宽依赖表示同一个父RDD的Partition被多个子RDD的Partition依赖，会引起Shuffle，总结：宽依赖我们形象的比喻为超生。 5、RDD任务划分 RDD任务切分中间分为：Application、Job、Stage和Task Application：初始化一个SparkContext即生成一个Application； Job：一个Action算子就会生成一个Job； Stage：Stage等于宽依赖(ShuffleDependency)的个数加1； Task：一个Stage阶段中，最后一个RDD的分区个数就是Task的个数。 注意：Application-&gt;Job-&gt;Stage-&gt;Task每一层都是1对n的关系。 RDD持久化1、RDD Cache缓存 RDD通过Cache或者Persist方法将前面的计算结果缓存，默认情况下会把数据以序列化的形式缓存在JVM的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action算子时，该RDD将会被缓存在计算节点的内存中，并供后面重用。 scala12345678// cache操作会增加血缘关系，不改变原有的血缘关系println(wordToOneRdd.toDebugString)// 数据缓存。wordToOneRdd.cache()// 可以更改存储级别//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2) 存储级别 scala12345678910111213object StorageLevel &#123; val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(true, true, true, false, 1) 缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。 Spark会自动对一些Shuffle操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点Shuffle失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用persist或cache。 2、RDD CheckPoint检查点 所谓的检查点其实就是通过将RDD中间结果写入磁盘 由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。 对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。 scala12345678910111213141516171819202122// 设置检查点路径sc.setCheckpointDir(\"./checkpoint1\")// 创建一个RDD，读取指定位置文件:hello ys ysval lineRdd: RDD[String] = sc.textFile(\"input/1.txt\")// 业务逻辑val wordRdd: RDD[String] = lineRdd.flatMap(line =&gt; line.split(\" \"))val wordToOneRdd: RDD[(String, Long)] = wordRdd.map &#123; word =&gt; &#123; (word, System.currentTimeMillis()) &#125;&#125;// 增加缓存,避免再重新跑一个job做checkpointwordToOneRdd.cache()// 数据检查点：针对wordToOneRdd做检查点计算wordToOneRdd.checkpoint()// 触发执行逻辑wordToOneRdd.collect().foreach(println) 缓存和检查点区别 Code123451）Cache缓存只是将数据保存起来，不切断血缘依赖。Checkpoint检查点切断血缘依赖。2）Cache缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint的数据通常存储在HDFS等容错、高可用的文件系统，可靠性高。3）建议对checkpoint()的RDD使用Cache缓存，这样checkpoint的job只需从Cache缓存中读取数据即可，否则需要再从头计算一次RDD。 scala123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.ysss.bigdata.spark.core.cacheimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object Spark02_Checkpoint &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setAppName(\"SparkCoreTest\").setMaster(\"local[*]\") val sc: SparkContext = new SparkContext(conf) // 设置检查点路径， 一般路径应该为分布式存储路径，HDFS sc.setCheckpointDir(\"cp\") val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4)) // TODO 检查点 // RDD的持久化可能会导致数据丢失，如果数据丢失，那么需要重新再次计算，性能不高 // 所以如果能够保证数据不丢，那么是一个好的选择 // 可以将数据保存到检查点中，这样是分布式存储，所以比较安全。 // 所以将数据保存到检查点前，需要设定检查点路径 val rdd1 = rdd.map( num =&gt; &#123; //println(\"num.....\") num &#125; ) // 检查点 // 检查点为了准确，需要重头再执行一遍，就等同于开启一个新的作业 // 为了提高效率，一般情况下，是先使用cache后在使用检查点 // 检查点会切断RDD的血缘关系。将当前检查点当成数据计算的起点。 // 持久化操作是不能切断血缘关系，因为一旦内存中数据丢失，无法恢复数据 val rdd2: RDD[Int] = rdd1.cache() rdd2.checkpoint() println(rdd2.toDebugString) println(rdd2.collect().mkString(\",\")) println(rdd2.toDebugString) println(\"**********************\") println(rdd2.collect().mkString(\",\")) sc.stop() &#125;&#125; RDD分区器Spark目前支持Hash分区和Range分区，和用户自定义分区。 Hash分区为当前的默认分区。 分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区，进而决定了Reduce的个数。 只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None 每个RDD的分区ID范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。 1) Hash分区：对于给定的key，计算其hashCode,并除以分区个数取余 2) Range分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序 文件读取与保存Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。 文件格式分为：text文件、json文件、csv文件、sequence文件以及Object文件； 文件系统分为：本地文件系统、HDFS、HBASE以及数据库。 累加器累加器用来把Executor端变量信息聚合到Driver端。 在Driver程序中定义的变量，在Executor端的每个Task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行merge。 系统累加器 scala1234567891011val rdd = sc.makeRDD(List(1,2,3,4,5))// 声明累加器var sum = sc.longAccumulator(\"sum\");rdd.foreach( num =&gt; &#123; // 使用累加器 sum.add(num) &#125;)// 获取累加器的值println(\"sum = \" + sum.value) 自定义累加器 scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// 自定义累加器// 1. 继承AccumulatorV2，并设定泛型// 2. 重写累加器的抽象方法class WordCountAccumulator extends AccumulatorV2[String, mutable.Map[String, Long]]&#123;var map : mutable.Map[String, Long] = mutable.Map()// 累加器是否为初始状态override def isZero: Boolean = &#123; map.isEmpty&#125;// 复制累加器override def copy(): AccumulatorV2[String, mutable.Map[String, Long]] = &#123; new WordCountAccumulator&#125;// 重置累加器override def reset(): Unit = &#123; map.clear()&#125;// 向累加器中增加数据 (In)override def add(word: String): Unit = &#123; // 查询map中是否存在相同的单词 // 如果有相同的单词，那么单词的数量加1 // 如果没有相同的单词，那么在map中增加这个单词 map(word) = map.getOrElse(word, 0L) + 1L&#125;// 合并累加器override def merge(other: AccumulatorV2[String, mutable.Map[String, Long]]): Unit = &#123; val map1 = map val map2 = other.value // 两个Map的合并 map = map1.foldLeft(map2)( ( innerMap, kv ) =&gt; &#123; innerMap(kv._1) = innerMap.getOrElse(kv._1, 0L) + kv._2 innerMap &#125; )&#125;// 返回累加器的结果 （Out）override def value: mutable.Map[String, Long] = map&#125; 广播变量广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。 scala1234567891011121314151617val rdd1 = sc.makeRDD(List( (\"a\",1), (\"b\", 2), (\"c\", 3), (\"d\", 4) ),4)val list = List( (\"a\",4), (\"b\", 5), (\"c\", 6), (\"d\", 7) )// 声明广播变量val broadcast: Broadcast[List[(String, Int)]] = sc.broadcast(list)val resultRDD: RDD[(String, (Int, Int))] = rdd1.map &#123; case (key, num) =&gt; &#123; var num2 = 0 // 使用广播变量 for ((k, v) &lt;- broadcast.value) &#123; if (k == key) &#123; num2 = v &#125; &#125; (key, (num, num2)) &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"}]},{"title":"sqoop常见问题汇总","slug":"sqoop常见问题汇总","date":"2020-06-17T12:14:17.000Z","updated":"2020-06-17T12:15:59.786Z","comments":true,"path":"2020/06/17/sqoop常见问题汇总/","link":"","permalink":"https://masteryang4.github.io/2020/06/17/sqoop%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/","excerpt":"","text":"概述Sqoop是连接关系型数据库和Hadoop的桥梁，主要有两个方面(导入和导出)。 目前在我的工程实践中，一般是将MySQL数据进行导入导出 Sqoop参数shell123456789/opt/module/sqoop/bin/sqoop import \\--connect \\--username \\--password \\--target-dir \\--delete-target-dir \\--num-mappers \\--fields-terminated-by \\--query \"$2\" ' and $CONDITIONS;' Sqoop导入导出Null存储一致性问题Hive中的Null在底层是以“\\N”来存储，而MySQL中的Null在底层就是Null。为了保证数据两端的一致性。 往hive导入数据时采用--null-string和--null-non-string。 在从hive导出数据时采用--input-null-string和--input-null-non-string两个参数。 Sqoop数据导出一致性问题场景：如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。 官网：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html Since Sqoop breaks down export process into multiple transactions, it is possible that a failed export job may result in partial data being committed to the database. This can further lead to subsequent jobs failing due to insert collisions in some cases, or lead to duplicated data in others. You can overcome this problem by specifying a staging table via the –staging-table option which acts as an auxiliary table that is used to stage exported data. The staged data is finally moved to the destination table in a single transaction. –staging-table方式 shell1sqoop export --connect jdbc:mysql://192.168.137.10:3306/user_behavior --username root --password 123456 --table app_cource_study_report --columns watch_video_cnt,complete_video_cnt,dt --fields-terminated-by \"\\t\" --export-dir \"/user/hive/warehouse/tmp.db/app_cource_study_analysis_$&#123;day&#125;\" --staging-table app_cource_study_report_tmp --clear-staging-table --input-null-string '\\N' Sqoop底层运行的任务是什么只有Map阶段，没有Reduce阶段的任务。 默认是4个MapTask。 Sqoop一天导入多少数据100万日活=》10万订单，1人10条，每天1g左右业务数据 Sqoop每天将1G的数据量导入到数仓。 Sqoop数据导出的时候一次执行多长时间每天晚上00:30开始执行，Sqoop任务一般情况40 -50分钟的都有。取决于数据量（11:11，6:18等活动在1个小时左右）。 Sqoop在导入数据的时候数据倾斜 https://blog.csdn.net/lizhiguo18/article/details/103969906 Sqoop 抽数的并行化主要涉及到两个参数：num-mappers：启动N个map来并行导入数据，默认4个；split-by：按照某一列来切分表的工作单元。 通过ROWNUM() 生成一个严格均匀分布的字段，然后指定为分割字段. Sqoop数据导出Parquet我在工程项目中经常遇到的问题 Ads层数据用Sqoop往MySql中导入数据的时候，如果用了orc（Parquet）不能导入，需转化成text格式 （1）创建临时表，把Parquet中表数据导入到临时表，把临时表导出到目标表用于可视化 （2）Sqoop里面有参数，可以直接把Parquet转换为text （3）ads层建表的时候就不要建Parquet表","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"sqoop","slug":"大数据/sqoop","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/sqoop/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"sqoop","slug":"sqoop","permalink":"https://masteryang4.github.io/tags/sqoop/"},{"name":"离线大数据","slug":"离线大数据","permalink":"https://masteryang4.github.io/tags/%E7%A6%BB%E7%BA%BF%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"spark系列之spark-streaming","slug":"spark系列之spark-streaming","date":"2020-06-17T08:44:51.000Z","updated":"2020-06-17T11:41:04.721Z","comments":true,"path":"2020/06/17/spark系列之spark-streaming/","link":"","permalink":"https://masteryang4.github.io/2020/06/17/spark%E7%B3%BB%E5%88%97%E4%B9%8Bspark-streaming/","excerpt":"","text":"SparkStreaming概述Spark Streaming是什么Spark Streaming用于流式数据的处理。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。 和Spark基于RDD的概念很相似，Spark Streaming使用离散化流(discretized stream)作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而DStream是由这些RDD所组成的序列(因此得名“离散化”)。 Spark Streaming架构整体架构图 spark-streaming架构图 背压机制Spark 1.5以前版本，用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现，此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。 为了更好的协调数据接收速率与资源处理能力，1.5版本开始Spark Streaming可以动态控制数据接收速率来适配集群数据处理能力。背压机制（即Spark Streaming Backpressure）: 根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。 通过属性spark.streaming.backpressure.enabled来控制是否启用backpressure机制，默认值false，即不启用。 DStream入门需求：使用netcat工具向9999端口不断的发送数据，通过SparkStreaming读取端口数据并统计不同单词出现的次数 maven依赖 xml12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt;&lt;/dependency&gt; WordCount案例代码 scala123456789101112131415161718192021222324252627282930object StreamWordCount &#123; def main(args: Array[String]): Unit = &#123; //1.初始化Spark配置信息 val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"StreamWordCount\") //2.初始化SparkStreamingContext val ssc = new StreamingContext(sparkConf, Seconds(3)) //3.通过监控端口创建DStream，读进来的数据为一行行 val lineStreams = ssc.socketTextStream(\"linux1\", 9999) //将每一行数据做切分，形成一个个单词 val wordStreams = lineStreams.flatMap(_.split(\" \")) //将单词映射成元组（word,1） val wordAndOneStreams = wordStreams.map((_, 1)) //将相同的单词次数做统计 val wordAndCountStreams = wordAndOneStreams.reduceByKey(_+_) //打印 wordAndCountStreams.print() //启动SparkStreamingContext ssc.start() ssc.awaitTermination() &#125;&#125; 启动程序并通过netcat发送数据： Code12nc -lk 9999hello ysss WordCount解析 Discretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，DStream是一系列连续的RDD来表示。每个RDD含有一段时间间隔内的数据。 DStream创建/数据源RDD队列测试过程中，可以通过使用ssc.queueStream(queueOfRDDs)来创建DStream，每一个推送到这个队列中的RDD，都会作为一个DStream处理。 需求：循环创建几个RDD，将RDD放入队列。通过SparkStream创建Dstream，计算WordCount scala1234567891011121314151617181920212223242526272829303132333435363738package com.ysss.bigdata.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.dstream.&#123;DStream, InputDStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import scala.collection.mutableobject SparkStreaming02_DStream_Queue &#123; def main(args: Array[String]): Unit = &#123; // TODO 配置对象 val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") // TODO 环境对象 val ssc = new StreamingContext(sparkConf, Seconds(3)) // TODO 数据处理 val que = new mutable.Queue[RDD[String]]() val queDS: InputDStream[String] = ssc.queueStream(que) queDS.print() // TODO 关闭连接环境 ssc.start() println(\"queue append item\") for ( i &lt;- 1 to 5 ) &#123; val rdd = ssc.sparkContext.makeRDD(List(\"1\",\"2\")) que += rdd Thread.sleep(2000) &#125; // block ssc.awaitTermination() &#125;&#125; 结果 Code123456789101112131415161718192021222324252627282930313233343536373839404142434445-------------------------------------------Time: 1539075280000 ms-------------------------------------------(4,60)(0,60)(6,60)(8,60)(2,60)(1,60)(3,60)(7,60)(9,60)(5,60)-------------------------------------------Time: 1539075284000 ms-------------------------------------------(4,60)(0,60)(6,60)(8,60)(2,60)(1,60)(3,60)(7,60)(9,60)(5,60)-------------------------------------------Time: 1539075288000 ms-------------------------------------------(4,30)(0,30)(6,30)(8,30)(2,30)(1,30)(3,30)(7,30)(9,30)(5,30)-------------------------------------------Time: 1539075292000 ms------------------------------------------- 扩展，从文件中读取 scala123456789101112131415161718192021222324252627282930313233package com.ysss.bigdata.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import scala.collection.mutableobject SparkStreaming03_DStream_File &#123; def main(args: Array[String]): Unit = &#123; // TODO 配置对象 val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") // TODO 环境对象 val ssc = new StreamingContext(sparkConf, Seconds(5)) // TODO 数据处理 // 从文件夹中读取新的文件数据，功能不稳定 ，所以不推荐使用 // flume更加专业，所以生产环境，监控文件或目录的变化，采集数据都使用flume val fileDS: DStream[String] = ssc.textFileStream(\"in\") val wordDS: DStream[String] = fileDS.flatMap(_.split(\" \")) val wordToOneDS: DStream[(String, Int)] = wordDS.map( (_, 1) ) val wordToCountDS: DStream[(String, Int)] = wordToOneDS.reduceByKey(_+_) wordToCountDS.print() // TODO 关闭连接环境 ssc.start() ssc.awaitTermination() &#125;&#125; 自定义数据源需要继承Receiver，并实现onStart、onStop方法来自定义数据源采集。 需求：自定义数据源，实现监控某个端口号，获取该端口号内容。 scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.ysss.bigdata.spark.streamingimport java.io.&#123;BufferedReader, InputStreamReader&#125;import java.net.Socketimport org.apache.spark.SparkConfimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.receiver.Receiverimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming04_DStream_DIY &#123; def main(args: Array[String]): Unit = &#123; // TODO 配置对象 val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") // TODO 环境对象 val ssc = new StreamingContext(sparkConf, Seconds(5)) // TODO 数据处理 // 自定义数据采集器 val myDS = ssc.receiverStream( new MyReceiver( \"localhost\", 9999 ) ) val wordDS: DStream[String] = myDS.flatMap(_.split(\" \")) val wordToOneDS: DStream[(String, Int)] = wordDS.map( (_, 1) ) val wordToCountDS: DStream[(String, Int)] = wordToOneDS.reduceByKey(_+_) wordToCountDS.print() // TODO 关闭连接环境 ssc.start() ssc.awaitTermination() &#125; /* 自定义数据采集器 模仿spark自带的socket采集器 1. 继承Receiver ,设定泛型（采集数据的类型）, 传递参数 2. 重写方法 */ // rdd cache, checkpoint class MyReceiver(host:String, port:Int) extends Receiver[String](StorageLevel.MEMORY_ONLY)&#123; private var socket: Socket = _ // 接收数据 def receive(): Unit = &#123; val reader = new BufferedReader( new InputStreamReader( socket.getInputStream, \"UTF-8\" ) ) var s : String = null // 网络编程中，获取的数据没有null的概念 // 如果网络编程中，需要明确告知服务器，客户端不再传数据，需要发送特殊的指令 // 文件读取时，如果读到结束的时候，获取的结果为null while ( (s = reader.readLine()) != null ) &#123; // 采集到数据后，进行封装(存储) if ( s != \"-END-\" ) &#123; store(s) &#125; else &#123; // stop // close // 重启 //restart(\"\") &#125; &#125; &#125; // 启动采集器 // 采集 &amp; 封装 override def onStart(): Unit = &#123; socket = new Socket(host, port) new Thread(\"Socket Receiver\") &#123; setDaemon(true) override def run() &#123; receive() &#125; &#125;.start() &#125; override def onStop(): Unit = &#123; if ( socket != null ) &#123; socket.close() &#125; &#125; &#125;&#125; kakfa数据源[重点]概述ReceiverAPI：需要一个专门的Executor去接收数据，然后发送给其他的Executor做计算。存在的问题，接收数据的Executor和计算的Executor速度会有所不同，特别在接收数据的Executor速度大于计算的Executor速度，会导致计算数据的节点内存溢出。 DirectAPI：是由计算的Executor来主动消费Kafka的数据，速度由自身控制。 kafka 0-8 Receiver 模式这种方式使用Receiver来获取数据。Receiver是使用Kafka的高层次Consumer API来实现的。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的（如果突然数据暴增，大量batch堆积，很容易出现内存溢出的问题），然后Spark Streaming启动的job会去处理那些数据。 然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的预写日志机制（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。 xml12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt;&lt;/dependency&gt; scala123456789101112131415161718192021222324252627282930313233343536373839404142package com.ysss.bigdata.spark.streamingimport java.io.&#123;BufferedReader, InputStreamReader&#125;import java.net.Socketimport org.apache.spark.SparkConfimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.kafka.KafkaUtilsimport org.apache.spark.streaming.receiver.Receiverimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming05_DStream_Kafka &#123; def main(args: Array[String]): Unit = &#123; // TODO 配置对象 val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") // TODO 环境对象 val ssc = new StreamingContext(sparkConf, Seconds(5)) // TODO 数据处理 // 使用0.8版本的kafka - 接收器方式 // 访问kakfa会有相应的工具类 val kafkaDS: ReceiverInputDStream[(String, String)] = KafkaUtils.createStream( ssc, \"linux1:2181,linux2:2181,linux3:2181\", \"ysss191125\", Map(\"ysss191125\" -&gt; 3) ) // Kafka消息传递的时候以k-v对 // k - 传值的时候提供的，默认为null,主要用于分区 // v - message kafkaDS.map(_._2).print() // TODO 关闭连接环境 ssc.start() ssc.awaitTermination() &#125;&#125; kafka 0-8 Direct 模式这种新的不基于Receiver的直接方式，是在Spark 1.3中引入的，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。 自动维护 offset scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.ysss.bigdata.spark.streamingimport kafka.serializer.StringDecoderimport org.apache.kafka.clients.consumer.ConsumerConfigimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka.KafkaUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming07_DStream_Kafka_Direct1 &#123; def main(args: Array[String]): Unit = &#123; // TODO 这种方式，可以保证数据不丢失，但是可能会出现数据重复消费 // TODO 环境对象 - 从checkpoint中读取数据偏移量 // checkpoint还保存了计算逻辑,不适合扩展功能 // checkpoint会延续计算，但是可能会压垮内存 // checkpoint一般的存储路径为HDFS，所以会导致小文件过多。性能受到影响 // 不推荐使用 val ssc: StreamingContext = StreamingContext.getActiveOrCreate(\"scp\", () =&gt; getStreamingContext) // TODO 关闭连接环境 ssc.start() ssc.awaitTermination() &#125; def getStreamingContext () = &#123; // TODO 配置对象 val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.checkpoint(\"scp\") // TODO 数据处理 // 使用0.8版本的kafka - Direct方式 - 自动维护Offset // TODO 默认情况下，SparkStreaming采用checkpoint来保存kafka的数据偏移量 // 访问kakfa会有相应的工具类 val kafkaParamMap = Map( ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; \"linux1:9092,linux2:9092,linux3:9092\", ConsumerConfig.GROUP_ID_CONFIG -&gt; \"ysss191125new\" ) val kafkaDS: InputDStream[(String, String)] = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder]( ssc, kafkaParamMap, Set(\"ysss191125new\") ) kafkaDS.map(_._2).print() kafkaDS.print() ssc &#125;&#125; 手动维护 offset scala1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.ysss.bigdata.spark.streamingimport kafka.common.TopicAndPartitionimport kafka.message.MessageAndMetadataimport kafka.serializer.StringDecoderimport org.apache.kafka.clients.consumer.ConsumerConfigimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka.&#123;HasOffsetRanges, KafkaUtils, OffsetRange&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming08_DStream_Kafka_Direc2 &#123; def main(args: Array[String]): Unit = &#123; // TODO 配置对象 val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(5)) // TODO 数据处理 // 使用0.8版本的kafka - Direct方式 - 手动维护Offset // 所谓的手动维护，其实就是开发人员自己获取偏移量，并进行保存处理。 // 通过保存的偏移量，可以动态获取kafka中指定位置的数据 // offset会保存到kakfa集群的系统主题中__consumer_offsets val kafkaMap = Map( ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; \"linux1:9092,linux2:9092,linux3:9092\", ConsumerConfig.GROUP_ID_CONFIG -&gt; \"ysss191125123\" ) val fromOffsets = Map( (TopicAndPartition(\"ysss191125new\", 0), 0L), (TopicAndPartition(\"ysss191125new\", 1), 1L), (TopicAndPartition(\"ysss191125new\", 2), 2L) ) // TODO 从kafka中获取指定topic中指定offset的数据 val kafkaDS: InputDStream[String] = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder, String]( ssc, kafkaMap, fromOffsets, (m:MessageAndMetadata[String, String]) =&gt; m.message() ) var offsetRanges = Array.empty[OffsetRange] // 转换 // 获取偏移量，一定要在最初的逻辑中获取，防止数据处理完毕后，无偏移量信息 kafkaDS.transform(rdd =&gt; &#123; // 获取RDD中的偏移量范围 // 默认Spark中的RDD是没有offsetRanges方法，所以必须转换类型后才能使用 // RDD 和 HasOffsetRanges有关系 offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd &#125;).foreachRDD(rdd=&gt;&#123; for (o &lt;- offsetRanges) &#123; println(s\"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;\") &#125; rdd.foreach(println) &#125;) ssc.start() ssc.awaitTermination() &#125;&#125; kafka 0-10 Direct 模式xml12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt;&lt;/dependency&gt; scala1234567891011121314151617181920212223242526272829303132333435363738394041package com.ysss.bigdata.spark.streamingimport org.apache.kafka.clients.consumer.&#123;ConsumerConfig, ConsumerRecord&#125;import org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming08_DStream_Kafka_Direc2 &#123; def main(args: Array[String]): Unit = &#123; // TODO 配置对象 val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(5)) // TODO 数据处理 // 使用0.10版本的kafka - Direct方式 - 自动维护Offset // LocationStrategy : 位置策略 // ConsumerStrategies : 消费策略 // TODO sealed : 用于修饰类的关键字，表示密封类 // 要求子类如果是样例类，必须全部在同一个源码文件中 val kafkaMap = Map( ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; \"linux1:9092,linux2:9092,linux3:9092\", ConsumerConfig.GROUP_ID_CONFIG -&gt; \"ysss191125123\", ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -&gt; \"org.apache.kafka.common.serialization.StringDeserializer\", ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -&gt; \"org.apache.kafka.common.serialization.StringDeserializer\" ) val kafkaDS: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String]( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String]( Set(\"ysss191125new\"), kafkaMap ) ) kafkaDS.map(_.value()).print() ssc.start() ssc.awaitTermination() &#125;&#125; 手动维护可参考官网，和0-8手动维护类似。 spark-streaming如何保证数据精准一次性处理呢？ scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.ysss.bigdata.spark.streamingimport kafka.common.TopicAndPartitionimport kafka.message.MessageAndMetadataimport kafka.serializer.StringDecoderimport org.apache.kafka.clients.consumer.ConsumerConfigimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka.&#123;HasOffsetRanges, KafkaUtils, OffsetRange&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming09_DStream_Kafka_Direc3 &#123; def main(args: Array[String]): Unit = &#123; // TODO 配置对象 val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(5)) // TODO 数据处理 // SparkStreaming消费Kafka数据时，手动维护offset的思路 // TODO 1. 从指定的位置获取当前业务中保存的数据偏移量 // mysql =&gt; message offset =&gt; 5 // TODO 2. 从kafka中对应的分区里根据偏移量获取数据 // topicAndPartition =&gt; topic : xxx, partition : 0, offset : 5 // TODO 3. 消费数据时，需要将消费数据的偏移量拿到。 // KafkaRDD =&gt; offsetRange =&gt; (5, 100) // TODO 4. 执行业务操作。要求，偏移量的更新和业务要求在同一个事务中 // Tx start // service // commit - offset -&gt; mysql // Tx commit // TODO 4.1 如果不使用事务，那么可能业务成功，但是offset提交失败 // 会导致数据重复消费 // TODO 4.2 如果不使用事务，那么可能offset提交成功，但是业务失败 // 会导致数据丢失 // TODO 4.3 分布式事务， 如果中间出现shuffle，怎么办？ // 所以需要将数据拉取到driver端进行事务操作，保证数据不会出现问题。 // 这样会导致driver的性能下降，所以其实不是一个好的选择。 // SparkStreaming =&gt; 基本要求： 不丢失数据 // Flink =&gt; 数据精准一次性处理。 ssc.start() ssc.awaitTermination() &#125;&#125; 消费kafka数据模式总结Code123456789101112131415161718192021- 0-8 ReceiverAPI: 1) 专门的Executor读取数据，速度不统一 2) 跨机器传输数据 3) Executor读取数据通过多个线程的方式，想要增加并行度，则需要多个流union 4) offset存储在zookeeper中- 0-8 DirectAPI: 1) Executor读取数据并计算 2) 增加Executor个数来增加消费的并行度 3) offset存储 a. CheckPoint(getActiveOrCreate方式创建StreamingContext) b. 手动维护(有事务的存储系统) 4) 获取offset必须在第一个调用的算子中： offsetRanges &#x3D; rdd.asInstanceOf[HasOffsetRanges].offsetRanges- 0-10 DirectAPI: 1) Executor读取数据并计算 2) 增加Executor个数来增加消费的并行度 3) offset存储 a. __consumer_offsets系统主题中 b. 手动维护(有事务的存储系统) DStream转换DStream上的操作与RDD的类似，分为Transformations（转换）和Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。 无状态转化操作无状态转化操作就是把简单的RDD转化操作应用到每个批次上，也就是转化DStream中的每一个RDD。部分无状态转化操作列在了下表中。注意，针对键值对的DStream转化操作(比如 reduceByKey())要添加import StreamingContext._才能在Scala中使用。 需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个DStream在内部是由许多RDD（批次）组成，且无状态转化操作是分别应用到每个RDD上的。 例如：reduceByKey()会归约每个时间区间中的数据，但不会归约不同区间之间的数据。 transformtransform允许DStream上执行任意的RDD-to-RDD函数。即使这些函数并没有在DStream的API中暴露出来，通过该函数可以方便的扩展Spark API。该函数每一批次调度一次。其实也就是对DStream中的RDD应用转换。 scala12345678910111213141516171819202122232425262728293031package com.ysss.bigdata.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming10_DStream_WordCount &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(5)) val socketDS: ReceiverInputDStream[String] = ssc.socketTextStream(\"localhost\", 9999) // TODO 可以将DStream转换为RDD进行操作。 // DStream =&gt; old RDD =&gt; new RDD =&gt; new DStream val resultDS: DStream[(String, Int)] = socketDS.transform( rdd =&gt; &#123; val flatRDD = rdd.flatMap(_.split(\" \")) val mapRDD = flatRDD.map((_, 1)) val reduceRDD = mapRDD.reduceByKey(_ + _) reduceRDD &#125; ) resultDS.print() ssc.start() ssc.awaitTermination() &#125;&#125; 相比直接在DStream上进行操作，transform的优势 scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.ysss.bigdata.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming11_DStream_Transform &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(5)) val socketDS: ReceiverInputDStream[String] = ssc.socketTextStream(\"localhost\", 9999) // TODO transform可以获取底层的RDD进行处理 // TODO transform可以周期性的执行driver的代码逻辑 // Code =&gt; Driver// val newDS: DStream[String] = socketDS.map(// dataString =&gt; &#123;// // Code = Executor// \"string : \" + dataString// &#125;// ) // Code = Driver // JDBC.getData(); val newDS1: DStream[String] = socketDS.transform( rdd =&gt; &#123; // Code = Driver // JDBC.getData(); println(Thread.currentThread().getName) rdd.map( dataString =&gt; &#123; // Code = Executor \"string : \" + dataString // JDBC.updateData(); &#125; ) &#125; ) newDS1.print() ssc.start() ssc.awaitTermination() &#125;&#125; join两个流之间的join需要两个流的批次大小一致，这样才能做到同时触发计算。计算过程就是对当前批次的两个流中各自的RDD进行join，与两个RDD的join效果相同。 scala123456789101112131415161718192021222324252627package com.ysss.bigdata.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming12_DStream_Join &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(5)) val socketDS1: ReceiverInputDStream[String] = ssc.socketTextStream(\"localhost\", 9999) val socketDS2: ReceiverInputDStream[String] = ssc.socketTextStream(\"localhost\", 8888) val ds1: DStream[(String, Int)] = socketDS1.map((_,1)) val ds2: DStream[(String, Int)] = socketDS2.map((_,1)) val joinDS: DStream[(String, (Int, Int))] = ds1.join(ds2) joinDS.print() ssc.start() ssc.awaitTermination() &#125;&#125; 有状态转化操作UpdateStateByKeyUpdateStateByKey原语用于记录历史记录，有时，我们需要在DStream中跨批次维护状态(例如流计算中累加wordcount)。针对这种情况，updateStateByKey()为我们提供了对一个状态变量的访问，用于键值对形式的DStream。给定一个由(键，事件)对构成的 DStream，并传递一个指定如何根据新的事件更新每个键对应状态的函数，它可以构建出一个新的 DStream，其内部数据为(键，状态) 对。 updateStateByKey() 的结果会是一个新的DStream，其内部的RDD 序列是由每个时间区间对应的(键，状态)对组成的。 updateStateByKey操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功能，需要做下面两步： 定义状态，状态可以是一个任意的数据类型。 定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更新。 使用updateStateByKey需要对检查点目录进行配置，会使用检查点来保存状态。 scala1234567891011121314151617181920212223242526272829303132333435363738package com.ysss.bigdata.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming13_DStream_State &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.checkpoint(\"scp\") val socketDS = ssc.socketTextStream(\"localhost\", 9999) val wordDS: DStream[String] = socketDS.flatMap(_.split(\" \")) val wordToOneDS: DStream[(String, Int)] = wordDS.map((_,1)) // TODO 使用有状态操作updateStateByKey保存数据 // SparkStreaming的状态保存依赖的是checkpoint,所以需要设定相关路径 val wordToCountDS: DStream[(String, Long)] = wordToOneDS.updateStateByKey[Long]( // 累加器 = 6 // UDAF = 8 // TODO 第一个参数表示相同key的value数据集合 // TODO 第二个参数表示相同key的缓冲区的数据 (seq: Seq[Int], buffer: Option[Long]) =&gt; &#123; // TODO 返回值表示更新后的缓冲区的值 val newBufferValue = buffer.getOrElse(0L) + seq.sum Option(newBufferValue) &#125; ) wordToCountDS.print() ssc.start() ssc.awaitTermination() &#125;&#125; WindowOperationsWindow Operations可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。 窗口时长：计算内容的时间范围； 滑动步长：隔多久触发一次计算。 注意：这两者都必须为采集周期大小的整数倍。 【回顾】scala语言中的window scala123456789101112131415161718192021222324package com.ysss.bigdata.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming14_DStream_Window &#123; def main(args: Array[String]): Unit = &#123; val list = List(1,2,3,4,5,6,7,8) // overflow : 滚动 -&gt; StackOverflowError -&gt; 栈溢出 // 滑动 // flatMap =&gt; 整体-&gt;个体 // sliding =&gt; 整体连续部分（3） -&gt; 整体 // 将sliding中的范围称之为窗口，其中的数据就称之为窗口数据 // 窗口可以动态调整，向后滑动。 val iterator: Iterator[List[Int]] = list.sliding(3, 2) while ( iterator.hasNext ) &#123; println(iterator.next()) &#125; &#125;&#125; window(windowLength, slideInterval): 基于对源DStream窗化的批次进行计算返回一个新的Dstream； scala12345678910111213141516171819202122232425262728293031323334package com.ysss.bigdata.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming15_DStream_Window1 &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(3)) // 滑窗 val socketDS: ReceiverInputDStream[String] = ssc.socketTextStream(\"localhost\", 9999) // 设定窗口。将2个采集周期的数据当成一个整体进行处理 // 默认窗口是可以滑动的。滑动的幅度为一个采集周期 // 可以动态改变滑动幅度 // 如果两个窗口移动过程中，没有重合的数据，称之为滚动窗口 // window方法的第一个参数表示窗口的范围大小，以采集周期为单位 // window方法的第二个参数表示窗口的滑动幅度，也表示计算的周期 val windowDS: DStream[String] = socketDS.window( Seconds(6), Seconds(3)) windowDS .flatMap(_.split(\" \")) .map((_,1)) .reduceByKey(_+_) .print() ssc.start() ssc.awaitTermination() &#125;&#125; reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V)对的DStream上调用此函数，会返回一个新(K,V)对的DStream，此处通过对滑动窗口中批次数据使用reduce函数来整合每个key的value值。 scala123456789101112131415161718192021222324252627package com.ysss.bigdata.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming17_DStream_Window3 &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(3)) // 滑窗 val socketDS: ReceiverInputDStream[String] = ssc.socketTextStream(\"localhost\", 9999) val wordToOneDS: DStream[(String, Int)] = socketDS .flatMap(_.split(\" \")) .map((_, 1)) val windowDS: DStream[(String, Int)] = wordToOneDS.reduceByKeyAndWindow( (x: Int, y: Int) =&gt; x + y, Seconds(6), Seconds(3) ) windowDS.print() ssc.start() ssc.awaitTermination() &#125;&#125; reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的reduce值都是通过用前一个窗的reduce值来递增计算。通过reduce进入到滑动窗口数据并”反向reduce”离开窗口的旧数据来实现这个操作。一个例子是随着窗口滑动对keys的“加”“减”计数。通过前边介绍可以想到，这个函数只适用于”可逆的reduce函数”，也就是这些reduce函数有相应的”反reduce”函数(以参数invFunc形式传入)。如前述函数，reduce任务的数量通过可选参数来配置。 scala1234567891011121314151617181920212223242526272829303132333435package com.ysss.bigdata.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming19_DStream_Window5 &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(3)) ssc.checkpoint(\"scp\") // 滑窗 val socketDS: ReceiverInputDStream[String] = ssc.socketTextStream(\"localhost\", 9999) val wordToOneDS: DStream[(String, Int)] = socketDS.map(num=&gt;(\"a\", 1)) val windowDS: DStream[(String, Int)] = wordToOneDS.reduceByKeyAndWindow( (x: Int, y: Int) =&gt; &#123; val sum = x + y println( sum + \"=\" + x + \"+\" + y ) sum &#125;, (x:Int, y:Int) =&gt; &#123; val diff = x - y println( diff + \"=\" + x + \"-\" + y ) diff &#125;, Seconds(6), Seconds(3) ) windowDS.print() ssc.start() ssc.awaitTermination() &#125;&#125; countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数； scala1234567891011121314151617181920212223242526package com.ysss.bigdata.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming18_DStream_Window4 &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(3)) ssc.checkpoint(\"scp\") // 滑窗 val socketDS: ReceiverInputDStream[String] = ssc.socketTextStream(\"localhost\", 9999) // 对窗口的数据进行计数，会使用checkpoint进行保存 val countDS: DStream[Long] = socketDS.countByWindow(Seconds(6), Seconds(3)) countDS.print() ssc.start() ssc.awaitTermination() &#125;&#125; DStream输出输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。与RDD中的惰性求值类似，如果一个DStream及其派生出的DStream都没有被执行输出操作，那么这些DStream就都不会被求值。如果StreamingContext中没有设定输出操作，整个context就都不会启动。 输出操作如下： print()：在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。在Python API中，同样的操作叫print()。 saveAsTextFiles(prefix, [suffix])：以text文件形式存储这个DStream的内容。每一批次的存储文件名基于参数中的prefix和suffix。”prefix-Time_IN_MS[.suffix]”。 saveAsObjectFiles(prefix, [suffix])：以Java对象序列化的方式将Stream中的数据保存为 SequenceFiles . 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”. Python中目前不可用。 saveAsHadoopFiles(prefix, [suffix])：将Stream中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”。Python API 中目前不可用。 foreachRDD(func)：这是最通用的输出操作，即将函数 func 用于产生于 stream的每一个RDD。其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统，如将RDD存入文件或者通过网络将其写入数据库。 通用的输出操作foreachRDD()，它用来对DStream中的RDD运行任意计算。这和transform() 有些类似，都可以让我们访问任意RDD。在foreachRDD()中，可以重用我们在Spark中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如MySQL的外部数据库中。 注意： 1) 连接不能写在driver层面（序列化） 2) 如果写在foreach则每个RDD中的每一条数据都创建，得不偿失； 3) 增加foreachPartition，在分区创建（获取）。 方法一：性能低，每个RDD要连接一次 scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.ysss.bigdata.spark.streamingimport java.sql.&#123;DriverManager, PreparedStatement&#125;import org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming20_DStream_Output &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(5)) val socketDS = ssc.socketTextStream(\"localhost\", 9999) // 将数据保存到MySQL数据库中 // id, name, age socketDS.foreachRDD(rdd=&gt;&#123; rdd.foreach(data=&gt;&#123; // 解决性能问题 val datas = data.split(\",\") val id = datas(0).toInt val name = datas(1) val age = datas(2).toInt // TODO 加载数据库驱动 Class.forName(\"com.mysql.jdbc.Driver\") // TODO 建立链接和操作对象 val conn = DriverManager.getConnection( \"jdbc:mysql://linux1:3306/rdd\", \"root\",\"000000\") val sql = \"insert into user (id ,name, age) values (?, ?, ?)\" val statement: PreparedStatement = conn.prepareStatement(sql) statement.setInt(1, id) statement.setString(2, name) statement.setInt(3, age) // TODO 操作数据 statement.executeUpdate() // TODO 关闭连接 statement.close() conn.close() println(\"数据保存成功！！！\") &#125;) &#125;) ssc.start() ssc.awaitTermination() &#125;&#125; 方法二：把连接放到foreachRDD外面，但是根本执行不了，因为所有的连接对象都不支持序列化操作 scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.ysss.bigdata.spark.streamingimport java.sql.&#123;DriverManager, PreparedStatement&#125;import org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming21_DStream_Output1 &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(5)) val socketDS = ssc.socketTextStream(\"localhost\", 9999) // 将数据保存到MySQL数据库中 // id, name, age // TODO 加载数据库驱动 Class.forName(\"com.mysql.jdbc.Driver\") // TODO 建立链接和操作对象 // TODO 所有的连接对象都不支持序列化操作 val conn = DriverManager.getConnection( \"jdbc:mysql://linux1:3306/rdd\", \"root\",\"000000\") val sql = \"insert into user (id ,name, age) values (?, ?, ?)\" val statement: PreparedStatement = conn.prepareStatement(sql) socketDS.foreachRDD(rdd=&gt;&#123; // TODO RDD的方法称之为算子，存在分布式计算，需要进行闭包检测 rdd.foreach(data=&gt;&#123; // 解决性能问题 val datas = data.split(\",\") val id = datas(0).toInt val name = datas(1) val age = datas(2).toInt statement.setInt(1, id) statement.setString(2, name) statement.setInt(3, age) // TODO 操作数据 //statement.addBatch() //statement.executeBatch() statement.executeUpdate() println(\"数据保存成功！！！\") &#125;) &#125;) // SparkException : Task not serializable // TODO 关闭连接 statement.close() conn.close() ssc.start() ssc.awaitTermination() &#125;&#125; 方法三：rdd.foreachPartition,以分区为单位进行遍历 scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.ysss.bigdata.spark.streamingimport java.sql.&#123;DriverManager, PreparedStatement&#125;import org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object SparkStreaming22_DStream_Output2 &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") val ssc = new StreamingContext(sparkConf, Seconds(5)) val socketDS = ssc.socketTextStream(\"localhost\", 9999) // 将数据保存到MySQL数据库中 // id, name, age socketDS.foreachRDD(rdd=&gt;&#123; //【注意】mapPartitions和foreachPartition的区别： // 以分区为单位进行转换 =&gt; 返回 //rdd.mapPartitions() // 以分区为单位进行遍历 =&gt; 不需要返回 rdd.foreachPartition( datas =&gt; &#123; // TODO 加载数据库驱动 Class.forName(\"com.mysql.jdbc.Driver\") // TODO 建立链接和操作对象 // TODO 所有的连接对象都不支持序列化操作 val conn = DriverManager.getConnection( \"jdbc:mysql://linux1:3306/rdd\", \"root\",\"000000\") val sql = \"insert into user (id ,name, age) values (?, ?, ?)\" val statement: PreparedStatement = conn.prepareStatement(sql) // datas 其实是scala的集合，所以不存在分布式计算的概念 datas.foreach( data =&gt; &#123; // 解决性能问题 val datas = data.split(\",\") val id = datas(0).toInt val name = datas(1) val age = datas(2).toInt statement.setInt(1, id) statement.setString(2, name) statement.setInt(3, age) // TODO 操作数据 //statement.addBatch() //statement.executeBatch() statement.executeUpdate() println(\"数据保存成功！！！\") &#125; ) // TODO 关闭连接 statement.close() conn.close() &#125; ) &#125;) ssc.start() ssc.awaitTermination() &#125;&#125; 优雅关闭流式任务需要7*24小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。 使用外部文件系统来控制内部程序关闭。 把spark.streaming.stopGracefullyOnShutdown参数设置成ture,Spark会在JVM关闭时正常关闭StreamingContext,而不是立马关闭 scala1sparkConf.set(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") 案例： scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package com.ysss.bigdata.spark.streamingimport java.sql.&#123;DriverManager, PreparedStatement, ResultSet&#125;import org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext, StreamingContextState&#125;object SparkStreaming23_Stop &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"wordcount\") // TODO 配置优雅地关闭 sparkConf.set(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") val ssc = new StreamingContext(sparkConf, Seconds(5)) val socketDS: ReceiverInputDStream[String] = ssc.socketTextStream(\"localhost\", 9999) val wordDS: DStream[String] = socketDS.flatMap(_.split(\" \")) val wordToOneDS: DStream[(String, Int)] = wordDS.map( (_, 1) ) val wordToCountDS: DStream[(String, Int)] = wordToOneDS.reduceByKey(_+_) wordToCountDS.print() ssc.start() new Thread( new Runnable &#123; override def run(): Unit = &#123; // TODO SparkStreaming是可以停止。但是停止的逻辑代码的位置？ // TODO stop方法不能放置在driver的主线程中。 // TODO 直接调用ssc的stop方法是不可以的。需要循环判断sparkStreaming是否应该关闭 while ( true ) &#123; // TODO 在Driver端应该设置标记，让当前关闭线程可以访问。可以动态改变状态。 // TODO 但是Driver端的标记何时更新，由谁更新都是不确定的。 // TODO 所以一般标记不是放置在Driver端，而是在第三方软件中：redis,zk,mysql,hdfs Class.forName(\"com.mysql.jdbc.Driver\") // TODO 建立链接和操作对象 // TODO 所有的连接对象都不支持序列化操作 val conn = DriverManager.getConnection( \"jdbc:mysql://linux1:3306/rdd\", \"root\",\"000000\") val sql = \"select age from user where id = 1\" val statement: PreparedStatement = conn.prepareStatement(sql) val rs: ResultSet = statement.executeQuery() rs.next() val age: Int = rs.getInt(1) if ( age &lt;= 20 ) &#123; // TODO 判断SSC的状态 val state: StreamingContextState = ssc.getState() if ( state == StreamingContextState.ACTIVE ) &#123; println(\"SparkStreaming的环境准备关闭...\") // TODO 优雅地关闭SSC // 将现有的数据处理完再关闭就是优雅地关闭 ssc.stop(true, true) System.exit(0) &#125; &#125; Thread.sleep(1000 * 5) &#125; &#125; &#125; ).start() ssc.awaitTermination() // TODO Thread 线程停止的方式？run方法执行完毕 // 为什么不调用stop方法停止线程？因为会出现数据安全问题 // i++ =&gt; 1), 2) // new Thread().stop() &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"},{"name":"spark-streaming","slug":"spark-streaming","permalink":"https://masteryang4.github.io/tags/spark-streaming/"},{"name":"实时大数据","slug":"实时大数据","permalink":"https://masteryang4.github.io/tags/%E5%AE%9E%E6%97%B6%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"redis为什么那么快","slug":"redis为什么那么快","date":"2020-06-16T15:55:38.000Z","updated":"2020-06-16T15:57:31.422Z","comments":true,"path":"2020/06/16/redis为什么那么快/","link":"","permalink":"https://masteryang4.github.io/2020/06/16/redis%E4%B8%BA%E4%BB%80%E4%B9%88%E9%82%A3%E4%B9%88%E5%BF%AB/","excerpt":"","text":"redis是单线程的，为什么那么快 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。 数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗 使用多路I/O复用模型，非阻塞IO 使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Redis","slug":"大数据/Redis","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Redis/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据库","slug":"数据库","permalink":"https://masteryang4.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"缓存","slug":"缓存","permalink":"https://masteryang4.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://masteryang4.github.io/tags/JavaWeb/"}]},{"title":"Spark的WordCount到底有几个RDD","slug":"Spark的WordCount到底有几个RDD","date":"2020-06-16T05:54:59.000Z","updated":"2020-06-16T05:57:32.010Z","comments":true,"path":"2020/06/16/Spark的WordCount到底有几个RDD/","link":"","permalink":"https://masteryang4.github.io/2020/06/16/Spark%E7%9A%84WordCount%E5%88%B0%E5%BA%95%E6%9C%89%E5%87%A0%E4%B8%AARDD/","excerpt":"","text":"简介 本文转载自 https://blog.csdn.net/zhongqi2513/article/details/81513587 这样的一句标准的sparkcore的wordcount的代码到底能要产生几个RDD呢。相信大家对于一个标准的WordCount的代码一定不陌生： scala12345sc.textFile(\"hdfs://myha01/wc/input/words.txt\") .flatMap(_.split(\" \")) .map((_,1)) .reduceByKey(_+_) .saveAsTextFile(\"hdfs://myha01/wc/output/\") 这局代码： 1、开始使用了一个textFile用来读取数据的方法 2、中间使用了三个标准的RDD的操作算子： flatMap(_.split(&quot; &quot;)) 负责把由每一行组成的RDD按照空格切开压平成标准的由单词组成的RDD map((_,1))负责把每个单词word变成（word,1）每个单词出现一次 reduceByKey(_+_)负责把按照key相同也就是单词相同的key-value划分成一组，然后每一组做count聚合，最终就得出了输入文件中，每个单词出现了多少次。 3、最后，使用了一个saveAsTextFile的方法来存储数据 那到底这句代码中执行过程中，是不是刚好每个算子生成一个RDD呢？ 很不幸，不是的。如果需要知晓答案，最好的方式，就是翻阅参与运算的每个算子到底做了什么事情。 解析接下来是详细分析： 1、首先看sc.textFile(“hdfs://myha01/wc/input/words.txt”)：textFile方法在SparkContext类中 接着看textFile中的hadoopFile方法的实现： 通过这个代码可以得知，在hadoopFile的内部产生了第一个RDD：HadoopRDD 接着回到textFile方法： 发现，其实返回的HadoopRDD又调用了map算子，看map算子的实现： map算子的内部实现中，又创建了一个RDD，这就是第二个RDD： MapPartitionsRDD 那也就是说，textFile算子的最终返回值就是第二个RDD：MapPartitionsRDD 接着看：flatMap(_.split(“ “))算子的操作实现：flatMap算子在RDD中 所以flatMap(_.split(“ “))算子操作产生了第三个RDD：MapPartitionsRDD 接着看map((_,1))算子操作：map算子在RDD类中 map((_,1))算子的具体实现依然是简单的new MapPartitionRDD的方式生成第四个RDD：MapPartitionsRDD 接着看：reduceByKey(+)算子的具体实现：reduceByKey在PairRDDFunctions类中 跳到： 跳到： 到这个地方说明：reduceByKey算子的返回值其实是创建了第五个RDD：ShuffledRDD 接着看：saveAsTextFile(“hdfs://myha01/wc/output/“)算子的具体实现：saveAsTextFile算子在RDD类中 this.mapPartitions这句代码在调用的时候，在mapPartitions的内部，其实又创建了第六个RDD：MapPartitionRDD 接着回到：saveAsTextFile方法的实现，其实返现，最后一句话在调用中，也会生成一个RDD 这就是第七个RDD：MapPartitionRDD 到底为止，其他的地方，是没有再产生RDD的。 所以按照刚才的分析得出的最终结论是： Code1234567第一个RDD：HadoopRDD第二个RDD：MapPartitionsRDD第三个RDD：MapPartitionsRDD第四个RDD：MapPartitionsRDD第五个RDD：ShuffledRDD第六个RDD：MapPartitionRDD第七个RDD：MapPartitionRDD 其实，在执行saveAsTextFile之前，我们可以通过RDD提供的toDebugString看到这些个算子在调用的时候到底产生了多少个RDD: 望各位仁兄牢记。如果不记得，请翻阅源码。本篇文章是基于最新的Spark-2.3.1的版本 小结7个RDD，2+1+1+1+2 scala12345sc.textFile(\"hdfs://myha01/wc/input/words.txt\") .flatMap(_.split(\" \")) .map((_,1)) .reduceByKey(_+_) .saveAsTextFile(\"hdfs://myha01/wc/output/\") 算子 产生的RDD sc.textFile(&quot;hdfs://myha01/wc/input/words.txt&quot;) 第一个RDD：HadoopRDD第二个RDD：MapPartitionsRDD .flatMap(_.split(&quot; &quot;)) 第三个RDD：MapPartitionsRDD .map((_,1)) 第四个RDD：MapPartitionsRDD .reduceByKey(_+_) 第五个RDD：ShuffledRDD .saveAsTextFile(&quot;hdfs://myha01/wc/output/&quot;) 第六个RDD：MapPartitionRDD第七个RDD：MapPartitionRDD","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"},{"name":"spark-core","slug":"spark-core","permalink":"https://masteryang4.github.io/tags/spark-core/"}]},{"title":"常用排序算法总结","slug":"常用排序算法总结","date":"2020-06-15T09:31:13.000Z","updated":"2020-06-15T09:35:34.260Z","comments":true,"path":"2020/06/15/常用排序算法总结/","link":"","permalink":"https://masteryang4.github.io/2020/06/15/%E5%B8%B8%E7%94%A8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/","excerpt":"","text":"冒泡排序java1234567891011121314151617181920212223242526272829303132333435363738394041/** * 冒泡排序 时间复杂度 O(n^2) 空间复杂度O(1) */public class BubbleSort &#123; public static void bubbleSort(int[] data) &#123; System.out.println(\"开始排序\"); int arrayLength = data.length; for (int i = 0; i &lt; arrayLength - 1; i++) &#123; boolean flag = false; for (int j = 0; j &lt; arrayLength - 1 - i; j++) &#123; if(data[j] &gt; data[j + 1])&#123; int temp = data[j + 1]; data[j + 1] = data[j]; data[j] = temp; flag = true; &#125; &#125; System.out.println(java.util.Arrays.toString(data)); if (!flag) break; &#125; &#125; public static void main(String[] args) &#123; int[] data = &#123; 9, -16, 21, 23, -30, -49, 21, 30, 30 &#125;; System.out.println(\"排序之前：\\n\" + java.util.Arrays.toString(data)); bubbleSort(data); System.out.println(\"排序之后：\\n\" + java.util.Arrays.toString(data)); &#125;&#125; 快速排序java1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.ys.shuzu;/** * 标准快排 * 【注意】 * 最左边为基准数（flag）的时候，从右开始往前遍历。 */public class Kuaisupaixu &#123; public static void quicksort(int[] arr, int left, int right) &#123; if (left &gt; right) &#123; return; &#125; int flag = arr[left]; int l = left; int r = right; int temp; while (l != r) &#123; while (arr[r] &gt;= flag &amp;&amp; l &lt; r) &#123; //【重点】 r -= 1; &#125; while (arr[l] &lt;= flag &amp;&amp; l &lt; r) &#123; l += 1; &#125; temp = arr[r]; arr[r] = arr[l]; arr[l] = temp; &#125; arr[left] = arr[l]; arr[l] = flag; quicksort(arr, left, l - 1); quicksort(arr, l + 1, right); &#125; public static void main(String[] args) &#123; int[] a = &#123;9, 2, 1, 5, 4, 8, 7, 6, 1, 0&#125;; quicksort(a, 0, a.length - 1); for (int i : a) &#123; System.out.print(i + \" \"); &#125; &#125;&#125; scala123456789101112/** * 快排 * 时间复杂度:平均时间复杂度为O(nlogn) * 空间复杂度:O(logn)，因为递归栈空间的使用问题 */def quickSort(list: List[Int]): List[Int] = list match &#123; case Nil =&gt; Nil case List() =&gt; List() case head :: tail =&gt; val (left, right) = tail.partition(_ &lt; head) quickSort(left) ::: head :: quickSort(right) &#125; 归并排序核心思想：不断的将大的数组分成两个小数组，直到不能拆分为止，即形成了单个值。此时使用合并的排序思想对已经有序的数组进行合并，合并为一个大的数据，不断重复此过程，直到最终所有数据合并到一个数组为止。 java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.ys.shuzu;import java.util.Arrays;/** * 【归并排序】 * 时间复杂度nlogn（平均，最好，最坏都是这个值） * 空间复杂度n（用空间换时间，时间上和快排差不多） */public class MergeSort &#123; public static void main(String[] args) &#123; int arr[] = &#123;8, 4, 5, 7, 1, 3, 6, 2&#125;; // int temp[] = new int[arr.length]; //归并排序需要一个额外空间 mergeSort(arr, 0, arr.length - 1, temp); System.out.println(\"归并排序后=\" + Arrays.toString(arr)); &#125; //分+合方法 public static void mergeSort(int[] arr, int left, int right, int[] temp) &#123; if (left &lt; right) &#123; int mid = (left + right) / 2; //中间索引 //向左递归进行分解 mergeSort(arr, left, mid, temp); //向右递归进行分解 mergeSort(arr, mid + 1, right, temp); //合并 merge(arr, left, mid, right, temp); &#125; &#125; //合并的方法 /** * @param arr 排序的原始数组 * @param left 左边有序序列的初始索引 * @param mid 中间索引 * @param right 右边索引 * @param temp 做中转的数组 */ public static void merge(int[] arr, int left, int mid, int right, int[] temp) &#123; int i = left; // 初始化i, 左边有序序列的初始索引 int j = mid + 1; //初始化j, 右边有序序列的初始索引 int t = 0; // 指向temp数组的当前索引 //(一) //先把左右两边(有序)的数据按照规则填充到temp数组 //直到左右两边的有序序列，有一边处理完毕为止 while (i &lt;= mid &amp;&amp; j &lt;= right) &#123;//继续 //如果左边的有序序列的当前元素，小于等于右边有序序列的当前元素 //即将左边的当前元素，填充到 temp数组 //然后 t++, i++ if (arr[i] &lt;= arr[j]) &#123; temp[t] = arr[i]; t += 1; i += 1; &#125; else &#123; //反之,将右边有序序列的当前元素，填充到temp数组 temp[t] = arr[j]; t += 1; j += 1; &#125; &#125; //(二) //把有剩余数据的一边的数据依次全部填充到temp while (i &lt;= mid) &#123; //左边的有序序列还有剩余的元素，就全部填充到temp temp[t] = arr[i]; t += 1; i += 1; &#125; while (j &lt;= right) &#123; //右边的有序序列还有剩余的元素，就全部填充到temp temp[t] = arr[j]; t += 1; j += 1; &#125; //(三) //将temp数组的元素拷贝到arr //注意，并不是每次都拷贝所有 t = 0; int tempLeft = left; // //第一次合并 tempLeft = 0 , right = 1 // tempLeft = 2 right = 3 // tL=0 ri=3 //最后一次 tempLeft = 0 right = 7 while (tempLeft &lt;= right) &#123; arr[tempLeft] = temp[t]; t += 1; tempLeft += 1; &#125; &#125;&#125; scala123456789101112/** * 快排 * 时间复杂度:O(nlogn) * 空间复杂度:O(n) */def merge(left: List[Int], right: List[Int]): List[Int] = (left, right) match &#123; case (Nil, _) =&gt; right case (_, Nil) =&gt; left case (x :: xTail, y :: yTail) =&gt; if (x &lt;= y) x :: merge(xTail, right) else y :: merge(left, yTail) &#125; 二分查找scala12345678910111213141516171819/** * 二分查找 时间复杂度O(log2n);空间复杂度O(1) */ def binarySearch(arr:Array[Int],left:Int,right:Int,findVal:Int): Int=&#123; if(left&gt;right)&#123;//递归退出条件，找不到，返回-1 -1 &#125; val midIndex = (left+right)/2 if (findVal &lt; arr(midIndex))&#123;//向左递归查找 binarySearch(arr,left,midIndex-1,findVal) &#125;else if(findVal &gt; arr(midIndex))&#123;//向右递归查找 binarySearch(arr,midIndex+1,right,findVal) &#125;else&#123;//查找到，返回下标 midIndex &#125;&#125; java12345678910111213141516171819202122232425262728293031323334353637package com.ys.chazhao;/** 查找数目超过半数的值并打印，如果没有就打印0** 方法二：快速排序，中间的值就是数量为半数的值。*///&#123;1,2,3,2,2,2,5,4,2&#125;public class Banshuchazhao &#123; public static int MoreThanHalfNum_Solution(int[] array) &#123; int res = array[0], count = 1; for (int i = 1; i &lt; array.length; i++) &#123; if (array[i] == res) count++; else &#123; count--; &#125; if (count == 0) &#123; res = array[i]; count = 1; &#125; &#125; // 验证 count = 0; for (int i = 0; i &lt; array.length; i++) &#123; if (array[i] == res) count++; &#125; return count &gt; array.length / 2 ? res : 0; &#125; public static void main(String[] args) &#123; int i = MoreThanHalfNum_Solution(new int[]&#123;1, 2, 3, 2, 2, 2, 5, 4, 2&#125;); System.out.println(i); &#125;&#125; 拓展需求：当一个有序数组中，有多个相同的数值时，如何将所有的数值都查找到。 代码实现如下： scala1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/* &#123;1,8, 10, 89, 1000, 1000，1234&#125; 当一个有序数组中，有多个相同的数值时，如何将所有的数值都查找到，比如这里的 1000. //分析 1. 返回的结果是一个可变数组 ArrayBuffer 2. 在找到结果时，向左边扫描，向右边扫描 [条件] 3. 找到结果后，就加入到ArrayBuffer */ def binarySearch2(arr: Array[Int], l: Int, r: Int, findVal: Int): ArrayBuffer[Int] = &#123; //找不到条件? if (l &gt; r) &#123; return ArrayBuffer() &#125; val midIndex = (l + r) / 2 val midVal = arr(midIndex) if (midVal &gt; findVal) &#123; //向左进行递归查找 binarySearch2(arr, l, midIndex - 1, findVal) &#125; else if (midVal &lt; findVal) &#123; //向右进行递归查找 binarySearch2(arr, midIndex + 1, r, findVal) &#125; else &#123; println(\"midIndex=\" + midIndex) //定义一个可变数组 val resArr = ArrayBuffer[Int]() //向左边扫描 var temp = midIndex - 1 breakable &#123; while (true) &#123; if (temp &lt; 0 || arr(temp) != findVal) &#123; break() &#125; if (arr(temp) == findVal) &#123; resArr.append(temp) &#125; temp -= 1 &#125; &#125; //将中间这个索引加入 resArr.append(midIndex) //向右边扫描 temp = midIndex + 1 breakable &#123; while (true) &#123; if (temp &gt; arr.length - 1 || arr(temp) != findVal) &#123; break() &#125; if (arr(temp) == findVal) &#123; resArr.append(temp) &#125; temp += 1 &#125; &#125; return resArr &#125; 二叉树相关二叉树的特点 （1）树执行查找、删除、插入的时间复杂度都是O(logN) （2）遍历二叉树的方法包括前序、中序、后序 （3）非平衡树指的是根的左右两边的子节点的数量不一致 （4）在非空二叉树中，第i层的结点总数不超过 , i&gt;=1； （5）深度为h的二叉树最多有个结点(h&gt;=1)，最少有h个结点； （6）对于任意一棵二叉树，如果其叶结点数为N0，而度数为2的结点总数为N2，则N0=N2+1； 定义节点以及前序、中序、后序遍历 scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147class TreeNode(treeNo:Int)&#123; val no = treeNo var left:TreeNode = null var right:TreeNode = null //后序遍历 def postOrder():Unit=&#123; //向左递归输出左子树 if(this.left != null)&#123; this.left.postOrder &#125; //向右递归输出右子树 if (this.right != null) &#123; this.right.postOrder &#125; //输出当前节点值 printf(\"节点信息 no=%d \\n\",no) &#125; //中序遍历 def infixOrder():Unit=&#123; //向左递归输出左子树 if(this.left != null)&#123; this.left.infixOrder() &#125; //输出当前节点值 printf(\"节点信息 no=%d \\n\",no) //向右递归输出右子树 if (this.right != null) &#123; this.right.infixOrder() &#125; &#125; //前序遍历 def preOrder():Unit=&#123; //输出当前节点值 printf(\"节点信息 no=%d \\n\",no) //向左递归输出左子树 if(this.left != null)&#123; this.left.postOrder() &#125; //向右递归输出右子树 if (this.right != null) &#123; this.right.preOrder() &#125; &#125; //后序遍历查找 def postOrderSearch(no:Int): TreeNode = &#123; //向左递归输出左子树 var resNode:TreeNode = null if (this.left != null) &#123; resNode = this.left.postOrderSearch(no) &#125; if (resNode != null) &#123; return resNode &#125; if (this.right != null) &#123; resNode = this.right.postOrderSearch(no) &#125; if (resNode != null) &#123; return resNode &#125; println(\"ttt~~\") if (this.no == no) &#123; return this &#125; resNode &#125; //中序遍历查找 def infixOrderSearch(no:Int): TreeNode = &#123; var resNode : TreeNode = null //先向左递归查找 if (this.left != null) &#123; resNode = this.left.infixOrderSearch(no) &#125; if (resNode != null) &#123; return resNode &#125; println(\"yyy~~\") if (no == this.no) &#123; return this &#125; //向右递归查找 if (this.right != null) &#123; resNode = this.right.infixOrderSearch(no) &#125; return resNode &#125; //前序查找 def preOrderSearch(no:Int): TreeNode = &#123; if (no == this.no) &#123; return this &#125; //向左递归查找 var resNode : TreeNode = null if (this.left != null) &#123; resNode = this.left.preOrderSearch(no) &#125; if (resNode != null)&#123; return resNode &#125; //向右边递归查找 if (this.right != null) &#123; resNode = this.right.preOrderSearch(no) &#125; return resNode &#125; //删除节点 //删除节点规则 //1如果删除的节点是叶子节点，则删除该节点 //2如果删除的节点是非叶子节点，则删除该子树 def delNode(no:Int): Unit = &#123; //首先比较当前节点的左子节点是否为要删除的节点 if (this.left != null &amp;&amp; this.left.no == no) &#123; this.left = null return &#125; //比较当前节点的右子节点是否为要删除的节点 if (this.right != null &amp;&amp; this.right.no == no) &#123; this.right = null return &#125; //向左递归删除 if (this.left != null) &#123; this.left.delNode(no) &#125; //向右递归删除 if (this.right != null) &#123; this.right.delNode(no) &#125; &#125;&#125; 定义二叉树，前序、中序、后序遍历，前序、中序、后序查找，删除节点 scala1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class BinaryTree&#123; var root:TreeNode = null //后序遍历 def postOrder(): Unit = &#123; if (root != null)&#123; root.postOrder() &#125;else &#123; println(\"当前二叉树为空，不能遍历\")&#125;&#125; //中序遍历 def infixOrder(): Unit = &#123; if (root != null)&#123; root.infixOrder() &#125;else &#123; println(\"当前二叉树为空，不能遍历\") &#125; &#125; //前序遍历 def preOrder(): Unit = &#123; if (root != null)&#123; root.preOrder() &#125;else &#123; println(\"当前二叉树为空，不能遍历\") &#125; &#125; //后序遍历查找 def postOrderSearch(no:Int): TreeNode = &#123; if (root != null) &#123; root.postOrderSearch(no) &#125;else&#123; null &#125; &#125; //中序遍历查找 def infixOrderSeacher(no:Int): TreeNode = &#123; if (root != null) &#123; return root.infixOrderSearch(no) &#125;else &#123; return null &#125; &#125; //前序查找 def preOrderSearch(no:Int): TreeNode = &#123; if (root != null) &#123; return root.preOrderSearch(no) &#125;else&#123; //println(\"当前二叉树为空，不能查找\") return null &#125; &#125;//删除节点 def delNode(no:Int): Unit = &#123; if (root != null) &#123; //先处理一下root是不是要删除的 if (root.no == no)&#123; root = null &#125;else &#123; root.delNode(no) &#125; &#125; &#125;","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://masteryang4.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://masteryang4.github.io/tags/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"springboot精简教程","slug":"springboot精简教程","date":"2020-06-14T14:55:58.000Z","updated":"2020-06-14T15:02:26.437Z","comments":true,"path":"2020/06/14/springboot精简教程/","link":"","permalink":"https://masteryang4.github.io/2020/06/14/springboot%E7%B2%BE%E7%AE%80%E6%95%99%E7%A8%8B/","excerpt":"","text":"SpringBoot2.1 Spring分布式架构 2.2 SpringBoot 概述Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。 该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。 通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者。 2.3 为什么要使用SpringBoot说到为什么使用Spring Boot, 就不得不提到Spring框架的前世今生 Spring框架由于其繁琐的配置，一度被人认为“配置地狱”，各种XML、Annotation配置混合使用，让人眼花缭乱，而且如果出错了也很难找出原因。 通过SpringMVC框架部署和发布web程序，需要和系统外服务器进行关联，操作繁琐不方便。 Spring Boot是由Spring官方推出的一个新框架，对Spring进行了高度封装，是Spring未来的发展方向。使用SpringBoot框架后，可以帮助开发者快速搭建Spring框架，也可以帮助开发者快速启动一个Web服务，无须依赖外部Servlet容器，使编码变得简单，使配置变得简单，使部署变得简单，使监控变得简单。 2.4 Spring 前世今生1) Spring1.x 时代 在Spring1.x时代，都是通过xml文件配置bean 随着项目的不断扩大，需要将xml配置分放到不同的配置文件中 需要频繁的在java类和xml配置文件中切换。 2) Spring2.x时代 随着JDK 1.5带来的注解支持，Spring2.x可以使用注解对Bean进行申明和注入，大大的减少了xml配置文件，同时也大大简化了项目的开发。 那么，问题来了，究竟是应该使用xml还是注解呢？ 最佳实践： 应用的基本配置用xml，比如：数据源、资源文件等； 业务开发用注解，比如：Service中注入bean等； 3) Spring3.x到Spring4.x 从Spring3.x开始提供了Java配置方式，使用Java配置方式可以更好的理解你配置的Bean，现在我们就处于这个时代，并且Spring4.x和Springboot都推荐使用java配置的式。 java1234567891011121314151617181920212223//Spring 1.X//使用基本的框架类及配置文件（.xml）实现对象的声明及对象关系的整合。org.springframework.core.io.ClassPathResourceorg.springframework.beans.factory.xml.XmlBeanFactoryorg.springframework.context.support.ClassPathXmlApplicationContext //Spring 2.X//使用注解代替配置文件中对象的声明。简化配置。org.springframework.stereotype.@Componentorg.springframework.stereotype.@Controllerorg.springframework.stereotype.@Serviceorg.springframework.stereotype.@Repositoryorg.springframework.stereotype.@Scopeorg.springframework.beans.factory.annotation.@Autowired //Spring 3.X//使用更强大的注解完全代替配置文件。org.springframework.context.annotation.AnnotationConfigApplicationContextorg.springframework.context.annotation.@Configurationorg.springframework.context.annotation.@Beanorg.springframework.context.annotation.@Valueorg.springframework.context.annotation.@Import //Spring 4.X//使用条件注解强化之前版本的注解。org.springframework.context.annotation.@Conditional 2.5 自动创建一个SpringBoot项目1) 在Idea中new→Module→Spring Initializr 2) 给工程命名、设置包名等，其他默认即可 3) 选择工程的版本 4) 点击Next ，给工程命名，然后点击Finish 2.6 手动创建一个SpringBoot 项目2.6.1 创建Maven项目2.6.2 集成Spring Boot框架 修改pom.xml文件，增加Spring Boot框架的依赖关系及对Web环境的支持。 xml12345678910111213141516&lt;project&gt; ... &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;/parent&gt; ... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; ...&lt;/project&gt; Spring Boot版本为官方最新正式版2.2.2.RELEASE 以往的项目中，所有类库的依赖关系都需要我们自己导入到pom.xml文件中，但是SpringBoot项目增加spring-boot-starter-web依赖后，会自动加载web环境配置相关依赖(SpringMVC,Tomcat)，简化了我们的操作。 spring-boot-starter-parent：继承Spring Boot的相关参数 spring-boot-starter-xxx：代表一个Spring Boot模块 spring-boot-starter-web：代表Web模块，在这个模块中包含了许多依赖的JAR包 扩展:修改一下Maven编译插件的版本 xml12345&lt;properties&gt; &lt;!-- 设置Maven编译插件的版本 SpringBoot高版本用的Maven插件版本比较 高，STS没支持到，需手动指定 --&gt; &lt;maven-jar-plugin.version&gt;3.1.1&lt;/maven-jar-plugin.version&gt;&lt;/properties&gt; 2.6.3 增加程序代码 在src/main/java目录中增加类com.atguigu.springboot.SpringBootSelfApplication，并增加相应代码。 java1234567891011package com.atguigu.springboot;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class SpringBootSelfApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootSelfApplication.class, args); &#125;&#125; SpringBoot项目中都会有一个以Application结尾的应用类，然后有一个标准的Java入口方法main方法。通过这个方法启动SpringBoot项目，方法中无需放入任何业务逻辑。 @SpringBootApplication注解是Spring Boot核心注解 右键点击项目或项目中的SpringBootSelfApplication类, 选择菜单Run as Spring BootApp，启动SpringBoot项目. 2.6.4 集成Tomcat服务器 SpringBoot内置了Tomcat，当增加Web依赖后执行main方法，等同于启动Tomcat服务器,默认端口号为8080。如果想具体指定,通过server.port来指定 默认情况下SpringBoot启动后，默认的context-path的值为/，从浏览器端访问项目时，,不需要加项目名，直接通过http://localhost:8080/请求名 来访问，如果想具体指定，通过server.servlet.context-path来指定 例如:在src/main/resources/目录中增加application.properties文件。 properties12server.servlet.context-path=/server.port=80 SpringBoot会自动读取src/main/resources/路径或着src/main/resources/config路径中的application.properties文件或application.yml文件。 2.6.5 为什么还会有配置文件Spring Boot我们称之为微框架，这里的“微”不是小和少的意思，而是“简”的意思，简单，简洁。 项目中大部分的基础配置由Spring Boot框架帮我们自动集成，简化了我们的配置，但是框架自身为了扩展性，依然需要提供配置文件。 上面的代码中只是简单的应用了Spring Boot框架，但是我们真正要做的是将SpringBoot应用到项目中，所以接下来我们增加对SpringMVC框架，Mybatis框架的集成。 2.7 SpringBoot 集成 Spring &amp; Spring Web MVC 基本的Spring Boot环境已经构建好了，现在需要配置Spring框架及SpringMVC框架的业务环境 2.7.1 @ComponentScan注解 通过@ComponentScan注解指定扫描的包 java1234567891011121314package com.atguigu.springboot;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.ComponentScan;@ComponentScan(basePackages=\"com.atguigu\")@SpringBootApplicationpublic class SpringBootSelfApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootSelfApplication.class, args); &#125;&#125; 默认扫描 默认扫描当前包com.atguigu.springboot和子包com.atguigu.springboot.* 如果还需要扫描其他的包，那么需要增加@ComponentScan注解,指定包名进行扫描。 2.7.2 增加控制器代码在src/main/java目录中增加类com.atguigu.springboot.controller.UserController，并增加相应代码。 java1234567891011121314151617181920package com.atguigu.springboot.controller;import java.util.HashMap;import java.util.Map;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;@Controllerpublic class UserController &#123; @ResponseBody //返回Json数据 @RequestMapping(\"/getAllUser\") //指定请求URL public Object getAllUser() &#123; Map&lt;String,String&gt; map = new HashMap&lt;&gt;(); map.put(\"username\", \"张三\"); return map; &#125;&#125; 2.7.3 执行main方法启动应用访问路径http://localhost:8080[/应用路径名称]/ getAllUser页面打印JSON字符串即可 2.7.4 @Controller和@RestController区别@RestController等同于@Controller + @ResponseBody，所以上面的代码可以变为： java1234567891011121314151617181920package com.atguigu.springboot.controller;import java.util.HashMap;import java.util.Map;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class UserController &#123; @RequestMapping(\"/getAllUser\") //指定请求URL public Object getAllUser() &#123; Map&lt;String,String&gt; map = new HashMap&lt;&gt;(); map.put(\"username\", \"张三\"); return map; &#125;&#125; 2.7.5 页面跳转[了解]1) 如果需要转发跳转Jsp页面,可参考如下步骤 在pom.xml中加入如下依赖 xml12345678&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt;&lt;/dependency&gt; 将jsp页面存放在src/main/webapp目录下,Springboot默认从该目录下查找jsp页面 在application.properties文件中配置： properties12spring.mvc.view.prefix=/ spring.mvc.view.suffix=.jsp 2) 如需要进行重定向，可参考如下步骤 在请求处理方法中的返回值前面加上”redirect:” 重定向的页面同样存放在src/main/webapp下 2.8 SpringBoot集成通用Mapper2.8.1 通用Mapper简介通用mapper可以极大的方便开发人员进行CRUD操作，提供极其方便的单表增删改查。 一句话简单说，它就是个辅助mybatis极简单表开发的组件。它不是为了替代mybatis，而是让mybatis的开发更方便。 2.8.2 集成通用Mapper1) 在pom.xml中加入通用Mapper的starter xml12345678910&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt; 2) 添加持久层代码 通用Mapper提供了Mapper接口，该接口中提供了常用的CRUD方法. 用户可以自己定义自己的Mapper接口，继承通用Mapper提供的Mapper接口， java1234567package com.atguigu.springboot.mapper;import com.atguigu.springboot.beans.User;import tk.mybatis.mapper.common.Mapper;public interface UserMapper extends Mapper&lt;User&gt; &#123;&#125; 3) 在src/main/resources下创建application.yml文件,配置数据源 yml1234567# jdbc配置spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/bigdata?serverTimezone=UTC username: root password: 1234 2.9 整合测试2.9.1 增加业务层代码1) 增加业务层接口 java123456789101112package com.atguigu.springboot.service;import java.util.List;import com.atguigu.springboot.beans.User;public interface UserService &#123; /** * 查询所有的用户 */ public List&lt;User&gt; selectAllUser();&#125; 2) 增加业务层实现类 java123456789101112131415161718192021package com.atguigu.springboot.service;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import com.atguigu.springboot.beans.User;import com.atguigu.springboot.mapper.UserMapper;@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired private UserMapper userMapper; @Override public List&lt;User&gt; selectAllUser() &#123; return userMapper.selectAll(); &#125;&#125; 2.9.2 增加控制层方法java1234567891011121314151617181920package com.atguigu.springboot.controller;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import com.atguigu.springboot.service.UserService;@RestControllerpublic class UserController &#123; @Autowired private UserService userService; @RequestMapping(\"/getAllUser\") //指定请求URL public Object getAllUser() &#123; return userService.selectAllUser(); &#125;&#125; 2.9.3 扫描Mapperjava1234567@MapperScan(basePackages = \"com.atguigu.springboot.mapper\")@SpringBootApplicationpublic class SpringBootSelfApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootSelfApplication.class, args); &#125;&#125; 2.9.4 测试访问路径http://localhost:8080[/应用路径名称]/ getAllUser页面打印JSON字符串即可 2.10 Restful风格URL2.10.1 REST 简介REST（Representational State Transfer）又被称作表现层状态转换。它涉及到三个重要名词： 资源 所谓资源简单讲就是服务所能提供的数据，可以是实体数据也可是媒体类型，图片、PDF、文本等 表现层 何为变现层？简单说就是将数据以某种方式展现给用户，或者给客户返回一张图片等等动作称之为表现，通常是已JSON或XML形式展现数据 状态转换 状态转换就是对数据进行一系列的操作，因为资源本身并非一尘不变，随着需求的变化而变化。一个资源可能会随着需求的变化而经历一个资源创建、修改、查询、删除等过程，REST风格正是基于HTTP协议运行的，HTTP协议又被称为无状态协议，所以资源的变化需要在服务端完成， 简单用一句话概括就是：REST风格使用URL定位资源，用HTTP动词（GET,POST,DELETE,PUT）描述操作。 2.10.1 REST 规定 GET请求 获取资源 例如：/emp/1 获取id=1的员工信息 POST请求 添加资源 例如：/emp 添加员工信息 PUT请求 更新资源 例如：/emp/1 更新id=1的员工信息 DELETE请求 删除资源 例如：/emp/1 删除id=1的员工信息 2.10.2 Resulful风格URL 和普通URL对比普通URL:localhost:8888/SpringBootSelf/selectUser?id=1001&amp;username=zhangsan Restful: localhost:8888/SpringBootSelf/selectUser/1001/zhangsan 2.10.3 如何在后台处理Restful风格URL中的参数 客户端的URL:localhost:8888/SpringBootSelf/selectUser/1001 在@RequestMapping注解中使用 {} 占位符对应实际URL中的参数 java1234@RequestMapping(\"/selectUser/&#123;ids&#125;\")public User selectUser(@PathVariable(\"ids\") Integer id ) &#123; return userService.doSelectUser(id); &#125; 在方法中使用@PathVariable注解指定将占位符对应的URL中的参数值赋值给方法的形参. java1234@RequestMapping(\"/selectUser/&#123;ids&#125;\")public User selectUser(@PathVariable(\"ids\") Integer id ) &#123; return userService.doSelectUser(id); &#125; 2.10.4 转换PUT请求和DELETE请求[了解] PUT请求和DELETE请求需要通过POST请求来转换 发送POST请求我们需要在form表单中发送，所以我们需要使用SpringBoot的模板 转换的步骤： 1、添加Thymeleaf模块 xml1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 添加该模块后在main/resources目录下创建templates目录 2、在templates目录下创建index.html页面，添加form表单，请求方式设置为post，表单中设置一个隐藏域，name属性值为_method,value值为put（转换为PUT请求时的值）或delete（转换为DELETE请求时的值） html1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;a href=\"/getEmp/1\"&gt;获取员工&lt;/a&gt; &lt;form action=\"/emp/4\" method=\"post\"&gt; &lt;input type=\"hidden\" name=\"_method\" value=\"delete\"&gt;&lt;br&gt; &lt;input type=\"submit\" value=\"删除员工\"&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 3、配置过滤器 创建一个类继承HiddenHttpMethodFilter 在类上添加@WebFilter java1234567import org.springframework.web.filter.HiddenHttpMethodFilter;import javax.servlet.annotation.WebFilter;@WebFilterpublic class MyFilter extends HiddenHttpMethodFilter &#123;&#125; 4、在启动类上添加@ServletComponentScan注解 java12345678@ServletComponentScan@MapperScan(basePackages = \"com.atguigu.springboot.mapper\")@SpringBootApplicationpublic class SpringBootSelfApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootSelfApplication.class, args); &#125;&#125;","categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://masteryang4.github.io/categories/JavaWeb/"},{"name":"springboot","slug":"JavaWeb/springboot","permalink":"https://masteryang4.github.io/categories/JavaWeb/springboot/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://masteryang4.github.io/tags/JavaWeb/"},{"name":"springboot","slug":"springboot","permalink":"https://masteryang4.github.io/tags/springboot/"}]},{"title":"spring精简教程","slug":"spring精简教程","date":"2020-06-14T14:55:47.000Z","updated":"2020-06-14T15:01:10.279Z","comments":true,"path":"2020/06/14/spring精简教程/","link":"","permalink":"https://masteryang4.github.io/2020/06/14/spring%E7%B2%BE%E7%AE%80%E6%95%99%E7%A8%8B/","excerpt":"","text":"简单了解框架框架，即framework。其实就是某种应用的半成品，就是一组组件，供你选用完成你自己的系统。简单说就是使用别人搭好的舞台，你来做表演。而且，框架一般是成熟的，不断升级的软件。 框架是对特定应用领域中的应用系统的部分设计和实现的整体结构。 因为软件系统发展到今天已经很复杂了，特别是服务器端软件，涉及到的知识，内容，问题太多。在某些方面使用别人成熟的框架，就相当于让别人帮你完成一些基础工作，你只需要集中精力完成系统的业务逻辑设计。而且框架一般是成熟，稳健的，他可以处理系统很多细节问题，比如，事务处理，安全性，数据流控制等问题。还有框架一般都经过很多人使用，所以结构很好，所以扩展性也很好，而且它是不断升级的，你可以直接享受别人升级代码带来的好处。 第1章 Spring1.1 Spring 概述1) Spring是一个开源框架 2) Spring为简化企业级开发而生，使用Spring，JavaBean就可以实现很多以前要靠EJB才能实现的功能。同样的功能，在EJB中要通过繁琐的配置和复杂的代码才能够实现，而在Spring中却非常的优雅和简洁。 3) Spring是一个IOC(DI)和AOP容器框架。 4) Spring的优良特性 ① 非侵入式：基于Spring开发的应用中的对象可以不依赖于Spring的API ② 依赖注入：DI——Dependency Injection，反转控制(IOC)最经典的实现。 ③ 面向切面编程：Aspect Oriented Programming——AOP ④ 容器：Spring是一个容器，因为它包含并且管理应用对象的生命周期 ⑤ 组件化：Spring实现了使用简单的组件配置组合成一个复杂的应用。在 Spring 中可以使用XML和Java注解组合这些对象。 ⑥ 一站式：在IOC和AOP的基础上可以整合各种企业应用的开源框架和优秀的第三方类库（实际上Spring自身也提供了表述层的SpringMVC和持久层的Spring JDBC）。 5) Spring模块 1.2 Spring HelloWorld1) 创建一个Maven版的Java工程 2) 在pom.xml中加入对Spring的依赖 xml12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.0.0.RELEASE&lt;/version&gt;&lt;/dependency&gt; 3) 创建Spring的核心配置文件 File-&gt;New-&gt;Spring Bean Configuration File 为文件取名字 例如：applicationContext.xml xml1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt;&lt;/beans&gt; 4) 编写组件 创建控制层组件 java12345678910package com.ys.spring.controller;/** * 控制层组件 处理客户端的请求，给客户端响应 */import com.ys.spring.service.UserService;public class UserController &#123; public void listAllUsers() &#123; &#125;&#125; 创建业务层组件接口 java1234567package com.ys.spring.service;/** * 业务层组件 处理业务逻辑 */public interface UserService &#123; public void doGetAllUser();&#125; 创建业务层组件实现类 java12345678910package com.ys.spring.service;import com.ys.spring.dao.UserDao;public class UserServiceImpl implements UserService&#123; @Override public void doGetAllUser() &#123; &#125;&#125; 创建持久层组件接口 java123456789package com.ys.spring.dao;/** * 持久层组件 负责数据库的CRUD操作 * */public interface UserDao &#123; public void selectAllUsers();&#125; 创建持久层组件实现类 java123456789package com.ys.spring.dao;public class UserDaoJdbcImpl implements UserDao &#123; @Override public void selectAllUsers() &#123; System.out.println(\"UserDaoJdbcImpl selectAllUsers Success .....\"); &#125;&#125; 在spring的核心配置文件中管理Bean xml123456789101112131415161718&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 管理组件 --&gt; &lt;!-- bean: 对应一个被Spring管理的组件对象 id: bean的唯一标识 class: 组件对象对应的类的全类名 --&gt; &lt;bean id=\"userController\" class=\"com.ys.spring.controller.UserController\"&gt; &lt;/bean&gt; &lt;bean id=\"userServiceImpl\" class=\"com.ys.spring.service.UserServiceImpl\"&gt; &lt;/bean&gt; &lt;bean id=\"userDaoJdbcImpl\" class=\"com.ys.spring.dao.UserDaoJdbcImpl\"&gt;&lt;/bean&gt;&lt;/beans&gt; 编写测试类 java123456789101112131415161718192021222324252627package com.ys.spring.test;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.ys.spring.controller.UserController;import com.ys.spring.dao.UserDao;import com.ys.spring.service.UserService;public class TestSpring &#123; @Test public void testSpringXML() &#123; //1. 先创建Spring的容器对象 ApplicationContext ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); //2. 从Spring的容器中获取Bean对象 UserController uc = ctx.getBean(\"userController\", UserController.class); System.out.println(\"uc:\" + uc); UserService us = ctx.getBean(\"userServiceImpl\",UserService.class); System.out.println(\"us:\" + us ); UserDao ud = ctx.getBean(\"userDaoJdbcImpl\",UserDao.class); System.out.println(\"ud:\" + ud ); &#125;&#125; 5) 组件装配 在控制层组件中定义业务层组件类型的属性 java123456789101112131415161718package com.ys.spring.controller;/** * 控制层组件 处理客户端的请求，给客户端响应 */import com.ys.spring.service.UserService;public class UserController &#123; private UserService userService ; public void setUserService(UserService userService) &#123; this.userService = userService; &#125; public void listAllUsers() &#123; userService.doGetAllUser(); &#125;&#125; 在业务层组件中定义持久层组件类型的属性 java12345678910111213141516package com.ys.spring.service;import com.ys.spring.dao.UserDao;public class UserServiceImpl implements UserService&#123; private UserDao userDao ; public void setUserDao(UserDao userDao) &#123; this.userDao = userDao; &#125; @Override public void doGetAllUser() &#123; userDao.selectAllUsers(); &#125;&#125; 在Spring的核心配置文件中完成组件装配 12345678910111213141516171819202122&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 管理组件 --&gt; &lt;!-- bean: 对应一个被Spring管理的组件对象 id: bean的唯一标识 class: 组件对象对应的类的全类名 --&gt; &lt;bean id=\"userController\" class=\"com.ys.spring.controller.UserController\"&gt; &lt;!-- 给属性注入值 --&gt; &lt;property name=\"userService\" ref=\"userServiceImpl\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"userServiceImpl\" class=\"com.ys.spring.service.UserServiceImpl\"&gt; &lt;property name=\"userDao\" ref=\"userDaoJdbcImpl\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"userDaoJdbcImpl\" class=\"com.ys.spring.dao.UserDaoJdbcImpl\"&gt;&lt;/bean&gt;&lt;/beans&gt; 测试控制层 业务层 持久层的调用 java1234567891011121314151617181920212223242526272829package com.ys.spring.test;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.ys.spring.controller.UserController;import com.ys.spring.dao.UserDao;import com.ys.spring.service.UserService;public class TestSpring &#123; @Test public void testSpringXML() &#123; //1. 先创建Spring的容器对象 ApplicationContext ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); //2. 从Spring的容器中获取Bean对象 UserController uc = ctx.getBean(\"userController\", UserController.class); System.out.println(\"uc:\" + uc); UserService us = ctx.getBean(\"userServiceImpl\",UserService.class); System.out.println(\"us:\" + us ); UserDao ud = ctx.getBean(\"userDaoJdbcImpl\",UserDao.class); System.out.println(\"ud:\" + ud ); uc.listAllUsers(); &#125;&#125; 1.3 基于注解开发Spring应用1.3.1 常用注解标识组件1) 普通组件： @Component 标识一个受Spring IOC容器管理的组件 2) 持久化层组件： @Repository 标识一个受Spring IOC容器管理的持久化层组件 3) 业务逻辑层组件： @Service 标识一个受Spring IOC容器管理的业务逻辑层组件 4) 表述层控制器组件： @Controller 标识一个受Spring IOC容器管理的表述层控制器组件 1.3.2 组件命名规则1) 默认情况：使用组件的简单类名首字母小写后得到的字符串作为bean的id 2) 使用组件注解的value属性指定bean的id 3) 注意：事实上Spring并没有能力识别一个组件到底是不是它所标记的类型，即使将@Respository注解用在一个表述层控制器组件上面也不会产生任何错误，所以@Respository、@Service、@Controller这几个注解仅仅是为了让开发人员自己 明确当前的组件扮演的角色。 1.3.3 Spring HelloWorld 注解版1) 在控制层 业务层 持久层组件标注对应的注解 在控制层组件标注注解 java123@Controllerpublic class UserController &#123;&#125; 在业务层组件标注注解 java123@Servicepublic class UserServiceImpl implements UserService&#123;&#125; 在持久层组件标注注解 java123@Repositorypublic class UserDaoJdbcImpl implements UserDao &#123;&#125; 2) 在Spring的核心配置文件中开启组件扫描 首先在xml文件中的namespace视图下勾选context名称空间 在Spring的核心配置文件中开启组件扫描 xml12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd\"&gt; &lt;!-- 组件扫描 base-package: 基本包 Spring会扫描通过base-package指定的包下以及子包下的组件，将带有Spring相关 注解的类管理到IOC容器中。 --&gt; &lt;context:component-scan base-package=\"com.ys.spring\"&gt;&lt;/context:component-scan&gt; &lt;/beans&gt; 3) 编写测试类 java1234567891011121314151617181920212223242526package com.ys.spring.test;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.ys.spring.controller.UserController;import com.ys.spring.dao.UserDao;import com.ys.spring.service.UserService;public class TestSpring &#123; @Test public void testSpringXML() &#123; //1. 先创建Spring的容器对象 ApplicationContext ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); //2. 从Spring的容器中获取Bean对象 UserController uc = ctx.getBean(\"userController\", UserController.class); System.out.println(\"uc:\" + uc); UserService us = ctx.getBean(\"userServiceImpl\",UserService.class); System.out.println(\"us:\" + us ); UserDao ud = ctx.getBean(\"userDaoJdbcImpl\",UserDao.class); System.out.println(\"ud:\" + ud ); &#125;&#125; 1.3.4 @Autowired注解1) @Autowired的工作机制 首先会通过当前被装配的属性的类型到IOC容器中去匹配对应的Bean对象,如果能唯一确定一个bean对象，则装配成功 当通过当前被装配的属性的类型匹配在IOC容器中匹配到多个对应的Bean对象时，会再使用当前被装配的属性的名字与匹配到的Bean对象的id值再进行唯一确定，如果能确定唯一一个，则装配成功，否则，抛出异常 expected single matching bean but found 2:userDaoJdbcImpl,userDaoMyBatisImpl 如果被装配的属性在IOC容器中匹配不到任何一个Bean对象，也会抛出异常 expected at least 1 bean which qualifies as autowire candidate forthis dependency. Dependency annotations: @org.springframework.beans.factory.annotation.Autowired(required=true)} 如果匹配到多个Bean的情况，并且通过属性名也无法唯一确定一个Bean的时候，可以手动通过@Qualifier注解来具体指定装配哪个Bean对象. @Autowired 注解中required的属性默认是true，表示属性必须被装配，可以改为false，表示可选.也就是有就装配，没有就不装配. @Autowired 和 @Qualifier 注解可以加在属性上，也可以加在方法上。 1.3.5 基于注解装配1) 在 控制层 和 业务层分别定义需要被装配的组件类型的属性,并在属性上标注注解 在控制层中定义业务层类型的属性 和 相关方法 java123456789@Controllerpublic class UserController &#123; @Autowired private UserService userService ; public void listAllUsers() &#123; userService.doGetAllUser(); &#125;&#125; 在业务层中定义持久层类型的属性 和相关方法 java123456789101112131415161718package com.ys.spring.service;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.stereotype.Service;import com.ys.spring.dao.UserDao;@Servicepublic class UserServiceImpl implements UserService&#123; @Autowired private UserDao userDao ; @Override public void doGetAllUser() &#123; userDao.selectAllUsers(); &#125;&#125; 在持久层中定义相关方法 java12345678@Repositorypublic class UserDaoJdbcImpl implements UserDao &#123; @Override public void selectAllUsers() &#123; System.out.println(\"UserDaoJdbcImpl selectAllUsers Success .....\"); &#125;&#125; 2) 在测试方法中测试 控制层 业务层 持久层的调用 java12345678910111213141516171819202122232425262728package com.ys.spring.test;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.ys.spring.controller.UserController;import com.ys.spring.dao.UserDao;import com.ys.spring.service.UserService;public class TestSpring &#123; @Test public void testSpringXML() &#123; //1. 先创建Spring的容器对象 ApplicationContext ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); //2. 从Spring的容器中获取Bean对象 UserController uc = ctx.getBean(\"userController\", UserController.class); System.out.println(\"uc:\" + uc); UserService us = ctx.getBean(\"userServiceImpl\",UserService.class); System.out.println(\"us:\" + us ); UserDao ud = ctx.getBean(\"userDaoJdbcImpl\",UserDao.class); System.out.println(\"ud:\" + ud ); uc.listAllUsers(); //【注意】id首字母要小写 &#125;&#125;","categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://masteryang4.github.io/categories/JavaWeb/"},{"name":"spring","slug":"JavaWeb/spring","permalink":"https://masteryang4.github.io/categories/JavaWeb/spring/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://masteryang4.github.io/tags/JavaWeb/"},{"name":"spring","slug":"spring","permalink":"https://masteryang4.github.io/tags/spring/"}]},{"title":"解决Github连不上、ping不通的问题","slug":"解决Github连不上、ping不通的问题","date":"2020-06-07T15:59:15.000Z","updated":"2020-06-07T16:10:43.018Z","comments":true,"path":"2020/06/07/解决Github连不上、ping不通的问题/","link":"","permalink":"https://masteryang4.github.io/2020/06/07/%E8%A7%A3%E5%86%B3Github%E8%BF%9E%E4%B8%8D%E4%B8%8A%E3%80%81ping%E4%B8%8D%E9%80%9A%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"修改host即可Github连不上、ping不通、git clone特别慢等现象，通常是因为github.global.ssl.fastly.net域名被限制了。 因此，只要找到你当前线路最快的ip，修改一下host就能提速。 步骤一、在网站 https://www.ipaddress.com 分别找这两个域名所对应的最快的ip地址 Code12github.global.ssl.fastly.netgithub.com 二、在C:\\Windows\\System32\\drivers\\etc\\hosts里面做映射 注意要以自己查到的这两个域名所对应的最快IP地址为准。 在hosts文件最下方添加即可。 示例： Code12199.232.69.194 github.global.ssl.fastly.net140.82.114.4 github.com 保存修改后，再登陆一般就木有问题了。","categories":[{"name":"Git&Github","slug":"Git-Github","permalink":"https://masteryang4.github.io/categories/Git-Github/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"Git&Github","slug":"Git-Github","permalink":"https://masteryang4.github.io/tags/Git-Github/"},{"name":"bug解决","slug":"bug解决","permalink":"https://masteryang4.github.io/tags/bug%E8%A7%A3%E5%86%B3/"}]},{"title":"Redis常见问题及扩展","slug":"Redis常见问题及扩展","date":"2020-06-07T15:04:34.000Z","updated":"2020-06-07T15:08:37.885Z","comments":true,"path":"2020/06/07/Redis常见问题及扩展/","link":"","permalink":"https://masteryang4.github.io/2020/06/07/Redis%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%8F%8A%E6%89%A9%E5%B1%95/","excerpt":"","text":"缓存穿透、缓存雪崩、缓存击穿1、缓存穿透是指查询一个一定不存在的数据。由于缓存命不中时会去查询数据库，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。 解决方案： 是将空对象也缓存起来，并给它设置一个很短的过期时间，最长不超过5分钟 采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力 布隆过滤器(bloom filter)： https://zhuanlan.zhihu.com/p/72378274 2、如果缓存集中在一段时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上，就会造成缓存雪崩。 解决方案： 尽量让失效的时间点不分布在同一个时间点 3、缓存击穿，是指一个key非常热点，在不停的扛着大并发，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞。 解决方案： 可以设置key永不过期 哨兵模式主从复制中反客为主的自动版，如果主机Down掉，哨兵会从从机中选择一台作为主机，并将它设置为其他从机的主机，而且如果原来的主机再次启动的话也会成为从机。 数据类型 类型 描述 string 字符串 list 可以重复的集合 set 不可以重复的集合 hash 类似于Map&lt;String,String&gt; zset(sorted set） 带分数的set 持久化1、RDB持久化： 在指定的时间间隔内持久化 服务shutdown会自动持久化 输入bgsave也会持久化 2、AOF : 以日志形式记录每个更新操作 Redis重新启动时读取这个文件，重新执行新建、修改数据的命令恢复数据。 保存策略： 推荐（并且也是默认）的措施为每秒持久化一次，这种策略可以兼顾速度和安全性。 缺点： 比起RDB占用更多的磁盘空间 恢复备份速度要慢 每次读写都同步的话，有一定的性能压力 存在个别Bug，造成恢复不能 选择策略： 官方推荐： 如果对数据不敏感，可以选单独用RDB；不建议单独用AOF，因为可能出现Bug;如果只是做纯内存缓存，可以都不用。 悲观锁、乐观锁悲观锁： 执行操作前假设当前的操作肯定（或有很大几率）会被打断（悲观）。基于这个假设，我们在做操作前就会把相关资源锁定，不允许自己执行期间有其他操作干扰。 乐观锁： 执行操作前假设当前操作不会被打断（乐观）。基于这个假设，我们在做操作前不会锁定资源，万一发生了其他操作的干扰，那么本次操作将被放弃。 Redis使用的就是乐观锁。 推荐参考： https://zhuanlan.zhihu.com/p/89620471","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Redis","slug":"大数据/Redis","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Redis/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Redis","slug":"Redis","permalink":"https://masteryang4.github.io/tags/Redis/"},{"name":"布隆过滤器","slug":"布隆过滤器","permalink":"https://masteryang4.github.io/tags/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"name":"缓存","slug":"缓存","permalink":"https://masteryang4.github.io/tags/%E7%BC%93%E5%AD%98/"}]},{"title":"大数据常用框架源码编译","slug":"大数据常用框架源码编译","date":"2020-06-07T12:28:17.000Z","updated":"2020-06-07T12:33:39.199Z","comments":true,"path":"2020/06/07/大数据常用框架源码编译/","link":"","permalink":"https://masteryang4.github.io/2020/06/07/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/","excerpt":"","text":"源码编译通用步骤一、搭建编译环境一般编译环境为Linux + JDK + Maven，有些框架可能需要别的环境支持，一般都会注明，在后面细说。以下教程都是基于Linux + JDK8环境编译。 Linux和JDK环境这里不再赘述 MAVEN环境搭建 bash123456789101112#1. 从apache网站拉取tar包并解压MVNTAR=$(curl http://maven.apache.org/download.cgi | grep -E \"&gt;apache-maven-.*bin\\.tar\\.gz&lt;\" | sed 's/.*a href=\"\\(.*\\)\".*/\\1/g')curl $MVNTAR | tar zxC /opt/modulemv /opt/module/$(basename $MVNTAR | cut -d - -f 1,2,3) /opt/module/maven#2. 配置环境变量vim /etc/profile.d/my_env.sh#添加如下内容并保存退出export M2_HOME=/opt/module/mavenexport MAVEN_HOME=/opt/module/mavenexport PATH=$&#123;MAVEN_HOME&#125;/bin:$&#123;PATH&#125; 完成后重启Xshell会话 二、下载源码下载你想要编译的框架的源码。一般源码下载有两种方式： 想编译的版本已经发布release版，但是由于兼容性原因需要重新编译。这种情况直接从框架官网下载源码包并解压即可。 想测试框架还没发布的最新功能。此时从git托管服务器拉取最新源码，这时，我们需要git环境 Git环境搭建 bash12sudo yum install -y epel-releasesudo yum install -y git 到 https://git-wip-us.apache.org/repos/asf 查看想要编译的框架的git服务器，拉取源码(以Hive为例) bash123456#新建源码存储目录mkdir -p /opt/software/sourcecd /opt/software/source#拉取源码git clone https://git-wip-us.apache.org/repos/asf/hive.git 进入拉取的源码目录，切换到自己想要的分支 bash123456#查看所有本地和远程分支，这里也可以切换到之前版本的分支cd hivegit branch -a#新建本地分支同步远程分支git checkout -b 3.1 origin/branch-3.1 如果想切换到特定release的源码，使用git tag命令 bash12345#查看所有taggit tag#切换到想要的tag，这里以release-3.1.2为例git checkout rel/release-3.1.2 三、查看编译说明一般来说，源码根目录都会有building.txt之类的文件作为编译说明，如果没有找到，也可以去官网查看编译说明。说明里一般都会注明前置要求，例如一些额外的编译环境要求等。 Hive没有前置要求，我们直接进入第四步 四、对源码做必要修改一般我们只有在框架不兼容的情况下我们需要重新编译，不兼容一般是由于框架依赖版本不一致造成的，一般我们只需要编辑框架的pom.xml文件修改依赖版本即可。但是有些依赖新版本和旧版本不兼容，此时我们就需要对源码进行更多的修改。这些修改最好在IDE中进行。 Hive的guava版本和Hadoop 3.1.3的不兼容，我们修改其为27.0-jre xml1234将&lt;guava.version&gt;19.0&lt;/guava.version&gt;修改为&lt;guava.version&gt;27.0-jre&lt;/guava.version&gt; 这个依赖新老版本就不兼容，修改版本后我们需要对源码进行必要修改。详细修改步骤会在另外一篇教程中讲述 五、编译准备工作全部做完，最后我们开始编译。一般的编译命令为： bash1mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=true 然后静待编译完成。这个过程会比较久，而且会从maven官网拉取大量jar包，所以要保证网络状况良好。 编译完成的Tar包的位置，各个框架都不一样，我们可以用下面的命令查找 bash1find ./ -name *.tar.gz Hive编译 拉取源码 bash12cd /opt/software/sourcegit clone https://git-wip-us.apache.org/repos/asf/hive.git 修改pom.xml，将guava的版本改为如下版本 Code1&lt;guava.version&gt;27.0-jre&lt;&#x2F;guava.version&gt; 修改以下文件中关于 com.google.common.util.concurrent.Futures#addCallback 的调用 src\\java\\org\\apache\\hadoop\\hive\\llap\\AsyncPbRpcProxy.java java123456789101112131415161718192021//173行Futures.addCallback( future, new ResponseCallback&lt;U&gt;( request.getCallback(), nodeId, this) ,executor);//278行Futures.addCallback(requestManagerFuture, new FutureCallback&lt;Void&gt;() &#123; @Override public void onSuccess(Void result) &#123; LOG.info(\"RequestManager shutdown\"); &#125; @Override public void onFailure(Throwable t) &#123; if (!(t instanceof CancellationException)) &#123; LOG.warn(\"RequestManager shutdown with error\", t); &#125; &#125;&#125;, requestManagerExecutor); src\\java\\org\\apache\\hadoop\\hive\\llap\\daemon\\impl\\AMReporter.java java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//162行Futures.addCallback(queueLookupFuture, new FutureCallback&lt;Void&gt;() &#123; @Override public void onSuccess(Void result) &#123; LOG.info(\"AMReporter QueueDrainer exited\"); &#125; @Override public void onFailure(Throwable t) &#123; if (t instanceof CancellationException &amp;&amp; isShutdown.get()) &#123; LOG.info(\"AMReporter QueueDrainer exited as a result of a cancellation after shutdown\"); &#125; else &#123; LOG.error(\"AMReporter QueueDrainer exited with error\", t); Thread.getDefaultUncaughtExceptionHandler().uncaughtException(Thread.currentThread(), t); &#125; &#125;&#125;, queueLookupExecutor);//266行Futures.addCallback(future, new FutureCallback&lt;Void&gt;() &#123; @Override public void onSuccess(Void result) &#123; LOG.info(\"Sent taskKilled for &#123;&#125;\", taskAttemptId); &#125; @Override public void onFailure(Throwable t) &#123; LOG.warn(\"Failed to send taskKilled for &#123;&#125;. The attempt will likely time out.\", taskAttemptId); &#125;&#125;, executor);//331行Futures.addCallback(future, new FutureCallback&lt;Void&gt;() &#123; @Override public void onSuccess(Void result) &#123; // Nothing to do. &#125; @Override public void onFailure(Throwable t) &#123; QueryIdentifier currentQueryIdentifier = amNodeInfo.getQueryIdentifier(); amNodeInfo.setAmFailed(true); LOG.warn(\"Heartbeat failed to AM &#123;&#125;. Marking query as failed. query=&#123;&#125;\", amNodeInfo.amNodeId, currentQueryIdentifier, t); queryFailedHandler.queryFailed(currentQueryIdentifier); &#125;&#125;, executor); src\\java\\org\\apache\\hadoop\\hive\\llap\\daemon\\impl\\LlapTaskReporter.java java12//131行Futures.addCallback(future, new HeartbeatCallback(errorReporter), heartbeatExecutor); src\\java\\org\\apache\\hadoop\\hive\\llap\\daemon\\impl\\TaskExecutorService.java java12345//178行Futures.addCallback(future, new WaitQueueWorkerCallback(), executionCompletionExecutorServiceRaw);//692行Futures.addCallback(future, wrappedCallback, executionCompletionExecutorService); src\\java\\org\\apache\\hadoop\\hive\\llap\\tezplugins\\LlapTaskSchedulerService.java java123456789//747行Futures.addCallback(nodeEnablerFuture, new LoggingFutureCallback(\"NodeEnablerThread\", LOG),nodeEnabledExecutor);//751行Futures.addCallback(delayedTaskSchedulerFuture, new LoggingFutureCallback(\"DelayedTaskSchedulerThread\", LOG),delayedTaskSchedulerExecutor);//755行Futures.addCallback(schedulerFuture, new LoggingFutureCallback(\"SchedulerThread\", LOG),schedulerExecutor); src\\java\\org\\apache\\hadoop\\hive\\ql\\exec\\tez\\WorkloadManager.java java12345678//1089行Futures.addCallback(future, FATAL_ERROR_CALLBACK, timeoutPool);//1923行Futures.addCallback(getFuture, this,timeoutPool);//1977行Futures.addCallback(waitFuture, this, timeoutPool); src\\test\\org\\apache\\hadoop\\hive\\ql\\exec\\tez\\SampleTezSessionState.java java123456789101112//121行Futures.addCallback(waitForAmRegFuture, new FutureCallback&lt;Boolean&gt;() &#123; @Override public void onSuccess(Boolean result) &#123; future.set(session); &#125; @Override public void onFailure(Throwable t) &#123; future.setException(t); &#125;&#125;,timeoutPool); 编译 bash1mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=true Tez编译 拉取源码 bash12cd /opt/software/sourcegit clone https://git-wip-us.apache.org/repos/asf/tez.git 安装Tez必要环境 bash1sudo yum install -y protobuf protobuf-static protobuf-devel 编译 查看编译说明，按照编译说明用下列命令编译 bash12cd tezmvn clean package -Dhadoop.version=3.1.3 -Phadoop28 -P\\!hadoop27 -DskipTests -Dmaven.javadoc.skip=true Phoenix编译 拉取源码 bash12cd /opt/software/sourcegit clone https://git-wip-us.apache.org/repos/asf/phoenix.git 编译 bash12cd phoenixmvn clean package -DskipTests -Dhbase.profile=2.2 -Dhbase.version=2.2.4 Spark编译 去spark官网下载源码，解压到/opt/software/source 进入该目录，编译 bash1./dev/make-distribution.sh --name without-hive --tgz -Pyarn -Phadoop-3.1 -Dhadoop.version=3.1.3 -Pparquet-provided -Porc-provided -Phadoop-provided","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"源码编译","slug":"大数据/源码编译","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hive","slug":"hive","permalink":"https://masteryang4.github.io/tags/hive/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"},{"name":"源码编译","slug":"源码编译","permalink":"https://masteryang4.github.io/tags/%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"},{"name":"phoenix","slug":"phoenix","permalink":"https://masteryang4.github.io/tags/phoenix/"},{"name":"tez","slug":"tez","permalink":"https://masteryang4.github.io/tags/tez/"}]},{"title":"HiveSQL之常用查询函数case","slug":"HiveSQL之常用查询函数case","date":"2020-05-26T15:53:58.000Z","updated":"2020-05-26T16:18:08.383Z","comments":true,"path":"2020/05/26/HiveSQL之常用查询函数case/","link":"","permalink":"https://masteryang4.github.io/2020/05/26/HiveSQL%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%9F%A5%E8%AF%A2%E5%87%BD%E6%95%B0case/","excerpt":"","text":"关键词：CASE WHEN THEN ELSE END数据准备 name dept_id sex 悟空 A 男 大海 A 男 宋宋 B 男 凤姐 A 女 婷姐 B 女 婷婷 B 女 需求求出不同部门男女各多少人。结果如下： Code12A 2 1B 1 2 创建本地emp_sex.txt，导入数据Code1234567[ys@hadoop102 datas]$ vim emp_sex.txt悟空 A 男大海 A 男宋宋 B 男凤姐 A 女婷姐 B 女婷婷 B 女 创建hive表并导入数据sql123456create table emp_sex(name string, dept_id string, sex string) row format delimited fields terminated by \"\\t\";load data local inpath '/opt/module/datas/emp_sex.txt' into table emp_sex; 查询数据sql12345678select dept_id, sum(case sex when '男' then 1 else 0 end) male_count, sum(case sex when '女' then 1 else 0 end) female_countfrom emp_sexgroup by dept_id; 首先注意CASE WHEN THEN ELSE END的缺一不可 也要注意sum函数的用法，sum(条件)是经常会用到的方法！！！ 比如sum(if XXX)就常在HiveSQL里面使用。 例如：sum(if(dt=&#39;2020-05-27&#39;, order_count,0 )) order_count，本质其实是一样的。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hive","slug":"大数据/hive","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hive","slug":"hive","permalink":"https://masteryang4.github.io/tags/hive/"},{"name":"SQL","slug":"SQL","permalink":"https://masteryang4.github.io/tags/SQL/"},{"name":"hivesql","slug":"hivesql","permalink":"https://masteryang4.github.io/tags/hivesql/"}]},{"title":"一段有趣的spark_aggregate代码","slug":"一段有趣的spark-aggregate代码","date":"2020-05-26T14:18:30.000Z","updated":"2020-05-29T14:14:17.186Z","comments":true,"path":"2020/05/26/一段有趣的spark-aggregate代码/","link":"","permalink":"https://masteryang4.github.io/2020/05/26/%E4%B8%80%E6%AE%B5%E6%9C%89%E8%B6%A3%E7%9A%84spark-aggregate%E4%BB%A3%E7%A0%81/","excerpt":"","text":"看到了一段非常有趣的关于spark中aggregate算子的代码，需要很细心才能给出正确答案。 在这里和大家分享。 代码示例scala12345678910111213141516import org.apache.spark.&#123;SparkConf, SparkContext&#125;object TrySpark &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setAppName(\"aggTest\").setMaster(\"local[*]\") val sc = new SparkContext(conf) val rdd = sc.makeRDD(Array(\"12\", \"234\", \"345\", \"4567\"), 2) val str: String = rdd.aggregate(\"0\")((a, b) =&gt; Math.max(a.length, b.length).toString, (x, y) =&gt; x + y) println(str) val str1: String = rdd.aggregate(\"\")((a, b) =&gt; Math.min(a.length, b.length).toString, (x, y) =&gt; x + y) println(str1) &#125;&#125; 前方高能输出结果1 Code1204311 输出结果2 Code1203411 惊不惊喜，刺不刺激（手动狗头）。 解析aggregate：行动算子，意为【聚合】 函数签名 def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U 函数说明 分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合 第一个括号内的参数为初始值 第二个括号中 第一个参数为分区内要执行的函数，初始值和分区内元素依次聚合 第二个参数为分区间要执行的函数，初始值和分区间元素依次聚合 代码详解： scala1val rdd = sc.makeRDD(Array(\"12\", \"234\", \"345\", \"4567\"), 2) scala1rdd.aggregate(\"0\")((a, b) =&gt; Math.max(a.length, b.length).toString, (x, y) =&gt; x + y) 首先注意rdd是两个分区，”12”, “234”一个分区，”345”, “4567”一个分区 执行分区内函数Math.max(a.length, b.length).toString 分区一 “0”，“12”执行函数，输出“2”，【注意：函数后面有个toString】【聚合：上一步输出作为下一步输入】 “2”，”234”执行函数，最终输出“3” 分区二 “0”，“345” =&gt; “3” “3”，”4567” =&gt; 最终 “4” 执行分区间函数(x, y) =&gt; x + y，其实就是一个字符串拼接，但是因为分区的原因 不一定哪个分区先执行完，所以会出现两种情况的字符串拼接：“034” or “043” scala1rdd.aggregate(\"\")((a, b) =&gt; Math.min(a.length, b.length).toString, (x, y) =&gt; x + y) rdd是两个分区，”12”, “234”一个分区，”345”, “4567”一个分区 执行分区内函数Math.min(a.length, b.length).toString 分区一 “”，“12”执行函数，输出“0”，【注意：函数后面有个toString】【聚合：上一步输出作为下一步输入】 “0”，”234”执行函数，最终输出“1”，【注意：“0”的长度是1】 分区二 “”，“345” =&gt; “0” “0”，”4567” =&gt; 最终 “1” 执行分区间函数(x, y) =&gt; x + y，字符串拼接，“”+“1”+“1” =&gt; “11”","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"}]},{"title":"spark常用算子join","slug":"spark常用算子join","date":"2020-05-25T11:52:38.000Z","updated":"2020-05-25T12:12:02.270Z","comments":true,"path":"2020/05/25/spark常用算子join/","link":"","permalink":"https://masteryang4.github.io/2020/05/25/spark%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90join/","excerpt":"","text":"简述JOIN函数签名 def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))] 函数说明 spark RDD 转换算子 (对照函数签名)在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素连接在一起的(K,(V,W))的RDD 重点示例 join leftOuterJoin rightOuterJoin fullOuterJoin scala1234567891011121314151617181920212223242526272829303132333435363738import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object JoinTest &#123; def main(args: Array[String]): Unit = &#123; //1.创建SparkConf val sparkConf: SparkConf = new SparkConf().setAppName(\"JoinTest\").setMaster(\"local[*]\") //2.创建SparkContext val sc = new SparkContext(sparkConf) //3.创建两个RDD val rdd1: RDD[(String, Int)] = sc.makeRDD(Array((\"a\", 1), (\"a\", 2), (\"b\", 1), (\"c\", 1))) val rdd2: RDD[(String, Int)] = sc.makeRDD(Array((\"a\", 1), (\"b\", 1), (\"b\", 2), (\"d\", 1))) //4.测试各种JOIN【 注意返回值 】 val result1: RDD[(String, (Int, Int))] = rdd1.join(rdd2) val result2: RDD[(String, (Int, Option[Int]))] = rdd1.leftOuterJoin(rdd2) val result3: RDD[(String, (Option[Int], Int))] = rdd1.rightOuterJoin(rdd2) val result4: RDD[(String, (Option[Int], Option[Int]))] = rdd1.fullOuterJoin(rdd2) //5.打印 result1.foreach(println) println(\"======================&gt;&gt;&gt;\") result2.foreach(println) println(\"======================&gt;&gt;&gt;\") result3.foreach(println) println(\"======================&gt;&gt;&gt;\") result4.foreach(println) //6.关闭连接 sc.stop() &#125;&#125; 输出结果： Code1234567891011121314151617181920212223(b,(1,1))(a,(1,1))(a,(2,1))(b,(1,2))&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt;&gt;(c,(1,None))(a,(1,Some(1)))(a,(2,Some(1)))(b,(1,Some(1)))(b,(1,Some(2)))&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt;&gt;(d,(None,1))(a,(Some(1),1))(a,(Some(2),1))(b,(Some(1),1))(b,(Some(1),2))&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt;&gt;(d,(None,Some(1)))(c,(Some(1),None))(a,(Some(1),Some(1)))(a,(Some(2),Some(1)))(b,(Some(1),Some(1)))(b,(Some(1),Some(2)))","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"}]},{"title":"scala样例类转换成为JSON字符串","slug":"scala样例类转换成为JSON字符串","date":"2020-05-25T11:08:21.000Z","updated":"2020-05-26T01:01:19.891Z","comments":true,"path":"2020/05/25/scala样例类转换成为JSON字符串/","link":"","permalink":"https://masteryang4.github.io/2020/05/25/scala%E6%A0%B7%E4%BE%8B%E7%B1%BB%E8%BD%AC%E6%8D%A2%E6%88%90%E4%B8%BAJSON%E5%AD%97%E7%AC%A6%E4%B8%B2/","excerpt":"","text":"JSON常用方法Java中并没有内置JSON的解析，因此使用JSON需要借助第三方类库。 几个常用的 JSON 解析类库： Gson: 谷歌开发的 JSON 库，功能十分全面。 FastJson: 阿里巴巴开发的 JSON 库，性能十分优秀。 Jackson: 社区十分活跃且更新速度很快。 maven依赖： xml12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.47&lt;/version&gt;&lt;/dependency&gt; JSON 对象与字符串的相互转化 方法 作用 JSON.parseObject() 从字符串解析 JSON 对象 JSON.parseArray() 从字符串解析 JSON 数组 JSON.toJSONString(obj/array) 将 JSON 对象或 JSON 数组转化为字符串 示例： java1234567891011121314import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;public class JSONTest &#123; public static void main(String[] args) &#123; //从字符串解析JSON对象 JSONObject obj = JSON.parseObject(\"&#123;\\\"name\\\":\\\"ys\\\"&#125;\"); System.out.println(obj); //&#123;\"name\":\"ys\"&#125; //将JSON对象转化为字符串 String objStr = JSON.toJSONString(obj); System.out.println(objStr); //&#123;\"name\":\"ys\"&#125; &#125;&#125; Scala样例类转换成JSON字符串将Scala样例类转换成为JSON字符串，JSON.toJSONString(obj)会失效，所以使用如下方法： maven依赖（json4s —&gt; json for scala）： xml12345&lt;dependency&gt; &lt;groupId&gt;org.json4s&lt;/groupId&gt; &lt;artifactId&gt;json4s-native_2.11&lt;/artifactId&gt; &lt;version&gt;3.5.4&lt;/version&gt;&lt;/dependency&gt; scala1234import org.json4s.native.Serializationimplicit val formats=org.json4s.DefaultFormats //隐式转换val orderInfoJson: String = Serialization.write(orderInfo) 示例 scala123456789101112131415161718192021222324import com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import com.atguigu.bean.UserInfoimport org.json4s.native.Serializationobject JsonStrTest &#123; def main(args: Array[String]): Unit = &#123; val userInfo = UserInfo(\"1001\",\"name1\",\"5\",\"2020-05-25\",\"male\") implicit val formats = org.json4s.DefaultFormats //println(JSON.toJSONString(userInfo)) //报错 val str = Serialization.write(userInfo) println(str) // &#123;\"id\":\"1001\",\"login_name\":\"name1\",\"user_level\":\"5\",\"birthday\":\"2020-05-25\",\"gender\":\"male\"&#125; val nObject: JSONObject = JSON.parseObject(str) //正常解析 println(nObject) // &#123;\"birthday\":\"2020-05-25\",\"login_name\":\"name1\",\"gender\":\"male\",\"user_level\":\"5\",\"id\":\"1001\"&#125; &#125;&#125; UserInfo.scala scala12345case class UserInfo(id:String, login_name:String, user_level:String, birthday:String, gender:String)","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"scala","slug":"大数据/scala","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/scala/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"scala","slug":"scala","permalink":"https://masteryang4.github.io/tags/scala/"}]},{"title":"[精]ElasticSearch总结与思考","slug":"ElasticSearch总结与思考","date":"2020-05-18T14:05:34.000Z","updated":"2020-06-09T15:22:04.442Z","comments":true,"path":"2020/05/18/ElasticSearch总结与思考/","link":"","permalink":"https://masteryang4.github.io/2020/05/18/ElasticSearch%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83/","excerpt":"","text":"简介概述 Elasticsearch，基于Lucene，隐藏复杂性，提供简单易用的RestfulAPI接口、JavaAPI接口（还有其他语言的API接口）。 Elasticsearch是一个实时分布式搜索和分析引擎。它用于全文搜索、结构化搜索、分析。 全文检索：将非结构化数据中的一部分信息提取出来,重新组织,使其变得有一定结构,然后对此有一定结构的数据进行搜索,从而达到搜索相对较快的目的。 倒排索引：简单举例：根据关键词找包含其的文章（正常思维：在文章中找关键词）。 结构化检索：我想搜索商品分类为日化用品的商品都有哪些，select * from products where category_id=’日化用品’。 数据分析：电商网站，最近7天牙膏这种商品销量排名前10的商家有哪些；新闻网站，最近1个月访问量排名前3的新闻版块是哪些。 可以作为一个大型分布式集群（数百台服务器）技术，处理PB级数据，服务大公司；也可以运行在单机上，服务小公司. 使用场景 维基百科，类似百度百科，牙膏，牙膏的维基百科，全文检索，高亮，搜索推荐。 The Guardian（国外新闻网站），类似搜狐新闻，用户行为日志（点击，浏览，收藏，评论）+ 社交网络数据（对某某新闻的相关看法），数据分析，给到每篇新闻文章的作者，让他知道他的文章的公众反馈（好，坏，热门，垃圾，鄙视，崇拜）。 Stack Overflow（国外的程序异常讨论论坛），IT问题，程序的报错，提交上去，有人会跟你讨论和回答，全文检索，搜索相关问题和答案，程序报错了，就会将报错信息粘贴到里面去，搜索有没有对应的答案。 GitHub（开源代码管理），搜索上千亿行代码。 国内：站内搜索（电商，招聘，门户，等等），IT系统搜索（OA，CRM，ERP，等等），数据分析（ES热门的一个使用场景）。 核心概念ElasticSearch与数据库类比 关系型数据库（如Mysql） 非关系型数据库（Elasticsearch） 数据库Database 索引Index 表Table 类型Type(6.0版本之后在一个索引下面只能有一个，7.0版本之后取消了Type) 数据行Row 文档Document(JSON格式) 数据列Column 字段Field 约束 Schema 映射Mapping 安装1）解压elasticsearch-6.6.0.tar.gz到/opt/module目录下 Code1[ys@hadoop102 software]$ tar -zxvf elasticsearch-6.6.0.tar.gz -C &#x2F;opt&#x2F;module&#x2F; 2）在/opt/module/elasticsearch-6.6.0路径下创建data文件夹 Code1[ys@hadoop102 elasticsearch-6.6.0]$ mkdir data 3）修改配置文件/opt/module/elasticsearch-6.6.0/config/elasticsearch.yml Code123[ys@hadoop102 config]$ pwd&#x2F;opt&#x2F;module&#x2F;elasticsearch-6.6.0&#x2F;config[ys@hadoop102 config]$ vim elasticsearch.yml yml1234567891011121314#-----------------------Cluster-----------------------cluster.name: my-application#-----------------------Node-----------------------node.name: node-102#-----------------------Paths-----------------------path.data: /opt/module/elasticsearch-6.6.0/datapath.logs: /opt/module/elasticsearch-6.6.0/logs#-----------------------Memory-----------------------bootstrap.memory_lock: falsebootstrap.system_call_filter: false#-----------------------Network-----------------------network.host: 192.168.9.102 #-----------------------Discovery-----------------------discovery.zen.ping.unicast.hosts: [\"192.168.9.102\"] （1）cluster.name 如果要配置集群需要两个节点上的elasticsearch配置的cluster.name相同，都启动可以自动组成集群，这里如果不改cluster.name则默认是cluster.name=my-application， （2）nodename随意取但是集群内的各节点不能相同 （3）修改后的每行前面不能有空格，修改后的“：”后面必须有一个空格 4）分发至hadoop103以及hadoop104，分发之后修改： Code1234567[ys@hadoop102 module]$ xsync elasticsearch-6.6.0&#x2F;node.name: node-103network.host: 192.168.9.103node.name: node-104network.host: 192.168.9.104 5）此时启动会报错，要配置linux系统环境（参考：http://blog.csdn.net/satiling/article/details/59697916） 6）启动Elasticsearch Code1[ys@hadoop102 elasticsearch-6.6.0]$ bin&#x2F;elasticsearch 7）测试elasticsearch Code12345678910111213141516171819[ys@hadoop102 elasticsearch-6.6.0]$ curl http:&#x2F;&#x2F;hadoop102:9200&#123; &quot;name&quot; : &quot;node-102&quot;, &quot;cluster_name&quot; : &quot;my-application&quot;, &quot;cluster_uuid&quot; : &quot;KOpuhMgVRzW_9OTjMsHf2Q&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;6.6.0&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;, &quot;build_hash&quot; : &quot;eb782d0&quot;, &quot;build_date&quot; : &quot;2018-06-29T21:59:26.107521Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;7.3.1&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 8）停止集群 Code1kill -9 进程号 9）群起脚本 Code1[ys@hadoop102 bin]$ vi es.sh shell123456789101112131415161718#!/bin/bashes_home=/opt/module/elasticsearchcase $1 in \"start\") &#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==============$i==============\" ssh $i \"source /etc/profile;$&#123;es_home&#125;/bin/elasticsearch &gt;/dev/null 2&gt;&amp;1 &amp;\" done&#125;;;\"stop\") &#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==============$i==============\" ssh $i \"ps -ef|grep $es_home |grep -v grep|awk '&#123;print \\$2&#125;'|xargs kill\" &gt;/dev/null 2&gt;&amp;1 done&#125;;;esac 可视化工具KibanaKibana的安装 1、将kibana压缩包上传到虚拟机指定目录 Code1[ys@hadoop102 software]$ tar -zxvf kibana-6.6.0-linux-x86_64.tar.gz -C &#x2F;opt&#x2F;module&#x2F; 2、修改相关配置，连接Elasticsearch Code1[ys@hadoop102 kibana]$ vim config&#x2F;kibana.yml yml12345678910# Kibana is served by a back end server. This setting specifies the port to use.server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is 'localhost', which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.server.host: \"192.168.9.102\"... ...... ...# The URL of the Elasticsearch instance to use for all your queries.elasticsearch.url: \"http://192.168.9.102:9200\" 3、启动Kibana Code1[ys@hadoop102 kibana]$ bin&#x2F;kibana 4、浏览器访问：hadoop102:5601 即可操作 操作命令行操作核心数据类型 字符串型：text(分词)、keyword(不分词) 数值型：long、integer、short、byte、double、float、half_float、scaled_float 日期类型：date Mapping1、手动创建 创建mapping Code1234567891011121314151617PUT my_index1&#123; &quot;mappings&quot;: &#123; &quot;_doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;username&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;pinyin&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 创建文档 Code1234PUT my_index1&#x2F;_doc&#x2F;1&#123; &quot;username&quot;:&quot;haha heihei&quot;&#125; 查询 Code12345678GET my_index1&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;username.pinyin&quot;: &quot;haha&quot; &#125; &#125;&#125; 2、自动创建 直接插入文档 Code123456PUT &#x2F;test_index&#x2F;_doc&#x2F;1&#123; &quot;username&quot;:&quot;alfred&quot;, &quot;age&quot;:1, &quot;birth&quot;:&quot;1991-12-15&quot;&#125; 查看mapping Code123456789101112131415161718192021222324252627GET &#x2F;test_index&#x2F;doc&#x2F;_mapping&#123; &quot;test_index&quot;: &#123; &quot;mappings&quot;: &#123; &quot;doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;birth&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;username&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; IK分词器分词器主要应用在中文上，在ES中字符串类型有keyword和text两种。keyword默认不进行分词，而text是将每一个汉字拆开称为独立的词，这两种都是不适用于生产环境。 keyword分词 Code1234GET _analyze&#123; &quot;keyword&quot;:&quot;我是程序员&quot;&#125; 结果展示（会报错error） text类型的分词 Code1234GET _analyze&#123; &quot;text&quot;:&quot;我是程序员&quot;&#125; 结果展示： Code123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;我&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 1, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;是&quot;, &quot;start_offset&quot;: 1, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;程&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 3, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;序&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 3 &#125;, &#123; &quot;token&quot;: &quot;员&quot;, &quot;start_offset&quot;: 4, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 4 &#125; ]&#125; IK分词器安装1）下载与安装的ES相对应的版本 2）解压elasticsearch-analysis-ik-6.6.0.zip，将解压后的IK文件夹拷贝到ES安装目录下的plugins目录下，并重命名文件夹为ik（什么名称都OK） Code1[ys@hadoop102 plugins]$ mkdir ik Code1[ys@hadoop102 software]$ unzip elasticsearch-analysis-ik-6.6.0.zip -d &#x2F;opt&#x2F;module&#x2F;elasticsearch-6.6.0&#x2F;plugins&#x2F;ik&#x2F; 3）分发分词器目录 Code1[ys@hadoop102 elasticsearch-6.6.0]$ xsync plugins&#x2F; 4）重新启动Elasticsearch，即可加载IK分词器 5）IK测试 ik_smart ：最少切分 ik_max_word：最细粒度划分 Code12345get _analyze&#123; &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;text&quot;:&quot;我是程序员&quot;&#125; Code12345678910111213141516171819202122232425&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;我&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;程序员&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125; ]&#125; ik_max_word Code1&quot;我&quot;,&quot;是&quot;,&quot;程序员&quot;,&quot;程序&quot;,&quot;员&quot; 检索文档【重点】向Elasticsearch增加数据 Code12345678PUT &#x2F;atguigu&#x2F;doc&#x2F;1&#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [&quot;sports&quot;, &quot;music&quot;]&#125; 查询数据 Code12# 协议方法 索引&#x2F;类型&#x2F;文档编号GET &#x2F;atguigu&#x2F;doc&#x2F;1 响应 Code1234567891011121314151617&#123; &quot;_index&quot;: &quot;atguigu&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &#x2F;&#x2F; 文档的原始数据JSON数据 &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125;&#125; 元数据查询Code1GET _cat&#x2F;indices 全文档检索Code12# 协议方法 索引&#x2F;类型&#x2F;_searchGET &#x2F;atguigu&#x2F;_doc&#x2F;_search 字段全值匹配检索[filter]Code123456789101112GET atguigu&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;about&quot;: &quot;I love to go rock climbing&quot; &#125; &#125; &#125; &#125;&#125; 字段分词匹配检索[match]Code12345678GET atguigu&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;about&quot;: &quot;I&quot; &#125; &#125;&#125; 字段模糊匹配检索[fuzzy]Code12345678910GET test&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;aa&quot;: &#123; &quot;value&quot;: &quot;我是程序&quot; &#125; &#125; &#125;&#125; 聚合检索Code1234567891011GET test&#x2F;_search&#123; &quot;aggs&quot;: &#123; &quot;groupby_aa&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;aa&quot;, &quot;size&quot;: 10 &#125; &#125; &#125;&#125; 分页检索Code123456GET movie_index&#x2F;movie&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot;: 1, &quot;size&quot;: 1&#125; 索引别名 _aliases索引别名就像一个快捷方式或软连接，可以指向一个或多个索引，也可以给任何一个需要索引名的API来使用。别名带给我们极大的灵活性，允许我们做下面这些： 1）给多个索引分组 (例如， last_three_months) 2）给索引的一个子集创建视图 3）在运行的集群中可以无缝的从一个索引切换到另一个索引 说白了就是功能更强大的视图 创建索引别名 建表时直接声明 Code12345678910111213141516171819202122232425262728293031PUT movie_chn_2020&#123; &quot;aliases&quot;: &#123; &quot;movie_chn_2020-query&quot;: &#123;&#125; &#125;, &quot;mappings&quot;: &#123; &quot;movie&quot;:&#123; &quot;properties&quot;: &#123; &quot;id&quot;:&#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;:&#123; &quot;type&quot;: &quot;text&quot; , &quot;analyzer&quot;: &quot;ik_smart&quot; &#125;, &quot;doubanScore&quot;:&#123; &quot;type&quot;: &quot;double&quot; &#125;, &quot;actorList&quot;:&#123; &quot;properties&quot;: &#123; &quot;id&quot;:&#123; &quot;type&quot;:&quot;long&quot; &#125;, &quot;name&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 为已存在的索引增加别名 Code123456POST _aliases&#123; &quot;actions&quot;: [ &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;, &quot;alias&quot;: &quot;movie_chn_2020-query&quot; &#125;&#125; ]&#125; 也可以通过加过滤条件缩小查询范围，建立一个子集视图 Code1234567891011121314POST _aliases&#123; &quot;actions&quot;: [ &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;, &quot;alias&quot;: &quot;movie_chn0919-query-zhhy&quot;, &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;actorList.id&quot;: &quot;3&quot; &#125; &#125; &#125; &#125; ]&#125; 查询别名：与使用普通索引没有区别 Code1GET movie_chn_2020-query&#x2F;_search 删除某个索引的别名 Code123456POST _aliases&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;, &quot;alias&quot;: &quot;movie_chn_2020-query&quot; &#125;&#125; ]&#125; 为某个别名进行无缝切换 Code1234567POST &#x2F;_aliases&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;, &quot;alias&quot;: &quot;movie_chn_2020-query&quot; &#125;&#125;, &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;movie_chn_yyyy&quot;, &quot;alias&quot;: &quot;movie_chn_2020-query&quot; &#125;&#125; ]&#125; 查询别名列表 Code1GET _cat&#x2F;aliases?v 索引模板Index Template 索引模板，顾名思义，就是创建索引的模具，其中可以定义一系列规则来帮助我们构建符合特定业务需求的索引的mappings和 settings，通过使用 Index Template 可以让我们的索引具备可预知的一致性。 常见的场景: 分割索引 分割索引就是根据时间间隔把一个业务索引切分成多个索引。比如把order_info 变成 order_info_20200101,order_info_20200102 ….. 这样做的好处有两个： 1、结构变化的灵活性：因为elasticsearch不允许对数据结构进行修改。但是实际使用中索引的结构和配置难免变化，那么只要对下一个间隔的索引进行修改，原来的索引位置原状。这样就有了一定的灵活性。 2、查询范围优化：因为一般情况并不会查询全部时间周期的数据，那么通过切分索引，物理上减少了扫描数据的范围，也是对性能的优化。 创建模板 Code123456789101112131415161718192021222324PUT _template&#x2F;template_movie2020&#123; &quot;index_patterns&quot;: [&quot;movie_test*&quot;], &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;aliases&quot; : &#123; &quot;&#123;index&#125;-query&quot;: &#123;&#125;, &quot;movie_test-query&quot;:&#123;&#125; &#125;, &quot;mappings&quot;: &#123; &quot;_doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;movie_name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; &#125; &#125; &#125; &#125;&#125; 其中 “index_patterns”: [“movie_test*”], 的含义就是凡是往movie_test开头的索引写入数据时，如果索引不存在，那么es会根据此模板自动建立索引。 在 “aliases” 中用{index}表示，获得真正的创建的索引名。 测试： Code12345POST movie_test_2020xxxx&#x2F;_doc&#123; &quot;id&quot;:&quot;333&quot;, &quot;name&quot;:&quot;zhang3&quot;&#125; 查看系统中已有的模板清单 Code1GET _cat&#x2F;templates 查看某个模板详情 Code123GET _template&#x2F;template_movie2020或者GET _template&#x2F;template_movie* JavaAPI操作maven依赖: xml1234567891011121314151617181920212223242526272829303132333435&lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpmime&lt;/artifactId&gt; &lt;version&gt;4.3.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.searchbox&lt;/groupId&gt; &lt;artifactId&gt;jest&lt;/artifactId&gt; &lt;version&gt;5.3.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna&lt;/artifactId&gt; &lt;version&gt;4.5.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.codehaus.janino&lt;/groupId&gt; &lt;artifactId&gt;commons-compiler&lt;/artifactId&gt; &lt;version&gt;2.7.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;versison&gt;6.6.0&lt;/version&gt;&lt;/dependency&gt; 单条写入数据 java1234567891011121314151617181920212223242526272829303132333435363738394041import com.ys.bean.Stu;import io.searchbox.client.JestClient;import io.searchbox.client.JestClientFactory;import io.searchbox.client.config.HttpClientConfig;import io.searchbox.core.Index;import java.io.IOException;public class ESWriter &#123; public static void main(String[] args) throws IOException &#123; //一、创建ES客户端对象 //1.1 创建ES客户端的工厂对象 JestClientFactory jestClientFactory = new JestClientFactory(); //1.2 创建配置信息 HttpClientConfig config = new HttpClientConfig.Builder(\"http://hadoop102:9200\").build(); jestClientFactory.setHttpClientConfig(config); //1.3 获取客户端对象 JestClient jestClient = jestClientFactory.getObject(); //二、写入数据 //2.1 创建Action对象 --&gt; Index Stu stu = new Stu(\"004\", \"少爷\"); Index index = new Index.Builder(stu) .index(\"stu_temp_01\") .type(\"_doc\") .id(\"1004\") .build(); //2.2 执行写入数据操作 jestClient.execute(index); //三、关闭资源 jestClient.shutdownClient(); &#125;&#125; 批量写入数据 java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import com.ys.bean.Stu;import io.searchbox.client.JestClient;import io.searchbox.client.JestClientFactory;import io.searchbox.client.config.HttpClientConfig;import io.searchbox.core.Bulk;import io.searchbox.core.Index;import java.io.IOException;public class ESWriterByBulk &#123; public static void main(String[] args) throws IOException &#123; //一、创建ES客户端对象 //1.1 创建ES客户端的工厂对象 JestClientFactory jestClientFactory = new JestClientFactory(); //1.2 创建配置信息 HttpClientConfig config = new HttpClientConfig.Builder(\"http://hadoop102:9200\").build(); jestClientFactory.setHttpClientConfig(config); //1.3 获取客户端对象 JestClient jestClient = jestClientFactory.getObject(); //二、批量写入 //2.1 准备数据 Stu stu1 = new Stu(\"008\", \"麻瓜\"); Stu stu2 = new Stu(\"009\", \"海格\"); //2.2 创建Bulk.Builder对象 Bulk.Builder builder = new Bulk.Builder(); //2.3 创建Index对象 Index index1 = new Index.Builder(stu1).id(\"1008\").build(); Index index2 = new Index.Builder(stu2).id(\"1009\").build(); //2.4 赋值默认的索引名称及类型名 builder.defaultIndex(\"stu_temp_01\"); builder.defaultType(\"_doc\"); //2.5 添加Index之Bulk builder.addAction(index1); builder.addAction(index2); //2.6 真正构建Bulk对象 Bulk bulk = builder.build(); //2.7 执行批量写入数据操作 jestClient.execute(bulk); //3.关闭连接 jestClient.shutdownClient(); &#125;&#125; 读取数据（这里不使用json串，可读性不好） java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import io.searchbox.client.JestClient;import io.searchbox.client.JestClientFactory;import io.searchbox.client.config.HttpClientConfig;import io.searchbox.core.Search;import io.searchbox.core.SearchResult;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.TermQueryBuilder;import org.elasticsearch.search.builder.SearchSourceBuilder;import java.io.IOException;import java.util.List;import java.util.Map;public class ESReader &#123; public static void main(String[] args) throws IOException &#123; //1.获取客户端对象 //1.1 创建ES客户端的工厂对象 JestClientFactory jestClientFactory = new JestClientFactory(); //1.2 创建配置信息 HttpClientConfig config = new HttpClientConfig.Builder(\"http://hadoop102:9200\").build(); jestClientFactory.setHttpClientConfig(config); //1.3 获取客户端对象 JestClient jestClient = jestClientFactory.getObject(); //2.读取数据 //2.0 创建查询条件 SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); BoolQueryBuilder boolQueryBuilder = new BoolQueryBuilder(); boolQueryBuilder.filter(new TermQueryBuilder(\"class_id\", \"190218\")); searchSourceBuilder.query(boolQueryBuilder); searchSourceBuilder.from(0); searchSourceBuilder.size(2); //2.1 创建Search对象 Search search = new Search.Builder(searchSourceBuilder.toString()) .addIndex(\"student\") .addType(\"_doc\") .build(); //2.2 执行查询操作 SearchResult searchResult = jestClient.execute(search); //2.3 解析searchResult System.out.println(\"查询数据\" + searchResult.getTotal() + \"条！\"); // [json对应map是常见操作] List&lt;SearchResult.Hit&lt;Map, Void&gt;&gt; hits = searchResult.getHits(Map.class); for (SearchResult.Hit&lt;Map, Void&gt; hit : hits) &#123; Map source = hit.source; for (Object key : source.keySet()) &#123; System.out.println(hit.id + \":\" + key.toString() + \":\" + source.get(key).toString()); &#125; System.out.println(\"*************\"); &#125; //3.关闭资源 jestClient.shutdownClient(); &#125;&#125; java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// Stu.javapublic class Stu &#123; private String id; private String name; public Stu() &#123; &#125; public Stu(String id, String name) &#123; this.id = id; this.name = name; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Stu stu = (Stu) o; if (id != null ? !id.equals(stu.id) : stu.id != null) return false; return name != null ? name.equals(stu.name) : stu.name == null; &#125; @Override public int hashCode() &#123; int result = id != null ? id.hashCode() : 0; result = 31 * result + (name != null ? name.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return \"Stu&#123;\" + \"id='\" + id + '\\'' + \", name='\" + name + '\\'' + '&#125;'; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"elasticsearch","slug":"大数据/elasticsearch","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/elasticsearch/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://masteryang4.github.io/tags/elasticsearch/"},{"name":"数据库","slug":"数据库","permalink":"https://masteryang4.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"MyISAM与InnoDB的区别(详)","slug":"MyISAM与InnoDB的区别-详","date":"2020-05-14T12:52:48.000Z","updated":"2020-05-14T13:47:24.563Z","comments":true,"path":"2020/05/14/MyISAM与InnoDB的区别-详/","link":"","permalink":"https://masteryang4.github.io/2020/05/14/MyISAM%E4%B8%8EInnoDB%E7%9A%84%E5%8C%BA%E5%88%AB-%E8%AF%A6/","excerpt":"","text":"MyISAM与InnoDB的区别（详）1.事务 InnoDB支持事务，MyISAM不支持。 对于InnoDB每一条SQL语言都默认封装成事务，自动提交，这样会影响速度，所以最好把多条SQL语言放在begin和commit之间，组成一个事务； 所以，博客中的《 MySQL事务相关 》一文，是基于InnoDB引擎的。 2.外键 InnoDB支持外键，而MyISAM不支持。 对一个包含外键的InnoDB表转为MYISAM会失败； 3.索引 InnoDB是聚集索引，使用B+Tree作为索引结构，数据文件是和（主键）索引绑在一起的（表数据文件本身就是按B+Tree组织的一个索引结构），必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。 MyISAM是非聚集索引，也是使用B+Tree作为索引结构，索引和数据文件是分离的（联系本文第9点），索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 也就是说：InnoDB的B+树主键索引的叶子节点就是数据文件，辅助索引的叶子节点是主键的值；而MyISAM的B+树主键索引和辅助索引的叶子节点都是数据文件的地址指针。 4.表的具体行数 InnoDB不保存表的具体行数，执行select count(*) from table时需要全表扫描。 而MyISAM用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快（注意不能加有任何WHERE条件）； 那么为什么InnoDB没有了这个变量呢？ ​ 因为InnoDB的事务特性，在同一时刻表中的行数对于不同的事务而言是不一样的，因此count统计会计算对于当前事务而言可以统计到的行数，而不是将总行数储存起来方便快速查询。InnoDB会尝试遍历一个尽可能小的索引除非优化器提示使用别的索引。如果二级索引不存在，InnoDB还会尝试去遍历其他聚簇索引. ​ 如果索引并没有完全处于InnoDB维护的缓冲区（Buffer Pool）中，count操作会比较费时。可以建立一个记录总行数的表并让你的程序在INSERT/DELETE时更新对应的数据。和上面提到的问题一样，如果此时存在多个事务的话这种方案也不太好用。如果得到大致的行数值已经足够满足需求可以尝试： SHOW TABLE STATUS 5.全文索引 Innodb不支持全文索引，而MyISAM支持全文索引，在涉及全文索引领域的查询效率上MyISAM速度更快高； 5.7以后的InnoDB支持全文索引了。 6.表压缩 MyISAM表格可以被压缩后进行查询操作,压缩表是不能进行修改的(除非先将表解除压缩，修改数据，然后再次压缩)。压缩表可以极大地减少磁盘空间占用，因此也可以减少磁盘I/O，从而提升查询性能，压缩表也支持索引，但索引也只是只读的。 7.锁粒度 InnoDB支持表、行(默认)级锁，而MyISAM支持表级锁。 InnoDB的行锁是实现在索引上的，而不是锁在物理行记录上。 潜台词是，如果访问没有命中索引，也无法使用行锁，将要退化为表锁 T_T。 8.主键 InnoDB表必须有主键（用户没有指定的话会自己找或生产一个主键），而Myisam可以没有 9.表数据文件存储 Innodb存储文件有frm、ibd，而Myisam是frm、MYD、MYI Innodb：frm是表定义文件，ibd是数据文件（共享表空间和单独表空间） Myisam：frm是表定义文件，myd是数据文件，myi是索引文件 索引选择 除非需要用到某些Innodb不具备的特性，并且没有其他办法可以代替，否则都应该优先选择innodb引擎。 参考文章 https://blog.csdn.net/qq_35642036/article/details/82820178 （里面的图片值得参考） https://www.cnblogs.com/timor0101/p/12883649.html","categories":[{"name":"SQL","slug":"SQL","permalink":"https://masteryang4.github.io/categories/SQL/"},{"name":"MySQL","slug":"SQL/MySQL","permalink":"https://masteryang4.github.io/categories/SQL/MySQL/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"数据库","slug":"数据库","permalink":"https://masteryang4.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"MySQL","slug":"MySQL","permalink":"https://masteryang4.github.io/tags/MySQL/"}]},{"title":"MySQL事务相关","slug":"MySQL事务相关","date":"2020-05-14T09:10:57.000Z","updated":"2020-05-14T12:21:18.503Z","comments":true,"path":"2020/05/14/MySQL事务相关/","link":"","permalink":"https://masteryang4.github.io/2020/05/14/MySQL%E4%BA%8B%E5%8A%A1%E7%9B%B8%E5%85%B3/","excerpt":"","text":"事务四大特性（ACID）1、原子性（Atomicity）： 事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。 事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。 也就是说事务是一个不可分割的整体。 的基本单位 2、一致性（Consistency）： 事务开始前和结束后，数据库的完整性约束没有被破坏 。 比如 A 向 B 转账，不可能 A 扣了钱，B 却没收到。 3、隔离性（Isolation）： 同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。 比如 A 正在从一张银行卡中取钱，在 A 取钱的过程结束前，B 不能向这张卡转账。 4、持久性（Durability）： 事务完成后，事务对数据库的所有更新将被保存到数据库，不 能回滚。 MySQL事务隔离级别多个事务之间隔离的，相互独立的。 但是如果多个事务操作同一批数据，则会引发一些问题，设置不同的隔离级别就可以解决这些问题。 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted） 是 是 是 不可重复读（read-committed） 否 是 是 可重复读（repeatable-read） 否 否 是 串行化（serializable） 否 否 否 隔离级别越高，效率越低。 大多数数据库的默认级别就是不可重复读（Read committed），比如Sql Server , Oracle 【注意】MySQL的默认事务隔离级别是——可重复读 事务并发存在的问题1、脏读：事务 A 读取了事务 B 更新的数据，然后 B 回滚操作，那么 A 读取到的数据是脏数据。 （一个事务，读取到另一个事务中没有提交的数据） 2、不可重复读：事务A多次读取同一数据，事务B在事务A多次读取的过程中，对数据做了更新并提交，导致事务 A多次读取同一数据时，结果不一致 。 （在同一个事务中，两次读取到的数据不一样 ） 3、幻读：系统管理员 A 将数据库中所有学生的成绩从具体分数改为 ABCDE 等级，但是系统管理员 B 就在这个时候插入了一条具体分数的记录，当系统管理员 A 改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 （一个事务操作(DML)数据表中所有记录，另一个事务添加了一条数据，则第一个事务查询不到添加的数据） （ 一个事务(同一个read view)在前后两次查询同一范围的时候，后一次查询看到了前一次查询没有看到的行） 可重复读的隔离级别下使用了MVCC机制，select操作不会更新版本号，是快照读（历史版本）； insert、update和delete会更新版本号，是当前读（当前版本）。 幻读只在当前读下才会出现。 不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。 解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表等方法 幻读产生的原因： 行锁只能锁住行，即使把所有的行记录都上锁，也阻止不了新插入的记录。 解决幻读的其他方法： 将两行记录间的空隙加上锁，阻止新记录的插入；这个锁称为间隙锁。","categories":[{"name":"SQL","slug":"SQL","permalink":"https://masteryang4.github.io/categories/SQL/"},{"name":"MySQL","slug":"SQL/MySQL","permalink":"https://masteryang4.github.io/categories/SQL/MySQL/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"数据库","slug":"数据库","permalink":"https://masteryang4.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"MySQL","slug":"MySQL","permalink":"https://masteryang4.github.io/tags/MySQL/"}]},{"title":"[精]zookeeper总结与思考","slug":"精-zookeeper总结与思考","date":"2020-05-14T08:30:13.000Z","updated":"2020-06-17T11:40:35.102Z","comments":true,"path":"2020/05/14/精-zookeeper总结与思考/","link":"","permalink":"https://masteryang4.github.io/2020/05/14/%E7%B2%BE-zookeeper%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83/","excerpt":"","text":"一、介绍概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。多作为集群提供服务的中间件. Zookeeper从设计模式角度来理解，是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生了变化，Zookeeper就负责通知已经在Zookeeper上注册的那些观察者做出相应的反应. 分布式系统: 分布式系统指由很多台计算机组成的一个整体。 这个整体一致对外,并且处理同一请求，系统对内透明，对外不透明。 内部的每台计算机都可以相互通信，例如使用RPC 或者是WebService。客户端向一个分布式系统发送的一次请求到接受到响应，有可能会经历多台计算机。 Zookeeper = 文件系统 + 通知机制 特点中心化集群，但是中心化集群易出现单点故障。 数据结构 应用场景提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。 二、安装及操作需要提前安装JDK 两种部署方式：本地模式（standalone），分布式模式 分布式安装部署 版本：zookeeper-3.4.10 1、规划 将在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。 2、解压安装 三台服务器分别解压：tar -zxvf zookeeper-3.4.10.tar.gz 解压后生成zookeeper-3.4.10目录 3、配置服务器编号 在zookeeper-3.4.10目录下创建zkData：mkdir -p zkData 进入目录：cd zkData 创建myid文件：touch myid 编辑文件：vim myid 在文件中添加与server对应的编号：比如hadoop02添加2； 在hadoop103、hadoop104上修改myid文件中内容为3、4 4、修改配置文件 zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg重命名为zoo.cfg：mv zoo_sample.cfg zoo.cfg 打开zoo.cfg文件：vim zoo.cfg 在文件中修改数据存储路径配置： dataDir=/opt/module/zookeeper-3.4.10/zkData 并且增加如下配置： #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 同步zoo.cfg配置文件到其他所有服务器 【配置参数解读】server.A=B:C:D A是一个数字，表示这个是第几号服务器【myid】； zk启动时读取myid文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口2888；【副本】 D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口3888。【选举信息】 【扩展】2181，客户端访问端口 5、相关操作 三台服务器在zookeeper-3.4.10下分别启动：bin/zkServer.sh start 查看状态：bin/zkServer.sh status shell123456789101112[ys@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower[ys@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: leader[ys@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower 客户端命令行操作启动客户端：bin/zkCli.sh 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] （详细信息）查看当前节点数据并能看到更新次数等数据 create 普通创建-s 含有序列-e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 三、内部原理【重点】选举机制【重点】 半数机制： 集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。 内部投票选举： Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。 【举例】五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。这些服务器依序启动，则： Code1234567891011121314151617181920212223242526因为一共5台服务器，只有超过半数以上，即最少启动3台服务器，集群才能正常工作。（1）服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成；服务器1状态保持为LOOKING；（2）服务器2启动，再发起一次选举。服务器1和2分别投自己一票，此时服务器1发现服务器2的id比自己大，更改选票投给服务器2；此时服务器1票数0票，服务器2票数2票，不够半数以上（3票），选举无法完成；服务器1，2状态保持LOOKING；（3）服务器3启动，发起一次选举。与上面过程一样，服务器1和2先投自己一票，然后因为服务器3id最大，两者更改选票投给为服务器3；此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数（3票），服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；（4）服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3；服务器4并更改状态为FOLLOWING；（5）服务器5启动，同4一样投票给3，此时服务器3一共5票，服务器5为0票；服务器5并更改状态为FOLLOWING；最终Leader是服务器3，状态为LEADING；其余服务器是Follower，状态为FOLLOWING。 参考文章： https://blog.csdn.net/weixin_43291055/article/details/95451357 选举机制文章推荐： https://www.cnblogs.com/shuaiandjun/p/9383655.html https://blog.csdn.net/wyqwilliam/article/details/83537139 节点类型 监听器原理【重点】 写数据流程 【案例】监听服务器节点动态上下线/zk工作机制 API操作：1、maven依赖 xml123456&lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.10&lt;/version&gt;&lt;/dependency&gt; 2、集群上创建/servers节点 shell12[zk: localhost:2181(CONNECTED) 10] create /servers \"servers\"Created /servers 3、服务器端向Zookeeper注册 java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import java.io.IOException;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.ZooDefs.Ids;public class DistributeServer &#123; private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = \"/servers\"; // 创建到zk的客户端连接 public void getConnect() throws IOException&#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; &#125; &#125;); &#125; // 注册服务器 public void registServer(String hostname) throws Exception&#123; String create = zk.create(parentNode + \"/server\", hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname +\" is online \"+ create); &#125; // 业务功能 public void business(String hostname) throws Exception&#123; System.out.println(hostname+\" is working ...\"); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 1获取zk连接 DistributeServer server = new DistributeServer(); server.getConnect(); // 2 利用zk连接注册服务器信息 server.registServer(args[0]); // 3 启动业务功能 server.business(args[0]); &#125;&#125; 4、客户端 java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;public class DistributeClient &#123; private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = \"/servers\"; // 创建到zk的客户端连接 public void getConnect() throws IOException &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 再次启动监听 try &#123; getServerList(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; // 获取服务器列表信息 public void getServerList() throws Exception &#123; // 1获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zk.getChildren(parentNode, true); // 2存储服务器信息列表 ArrayList&lt;String&gt; servers = new ArrayList&lt;&gt;(); // 3遍历所有节点，获取节点中的主机名称信息 for (String child : children) &#123; byte[] data = zk.getData(parentNode + \"/\" + child, false, null); servers.add(new String(data)); &#125; // 4打印服务器列表信息 System.out.println(servers); &#125; // 业务功能 public void business() throws Exception&#123; System.out.println(\"client is working ...\");Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 1获取zk连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 2获取servers的子节点信息，从中获取服务器信息列表 client.getServerList(); // 3业务进程启动 client.business(); &#125;&#125; 四、其他注意点：1、zk常用端口号： 2181，客户端访问端口2888，zk内部信息通讯（数据）3888，zk选举专用 2、zk不能越级创建节点； 且创建节点一般要带有数据（除非数据是null），否则创建会失败 shell1234567891011121314[zk: localhost:2181(CONNECTED) 1] create /ys/sss \"666\"Node does not exist: /ys/sss[zk: localhost:2181(CONNECTED) 2] create /ys \"666\" Created /ys...[zk: localhost:2181(CONNECTED) 16] create /ss nullCreated /ys [zk: localhost:2181(CONNECTED) 17] ls /[cluster, configs, controller, brokers, zookeeper, overseer, admin, isr_change_notification, controller_epoch, druid, aliases.json, live_nodes, collections, overseer_elect, spark, clusterstate.json, consumers, 【ss】, latest_producer_id_block, config, hbase, kylin][zk: localhost:2181(CONNECTED) 18] ls /ss[][zk: localhost:2181(CONNECTED) 19] get /ssnull... 常考面试题 请简述ZooKeeper的选举机制 半数机制：2n+1 10 台服务器：3 台 zk 20 台服务器：5 台 zk 100 台服务器：11 台 zk 【注意】台数并不是越多越好。 太多选举时间过长影响性能。 ZooKeeper的监听原理 ZooKeeper的常用命令 ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？ 部署方式单机模式、集群模式 角色：Leader和Follower 集群最少需要机器数：3","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"zookeeper","slug":"大数据/zookeeper","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/zookeeper/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://masteryang4.github.io/tags/zookeeper/"},{"name":"分布式","slug":"分布式","permalink":"https://masteryang4.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"[精]Redis总结与思考","slug":"精-Redis总结与思考","date":"2020-05-12T14:20:49.000Z","updated":"2020-06-09T15:19:27.345Z","comments":true,"path":"2020/05/12/精-Redis总结与思考/","link":"","permalink":"https://masteryang4.github.io/2020/05/12/%E7%B2%BE-Redis%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83/","excerpt":"","text":"Redis介绍及安装Redis简介1、Redis是最常用的非关系型数据库（NoSQL）——不依赖业务逻辑方式存储，而以简单的key-value模式存储。 常见的NoSQL数据库： ​ Memcached,Redis,MongoDB,HBase 2、Redis有16个库，编号为0~15，默认使用0号库。 3、Redis使用的是单线程+多路IO复用技术（Linux系统特有）。 Redis安装及启动1、Redis安装步骤： 首先保证有gcc-c++工具，否则先执行：yum install gcc-c++ 下载获得redis-3.2.5.tar.gz后将它放入Linux目录 解压命令:tar -zxvf redis-3.2.5.tar.gz 解压完成后进入目录:cd redis-3.2.5 在redis-3.2.5目录下执行make命令 在redis-3.2.5目录下执行make install命令 2、Redis默认安装目录：/usr/local/bin redis-benchmark：性能测试工具，可以在自己本子运行，看看自己本子性能如何(服务启动起来后执行) redis-check-aof：修复有问题的AOF文件 redis-check-rdb：修复有问题RDB文件 redis-sentinel：Redis集群使用 redis-server：Redis服务器启动命令 redis-cli：客户端，操作入口 3、Redis启动： 备份redis.conf：拷贝一份redis.conf到其他目录 修改redis.conf文件将里面的daemonize no 改成 yes(128行)，让服务在后台启动 启动命令：执行 redis-server /root/myredis/redis.conf 用客户端访问: redis-cli -p 6379 关闭：客户端中输入shutdown，redis-server进程就已关闭。之后Ctrl+c退出客户端即可。 Redis数据类型 常用五大数据类型：String,list,set,hash,zset 五大数据类型常用指令： 0、Key Key常用指令 keys * 查询当前库的所有键 exists &lt;key&gt; 判断某个键是否存在 type &lt;key&gt; 查看键对应的数据的类型 del &lt;key&gt; 删除某个键 expire &lt;key&gt; &lt;seconds&gt; 为键值设置过期时间，单位秒 ttl &lt;key&gt; 查看还有多少秒过期，-1表示永不过期，-2表示已过期 dbsize 查看当前数据库的key的数量 flushdb 清空当前库 flushall 通杀全部库 1、String String类型是二进制安全的。意味着Redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。 String类型是Redis最基本的数据类型，一个Redis中字符串value最多可以是512M String常用指令 get &lt;key&gt; 查询对应键值 set &lt;key&gt; &lt;value&gt; 添加键值对 append &lt;key&gt; &lt;value&gt; 将给定的&lt;value&gt;追加到原值的末尾 strlen &lt;key&gt; 获得值的长度 setnx &lt;key&gt; &lt;value&gt; 只有在 key 不存在时设置 key 的值 incr &lt;key&gt; 将 key 中储存的数字值增1。只能对数字值操作，如果为空，新增值为1 decr &lt;key&gt; 将 key 中储存的数字值减1。只能对数字值操作，如果为空，新增值为-1 incrby / decrby &lt;key&gt; &lt;步长&gt; 将 key 中储存的数字值增减。自定义步长 mset &lt;key1&gt; &lt;value1&gt; &lt;key2&gt; &lt;value2&gt; … 同时设置一个或多个 key-value对 mget &lt;key1&gt; &lt;key2&gt; &lt;key3&gt; … 同时获取一个或多个 value msetnx &lt;key1&gt; &lt;value1&gt; &lt;key2&gt; &lt;value2&gt; … 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。 getrange &lt;key&gt; &lt;起始位置&gt; &lt;结束位置&gt; 获得值的范围，类似java中的substring setrange &lt;key&gt; &lt;起始位置&gt; &lt;value&gt; 用 &lt;value&gt;覆写&lt;key&gt;所储存的字符串值，从&lt;起始位置&gt;开始 setex &lt;key&gt; &lt;过期时间&gt; &lt;value&gt; 设置键值的同时，设置过期时间，单位秒 getset &lt;key&gt; &lt;value&gt; 以新换旧，设置了新值同时获得旧值 2、List 单键多值 Redis 列表是简单的字符串列表，按照插入顺序排序。 它的底层实际是个双向链表，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。 List常用指令 lpush/rpush &lt;key&gt; &lt;value1&gt; &lt;value2&gt; … 从左边/右边插入一个或多个值 lpop/rpop &lt;key&gt; 从左边/右边吐出一个值。值在键在，值亡键亡。 rpoplpush &lt;key1&gt; &lt;key2&gt; 从&lt;key1&gt;列表右边吐出一个值，插到&lt;key2&gt;列表左边 lrange &lt;key&gt; &lt;start&gt; &lt;stop&gt; 按照索引下标获得元素(从左到右) lindex &lt;key&gt; &lt;index&gt; 按照索引下标获得元素(从左到右) llen &lt;key&gt; 获得列表长度 linsert &lt;key&gt; before &lt;value&gt; &lt;newvalue&gt; 在&lt;value&gt;的前面插入&lt;newvalue&gt; lrem &lt;key&gt; &lt;n&gt; &lt;value&gt; 从左边删除n个value(从左到右) 3、Set Redis的Set是string类型的无序集合 它底层其实是一个value为null的hash表,所以添加，删除，查找的复杂度都是O(1)。 Set常用指令 sadd &lt;key&gt; &lt;value1&gt; &lt;value2&gt; … 将一个或多个 member 元素加入到集合 key 当中，已经存在于集合的 member 元素将被忽略。 smembers &lt;key&gt; 取出该集合的所有值 sismember &lt;key&gt; &lt;value&gt; 判断集合&lt;key&gt;是否为含有该&lt;value&gt;值，有返回1，没有返回0 scard &lt;key&gt; 返回该集合的元素个数。 srem &lt;key&gt; &lt;value1&gt; &lt;value2&gt; … 删除集合中的某个元素。 spop &lt;key&gt; &lt;n&gt; 随机从该集合中吐出一个或多个值。 srandmember &lt;key&gt; &lt;n&gt; 随机从该集合中取出n个值。不会从集合中删除。 sinter &lt;key1&gt; &lt;key2&gt; 返回两个集合的交集元素。 sunion &lt;key1&gt; &lt;key2&gt; 返回两个集合的并集元素。 sdiff &lt;key1&gt; &lt;key2&gt; 返回两个集合的差集元素。 4、Hash Redis hash 是一个键值对集合。 Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 类似Java里面的Map&lt;String,Object&gt; Hash常用指令 hset &lt;key&gt; &lt;field&gt; &lt;value&gt; 给&lt;key&gt;集合中的&lt;field&gt;键赋值&lt;value&gt; hget &lt;key&gt; &lt;field&gt; 从&lt;key&gt;集合&lt;field&gt;取出 value hmset &lt;key&gt; &lt;field1&gt; &lt;value1&gt; &lt;field2&gt; &lt;value2&gt;… 批量设置hash的值 hexists key &lt;field&gt; 查看哈希表 key 中，给定域 field 是否存在 hkeys &lt;key&gt; 列出该hash集合的所有field hvals &lt;key&gt; 列出该hash集合的所有value hincrby &lt;key&gt; &lt;field&gt; &lt;increment&gt; 为哈希表 key 中的域 field 的值加上增量 increment hsetnx &lt;key&gt; &lt;field&gt; &lt;value&gt; 将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在 5、zset (sorted set) Redis有序集合zset与普通集合set非常相似，是一个没有重复元素的字符串集合。 有序集合的所有成员都关联了一个评分（score） ，这个评分（score）被用来按照从最低分到最高分的方式排序集合中的成员。（集合的成员是唯一的，但是评分可以是重复了的） 因为元素是有序的, 所以你也可以很快的根据评分（score）或者次序（position）来获取一个范围的元素。访问有序集合的中间元素也是非常快的,因此你能够使用有序集合作为一个没有重复成员的智能列表。 zset常用指令 zadd &lt;key&gt; &lt;score1&gt; &lt;value1&gt; &lt;score2&gt; &lt;value2&gt;… 将一个或多个 member 元素及其 score 值加入到有序集 key 当中 zrange &lt;key&gt; &lt;start&gt; &lt;stop&gt; [WITHSCORES] 返回有序集 key 中，下标在&lt;start&gt; &lt;stop&gt;之间的元素。带WITHSCORES，可以让分数一起和值返回到结果集。 zrangebyscore key min max [withscores] [limit offset count] 返回有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。有序集成员按 score 值递增(从小到大)次序排列 zrevrangebyscore key max min [withscores] [limit offset count] 同上，改为从大到小排列 zincrby &lt;key&gt; &lt;increment&gt; &lt;value&gt; 为元素的score加上增量 zrem &lt;key&gt; &lt;value&gt; 删除该集合下，指定值的元素 zcount &lt;key&gt; &lt;min&gt; &lt;max&gt; 统计该集合，分数区间内的元素个数 zrank &lt;key&gt; &lt;value&gt; 返回该值在集合中的排名，从0开始 Redis的Java客户端Jedismaven依赖： xml12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.8.1&lt;/version&gt;&lt;/dependency&gt; 注意事项： 禁用Linux的防火墙： 临时禁用：service iptables stop 关闭开机自启：chkconfig iptables off redis.conf中注释掉bind 127.0.0.1（61行） ,然后 protect-mode（80行）设置为 no。 Jedis测试连通性 java12345678public class Demo01 &#123; public static void main(String[] args) &#123; //连接本地的 Redis 服务 Jedis jedis = new Jedis(\"127.0.0.1\",6379); //查看服务是否运行，打出pong表示OK System.out.println(\"connection is OK==========&gt;: \"+jedis.ping()); &#125;&#125; Jedis-API: Key java12345678//keySet&lt;String&gt; keys = jedis.keys(\"*\");for (Iterator iterator = keys.iterator(); iterator.hasNext();) &#123; String key = (String) iterator.next(); System.out.println(key);&#125;System.out.println(\"jedis.exists====&gt;\"+jedis.exists(\"k2\"));System.out.println(jedis.ttl(\"k1\")); Jedis-API: String java12345System.out.println(jedis.get(\"k1\"));jedis.set(\"k4\",\"k4_Redis\");System.out.println(\"----------------------------------------\");jedis.mset(\"str1\",\"v1\",\"str2\",\"v2\",\"str3\",\"v3\");System.out.println(jedis.mget(\"str1\",\"str2\",\"str3\")); Jedis-API: List java1234List&lt;String&gt; list = jedis.lrange(\"mylist\",0,-1); for (String element : list) &#123; System.out.println(element); &#125; Jedis-API: Set java123456789jedis.sadd(\"orders\",\"jd001\");jedis.sadd(\"orders\",\"jd002\");jedis.sadd(\"orders\",\"jd003\");Set&lt;String&gt; set1 = jedis.smembers(\"orders\");for (Iterator iterator = set1.iterator(); iterator.hasNext();) &#123; String string = (String) iterator.next(); System.out.println(string);&#125;jedis.srem(\"orders\",\"jd002\"); Jedis-API: hash[注意] java1234567891011jedis.hset(\"hash1\",\"userName\",\"lisi\");System.out.println(jedis.hget(\"hash1\",\"userName\"));Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;(); //【注意】map.put(\"telphone\",\"13810169999\");map.put(\"address\",\"atguigu\");map.put(\"email\",\"abc@163.com\");jedis.hmset(\"hash2\",map);List&lt;String&gt; result = jedis.hmget(\"hash2\", \"telphone\",\"email\");for (String element : result) &#123; System.out.println(element);&#125; Jedis-API: zset java123456789jedis.zadd(\"zset01\",60d,\"v1\");jedis.zadd(\"zset01\",70d,\"v2\");jedis.zadd(\"zset01\",80d,\"v3\");jedis.zadd(\"zset01\",90d,\"v4\");Set&lt;String&gt; s1 = jedis.zrange(\"zset01\",0,-1);for (Iterator iterator = s1.iterator(); iterator.hasNext();) &#123; String string = (String) iterator.next(); System.out.println(string);&#125; Redis事务 Redis事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 Redis事务的主要作用就是串联多个命令防止别的命令插队 悲观锁(Pessimistic Lock)，顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 乐观锁(Optimistic Lock)， 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量。Redis就是利用这种check-and-set机制实现事务的。 三特性： 1、单独的隔离操作 事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 2、没有隔离级别的概念 队列中的命令没有提交之前都不会实际的被执行，因为事务提交前任何指令都不会被实际执行，也就不存在“事务内的查询要看到事务里的更新，在事务外查询不能看到”这个让人万分头痛的问题 3、不保证原子性 Redis同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚 Redis持久化1、RDB （Redis DataBase） 在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里。 备份是如何执行的： Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。 关于fork：在Linux程序中，fork()会产生一个和父进程完全相同的子进程，但子进程在此后多会exec系统调用，出于效率考虑，Linux中引入了“写时复制技术”，一般情况父进程和子进程会共用同一段物理内存，只有进程空间的各段的内容要发生变化时，才会将父进程的内容复制一份给子进程。 在redis.conf中配置文件名称，默认为dump.rdb RDB优缺点： 优点 节省磁盘空间 恢复速度快 rdb的缺点 虽然Redis在fork时使用了写时拷贝技术,但是如果数据庞大时还是比较消耗性能。 在备份周期在一定间隔时间做一次备份，所以如果Redis意外down掉的话，就会丢失最后一次快照后的所有修改。 2、AOF （Append Of File） 以日志的形式来记录每个写操作，将Redis执行过的所有写指令记录下来(读操作不记录)，只许追加文件但不可以改写文件，Redis启动之初会读取该文件重新构建数据，换言之，Redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。 AOF默认不开启，需要手动在配置文件中配置 可以在redis.conf中配置文件名称，默认为 appendonly.aof AOF和RDB同时开启，系统默认取AOF的数据 AOF文件故障恢复： AOF文件的保存路径，同RDB的路径一致。 如遇到AOF文件损坏，可通过 redis-check-aof --fix appendonly.aof 进行恢复 Rewrite： AOF采用文件追加方式，文件会越来越大为避免出现此种情况，新增了重写机制,当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集.可以使用命令bgrewriteaof。 AOF优缺点： 优点： 备份机制更稳健，丢失数据概率更低。 可读的日志文本，通过操作AOF稳健，可以处理误操作。 缺点： 比起RDB占用更多的磁盘空间。 恢复备份速度要慢。 每次读写都同步的话，有一定的性能压力。 存在个别Bug，造成恢复不能。 用哪个好呢 官方推荐两个都启用。 如果对数据不敏感，可以选单独用RDB。 不建议单独用 AOF，因为可能会出现Bug。 如果只是做纯内存缓存，可以都不用。 Redis主从复制概念：主从复制，就是主机数据更新后根据配置和策略，自动同步到备机的master/slaver机制，Master以写为主，Slave以读为主。 用处：读写分离，性能扩展。容灾快速回复。 Code1234567891011配从(服务器)不配主(服务器):- 拷贝多个redis.conf文件include- 开启daemonize yes- Pid文件名字pidfile- 指定端口port- Log文件名字- Dump.rdb名字dbfilename- Appendonly 关掉或者换名字info replication:打印主从复制的相关信息slaveof &lt;ip&gt; &lt;port&gt; :成为某个实例的从服务器 一主二仆模式： 复制原理： 每次从机联通后，都会给主机发送sync指令 主机立刻进行存盘操作，发送RDB文件，给从机 从机收到RDB文件后，进行全盘加载 之后每次主机的写操作，都会立刻发送给从机，从机执行相同的命令 薪火相传： 上一个slave可以是下一个slave的Master，slave同样可以接收其他slaves的连接和同步请求，那么该slave作为了链条中下一个的master, 可以有效减轻master的写压力,去中心化降低风险。 用 slaveof &lt;ip&gt; &lt;port&gt; 中途变更转向:会清除之前的数据，重新建立拷贝最新的 风险是一旦某个slave宕机，后面的slave都没法备份 反客为主： 当一个master宕机后，后面的slave可以立刻升为master，其后面的slave不用做任何修改。。 用 slaveof no one 将从机变为主机。 哨兵模式(sentinel)反客为主的自动版，能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库。 Code1234567891、配置哨兵：调整为一主二仆模式自定义的&#x2F;myredis目录下新建sentinel.conf文件在配置文件中填写内容： sentinel monitor mymaster 127.0.0.1 6379 1其中mymaster为监控对象起的服务器名称， 1 为 至少有多少个哨兵同意迁移的数量。 2、启动哨兵执行redis-sentinel &#x2F;myredis&#x2F;sentinel.conf 故障恢复： 1、新主登基 从下线的主服务的所有从服务里面挑选一个从服务，将其转成主服务选择条件依次为：（1）选择优先级靠前的（2）选择偏移量最大的（3）选择runid最小的从服务 2、群仆俯首 挑选出新的主服务之后，sentinel 向原主服务的从服务发送 slaveof 新主服务 的命令，复制新master 3、旧主俯首 当已下线的服务重新上线时，sentinel会向其发送slaveof命令，让其成为新主的从 优先级在redis.conf中slave-priority 100偏移量是指获得原主数据最多的每个redis实例启动后都会随机生成一个40位的runid","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Redis","slug":"大数据/Redis","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Redis/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据库","slug":"数据库","permalink":"https://masteryang4.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Redis","slug":"Redis","permalink":"https://masteryang4.github.io/tags/Redis/"},{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://masteryang4.github.io/tags/JavaWeb/"}]},{"title":"JUnit常用注解","slug":"JUnit常用注解","date":"2020-05-12T11:49:00.000Z","updated":"2020-05-12T12:14:44.974Z","comments":true,"path":"2020/05/12/JUnit常用注解/","link":"","permalink":"https://masteryang4.github.io/2020/05/12/JUnit%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3/","excerpt":"","text":"JUnit常用注解JUnit是 Java平台最常用的测试框架 。 本文重点阐述JUnit4版本的@Before、@After、@BeforeClass、@AfterClass四个注解。 JUnit4，JUnit5注解对比 JUnit4 JUnit5 功能 @BeforeClass @BeforeAll 在当前类的所有测试方法之前执行。注解在【静态方法】上。 @AfterClass @AfterAll 在当前类中的所有测试方法之后执行。注解在【静态方法】上。 @Before @BeforeEach 在每个测试方法之前执行。注解在【非静态方法】上。 @After @AfterEach 在每个测试方法之后执行。注解在【非静态方法】上。 为什么 JUnit中@BeforeClass和@AfterClass标注的方法必须是static的 ？ 其实和JUnit的运行机制有关： 在JUnit中：每运行一个@Test方法，就会为该测试类新建一个新的实例。所以@BeforeClass和@AfterClass必须是static的，因为运行他们的时候，测试类还没有实例化。 这种设计有助于提高测试方法之间的独立性，因为每个@Test执行的时候，都新建了一个实例，这样的话，可以避免测试方法之间重用各个@Test方法里面的变量值。 示例： java12345678910111213141516import org.junit.Test;public class JUintDemo &#123; int i = 2; @Test public void test1() &#123; int i = 1; System.out.println(\"test1 i=\" + i); //test1 i=1 &#125; @Test public void test2() &#123; System.out.println(\"test2 i=\" + i); //test2 i=2 &#125;&#125; 代码示例java1234567891011121314151617181920212223242526272829303132333435import org.junit.*;public class JunitTest &#123; @BeforeClass //【静态方法】 public static void beforeClass() &#123; System.out.println(\"before class:begin this class================\"); &#125; @AfterClass //【静态方法】 public static void afterClass() &#123; System.out.println(\"after class:end this class=================\"); &#125; @Before public void before() &#123; System.out.println(\"before:begin test\"); &#125; @After public void after() &#123; System.out.println(\"after:end test\"); &#125; @Test public void Test() &#123; System.out.println(\"[this is a test!]\"); &#125; @Test public void Test2() &#123; System.out.println(\"[this is another test!!!!!]\"); &#125;&#125; 执行整个JunitTest文件，输出结果： Code12345678before class:begin this class&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;before:begin test[this is a test!]after:end testbefore:begin test[this is another test!!!!!]after:end testafter class:end this class&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 小结一整个JUnit4的单元测试用例执行顺序为： ​ @BeforeClass -&gt; @Before -&gt; @Test -&gt; @After -&gt; @AfterClass; 每一个单独的测试方法的调用顺序为： ​ @Before -&gt; @Test -&gt; @After;","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"JUnit","slug":"JUnit","permalink":"https://masteryang4.github.io/tags/JUnit/"},{"name":"单元测试","slug":"单元测试","permalink":"https://masteryang4.github.io/tags/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/"}]},{"title":"[SparkSQL]UDAF自定义聚合函数","slug":"SparkSQL-UDAF自定义聚合函数","date":"2020-05-05T13:30:07.000Z","updated":"2020-05-05T13:45:04.384Z","comments":true,"path":"2020/05/05/SparkSQL-UDAF自定义聚合函数/","link":"","permalink":"https://masteryang4.github.io/2020/05/05/SparkSQL-UDAF%E8%87%AA%E5%AE%9A%E4%B9%89%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0/","excerpt":"","text":"[SparkSQL]UDAF自定义聚合函数SparkSql中，用户可以设定自己的自定义聚合函数（UserDefinedAggregateFunction）。 需求：实现平均年龄 user.json 文件： json123&#123;\"username\": \"lisi\",\"userage\": 40&#125;&#123;\"username\": \"zhangsan\",\"userage\": 30&#125;&#123;\"username\": \"wangwu\",\"userage\":20&#125; UDAF - 弱类型scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import org.apache.spark.SparkConfimport org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types.&#123;DataType, DoubleType, LongType, StructField, StructType&#125;import org.apache.spark.sql.&#123;DataFrame, Row, SparkSession&#125;import org.apache.spark.util.AccumulatorV2object SparkSQL_UDAF01 &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"sparksql\") val spark = SparkSession.builder().config(sparkConf).getOrCreate() // TODO 读取JSON数据 val df: DataFrame = spark.read.json(\"input/user.json\") // TODO 使用自定义聚合函数实现年龄的平均值计算 // buffer // select avg(age) from user // 创建自定义函数 val udaf = new MyAvgAgeUDAF // 注册UDAF函数 spark.udf.register(\"avgAge\", udaf) df.createTempView(\"user\") spark.sql(\"select avgAge(userage) from user\").show spark.close &#125; /* * TODO 自定义聚合函数（UDAF） * 1. 继承UserDefinedAggregateFunction * 2. 重写方法 */ class MyAvgAgeUDAF extends UserDefinedAggregateFunction &#123; // TODO 传入聚合函数的数据结构 // 1 =&gt; age =&gt; Long override def inputSchema: StructType = &#123; StructType(Array( StructField(\"age\", LongType) )) &#125; // TODO 用于计算的缓冲区的数据结构 override def bufferSchema: StructType = &#123; StructType(Array( StructField(\"totalage\", LongType), StructField(\"totalcnt\", LongType) )) &#125; // TODO 输出结果的类型 override def dataType: DataType = DoubleType // TODO 函数稳定性（幂等性） // 给函数相同的输入值，计算结果也相同 override def deterministic: Boolean = true // TODO 用于计算的缓冲区初始化 override def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = 0L buffer(1) = 0L &#125; // TODO 将输入的值更新到缓冲区中 override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; buffer(0) = buffer.getLong(0) + input.getLong(0) buffer(1) = buffer.getLong(1) + 1L &#125; // TODO 合并缓冲区 // MutableAggregationBuffer 继承了Row override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0) buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1) &#125; // TODO 计算结果 override def evaluate(buffer: Row): Any = &#123; buffer.getLong(0).toDouble / buffer.getLong(1) &#125; &#125;&#125; 输出： sql12345+---------------------+|myavgageudaf(userage)|+---------------------+| 30.0|+---------------------+ UDAF - 强类型scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, Dataset, Encoder, Encoders, SparkSession, TypedColumn&#125;import org.apache.spark.sql.expressions.Aggregatorobject UDAF02 &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"myudaf\") val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate() import spark.implicits._ val df: DataFrame = spark.read.json(\"input/user.json\") //封装为DataSet val ds: Dataset[User01] = df.as[User01] //创建聚合函数 var myAgeUdtf1 = new MyAveragUDAF1 //将聚合函数转换为查询的列 val col: TypedColumn[User01, Double] = myAgeUdtf1.toColumn //查询 ds.select(col).show() &#125; //输入数据类型 case class User01(username: String, userage: Long) //缓存类型 case class AgeBuffer(var sum: Long, var count: Long) /** * 定义类继承org.apache.spark.sql.expressions.Aggregator * 重写类中的方法 */ class MyAveragUDAF1 extends Aggregator[User01, AgeBuffer, Double] &#123; override def zero: AgeBuffer = &#123; AgeBuffer(0L, 0L) &#125; override def reduce(b: AgeBuffer, a: User01): AgeBuffer = &#123; b.sum = b.sum + a.userage b.count = b.count + 1 b &#125; override def merge(b1: AgeBuffer, b2: AgeBuffer): AgeBuffer = &#123; b1.sum = b1.sum + b2.sum b1.count = b1.count + b2.count b1 &#125; override def finish(buff: AgeBuffer): Double = &#123; buff.sum.toDouble / buff.count &#125; //DataSet默认额编解码器，用于序列化，固定写法 //自定义类型就是produce 自带类型根据类型选择 override def bufferEncoder: Encoder[AgeBuffer] = &#123; Encoders.product &#125; override def outputEncoder: Encoder[Double] = &#123; Encoders.scalaDouble &#125; &#125;&#125; 输出： sql12345+-----------------------------------------------------+|MyAveragUDAF1(com.atguigu.sparksql.UDAF_qiang$User01)|+-----------------------------------------------------+| 30.0|+-----------------------------------------------------+","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"},{"name":"sparksql","slug":"sparksql","permalink":"https://masteryang4.github.io/tags/sparksql/"}]},{"title":"HashMap文章推荐","slug":"HashMap文章推荐","date":"2020-04-30T15:42:39.000Z","updated":"2020-05-09T13:27:03.153Z","comments":true,"path":"2020/04/30/HashMap文章推荐/","link":"","permalink":"https://masteryang4.github.io/2020/04/30/HashMap%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90/","excerpt":"","text":"HashMap文章推荐Java 8系列之重新认识HashMap 【强烈推荐】来自美团技术团队，里面的参考文章也非常好 《吊打面试官》系列-HashMap 《吊打面试官》系列-ConcurrentHashMap &amp; HashTable 来自敖丙（蘑菇街大佬），从面试官角度阐述关键技术点，十分硬核，全是干货。 一个HashMap跟面试官扯了半个小时 面试者角度阐述HashMap。 有空闲时间的话，我自己也会出一篇，甚至是一系列的HashMap文章， 比如 源码分析， 知识点总结， 常考面试题归档 等等","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"集合","slug":"Java/集合","permalink":"https://masteryang4.github.io/categories/Java/%E9%9B%86%E5%90%88/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"hashmap","slug":"hashmap","permalink":"https://masteryang4.github.io/tags/hashmap/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"Java IO相关总结归纳","slug":"Java-IO相关总结归纳","date":"2020-04-29T14:12:12.000Z","updated":"2020-04-29T14:14:13.243Z","comments":true,"path":"2020/04/29/Java-IO相关总结归纳/","link":"","permalink":"https://masteryang4.github.io/2020/04/29/Java-IO%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93%E5%BD%92%E7%BA%B3/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"[spark]十一种方式实现WordCount","slug":"spark-十一种方式实现WordCount","date":"2020-04-27T14:23:10.000Z","updated":"2020-04-27T15:12:53.079Z","comments":true,"path":"2020/04/27/spark-十一种方式实现WordCount/","link":"","permalink":"https://masteryang4.github.io/2020/04/27/spark-%E5%8D%81%E4%B8%80%E7%A7%8D%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0WordCount/","excerpt":"","text":"[Spark]十一种方式实现WordCount使用Spark中的11种方法实现经典的WordCount算法。 其中，10种SparkRDD（算子）+ 1种自定义累加器实现。 特朗普：没人比我更懂WordCount！（滑稽） Why WordCount？ 大数据中最经典的算法，相当于编程语言中的“Hello World”。 在大数据处理中，大多数复杂的问题通常被拆分成一个个小问题，这些小问题一般都是基于WordCount算法。所以，WordCount是重中之重，是大数据处理算法的基石。 10种Spark算子实现scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable/** * spark-使用十种[算子]实现wordcount */object RDDWordcount &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark\") val sc = new SparkContext(sparkConf) // val rdd = sc.textFile(\"input/wc.txt\").flatMap(datas =&gt; &#123; // datas.split(\" \") // &#125;) val rdd = sc.makeRDD(List(\"hadoop\", \"hello\", \"spark\", \"hello\", \"scala\", \"hello\", \"scala\", \"spark\")) println(\"=================1====================\") rdd.countByValue().foreach(println) println(\"=================2====================\") rdd.map((_, 1)).countByKey().foreach(println) println(\"=================3====================\") rdd.map((_, 1)).reduceByKey(_ + _).collect().foreach(println) println(\"=================4====================\") rdd.map((_, 1)).groupByKey().mapValues(_.size).collect().foreach(println) println(\"=================5====================\") rdd.map((_, 1)).aggregateByKey(0)(_ + _, _ + _).collect().foreach(println) println(\"=================6====================\") rdd.map((_, 1)).foldByKey(0)(_ + _).collect().foreach(println) println(\"=================7====================\") rdd.map((_, 1)).combineByKey( (num: Int) =&gt; num, (x: Int, y: Int) =&gt; &#123; x + y &#125;, (x: Int, y: Int) =&gt; &#123; x + y &#125; ).collect().foreach(println) println(\"=================8====================\") rdd.map((_, 1)).groupBy(_._1).map(kv =&gt; &#123; (kv._1, kv._2.size) &#125;).collect().foreach(println) println(\"=================9====================\") rdd.aggregate(mutable.Map[String, Int]())( (map, word) =&gt; &#123; map(word) = map.getOrElse(word, 0) + 1 map &#125;, (map1, map2) =&gt; &#123; map1.foldLeft(map2)( (finalMap, kv) =&gt; &#123; finalMap(kv._1) = finalMap.getOrElse(kv._1, 0) + kv._2 finalMap &#125; ) &#125; ).foreach(println) println(\"=================10====================\") rdd.map(s =&gt; mutable.Map(s -&gt; 1)).fold(mutable.Map[String, Int]())( (map1, map2) =&gt; &#123; map1.foldLeft(map2)( (finalMap, kv) =&gt; &#123; finalMap(kv._1) = finalMap.getOrElse(kv._1, 0) + kv._2 finalMap &#125; ) &#125; ).foreach(println) sc.stop() &#125;&#125; 输出结果： scala1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950=================1====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================2====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================3====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================4====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================5====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================6====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================7====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================8====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================9====================(hadoop,1)(spark,2)(scala,2)(hello,3)=================10====================(hadoop,1)(spark,2)(scala,2)(hello,3) 自定义累加器实现scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import org.apache.spark.rdd.RDDimport org.apache.spark.util.AccumulatorV2import org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutableobject MyAccTest &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setAppName(\"acc\").setMaster(\"local[*]\") val sc: SparkContext = new SparkContext(conf) // TODO Spark - 自定义累加器 - wordcount // 累加器可以不使用shuffle就完成数据的聚合功能 val rdd: RDD[String] = sc.makeRDD(List(\"hadoop spark\", \"hello\", \"spark\", \"hello\", \"scala\", \"hello\", \"scala\", \"spark\")) // TODO 1. 创建累加器 val acc = new WordCountAccumulator // TODO 2. 向Spark注册累加器 sc.register(acc, \"wordcount\") // TODO 3. 使用累加器 rdd.foreach( words =&gt; &#123; val ws = words.split(\" \") ws.foreach( word =&gt; &#123; acc.add(word) &#125; ) &#125; ) println(acc.value) //Map(hadoop -&gt; 1, spark -&gt; 3, scala -&gt; 2, hello -&gt; 3) sc.stop() &#125; // 自定义累加器 Map&#123;(Word - Count), (Word - Count)&#125; // 1, 继承AccumulatorV2, 定义泛型 // IN : 向累加器传递的值的类型 , Out : 累加器的返回结果类型 // 2. 重写方法 class WordCountAccumulator extends AccumulatorV2[String, mutable.Map[String, Int]] &#123; var innerMap = mutable.Map[String, Int]() // TODO 累加器是否初始化 // Z override def isZero: Boolean = innerMap.isEmpty // TODO 复制累加器 override def copy(): AccumulatorV2[String, mutable.Map[String, Int]] = &#123; new WordCountAccumulator &#125; // TODO 重置累加器 override def reset(): Unit = &#123; innerMap.clear() &#125; // TODO 累加数据 override def add(word: String): Unit = &#123; val cnt = innerMap.getOrElse(word, 0) innerMap.update(word, cnt + 1) &#125; // TODO 合并累加器 override def merge(other: AccumulatorV2[String, mutable.Map[String, Int]]): Unit = &#123; // 两个Map的合并 var map1 = this.innerMap var map2 = other.value innerMap = map1.foldLeft(map2)( (map, kv) =&gt; &#123; val k = kv._1 val v = kv._2 map(k) = map.getOrElse(k, 0) + v map &#125; ) &#125; // TODO 获取累加器的值，就是累加器的返回结果 override def value: mutable.Map[String, Int] = innerMap &#125;&#125; 输出结果： scala1Map(spark -&gt; 3, hadoop -&gt; 1, scala -&gt; 2, hello -&gt; 3)","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"},{"name":"scala","slug":"scala","permalink":"https://masteryang4.github.io/tags/scala/"},{"name":"wordcount","slug":"wordcount","permalink":"https://masteryang4.github.io/tags/wordcount/"}]},{"title":"kafka高效读写数据","slug":"kafka高效读写数据","date":"2020-04-27T12:28:01.000Z","updated":"2020-06-18T06:40:37.634Z","comments":true,"path":"2020/04/27/kafka高效读写数据/","link":"","permalink":"https://masteryang4.github.io/2020/04/27/kafka%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE/","excerpt":"","text":"kafka高效读写数据一、分布式集群Kafka本身是分布式集群；同时采用分区技术，并发度高。 zookeeper在kafka中的作用：kafka集群中有一个broker会被选举成controller，负责管理集群broker的上下线，所有的topic分区副本分配和leader选举等工作。controller的管理工作都依赖于zk。 二、顺序写磁盘Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。 三、零复制技术kafka零复制技术示意图： java复制技术示意图： （仅仅复制文件，没有对于文件的应用，效率很低。 文件要经过操作系统层（OS层）Buffer缓存传给java应用层输入流，输入流再将数据写到输出流，输出流将数据写到OS层缓存，缓存在将数据写到新的文件。。。） 因为java复制技术在拷贝文件时效率较低，所以对上图做出优化，如下图所示： （应用层通知操作系统层：仅仅是复制文件，所以操作系统层就不会将数据传给应用层，直接在操作系统层复制文件即可。）","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"kafka","slug":"大数据/kafka","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"kafka","slug":"kafka","permalink":"https://masteryang4.github.io/tags/kafka/"}]},{"title":"flume总结与思考","slug":"精-flume总结与思考","date":"2020-04-24T15:50:53.000Z","updated":"2020-05-14T08:34:21.325Z","comments":true,"path":"2020/04/24/精-flume总结与思考/","link":"","permalink":"https://masteryang4.github.io/2020/04/24/%E7%B2%BE-flume%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"flume","slug":"大数据/flume","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flume/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"flume","slug":"flume","permalink":"https://masteryang4.github.io/tags/flume/"}]},{"title":"kafka分区分配策略","slug":"kafka分区分配策略","date":"2020-04-23T09:09:56.000Z","updated":"2020-06-18T06:40:54.884Z","comments":true,"path":"2020/04/23/kafka分区分配策略/","link":"","permalink":"https://masteryang4.github.io/2020/04/23/kafka%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/","excerpt":"","text":"kafka分区分配策略kafka系列总结之：kafka分区分配策略[转载&amp;归纳] kafka是由Apache软件基金会开发的一个开源流处理平台，由Scala和Java编写。 Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据 kafka官网： kafka.apache.org kafka分区分配策略文章索引 1、 Kafka分区分配策略（1）——RangeAssignor 2、 Kafka分区分配策略（2）——RoundRobinAssignor和StickyAssignor 3、 Kafka分区分配策略（3）——自定义分区分配策略 4、 Kafka分区分配策略（4）——分配的实施 [注]作者为 《深入理解Kafka:核心设计与实践原理》 的作者：朱忠华老师 作者更多kafka技术文章： https://blog.csdn.net/u013256816/category_6500871.html 作者个人博客： http://honeypps.com/ 作者CSDN博客： https://blog.csdn.net/u013256816","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"kafka","slug":"大数据/kafka","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"kafka","slug":"kafka","permalink":"https://masteryang4.github.io/tags/kafka/"}]},{"title":"scala中的flatMap和foldLeft函数","slug":"scala中的flatMap和foldLeft函数","date":"2020-04-21T14:34:32.000Z","updated":"2020-04-27T15:10:43.047Z","comments":true,"path":"2020/04/21/scala中的flatMap和foldLeft函数/","link":"","permalink":"https://masteryang4.github.io/2020/04/21/scala%E4%B8%AD%E7%9A%84flatMap%E5%92%8CfoldLeft%E5%87%BD%E6%95%B0/","excerpt":"","text":"scala中的flatMap和foldLeft函数scala由于其函数式编程的特性，在大数据的处理中被广泛使用。 此文针对scala集合中两个常用的，却不太好理解的函数进行示例讲解。 flatMapscala中最重要的函数之一，映射扁平化 把握以下三点即可： 1、flatMap = map + flatten 2、什么类型调用的flatMap方法，则返回的也是什么类型 3、先对集合中的每个元素进行map， ​ 再对map后的每个元素（map后的每个元素必须还是集合）中的每个元素进行flatten [注] 进行map的对象可以是只含一层的集合，但进行flatten操作的对象必需是至少含两层的集合 map和flatten示例： scala123456789101112131415object Test0001 &#123; def main(args: Array[String]): Unit = &#123; val list = List(1,2,3,4) // 集合映射 println(\"map =&gt; \" + list.map(x=&gt;&#123;x*2&#125;)) //map =&gt; List(2, 4, 6, 8) println(\"map =&gt; \" + list.map(x=&gt;x*2)) //map =&gt; List(2, 4, 6, 8) println(\"map =&gt; \" + list.map(_*2)) //map =&gt; List(2, 4, 6, 8) // 集合扁平化 val list1 = List( List(1,2), List(3,4) ) println(\"flatten =&gt;\" + list1.flatten) //flatten =&gt;List(1, 2, 3, 4) &#125;&#125; flatMap示例一： scala123val words = Set(\"scala\", \"spark\", \"hadoop\")val result = words.flatMap(x =&gt; x.toUpperCase)println(result) //Set(A, L, P, C, H, K, R, O, D, S) flatMap示例二： scala123456val tuples: List[(String, Int)] = List((\"Hello Scala\", 4), (\"Hello Spark\", 2))val strings: List[String] = tuples.map(t=&gt;&#123;(t._1+\" \")*t._2&#125;)//List(Hello Scala Hello Scala Hello Scala Hello Scala , Hello Spark Hello Spark )val flatMapList: List[String] = strings.flatMap(t=&gt;&#123;t.split(\" \")&#125;)//List(Hello, Scala, Hello, Scala, Hello, Scala, Hello, Scala, Hello, Spark, Hello, Spark) flatMap示例三： scala1234567val linesList = List((\"Hello Scala\", 4), (\"Hello Spark\", 2))val flatMapList: List[(String, Int)] = linesList.flatMap(t =&gt; &#123; val line: String = t._1 val words = line.split(\" \") words.map(w =&gt; (w, t._2))&#125;)println(flatMapList) //List((Hello,4), (Scala,4), (Hello,2), (Spark,2)) 根据上述三个原则即可算出函数结果。 foldLeft集合折叠函数，fold、foldRight底层都是基于foldLeft函数。 所以本文用到的函数可以不用严格区分，主要阐述其原理。 scala1def fold[A1 &gt;: A](z: A1)(op: (A1, A1) =&gt; A1): A1 = foldLeft(z)(op) 就是将集合的数据和集合之外的数据进行聚合操作。 fold方法有函数柯里化，有2个参数列表 第一个参数列表：集合之外的数据 第二个参数列表：表示计算规则 fold示例一： scala12345val list = List(1, 2, 3, 4)// 集合折叠println(\"fold =&gt; \" + list.fold(0)(_+_)) //10// 集合折叠(左)println(\"foldLeft =&gt; \" + list.foldLeft(0)(_+_)) //10 fold示例二： scala123456789101112131415161718192021object Scala21_Collection_Method4 &#123; def main(args: Array[String]): Unit = &#123; // 将两个Map集合进行合并(merge)处理 val map1 = mutable.Map(\"a\" -&gt; 1, \"b\" -&gt; 2, \"c\" -&gt; 3) val map2 = mutable.Map(\"a\" -&gt; 4, \"d\" -&gt; 5, \"c\" -&gt; 6) // Map( \"a\"-&gt;5, \"b\"-&gt;2, \"c\"-&gt;9 ,\"d\"-&gt;5) val map3 = map2.foldLeft(map1)( (map, kv) =&gt; &#123; val k = kv._1 val v = kv._2 //map.update(k, map.getOrElse(k, 0) + v) map(k) = map.getOrElse(k, 0) + v map &#125; ) println(map3) //Map(b -&gt; 2, d -&gt; 5, a -&gt; 5, c -&gt; 9) println(map1) //Map(b -&gt; 2, d -&gt; 5, a -&gt; 5, c -&gt; 9) println(map2) //Map(d -&gt; 5, a -&gt; 4, c -&gt; 6) &#125;&#125; 原理示意图如下： 总结： 其实，在foldleft函数中，第二个参数规定的就是， foldleft第一个参数和foldleft调用者的第一个元素的运算规则 可以用如下公式理解： a. foldLeft( b )( (b,a的第一个元素)=&gt;{} ) （对应上面示意图：红色块为b，蓝色块为a） 只不过在此公式中b和a的第一个元素都是动态变化的： ​ b一直在迭代，a会继续往后顺序取后面的值。 ​ 其实函数最终返回值就是b的值（上面的例子map1和map3相等也能证明这一点，本质就是map1把值赋给了map3），且a不发生改变。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"scala","slug":"大数据/scala","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/scala/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"scala","slug":"scala","permalink":"https://masteryang4.github.io/tags/scala/"}]},{"title":"Java空指针问题的本质","slug":"Java空指针问题的本质","date":"2020-04-18T15:58:28.000Z","updated":"2020-04-18T16:00:16.725Z","comments":true,"path":"2020/04/18/Java空指针问题的本质/","link":"","permalink":"https://masteryang4.github.io/2020/04/18/Java%E7%A9%BA%E6%8C%87%E9%92%88%E9%97%AE%E9%A2%98%E7%9A%84%E6%9C%AC%E8%B4%A8/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"面试题：sleep和wait的区别","slug":"sleep和wait的区别小结","date":"2020-04-14T08:01:30.000Z","updated":"2020-04-16T15:49:47.457Z","comments":true,"path":"2020/04/14/sleep和wait的区别小结/","link":"","permalink":"https://masteryang4.github.io/2020/04/14/sleep%E5%92%8Cwait%E7%9A%84%E5%8C%BA%E5%88%AB%E5%B0%8F%E7%BB%93/","excerpt":"","text":"Java中sleep和wait方法的区别 sleep和wait都能使线程处于阻塞状态，但二者有着本质区别。 代码示例java123456789101112131415161718public class test_thread &#123; public static void main(String[] args) throws Exception &#123; Thread t1 = new Thread(); Thread t2 = new Thread(); //【本质区别】静态方法和成员方法 //【静态方法】，绑定的是类。休眠的线程不是t1，是当前运行的main线程 //和对象都没有关系，所以不存在什么对象锁 t1.sleep(1000); Thread.sleep(1000); //【成员方法】，等待的线程就是t2 //有同步/synchronized关键字才能拿到对象锁。 t2.wait(); t2.wait(1000);//wait也可以加等待时间 //【扩展】scala中的伴生对象就是对静态语法的模拟 &#125;&#125; 总结 【核心】静态方法、成员方法 sleep是Thread类的静态方法。sleep的作用是让线程休眠道制定的时间，在时间到达时恢复，也就是说sleep将在接到时间到达事件事恢复线程执行。 wait是Object的方法，也就是说可以对任意一个对象调用wait方法，调用wait方法将会属将调用者的线程挂起，直到其他线程调用同一个对象的notify方法才会重新激活调用者。 sleep方法没有释放锁（lock），而wait方法释放了锁，使得其他线程可以使用同步控制块或者方法。 【使用范围】 wait，notify和notifyAll只能在同步控制方法或者同步控制块里面使用， 而sleep可以在任何地方使用","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"多线程","slug":"Java/多线程","permalink":"https://masteryang4.github.io/categories/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"Git使用小结","slug":"Git使用小结","date":"2020-04-12T16:25:50.000Z","updated":"2020-06-07T16:02:10.436Z","comments":true,"path":"2020/04/13/Git使用小结/","link":"","permalink":"https://masteryang4.github.io/2020/04/13/Git%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93/","excerpt":"","text":"Git使用小结小结Git常用指令，以及如何将本地代码同步/更新到Github的常用指令 一、初始配置git安装完成后，需要设置一下，在命令行输入 Code12$ git config --global user.name &quot;Your Name&quot;$ git config --global user.email &quot;email@example.com&quot; –global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。 二、常用指令进入到自己的项目文件下右键选择Git Bash Here打开git客户端 初始化项目： Code1git init 将文件添加到本地仓库： Code1git add 将文件提交到仓库 Code1git commit -m &quot;注释内容&quot; 关联远程项目（你的远程仓库地址） Code1git remote add origin https:&#x2F;&#x2F;github.com&#x2F;xxxx&#x2F;xxx.git 本地推送到远程（ 第一次推送master分支的所有内容） 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 Code1git push -u origin master 查看Git状态 Code1git status 三、更新文件到GithubCode1234git addgit commit -m &quot;注释内容&quot;git pull origin master #从远程抓取分支，使用git pull，如果有冲突，要先处理冲突git push origin master 查看远程库信息： Code1git remote -v 会显示可以抓取和推送的origin的地址。如果没有推送权限，就看不到push的地址。","categories":[{"name":"Git&Github","slug":"Git-Github","permalink":"https://masteryang4.github.io/categories/Git-Github/"}],"tags":[{"name":"Git&Github","slug":"Git-Github","permalink":"https://masteryang4.github.io/tags/Git-Github/"}]},{"title":"关于i=i++的分析与思考","slug":"关于i-i-的分析与思考","date":"2020-04-10T16:00:30.000Z","updated":"2020-04-18T15:22:45.874Z","comments":true,"path":"2020/04/11/关于i-i-的分析与思考/","link":"","permalink":"https://masteryang4.github.io/2020/04/11/%E5%85%B3%E4%BA%8Ei-i-%E7%9A%84%E5%88%86%E6%9E%90%E4%B8%8E%E6%80%9D%E8%80%83/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"关于HashMap的两道小面试题","slug":"关于HashMap的两道小面试题","date":"2020-04-10T16:00:16.000Z","updated":"2020-04-18T15:23:25.435Z","comments":true,"path":"2020/04/11/关于HashMap的两道小面试题/","link":"","permalink":"https://masteryang4.github.io/2020/04/11/%E5%85%B3%E4%BA%8EHashMap%E7%9A%84%E4%B8%A4%E9%81%93%E5%B0%8F%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"集合","slug":"Java/集合","permalink":"https://masteryang4.github.io/categories/Java/%E9%9B%86%E5%90%88/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"String_StringBuffer_StringBuilder分析总结","slug":"String_StringBuffer_StringBuilder分析总结","date":"2020-04-10T16:00:03.000Z","updated":"2020-04-24T15:48:19.991Z","comments":true,"path":"2020/04/11/String_StringBuffer_StringBuilder分析总结/","link":"","permalink":"https://masteryang4.github.io/2020/04/11/String_StringBuffer_StringBuilder%E5%88%86%E6%9E%90%E6%80%BB%E7%BB%93/","excerpt":"","text":"String_StringBuffer_StringBuilder分析总结本文对Java语言中的String，StringBuffer，StringBuilder类进行分析对比， 并String类型进行简单原理分析。 String，StringBuffer，StringBuilder的区别1、可变与不可变 String类中使用字符数组保存字符串，如下就是，因为有“final”修饰符，所以可以知道string对象是不可变的。 java1private final char value[]; StringBuilder与StringBuffer都继承自AbstractStringBuilder类，在AbstractStringBuilder中也是使用字符数组保存字符串，如下就是，可知这两种对象都是可变的。 java1char[] value; 2、是否多线程安全 String中的对象是不可变的，也就可以理解为常量，显然线程安全。 AbstractStringBuilder是StringBuilder与StringBuffer的公共父类，定义了一些字符串的基本操作，如expandCapacity、append、insert、indexOf等公共方法。 StringBuilder并没有对方法进行加同步锁，所以是非线程安全的。 ​ StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。看如下源码： java12345678public synchronized StringBuffer reverse() &#123; super.reverse(); return this;&#125;public int indexOf(String str) &#123; return indexOf(str, 0); //存在 public synchronized int indexOf(String str, int fromIndex) 方法&#125; 3、StringBuilder与StringBuffer共同点 StringBuilder与StringBuffer有公共父类AbstractStringBuilder(抽象类)。 抽象类与接口的其中一个区别是：抽象类中可以定义一些子类的公共方法，子类只需要增加新的功能，不需要重复写已经存在的方法；而接口中只是对方法的申明和常量的定义。 StringBuilder、StringBuffer的方法都会调用AbstractStringBuilder中的公共方法，如super.append(…)。只是StringBuffer会在方法上加synchronized关键字，进行同步。 如果程序不是多线程的，那么使用StringBuilder效率高于StringBuffer。 String相关String类部分源码： java123456789101112131415161718192021222324252627282930313233343536public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; //... public String() &#123; this.value = \"\".value; &#125; public String(String original) &#123; this.value = original.value; this.hash = original.hash; &#125; public String(char value[]) &#123; this.value = Arrays.copyOf(value, value.length); &#125; //... public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h; &#125; 说明： private final char value[];说明String不可变 其实不可变指的是其字符串内容不可变，字符串对象的地址其实是可以改变的，示例如下： java1234String a = \"ABCabc\";System.out.println(\"a = \" + a); //a = ABCabca = a.replace('A', 'a');System.out.println(\"a = \" + a); //a = aBCabc ​ 这个例子的本质是，字符串对象a指向了一个新的字符串数组。 如果真的要去修改String内容的话，其实也是可以的，使用反射机制就可以实现，示例如下： java123456789101112131415161718192021public static void testReflection() throws Exception &#123; //创建字符串\"Hello World\"， 并赋给引用s String s = \"Hello World\"; System.out.println(\"s = \" + s); //Hello World //获取String类中的value字段 Field valueFieldOfString = String.class.getDeclaredField(\"value\"); //改变value属性的访问权限 valueFieldOfString.setAccessible(true); //获取s对象上的value属性的值 char[] value = (char[]) valueFieldOfString.get(s); //改变value所引用的数组中的第5个字符 value[5] = '_'; System.out.println(\"s = \" + s); //Hello_World&#125; 参考博客： https://www.cnblogs.com/leskang/p/6110631.html https://www.cnblogs.com/xudong-bupt/p/3961159.html","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"Java中final关键字小结","slug":"Java中final关键字小结","date":"2020-04-10T15:59:41.000Z","updated":"2020-04-16T15:44:18.891Z","comments":true,"path":"2020/04/10/Java中final关键字小结/","link":"","permalink":"https://masteryang4.github.io/2020/04/10/Java%E4%B8%ADfinal%E5%85%B3%E9%94%AE%E5%AD%97%E5%B0%8F%E7%BB%93/","excerpt":"","text":"Java中final关键字小结一、final、finally、finalize的区别final： 修饰符（关键字）有三种用法：修饰类、变量和方法。 修饰类时，意味着它不能再派生出新的子类，即不能被继承，因此它和 abstract 是反义词。 修饰变量时，该变量使用中不被改变，必须在声明时给定初值，在引用中只能读取不可修改，即为常量。（下一节代码示例） 修饰方法时，也同样只能使用，不能在子类中被重写。 finally: 通常放在 try…catch 的后面构造最终执行代码块，这就意味着程序无论正常执行还是发生异常，这里的代码只要 JVM 不关闭都能执行，可以将释放外部资源的代码写在finally块中。 finalize： Object 类中定义的方法。 Java 中允许使用 finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。 这个方法是由垃圾收集器在销毁对象时调用的，通过重写 finalize() 方法可以整理系统资源或者执行其他清理工作。 二、代码示例java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class zd_important_test_nbst &#123; public static void main(String[] args) &#123; UserTest userTest = new UserTest(); System.out.println(userTest.getA()); UserTest userTest1 = new UserTest(6); System.out.println(userTest1.getA()); &#125;&#125;class UserTest &#123; /** * final修饰的变量，要么一开始就初始化（饿汉式），要么就在构造方法里初始化（懒汉式）。 * 一旦初始化完成，就不能修改。 * * 【String中同样，使用private final修饰char[]】所以String是不可变的。 * （通过反射可以破坏其不可变性） * 其他博客会提到上述内容。 */ private final int a; public UserTest() &#123; super(); a = 1; &#125; public UserTest(int a) &#123; this.a = a; &#125; public int getA() &#123; return a; &#125; //会报错，因为a是final// public void setA(int b) &#123;// this.a = b;// &#125; /** * 以下为idea默认生成的hashcode和equals，可忽略 * * 在Object的源码中，hashcode是native方法，使用c语言实现的，综合类的信息计算出的hashcode值 * equals底层就是“==” */ @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; UserTest userTest = (UserTest) o; return a == userTest.a; &#125; @Override public int hashCode() &#123; return Objects.hash(a); &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"Java静态代码块的加载时机","slug":"Java静态代码块的加载时机","date":"2020-04-10T15:57:07.000Z","updated":"2020-04-18T15:22:19.726Z","comments":true,"path":"2020/04/10/Java静态代码块的加载时机/","link":"","permalink":"https://masteryang4.github.io/2020/04/10/Java%E9%9D%99%E6%80%81%E4%BB%A3%E7%A0%81%E5%9D%97%E7%9A%84%E5%8A%A0%E8%BD%BD%E6%97%B6%E6%9C%BA/","excerpt":"","text":"Java静态代码块的加载时机在java中，静态代码块其实并不是随着类的加载而加载。","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"SQL的执行顺序问题","slug":"SQL的执行顺序问题","date":"2020-04-10T15:44:24.000Z","updated":"2020-04-14T13:41:32.432Z","comments":true,"path":"2020/04/10/SQL的执行顺序问题/","link":"","permalink":"https://masteryang4.github.io/2020/04/10/SQL%E7%9A%84%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%E9%97%AE%E9%A2%98/","excerpt":"","text":"SQL的执行顺序问题众所周知，sql的执行顺序： sql1from... where...group by... having....select ... order by... limit 但是，有一个“bug”，在 MySQL 中： sql1SELECT title, COUNT(title) AS t FROM table GROUP BY title HAVING t &gt;= 2 这样的语句是可以执行的。 正常来说，having在select之前执行，但是却可以使用select的别名，这是为什么呢？ 查阅了一切资料，做出如下解释： 解释一 mysql的处理方式是中间生成虚拟表（或者叫临时表），而这个虚拟表的生成的列靠的就是select。 所以猜测类似having之后的操作，其实内部已经根据select生成了虚拟表，列自然也是as后的。 解释二 之所以MYSQL可以这么做是因为MYSQL用的是临时表， 在having前已经产生了数据，所以可以用别名，但SQL Sever不可以，SQL是在having后才Select。","categories":[{"name":"SQL","slug":"SQL","permalink":"https://masteryang4.github.io/categories/SQL/"},{"name":"MySQL","slug":"SQL/MySQL","permalink":"https://masteryang4.github.io/categories/SQL/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://masteryang4.github.io/tags/MySQL/"}]},{"title":"LeetCode经典10道题","slug":"LeetCode经典10道题","date":"2020-03-22T15:36:52.000Z","updated":"2020-06-02T15:12:37.693Z","comments":true,"path":"2020/03/22/LeetCode经典10道题/","link":"","permalink":"https://masteryang4.github.io/2020/03/22/LeetCode%E7%BB%8F%E5%85%B810%E9%81%93%E9%A2%98/","excerpt":"","text":"LeetCode题目精选1. 两数之和链接：https://leetcode-cn.com/problems/two-sum/ 给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 Code1234给定 nums &#x3D; [2, 7, 11, 15], target &#x3D; 9因为 nums[0] + nums[1] &#x3D; 2 + 7 &#x3D; 9所以返回 [0, 1] 题解： java12345678910111213class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; int complement = target - nums[i]; if (map.containsKey(complement)) &#123; return new int[] &#123; map.get(complement), i &#125;; &#125; map.put(nums[i], i); &#125; throw new IllegalArgumentException(\"No two sum solution\"); &#125;&#125; 2. 爬楼梯链接：https://leetcode-cn.com/problems/climbing-stairs/ 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 示例 1： Code12345输入： 2输出： 2解释： 有两种方法可以爬到楼顶。1. 1 阶 + 1 阶2. 2 阶 示例 2： Code123456输入： 3输出： 3解释： 有三种方法可以爬到楼顶。1. 1 阶 + 1 阶 + 1 阶2. 1 阶 + 2 阶3. 2 阶 + 1 阶 题解： java1234567891011121314public class Solution &#123; public int climbStairs(int n) &#123; if (n == 1) &#123; return 1; &#125; int[] dp = new int[n + 1]; dp[1] = 1; dp[2] = 2; for (int i = 3; i &lt;= n; i++) &#123; dp[i] = dp[i - 1] + dp[i - 2]; &#125; return dp[n]; &#125;&#125; 3. 翻转二叉树链接：https://leetcode-cn.com/problems/invert-binary-tree/ 翻转一棵二叉树。 示例： 输入： Code12345 4 &#x2F; \\ 2 7 &#x2F; \\ &#x2F; \\1 3 6 9 输出： Code12345 4 &#x2F; \\ 7 2 &#x2F; \\ &#x2F; \\9 6 3 1 题解： java12345678910public TreeNode invertTree(TreeNode root) &#123; if (root == null) &#123; return null; &#125; TreeNode right = invertTree(root.right); TreeNode left = invertTree(root.left); root.left = right; root.right = left; return root;&#125; 4. 反转链表链接：https://leetcode-cn.com/problems/reverse-linked-list/ 反转一个单链表。 示例: Code12输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 题解： java1234567891011public ListNode reverseList(ListNode head) &#123; ListNode prev = null; ListNode curr = head; while (curr != null) &#123; ListNode nextTemp = curr.next; curr.next = prev; prev = curr; curr = nextTemp; &#125; return prev;&#125; 5. LRU缓存机制链接：https://leetcode-cn.com/problems/lru-cache/ 运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。 获取数据 get(key) - 如果密钥 (key) 存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。写入数据 put(key, value) - 如果密钥不存在，则写入其数据值。当缓存容量达到上限时，它应该在写入新数据之前删除最近最少使用的数据值，从而为新的数据值留出空间。 进阶: 你是否可以在 O(1) 时间复杂度内完成这两种操作？ 示例: Code1234567891011LRUCache cache &#x3D; new LRUCache( 2 &#x2F;* 缓存容量 *&#x2F; );cache.put(1, 1);cache.put(2, 2);cache.get(1); &#x2F;&#x2F; 返回 1cache.put(3, 3); &#x2F;&#x2F; 该操作会使得密钥 2 作废cache.get(2); &#x2F;&#x2F; 返回 -1 (未找到)cache.put(4, 4); &#x2F;&#x2F; 该操作会使得密钥 1 作废cache.get(1); &#x2F;&#x2F; 返回 -1 (未找到)cache.get(3); &#x2F;&#x2F; 返回 3cache.get(4); &#x2F;&#x2F; 返回 4 题解： java12345678910111213141516171819202122232425262728class LRUCache extends LinkedHashMap&lt;Integer, Integer&gt;&#123; private int capacity; public LRUCache(int capacity) &#123; super(capacity, 0.75F, true); this.capacity = capacity; &#125; public int get(int key) &#123; return super.getOrDefault(key, -1); &#125; public void put(int key, int value) &#123; super.put(key, value); &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) &#123; return size() &gt; capacity; &#125;&#125;/** * LRUCache 对象会以如下语句构造和调用: * LRUCache obj = new LRUCache(capacity); * int param_1 = obj.get(key); * obj.put(key,value); */ 6. 最长回文子串链接：https://leetcode-cn.com/problems/longest-palindromic-substring/ 给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。 示例 1： Code123输入: &quot;babad&quot;输出: &quot;bab&quot;注意: &quot;aba&quot; 也是一个有效答案。 示例 2： Code12输入: &quot;cbbd&quot;输出: &quot;bb&quot; 题解： java1234567891011121314151617181920212223public String longestPalindrome(String s) &#123; if (s == null || s.length() &lt; 1) return \"\"; int start = 0, end = 0; for (int i = 0; i &lt; s.length(); i++) &#123; int len1 = expandAroundCenter(s, i, i); int len2 = expandAroundCenter(s, i, i + 1); int len = Math.max(len1, len2); if (len &gt; end - start) &#123; start = i - (len - 1) / 2; end = i + len / 2; &#125; &#125; return s.substring(start, end + 1);&#125;private int expandAroundCenter(String s, int left, int right) &#123; int L = left, R = right; while (L &gt;= 0 &amp;&amp; R &lt; s.length() &amp;&amp; s.charAt(L) == s.charAt(R)) &#123; L--; R++; &#125; return R - L - 1;&#125; 7. 有效的括号链接：https://leetcode-cn.com/problems/valid-parentheses/ 给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串，判断字符串是否有效。 有效字符串需满足： 1. 左括号必须用相同类型的右括号闭合。 2. 左括号必须以正确的顺序闭合。 注意空字符串可被认为是有效字符串。 示例 1: Code12输入: &quot;()&quot;输出: true 示例 2: Code12输入: &quot;()[]&#123;&#125;&quot;输出: true 示例 3: Code12输入: &quot;(]&quot;输出: false 示例 4: Code12输入: &quot;([)]&quot;输出: false 示例 5: Code12输入: &quot;&#123;[]&#125;&quot;输出: true 题解： java1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123; // Hash table that takes care of the mappings. private HashMap&lt;Character, Character&gt; mappings; // Initialize hash map with mappings. This simply makes the code easier to read. public Solution() &#123; this.mappings = new HashMap&lt;Character, Character&gt;(); this.mappings.put(')', '('); this.mappings.put('&#125;', '&#123;'); this.mappings.put(']', '['); &#125; public boolean isValid(String s) &#123; // Initialize a stack to be used in the algorithm. Stack&lt;Character&gt; stack = new Stack&lt;Character&gt;(); for (int i = 0; i &lt; s.length(); i++) &#123; char c = s.charAt(i); // If the current character is a closing bracket. if (this.mappings.containsKey(c)) &#123; // Get the top element of the stack. If the stack is empty, set a dummy value of '#' char topElement = stack.empty() ? '#' : stack.pop(); // If the mapping for this bracket doesn't match the stack's top element, return false. if (topElement != this.mappings.get(c)) &#123; return false; &#125; &#125; else &#123; // If it was an opening bracket, push to the stack. stack.push(c); &#125; &#125; // If the stack still contains elements, then it is an invalid expression. return stack.isEmpty(); &#125;&#125; 8. 数组中的第K个最大元素链接：https://leetcode-cn.com/problems/kth-largest-element-in-an-array/ 在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。 示例 1: Code12输入: [3,2,1,5,6,4] 和 k &#x3D; 2输出: 5 示例 2: Code12输入: [3,2,3,1,2,4,5,5,6] 和 k &#x3D; 4输出: 4 说明: 你可以假设 k 总是有效的，且 1 ≤ k ≤ 数组的长度。 题解： java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.util.Random;class Solution &#123; int [] nums; public void swap(int a, int b) &#123; int tmp = this.nums[a]; this.nums[a] = this.nums[b]; this.nums[b] = tmp; &#125; public int partition(int left, int right, int pivot_index) &#123; int pivot = this.nums[pivot_index]; // 1. move pivot to end swap(pivot_index, right); int store_index = left; // 2. move all smaller elements to the left for (int i = left; i &lt;= right; i++) &#123; if (this.nums[i] &lt; pivot) &#123; swap(store_index, i); store_index++; &#125; &#125; // 3. move pivot to its final place swap(store_index, right); return store_index; &#125; public int quickselect(int left, int right, int k_smallest) &#123; /* Returns the k-th smallest element of list within left..right. */ if (left == right) // If the list contains only one element, return this.nums[left]; // return that element // select a random pivot_index Random random_num = new Random(); int pivot_index = left + random_num.nextInt(right - left); pivot_index = partition(left, right, pivot_index); // the pivot is on (N - k)th smallest position if (k_smallest == pivot_index) return this.nums[k_smallest]; // go left side else if (k_smallest &lt; pivot_index) return quickselect(left, pivot_index - 1, k_smallest); // go right side return quickselect(pivot_index + 1, right, k_smallest); &#125; public int findKthLargest(int[] nums, int k) &#123; this.nums = nums; int size = nums.length; // kth largest is (N - k)th smallest return quickselect(0, size - 1, size - k); &#125;&#125; 9. 实现 Trie (前缀树)实现一个 Trie (前缀树)，包含 insert, search, 和 startsWith 这三个操作。 示例: Code12345678Trie trie &#x3D; new Trie();trie.insert(&quot;apple&quot;);trie.search(&quot;apple&quot;); &#x2F;&#x2F; 返回 truetrie.search(&quot;app&quot;); &#x2F;&#x2F; 返回 falsetrie.startsWith(&quot;app&quot;); &#x2F;&#x2F; 返回 truetrie.insert(&quot;app&quot;); trie.search(&quot;app&quot;); &#x2F;&#x2F; 返回 true 说明: 你可以假设所有的输入都是由小写字母 a-z 构成的。 保证所有输入均为非空字符串。 题解： java1234567891011121314151617181920212223242526272829303132333435363738394041class Trie &#123; private TrieNode root; public Trie() &#123; root = new TrieNode(); &#125; // Inserts a word into the trie. public void insert(String word) &#123; TrieNode node = root; for (int i = 0; i &lt; word.length(); i++) &#123; char currentChar = word.charAt(i); if (!node.containsKey(currentChar)) &#123; node.put(currentChar, new TrieNode()); &#125; node = node.get(currentChar); &#125; node.setEnd(); &#125; // search a prefix or whole key in trie and // returns the node where search ends private TrieNode searchPrefix(String word) &#123; TrieNode node = root; for (int i = 0; i &lt; word.length(); i++) &#123; char curLetter = word.charAt(i); if (node.containsKey(curLetter)) &#123; node = node.get(curLetter); &#125; else &#123; return null; &#125; &#125; return node; &#125; // Returns if the word is in the trie. public boolean search(String word) &#123; TrieNode node = searchPrefix(word); return node != null &amp;&amp; node.isEnd(); &#125;&#125; 10. 编辑距离链接：https://leetcode-cn.com/problems/edit-distance/ 给定两个单词 word1 和 word2，计算出将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 1. 插入一个字符 2. 删除一个字符 3. 替换一个字符 示例 1: Code123456输入: word1 &#x3D; &quot;horse&quot;, word2 &#x3D; &quot;ros&quot;输出: 3解释: horse -&gt; rorse (将 &#39;h&#39; 替换为 &#39;r&#39;)rorse -&gt; rose (删除 &#39;r&#39;)rose -&gt; ros (删除 &#39;e&#39;) 示例 2: Code12345678输入: word1 &#x3D; &quot;intention&quot;, word2 &#x3D; &quot;execution&quot;输出: 5解释: intention -&gt; inention (删除 &#39;t&#39;)inention -&gt; enention (将 &#39;i&#39; 替换为 &#39;e&#39;)enention -&gt; exention (将 &#39;n&#39; 替换为 &#39;x&#39;)exention -&gt; exection (将 &#39;n&#39; 替换为 &#39;c&#39;)exection -&gt; execution (插入 &#39;u&#39;) 题解： java1234567891011121314151617181920212223242526272829303132333435class Solution &#123; public int minDistance(String word1, String word2) &#123; int n = word1.length(); int m = word2.length(); // if one of the strings is empty if (n * m == 0) return n + m; // array to store the convertion history int [][] d = new int[n + 1][m + 1]; // init boundaries for (int i = 0; i &lt; n + 1; i++) &#123; d[i][0] = i; &#125; for (int j = 0; j &lt; m + 1; j++) &#123; d[0][j] = j; &#125; // DP compute for (int i = 1; i &lt; n + 1; i++) &#123; for (int j = 1; j &lt; m + 1; j++) &#123; int left = d[i - 1][j] + 1; int down = d[i][j - 1] + 1; int left_down = d[i - 1][j - 1]; if (word1.charAt(i - 1) != word2.charAt(j - 1)) left_down += 1; d[i][j] = Math.min(left, Math.min(down, left_down)); &#125; &#125; return d[n][m]; &#125;&#125;","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://masteryang4.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://masteryang4.github.io/tags/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"HashMap底层实现源码分析","slug":"HashMap底层实现源码分析","date":"2020-03-16T15:21:13.000Z","updated":"2020-05-09T13:27:22.937Z","comments":true,"path":"2020/03/16/HashMap底层实现源码分析/","link":"","permalink":"https://masteryang4.github.io/2020/03/16/HashMap%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"HashMap底层实现原理0.样例数据java123456789101112131415161718192021222324252627282930313233343536373839import java.util.HashMap;import java.util.Iterator;import java.util.Map;import java.util.Set;public class CollectionTest &#123; public static void main(String[] args) &#123; //唯一的工作初始化负债因子（this.loadFactor = DEFAULT_LOAD_FACTOR）为0.75f Map&lt;String,Integer&gt; map = new HashMap&lt;&gt;(); int count = 1; //添加kv for (char i = 65; i &lt; 91; i++) &#123; map.put(String.valueOf(i),count); count++; &#125; //第一种遍历方式 Set&lt;String&gt; keySet = map.keySet(); Iterator&lt;String&gt; iterator = keySet.iterator(); while (iterator.hasNext()) &#123; String key = iterator.next(); System.out.println(key+ \" =&gt; \" + map.get(key)); &#125; System.out.println(\"******************************\"); //第二种遍历方式 Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; iteratorMap = map.entrySet().iterator(); while (iteratorMap.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; mapEntry = iteratorMap.next(); System.out.println(mapEntry); &#125; System.out.println(\"******************************\"); //第三种遍历方式 for (Map.Entry&lt;String, Integer&gt; entry : map.entrySet()) &#123; System.out.println(entry.getKey() + \" =&gt; \" + entry.getValue()); &#125; &#125;&#125; 1. 类信息java1public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; 2. 基本属性java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125private static final long serialVersionUID = 362498820763181265L; //序列化版本号static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 默认容量16(左移4位相当于乘以2的4次方)static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//最大容量（1073741824）static final float DEFAULT_LOAD_FACTOR = 0.75f;//默认负载因子static final int TREEIFY_THRESHOLD = 8; //链表节点转换红黑树节点的阈值static final int UNTREEIFY_THRESHOLD = 6; //红黑树节点转换链表节点的阈值static final int MIN_TREEIFY_CAPACITY = 64;// 转红黑树时, table的最小长度// 基本hash节点, 继承自Entry，此时的Node节点就是相当于Entry节点的实现static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + \"=\" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125;transient Node&lt;K,V&gt;[] table; //hashMap数组的表示transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; //entry节点transient int size; //数组长度transient int modCount; //添加的元素个数int threshold; //合理的初始化数组长度，根据tableSizeFor()得到，用于手动设置时使用final float loadFactor; //负载因子，用于手动设置时使用//构造器一：定义Node[]数组初始长度public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); //为Node[]数组设置负债因子 this.loadFactor = loadFactor; //为Node[]数组设置一个合理的值 this.threshold = tableSizeFor(initialCapacity);&#125;//初始化Node[]数组长度，根据传入的值以2的n次方对数组进行扩容//（例如：存入传入值为9，数组容量为16，在(8,16]范围内将不会再次扩容）。static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= 1 &lt;&lt; 30) ? 1 &lt;&lt; 30 : n + 1;&#125;//构造器二：调用HashMap(int initialCapacity, float loadFactor)构造器public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;//构造器三：仅创建HashMap对象，并初始化负债因子为0.75fpublic HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR;&#125;// 红黑树节点static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; /** * Returns root of tree containing this node. */ final TreeNode&lt;K,V&gt; root() &#123; for (TreeNode&lt;K,V&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; &#125; //...&#125; 3. hash算法HashMap定位数组索引位置，直接决定了hash方法的离散性能。下面是定位哈希桶数组的源码： java123456789101112131415161718192021222324252627// 计算key的hash值static final int hash(Object key) &#123; int h; // 1.先拿到key的hashCode值,基本数据类型会使用其包装类重载的hashCode()方法去计算hash值，引用数据类型根据是否重写去计算 // 2.将hashCode的高16位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; // 将(tab.length - 1) 与 hash值进行&amp;运算 int index = (tab.length - 1) &amp; hash;&#125;//对值进行Hash计算public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = valu /** * 当KEY值为A测试数据，A的hash为: 31 * hash + ANSI码值65 * 当KEY值为AB测试数据，AB的hash为：31 * 65 + 66 */ for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h;&#125; HashMap底层数组的长度总是2的n次方，并且取模运算为“h mod table.length”，对应上面的公式，可以得到该运算等同于“h &amp; (table.length - 1)”。这是HashMap在速度上的优化，因为&amp;比%具有更高的效率。 在JDK1.8的实现中，还优化了高位运算的算法，将hashCode的高16位与hashCode进行异或运算，主要是为了在table的length较小的时候，让高位也参与运算，并且不会有太大的开销。 4. get方法java1234567891011121314151617181920212223242526272829303132//调用的GET方法public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;//实际执行的GET方法final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // table不为空 &amp;&amp; table长度大于0 &amp;&amp; table索引位置(根据hash值计算出)节点不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // first的key等于传入的key则返回first对象 if (first.hash == hash &amp;&amp; ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //first的key不等于传入的key则说明是链表，向下遍历 if ((e = first.next) != null) &#123; // 判断是否为TreeNode，是则为红黑树 // 如果是红黑树节点，则调用红黑树的查找目标节点方法getTreeNode if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //走下列步骤表示是链表，循环至节点的key与传入的key值相等 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; //找不到符合的返回空 return null;&#125; 5. put方法java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364//掉用的PUT方法，hash(key)调用本例中的hash()方法public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; //实际执行的PUT方法 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // table是否为空或者length等于0, 如果是则调用resize方法进行初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 通过hash值计算索引位置, 如果table表该索引位置节点为空则新增一个 if ((p = tab[i = (n - 1) &amp; hash]) == null) // 将索引位置的头节点赋值给p tab[i] = newNode(hash, key, value, null); else &#123; // table表该索引位置不为空 Node&lt;K,V&gt; e; K k; //判断p节点的hash值和key值是否跟传入的hash值和key值相等 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果相等, 则p节点即为要查找的目标节点，赋值给e // 判断p节点是否为TreeNode, 如果是则调用红黑树的putTreeVal方法查找目标节点 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 走到这代表p节点为普通链表节点 else &#123; // 遍历此链表, binCount用于统计节点数 for (int binCount = 0; ; ++binCount) &#123; //p.next为空代表目标节点不存在 if ((e = p.next) == null) &#123; //新增一个节点插入链表尾部 p.next = newNode(hash, key, value, null); //如果节点数目超过8个，调用treeifyBin方法将该链表转换为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //e节点的hash值和key值都与传入的相等, 则e即为目标节点,跳出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // e不为空则代表根据传入的hash值和key值查找到了节点,将该节点的value覆盖,返回oldValue if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); // 用于LinkedHashMap return oldValue; &#125; &#125; //map修改次数加1 ++modCount; //map节点数加1，如果超过阀值，则扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); // 用于LinkedHashMap return null;&#125; 从上面的源码分析可以看出 1、如果节点已经存在，则更新原值 2、如果节点不存在，则插入数组中，如果数组已经有值，则判断是非是红黑树，如果是，则调用红黑树方法插入 3、如果插入的是链表，插入尾部，然后判断节点数是否超过8，如果超过，则转换为红黑树 4、先插入的数据，后面判断是否超过阀值再进行的扩容 putTreeVal,插入红黑树方法就不看了，看下treeifyBin方法，该方法是将链表转化为红黑树, java123456789101112131415161718192021222324final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; // table为空或者table的长度小于64, 进行扩容 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); // 根据hash值计算索引值, 遍历该索引位置的链表 else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); // 链表节点转红黑树节点 if (tl == null) // tl为空代表为第一次循环 hd = p; // 头结点 else &#123; p.prev = tl; // 当前节点的prev属性设为上一个节点 tl.next = p; // 上一个节点的next属性设置为当前节点 &#125; tl = p; // tl赋值为p, 在下一次循环中作为上一个节点 &#125; while ((e = e.next) != null); // e指向下一个节点 // 将table该索引位置赋值为新转的TreeNode的头节点 if ((tab[index] = hd) != null) hd.treeify(tab); // 以头结点为根结点, 构建红黑树 &#125;&#125; 可以看到，会先判断tab的节点数是否超过64，如果没超过，则进行扩容，如果超过了才会转换为红黑树 可以得到两个结论 1、什么时候转换为红黑树 当链表数目超过8,并且map节点数量超过64，才会转换为红黑树 2、什么时候扩容（前提是map数目没有超过最大容量值 1&lt;&lt;30 ） 新增节点时，发生了碰撞，并且节点数目超过阀值 新增节点时，发生了碰撞，节点数量木有超过阀值，但是链表数目&gt;8,map节点&lt;64时 再看下resize()方法 java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788final Node&lt;K,V&gt;[] resize() &#123; //oldTab保存未扩容的tab Node&lt;K,V&gt;[] oldTab = table; //oldTab最大容量 int oldCap = (oldTab == null) ? 0 : oldTab.length; //oldTab阀值 int oldThr = threshold; int newCap, newThr = 0; //如果老map有值 if (oldCap &gt; 0) &#123; // 老table的容量超过最大容量值，设置阈值为Integer.MAX_VALUE，返回老表 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; //老table的容量没有超过最大容量值，将新容量赋值为老容量*2，如果新容量&lt;最大容量并且老容量&gt;=16, 则将新阈值设置为原来的两倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // 老表的容量为0, 老表的阈值大于0, 是因为初始容量被放入阈值 newCap = oldThr; // 则将新表的容量设置为老表的阈值 else &#123; //老表的容量为0, 老表的阈值为0, 则为空表，设置默认容量和阈值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 如果新阈值为空, 则通过新的容量*负载因子获得新阈值 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; // 将当前阈值赋值为刚计算出来的新的阈值 threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 将当前的表赋值为新定义的表 // 如果老表不为空, 则需遍历将节点赋值给新表 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; // 将索引值为j的老表头节点赋值给e oldTab[j] = null; //将老表的节点设置为空, 以便垃圾收集器回收空间 // 如果e.next为空, 则代表老表的该位置只有1个节点, // 通过hash值计算新表的索引位置, 直接将该节点放在该位置 if (e.next == null) // newTab[e.hash &amp; (newCap - 1)] = e; //e.next不为空,判断是否是红黑树 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //是普通链表 else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; //如果e的hash值与老表的容量进行与运算为0,则扩容后的索引位置跟老表的索引位置一样 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; //如果e的hash值与老表的容量进行与运算为1,则扩容后的索引位置为: // 老表的索引位置＋oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; // 最后一个节点的next设为空 newTab[j] = loHead; // 将原索引位置的节点设置为对应的头结点 &#125; if (hiTail != null) &#123; hiTail.next = null; // 最后一个节点的next设为空 newTab[j + oldCap] = hiHead; // 将索引位置为原索引+oldCap的节点设置为对应的头结点 &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 可以看出，扩容时，节点重hash只分布在原索引位置与原索引+oldCap位置，为什么呢 假设老表的容量为16，即oldCap=16，则新表容量为16*2=32，假设节点1的hash值为0000 0000 0000 0000 0000 1111 0000 1010，节点2的hash值为0000 0000 0000 0000 0000 1111 0001 1010，则节点1和节点2在老表的索引位置计算如下图计算1，由于老表的长度限制，节点1和节点2的索引位置只取决于节点hash值的最后4位。再看计算2，计算2为新表的索引计算，可以知道如果两个节点在老表的索引位置相同，则新表的索引位置只取决于节点hash值倒数第5位的值，而此位置的值刚好为老表的容量值16，此时节点在新表的索引位置只有两种情况：原索引位置和原索引+oldCap位置（在此例中即为10和10+16=26）。由于结果只取决于节点hash值的倒数第5位，而此位置的值刚好为老表的容量值16，因此此时新表的索引位置的计算可以替换为计算3，直接使用节点的hash值与老表的容量16进行位于运算，如果结果为0则该节点在新表的索引位置为原索引位置，否则该节点在新表的索引位置为原索引+oldCap位置。 6. remove()方法java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; // 如果table不为空并且根据hash值计算出来的索引位置不为空, 将该位置的节点赋值给p if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; // 如果p的hash值和key都与入参的相同, 则p即为目标节点, 赋值给node if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; // 否则向下遍历节点 if (p instanceof TreeNode) // 如果p是TreeNode则调用红黑树的方法查找节点 node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; do &#123; // 遍历链表查找符合条件的节点 // 当节点的hash值和key与传入的相同,则该节点即为目标节点 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; // 赋值给node, 并跳出循环 break; &#125; p = e; // p节点赋值为本次结束的e &#125; while ((e = e.next) != null); // 指向像一个节点 &#125; &#125; // 如果node不为空(即根据传入key和hash值查找到目标节点)，则进行移除操作 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) // 如果是TreeNode则调用红黑树的移除方法 ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); // 走到这代表节点是普通链表节点 // 如果node是该索引位置的头结点则直接将该索引位置的值赋值为node的next节点 else if (node == p) tab[index] = node.next; // 否则将node的上一个节点的next属性设置为node的next节点, // 即将node节点移除, 将node的上下节点进行关联(链表的移除) else p.next = node.next; ++modCount; // 修改次数+1 --size; // table的总节点数-1 afterNodeRemoval(node); // 供LinkedHashMap使用 return node; // 返回被移除的节点 &#125; &#125; return null;&#125; 7. JDK1.7和1.8的区别1、JDK1.7的时候使用的是数组+ 单链表的数据结构。但是在JDK1.8及之后时，使用的是数组+链表+红黑树的数据结构（当链表的深度达到8的时候，也就是默认阈值，就会自动扩容把链表转成红黑树的数据结构来把时间复杂度从O（n）变成O（logN）提高了效率） 2、JDK1.7用的是头插法，而JDK1.8及之后使用的都是尾插法，那么他们为什么要这样做呢？因为JDK1.7是用单链表进行的纵向延伸，当采用头插法时会容易出现逆序且环形链表死循环问题。但是在JDK1.8之后是因为加入了红黑树使用尾插法，能够避免出现逆序且链表死循环的问题。 3、扩容后数据存储位置的计算方式也不一样：1. 在JDK1.7的时候是直接用hash值和需要扩容的二进制数进行&amp;（这里就是为什么扩容的时候为啥一定必须是2的多少次幂的原因所在，因为如果只有2的n次幂的情况时最后一位二进制数才一定是1，这样能最大程度减少hash碰撞）（hash值 &amp; length-1），而在JDK1.8的时候直接用了JDK1.7的时候计算的规律，也就是扩容前的原始位置+扩容的大小值=JDK1.8的计算方式，而不再是JDK1.7的那种异或的方法。但是这种方式就相当于只需要判断Hash值的新增参与运算的位是0还是1就直接迅速计算出了扩容后的储存方式。 4、jdk1.7 先扩容再put ，jdk1.8 先put再扩容","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"集合","slug":"Java/集合","permalink":"https://masteryang4.github.io/categories/Java/%E9%9B%86%E5%90%88/"}],"tags":[{"name":"源码","slug":"源码","permalink":"https://masteryang4.github.io/tags/%E6%BA%90%E7%A0%81/"},{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"hashmap","slug":"hashmap","permalink":"https://masteryang4.github.io/tags/hashmap/"}]},{"title":"Comparable和Comparator底层源码分析","slug":"Comparable和Comparator底层源码分析","date":"2020-03-15T15:52:58.000Z","updated":"2020-04-18T15:17:34.133Z","comments":true,"path":"2020/03/15/Comparable和Comparator底层源码分析/","link":"","permalink":"https://masteryang4.github.io/2020/03/15/Comparable%E5%92%8CComparator%E5%BA%95%E5%B1%82%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"1. Comparable源码分析1.1创建Java工程，实现Comparable接口java123456789101112131415161718192021222324252627282930313233343536373839404142434445import java.io.Serializable;//实现Serializable，标识该类可被序列化//实现Comparable接口，让此类可以利用Collections.sort()进行排序public class User&lt;T extends User&gt; implements Serializable,Comparable&lt;T&gt;&#123; private String name; private int age; private transient String address;//transient修饰，标识该类序列化时此字段不需要进行存储 public User(String name)&#123; this.name = name; &#125; public User(String name,int age,String address)&#123; this(name); this.age = age; this.address = address; &#125; public String getName() &#123; return name; &#125; public int getAge() &#123; return age; &#125; public String getAddress() &#123; return address; &#125; @Override public int compareTo(T o) &#123; //在此处打上断点，方便进行调试 int returnInt = 0; if(age&gt;o.getAge())&#123; returnInt=1; &#125;else if(age==o.getAge())&#123; returnInt=0; &#125;else if(age&lt;o.getAge())&#123; returnInt=-1; &#125; return returnInt; &#125;&#125; 1.2 编写测试类java123456789101112131415161718192021222324252627282930313233import java.util.ArrayList;import java.util.Collections;import java.util.List;public class TestComparable &#123; public static void main(String[] args) &#123; User u1 = new User(\"caililiang1\",20,\"hubei1\"); User u2 = new User(\"caililiang2\",30,\"hubei2\"); User u3 = new User(\"caililiang3\",25,\"hubei3\"); User u4 = new User(\"caililiang4\",28,\"hubei4\"); User u5 = new User(\"caililiang5\",23,\"hubei5\"); List&lt;User&gt; list = new ArrayList&lt;User&gt;(); list.add(u1); list.add(u2); list.add(u3); list.add(u4); list.add(u5); for(int i=0;i&lt;list.size();i++)&#123; User u =list.get(i); System.out.println(u.getName()+\"---&gt;\"+u.getAge()); &#125; System.out.println(\"排序后---------------------\"); //在此处打上断点，方便进行调试 Collections.sort(list); for(int i=0;i&lt;list.size();i++)&#123; User u =list.get(i); System.out.println(u.getName()+\"---&gt;\"+u.getAge()); &#125; &#125;&#125; 1.3 Collections类中的泛型方法sort()java1234567// 此处 &lt;T extends Comparable&lt;? super T&gt;&gt; 的意思是：// 1.&lt;T extends Comparable&gt;表示比较对象的类必须是Comparable 的子类。// 2.Comparable&lt;? super T&gt;表示是Comparable实现类及以上。public static &lt;T extends Comparable&lt;? super T&gt;&gt; void sort(List&lt;T&gt; list) &#123; //调用List接口中的sort()方法 list.sort(null); &#125; 1.4 List接口中的默认方法sort()java1234567891011// 由于本例中采用的是ArrayList集合，ArrayList集合对List接口中的sort()方法进行了重写，// 因此实际在DeBug的过程中会执行ArrayLIst类中的sort()方法 default void sort(Comparator&lt;? super E&gt; c) &#123; Object[] a = this.toArray(); Arrays.sort(a, (Comparator) c); ListIterator&lt;E&gt; i = this.listIterator(); for (Object e : a) &#123; i.next(); i.set((E) e); &#125; &#125; 1.5 ArrayList集合中的方法sort()java1234567891011@Override@SuppressWarnings(\"unchecked\")public void sort(Comparator&lt;? super E&gt; c) &#123; final int expectedModCount = modCount; //此方法直接调用Arrays类中sort()方法 Arrays.sort((E[]) elementData, 0, size, c); if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; modCount++;&#125; 1.6 Arrays类中的sort()方法java12345678910111213141516171819202122public static &lt;T&gt; void sort(T[] a, int fromIndex, int toIndex, Comparator&lt;? super T&gt; c) &#123; //在1.3中传入的 c值为null,所以调用sort(a, fromIndex, toIndex)方法 if (c == null) &#123; sort(a, fromIndex, toIndex); &#125; else &#123; rangeCheck(a.length, fromIndex, toIndex); if (LegacyMergeSort.userRequested) legacyMergeSort(a, fromIndex, toIndex, c); else TimSort.sort(a, fromIndex, toIndex, c, null, 0, 0); &#125;&#125;public static void sort(Object[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); if (LegacyMergeSort.userRequested) //归并排序 legacyMergeSort(a, fromIndex, toIndex); else //二进制插入排序 ComparableTimSort.sort(a, fromIndex, toIndex, null, 0, 0);&#125; 解析：源码里首先判断是否采用传统的排序方法,LegacyMergeSort.userRequested属性默认为false,也就是说默认选中 ComparableTimSort.sort(a)方法(传统归并排序在1.5及之前是默认排序方法，1.5之后默认执行ComparableTimSort.sort()方法。除非程序中强制要求使用传统归并排序,语句如下：System.setProperty(“java.util.Arrays.useLegacyMergeSort”, “true”))继续看 ComparableTimSort.sort(a)源码 1.7 ComparableTimSort类中的sort()方法java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687static void sort(Object[] a, int lo, int hi, Object[] work, int workBase, int workLen) &#123; assert a != null &amp;&amp; lo &gt;= 0 &amp;&amp; lo &lt;= hi &amp;&amp; hi &lt;= a.length; //nRemaining表示没有排序的对象个数，方法执行前，如果这个数小于2，就不需要排序了。 //如果2&lt;= nRemaining &lt;=32,即MIN_MERGE的初始值，表示需要排序的数组是小数组 //可以使用mini-TimSort方法进行排序，否则需要使用归并排序。 int nRemaining = hi - lo; if (nRemaining &lt; 2) return; // Arrays of size 0 and 1 are always sorted // If array is small, do a \"mini-TimSort\" with no merges if (nRemaining &lt; MIN_MERGE) &#123; //调用重写的compareTo()方法 int initRunLen = countRunAndMakeAscending(a, lo, hi); //只看这一句 binarySort(a, lo, hi, lo + initRunLen); return; &#125; ...... &#125;//这里才是真正的调用compareTo()方法对当前对象进行比较 private static int countRunAndMakeAscending(Object[] a, int lo, int hi) &#123; assert lo &lt; hi; int runHi = lo + 1; if (runHi == hi) return 1; // Find end of run, and reverse range if descending if (((Comparable) a[runHi++]).compareTo(a[lo]) &lt; 0) &#123; // 降序排列 while (runHi &lt; hi &amp;&amp; ((Comparable) a[runHi]).compareTo(a[runHi - 1]) &lt; 0) runHi++; reverseRange(a, lo, runHi); &#125; else &#123;// 升序排列 while (runHi &lt; hi &amp;&amp; ((Comparable) a[runHi]).compareTo(a[runHi - 1]) &gt;= 0) runHi++; &#125; return runHi - lo; &#125;//这里才是真正的进行排序。 private static void binarySort(Object[] a, int lo, int hi, int start) &#123; assert lo &lt;= start &amp;&amp; start &lt;= hi; if (start == lo) start++; for ( ; start &lt; hi; start++) &#123; Comparable pivot = (Comparable) a[start]; // Set left (and right) to the index where a[start] (pivot) belongs int left = lo; int right = start; assert left &lt;= right; /* * Invariants: * pivot &gt;= all in [lo, left). * pivot &lt; all in [right, start). */ while (left &lt; right) &#123; int mid = (left + right) &gt;&gt;&gt; 1; if (pivot.compareTo(a[mid]) &lt; 0) right = mid; else left = mid + 1; &#125; assert left == right; /* * The invariants still hold: pivot &gt;= all in [lo, left) and * pivot &lt; all in [left, start), so pivot belongs at left. Note * that if there are elements equal to pivot, left points to the * first slot after them -- that's why this sort is stable. * Slide elements over to make room for pivot. */ int n = start - left; // The number of elements to move // Switch is just an optimization for arraycopy in default case switch (n) &#123; case 2: a[left + 2] = a[left + 1]; case 1: a[left + 1] = a[left]; break; default: System.arraycopy(a, left, a, left + 1, n); &#125; a[left] = pivot; &#125; &#125; 2. Comparator源码分析2.1 创建JavaBeanjava123456789101112131415161718192021222324252627282930import java.io.Serializable;public class People implements Serializable &#123; private String name; private int age; private transient String address;//transient修饰，标识该类序列化时此字段不需要进行存储 public People(String name)&#123; this.name = name; &#125; public People(String name,int age,String address)&#123; this(name); this.age = age; this.address = address; &#125; public String getName() &#123; return name; &#125; public int getAge() &#123; return age; &#125; public String getAddress() &#123; return address; &#125;&#125; 2.2 创建外部比较器java123456789101112131415import java.util.Comparator;public class PeopleComparator&lt;T extends People&gt; implements Comparator&lt;T&gt; &#123; public int compare(T o1, T o2) &#123; int returnInt = 0; if(o1.getAge()&gt;o2.getAge())&#123; returnInt = 1; &#125;else if(o1.getAge()==o2.getAge())&#123; returnInt = 0; &#125;else if(o1.getAge()&lt;o2.getAge())&#123; returnInt = -1; &#125; return returnInt; &#125;&#125; 2.3 创建测试类java12345678910111213141516171819202122232425262728293031323334import java.util.ArrayList;import java.util.Collections;import java.util.List;public class TestComparator &#123; public static void main(String[] args) &#123; People u1 = new People(\"caililiang1\",20,\"hubei1\"); People u2 = new People(\"caililiang2\",30,\"hubei2\"); People u3 = new People(\"caililiang3\",25,\"hubei3\"); People u4 = new People(\"caililiang4\",28,\"hubei4\"); People u5 = new People(\"caililiang5\",23,\"hubei5\"); List&lt;People&gt; list = new ArrayList&lt;People&gt;(); list.add(u1); list.add(u2); list.add(u3); list.add(u4); list.add(u5); for(int i=0;i&lt;list.size();i++)&#123; People u =list.get(i); System.out.println(u.getName()+\"---&gt;\"+u.getAge()); &#125; System.out.println(\"排序后---------------------\"); Collections.sort(list,new PeopleComparator()); for(int i=0;i&lt;list.size();i++)&#123; People u =list.get(i); System.out.println(u.getName()+\"---&gt;\"+u.getAge()); &#125; &#125;&#125; 2.4 Collections类中的泛型方法sort()java12345 @SuppressWarnings(&#123;\"unchecked\", \"rawtypes\"&#125;)//此处调用的是sort方法的重载方法，与案例一中不同 public static &lt;T&gt; void sort(List&lt;T&gt; list, Comparator&lt;? super T&gt; c) &#123; list.sort(c); &#125; 2.5 ArrayList集合中的方法sort()java12345678910@Override@SuppressWarnings(\"unchecked\")public void sort(Comparator&lt;? super E&gt; c) &#123; final int expectedModCount = modCount; Arrays.sort((E[]) elementData, 0, size, c); if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; modCount++;&#125; 2.6 Arrays类中的sort()方法java123456789101112public static &lt;T&gt; void sort(T[] a, int fromIndex, int toIndex, Comparator&lt;? super T&gt; c) &#123; if (c == null) &#123; sort(a, fromIndex, toIndex); &#125; else &#123; rangeCheck(a.length, fromIndex, toIndex); if (LegacyMergeSort.userRequested) legacyMergeSort(a, fromIndex, toIndex, c); else //本次进入这里进行排序 TimSort.sort(a, fromIndex, toIndex, c, null, 0, 0); &#125;&#125; 2.7 TimSort类下的sort()方法java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758static &lt;T&gt; void sort(T[] a, int lo, int hi, Comparator&lt;? super T&gt; c, T[] work, int workBase, int workLen) &#123; assert c != null &amp;&amp; a != null &amp;&amp; lo &gt;= 0 &amp;&amp; lo &lt;= hi &amp;&amp; hi &lt;= a.length; int nRemaining = hi - lo; if (nRemaining &lt; 2) return; // Arrays of size 0 and 1 are always sorted // If array is small, do a \"mini-TimSort\" with no merges if (nRemaining &lt; MIN_MERGE) &#123; int initRunLen = countRunAndMakeAscending(a, lo, hi, c); binarySort(a, lo, hi, lo + initRunLen, c); return; &#125; private static &lt;T&gt; void binarySort(T[] a, int lo, int hi, int start, Comparator&lt;? super T&gt; c) &#123; assert lo &lt;= start &amp;&amp; start &lt;= hi; if (start == lo) start++; for ( ; start &lt; hi; start++) &#123; T pivot = a[start]; // Set left (and right) to the index where a[start] (pivot) belongs int left = lo; int right = start; assert left &lt;= right; /* * Invariants: * pivot &gt;= all in [lo, left). * pivot &lt; all in [right, start). */ while (left &lt; right) &#123; int mid = (left + right) &gt;&gt;&gt; 1; if (c.compare(pivot, a[mid]) &lt; 0) right = mid; else left = mid + 1; &#125; assert left == right; /* * The invariants still hold: pivot &gt;= all in [lo, left) and * pivot &lt; all in [left, start), so pivot belongs at left. Note * that if there are elements equal to pivot, left points to the * first slot after them -- that's why this sort is stable. * Slide elements over to make room for pivot. */ int n = start - left; // The number of elements to move // Switch is just an optimization for arraycopy in default case switch (n) &#123; case 2: a[left + 2] = a[left + 1]; case 1: a[left + 1] = a[left]; break; default: System.arraycopy(a, left, a, left + 1, n); &#125; a[left] = pivot; &#125;&#125; 3. 总结 Comparable 此接口强行对实现它的每个类的对象进行整体排序。这种排序被称为类的自然排序，类的compareTo()方法被称为它的自然比较方法。 实现此接口的对象列表（集合和数组）可以通过 Collections.sort和 Arrays.sort 进行自动排序。实现此接口的对象可以用作有序映射中的键或有序集合中的元素，无需指定比较器。 Arrays.sort(people) Comparator 是比较器，排序时，需要新建比较器对象，将比较器和对象一起传递过去就可以比大小，可称为“外部排序”。比较器是定义在要比较对象的外部的, 必须要重写compare()方法，而需要比较的类的结构不需要有任何变化。并且在Comparator 里面用户可以自己实现复杂的可以通用的逻辑,使其可以匹配一些比较简单的对象,那样就可以节省很多重复劳动了。 Arrays.sort(people,new PersonCompartor()); 关于两个类的具体应用场景可以理解为，自己在创建一个工程时可以使用Comparable进行排序，当工程创建完毕时添加新的排序功能时，可以使用Comparator，无需改变类的结构。","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"源码","slug":"源码","permalink":"https://masteryang4.github.io/tags/%E6%BA%90%E7%A0%81/"},{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"Hexo博客安装及部署","slug":"Hexo博客安装及部署","date":"2020-03-12T06:40:31.000Z","updated":"2020-04-18T15:53:17.599Z","comments":true,"path":"2020/03/12/Hexo博客安装及部署/","link":"","permalink":"https://masteryang4.github.io/2020/03/12/Hexo%E5%8D%9A%E5%AE%A2%E5%AE%89%E8%A3%85%E5%8F%8A%E9%83%A8%E7%BD%B2/","excerpt":"安装nodejs node -v #查看node版本npm -v #查看npm版本npm install -g cnpm –registry=http://registry.npm.taobao.org #安装淘宝的cnpm 管理器cnpm -v #查看cnpm版本 hexo安装及配置 hexo -v #查看hexo版本mkdir blog #创建blog目录cd blog #进入blog目录sudo hexo init #生成博客 初始化博客 hexo s #启动本地博客服务http://localhost:4000/ #本地访问地址hexo n “我的第一篇文章” #创建新的文章 在blog目录下","text":"安装nodejs node -v #查看node版本npm -v #查看npm版本npm install -g cnpm –registry=http://registry.npm.taobao.org #安装淘宝的cnpm 管理器cnpm -v #查看cnpm版本 hexo安装及配置 hexo -v #查看hexo版本mkdir blog #创建blog目录cd blog #进入blog目录sudo hexo init #生成博客 初始化博客 hexo s #启动本地博客服务http://localhost:4000/ #本地访问地址hexo n “我的第一篇文章” #创建新的文章 在blog目录下 hexo clean #清理hexo g #生成#Github创建一个新的仓库 YourGithubName.github.iocnpm install –save hexo-deployer-git #在blog目录下安装git部署插件 配置_config.yml yml123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:type: gitrepo: https://github.com/YourGithubName/YourGithubName.github.io.gitbranch: master 部署到Github仓库里 hexo d https://YourGithubName.github.io/ #访问这个地址可以查看博客 yilia主题配置 下载yilia主题到本地 git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 修改hexo根目录下的 _config.yml 文件 yml1theme: yilia 部署到github hexo clean #清理一下 hexo g #生成 hexo d #部署到远程Github仓库 查看博客 ： https://YourGithubName.github.io/","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://masteryang4.github.io/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://masteryang4.github.io/tags/Hexo/"},{"name":"博客","slug":"博客","permalink":"https://masteryang4.github.io/tags/%E5%8D%9A%E5%AE%A2/"}]},{"title":"ArrayList底层实现源码分析(JDK1.8)","slug":"ArrayList底层实现源码分析_JDK1.8","date":"2020-03-12T03:30:45.000Z","updated":"2020-04-18T15:16:40.883Z","comments":true,"path":"2020/03/12/ArrayList底层实现源码分析_JDK1.8/","link":"","permalink":"https://masteryang4.github.io/2020/03/12/ArrayList%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90_JDK1.8/","excerpt":"1. 类信息1public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; 2. 基本属性","text":"1. 类信息java1public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; 2. 基本属性 java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//定义序列化ID，主要是为了表示不同的版本的兼容性private static final long serialVersionUID = 8683452581122892189L;//默认的数组存储容量(ArrayList底层是数组结构)private static final int DEFAULT_CAPACITY = 10;//当指定数组的容量为0时使用这个常量赋值private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;//默认空参构造函数时使用这个常量赋值private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;//真正存放数据的对象数组，transient标识不被序列化transient Object[] elementData;//数组中的真实元素个数，该值小于或等于elementData.lengthprivate int size;//最大数组长度：0x7fffffff - 8private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;//构造器一：创建具有初始化长度的listpublic ArrayList(int initialCapacity) &#123; //对传入的值进行合法检测 if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125;&#125;//构造器二：默认空参构造器public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125;//构造器三：创建具有初始化值的集合，可传入的集合类型父类是Collection即可，此处是多态的一个应用public ArrayList(Collection&lt;? extends E&gt; c) &#123; //将传入的集合转化为数组 elementData = c.toArray(); //判断elementData数组长度 if ((size = elementData.length) != 0) &#123; // elementData转化的数组如果不是Object的子类，就对当前数组进行复制，重新赋值给elementData if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123;//如果数组长度为0，复制为EMPTY_ELEMENTDATA this.elementData = EMPTY_ELEMENTDATA; &#125;&#125; 3. add(E e) 方法ArrayList集合创建时，默认初始化长度为0，通过add( )方法在添加元素时对数组长度进行动态赋值。添加第一个元素时，长度为10。当添加的元素个数超过10时，会进行首次扩容，容量为原数组长度的1.5倍。 java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 //此方法是添加元素的方法，另外还有一个重载方法 public boolean add(E e) &#123; //调用ensureCapacityInternal方法，初始化数组长度（默认为10） ensureCapacityInternal(size + 1); //为数组复制 elementData[size++] = e; return true; &#125;//初始化数组长度，默认值为10 private void ensureCapacityInternal(int minCapacity) &#123; //判断如果数组长度为0，对长度进行初始化 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; //从默认数组长度（10）和添加的元素个数（添加第一个元素时size=0,minCapacity=size+1）中取出最大值 //作为数组初始化长度 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; //再次确定数组容量 ensureExplicitCapacity(minCapacity); &#125; //再次确定数组容量 private void ensureExplicitCapacity(int minCapacity) &#123; //对数组元素个数进行统计 modCount++; //如果数组长度超过10，就对数组长度进行扩容 //那第一次扩容举例：minCapacity值为11，DEFAULT_CAPACITY值为10 if (minCapacity - elementData.length &gt; 0) //对数组进行扩容，默认为老数组的1.5倍 grow(minCapacity); &#125; //对数组进行扩容，默认为老数组的1.5倍 private void grow(int minCapacity) &#123; //老数组容量：minCapacity int oldCapacity = elementData.length; //新数组容量：是老数组长度的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //对新数组容量进行合法检测 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //MAX_ARRAY_SIZE：0x7fffffff - 8 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) //如果超过最大数组长度，再次进行扩容 newCapacity = hugeCapacity(minCapacity); //对原数组进行复制 elementData = Arrays.copyOf(elementData, newCapacity); &#125; private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) throw new OutOfMemoryError(); //三元运算符，如果超过最大数组长度返回Integer最大值：0x7fffffff return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; 4. add (int idnex,E element)从源码中可以看出，与add(E e)方法大致一致，主要的差异是增加了一行代码：System.arraycopy(elementData, index, elementData, index + 1, size - index)，从index位置开始以及之后的数据，整体拷贝到index+1开始的位置，然后再把新加入的数据放在index这个位置，而之前的数据不需要移动。 java123456789101112131415161718//在指定位置添加元素public void add(int index, E element) &#123; //判断index是否在范围内 rangeCheckForAdd(index); //与add(E e)方法一致，对数组长度进行初始化 ensureCapacityInternal(size + 1); //对原数组从index位置进行拷贝，复制到index+1的位置，elementData[index]此时为空 //System.arraycopy是一个native方法，意味着这个方法是C/C++语言实现的，我们无法再以普通的方式去查看这些方法了 System.arraycopy(elementData, index, elementData, index + 1, size - index); //为该下标赋值 elementData[index] = element; size++;&#125;//判断index是否在范围内的具体实现private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; arraycopy(elementData, index, elementData, index + 1, size - index)函数中各个参数对应的意义：（原数组，原数组的开始位置，目标数组，目标数组的开始位置，拷贝的个数） 5. remove(int index)java12345678910111213141516171819202122232425262728293031323334353637383940414243444546 //移除指定index下的元素 public E remove(int index) &#123; //index是否合法检测 rangeCheck(index); modCount++; //指定index下的元素 E oldValue = elementData(index); //移除后数组长度 int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); //为最后一个元素赋值为null elementData[--size] = null; return oldValue; &#125; //返回指定index下的元素E elementData(int index) &#123; return (E) elementData[index]; &#125; //根据元素（对象）移除该元素 public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; //类似于remove()方法 private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; &#125; remove方法与add正好是一个相反的操作，移除一个元素，会影响到一批数字的位置移动，所以也是比较耗性能。核心代码都是调用了java.lang.System.arraycopy(Object src, int srcPos, Object dest, int destPos, int length)方法 6. get(int index)java123456//根据指定下标获取元素值public E get(int index) &#123; rangeCheck(index); return elementData(index);&#125; 7. set(int index, E element)java12345678//修改指定index下的元素值public E set(int index, E element) &#123; rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; 8. clear()java12345678910//清空所有元素public void clear() &#123; modCount++; // clear to let GC do its work for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0;&#125; 9. contains(Object o)java1234567891011121314151617//查询是否包含某个元素public boolean contains(Object o) &#123; return indexOf(o) &gt;= 0;&#125;//具体的实现方法，如果不包含返回-1public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; 10. 总结 基于数组实现的List在随机访问和遍历的效率比较高，但是往指定位置加入元素或者删除指定位置的元素效率比较低。","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"集合","slug":"Java/集合","permalink":"https://masteryang4.github.io/categories/Java/%E9%9B%86%E5%90%88/"}],"tags":[{"name":"源码","slug":"源码","permalink":"https://masteryang4.github.io/tags/%E6%BA%90%E7%A0%81/"},{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]}]}