{"meta":{"title":"MasterYangBlog","subtitle":"","description":"","author":"Yang4","url":"https://masteryang4.github.io","root":"/"},"pages":[{"title":"About me","date":"2020-04-15T16:13:50.000Z","updated":"2020-05-06T06:53:14.123Z","comments":true,"path":"about/index.html","permalink":"https://masteryang4.github.io/about/index.html","excerpt":"","text":"json12345678&#123; \"name\": \"Yang Sen\", \"sex\": \" ♂ \", \"address\": \"Shanghai.China\", \"Github\": \"https://github.com/masteryang4\", \"Blog\": \"www.yangsen94.top\", \"E-mail\": \"2692474773@qq.com\"&#125; 技术栈：Java，Python，Scala，大数据，Web技术等 欢迎互相交♂流♂学习~"},{"title":"文章分类","date":"2020-04-13T13:58:22.000Z","updated":"2020-04-15T15:54:13.404Z","comments":true,"path":"categories/index.html","permalink":"https://masteryang4.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-04-13T13:58:35.000Z","updated":"2020-05-21T12:34:33.889Z","comments":true,"path":"link/index.html","permalink":"https://masteryang4.github.io/link/index.html","excerpt":"","text":"Codesheep程序羊： https://www.codesheep.cn/ 廖雪峰的官方网站： https://www.liaoxuefeng.com/ 牛客网： https://www.nowcoder.com/ 左耳朵耗子（酷壳）： https://coolshell.cn/ 敖丙三太子： https://github.com/AobingJava 知乎： https://www.zhihu.com/ 王道计算机考研： http://cskaoyan.com/forum.php 尚硅谷： http://www.atguigu.com/"},{"title":"留言板","date":"2020-04-19T12:00:02.000Z","updated":"2020-04-19T12:10:25.372Z","comments":true,"path":"messageboard/index.html","permalink":"https://masteryang4.github.io/messageboard/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-04-13T13:57:16.000Z","updated":"2020-04-15T15:58:24.136Z","comments":true,"path":"tags/index.html","permalink":"https://masteryang4.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"解决Github连不上、ping不通的问题","slug":"解决Github连不上、ping不通的问题","date":"2020-06-07T15:59:15.000Z","updated":"2020-06-07T16:10:43.018Z","comments":true,"path":"2020/06/07/解决Github连不上、ping不通的问题/","link":"","permalink":"https://masteryang4.github.io/2020/06/07/%E8%A7%A3%E5%86%B3Github%E8%BF%9E%E4%B8%8D%E4%B8%8A%E3%80%81ping%E4%B8%8D%E9%80%9A%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"修改host即可Github连不上、ping不通、git clone特别慢等现象，通常是因为github.global.ssl.fastly.net域名被限制了。 因此，只要找到你当前线路最快的ip，修改一下host就能提速。 步骤一、在网站 https://www.ipaddress.com 分别找这两个域名所对应的最快的ip地址 Code12github.global.ssl.fastly.netgithub.com 二、在C:\\Windows\\System32\\drivers\\etc\\hosts里面做映射 注意要以自己查到的这两个域名所对应的最快IP地址为准。 在hosts文件最下方添加即可。 示例： Code12199.232.69.194 github.global.ssl.fastly.net140.82.114.4 github.com 保存修改后，再登陆一般就木有问题了。","categories":[{"name":"Git&Github","slug":"Git-Github","permalink":"https://masteryang4.github.io/categories/Git-Github/"}],"tags":[{"name":"Git&Github","slug":"Git-Github","permalink":"https://masteryang4.github.io/tags/Git-Github/"},{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"bug解决","slug":"bug解决","permalink":"https://masteryang4.github.io/tags/bug%E8%A7%A3%E5%86%B3/"}]},{"title":"Redis常见问题及扩展","slug":"Redis常见问题及扩展","date":"2020-06-07T15:04:34.000Z","updated":"2020-06-07T15:08:37.885Z","comments":true,"path":"2020/06/07/Redis常见问题及扩展/","link":"","permalink":"https://masteryang4.github.io/2020/06/07/Redis%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%8F%8A%E6%89%A9%E5%B1%95/","excerpt":"","text":"缓存穿透、缓存雪崩、缓存击穿1、缓存穿透是指查询一个一定不存在的数据。由于缓存命不中时会去查询数据库，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。 解决方案： 是将空对象也缓存起来，并给它设置一个很短的过期时间，最长不超过5分钟 采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力 布隆过滤器(bloom filter)： https://zhuanlan.zhihu.com/p/72378274 2、如果缓存集中在一段时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上，就会造成缓存雪崩。 解决方案： 尽量让失效的时间点不分布在同一个时间点 3、缓存击穿，是指一个key非常热点，在不停的扛着大并发，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞。 解决方案： 可以设置key永不过期 哨兵模式主从复制中反客为主的自动版，如果主机Down掉，哨兵会从从机中选择一台作为主机，并将它设置为其他从机的主机，而且如果原来的主机再次启动的话也会成为从机。 数据类型 类型 描述 string 字符串 list 可以重复的集合 set 不可以重复的集合 hash 类似于Map&lt;String,String&gt; zset(sorted set） 带分数的set 持久化1、RDB持久化： 在指定的时间间隔内持久化 服务shutdown会自动持久化 输入bgsave也会持久化 2、AOF : 以日志形式记录每个更新操作 Redis重新启动时读取这个文件，重新执行新建、修改数据的命令恢复数据。 保存策略： 推荐（并且也是默认）的措施为每秒持久化一次，这种策略可以兼顾速度和安全性。 缺点： 比起RDB占用更多的磁盘空间 恢复备份速度要慢 每次读写都同步的话，有一定的性能压力 存在个别Bug，造成恢复不能 选择策略： 官方推荐： 如果对数据不敏感，可以选单独用RDB；不建议单独用AOF，因为可能出现Bug;如果只是做纯内存缓存，可以都不用。 悲观锁、乐观锁悲观锁： 执行操作前假设当前的操作肯定（或有很大几率）会被打断（悲观）。基于这个假设，我们在做操作前就会把相关资源锁定，不允许自己执行期间有其他操作干扰。 乐观锁： 执行操作前假设当前操作不会被打断（乐观）。基于这个假设，我们在做操作前不会锁定资源，万一发生了其他操作的干扰，那么本次操作将被放弃。 Redis使用的就是乐观锁。 推荐参考： https://zhuanlan.zhihu.com/p/89620471","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Redis","slug":"大数据/Redis","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Redis/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Redis","slug":"Redis","permalink":"https://masteryang4.github.io/tags/Redis/"},{"name":"布隆过滤器","slug":"布隆过滤器","permalink":"https://masteryang4.github.io/tags/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"name":"缓存","slug":"缓存","permalink":"https://masteryang4.github.io/tags/%E7%BC%93%E5%AD%98/"}]},{"title":"大数据常用框架源码编译","slug":"大数据常用框架源码编译","date":"2020-06-07T12:28:17.000Z","updated":"2020-06-07T12:33:39.199Z","comments":true,"path":"2020/06/07/大数据常用框架源码编译/","link":"","permalink":"https://masteryang4.github.io/2020/06/07/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/","excerpt":"","text":"源码编译通用步骤一、搭建编译环境一般编译环境为Linux + JDK + Maven，有些框架可能需要别的环境支持，一般都会注明，在后面细说。以下教程都是基于Linux + JDK8环境编译。 Linux和JDK环境这里不再赘述 MAVEN环境搭建 bash123456789101112#1. 从apache网站拉取tar包并解压MVNTAR=$(curl http://maven.apache.org/download.cgi | grep -E \"&gt;apache-maven-.*bin\\.tar\\.gz&lt;\" | sed 's/.*a href=\"\\(.*\\)\".*/\\1/g')curl $MVNTAR | tar zxC /opt/modulemv /opt/module/$(basename $MVNTAR | cut -d - -f 1,2,3) /opt/module/maven#2. 配置环境变量vim /etc/profile.d/my_env.sh#添加如下内容并保存退出export M2_HOME=/opt/module/mavenexport MAVEN_HOME=/opt/module/mavenexport PATH=$&#123;MAVEN_HOME&#125;/bin:$&#123;PATH&#125; 完成后重启Xshell会话 二、下载源码下载你想要编译的框架的源码。一般源码下载有两种方式： 想编译的版本已经发布release版，但是由于兼容性原因需要重新编译。这种情况直接从框架官网下载源码包并解压即可。 想测试框架还没发布的最新功能。此时从git托管服务器拉取最新源码，这时，我们需要git环境 Git环境搭建 bash12sudo yum install -y epel-releasesudo yum install -y git 到 https://git-wip-us.apache.org/repos/asf 查看想要编译的框架的git服务器，拉取源码(以Hive为例) bash123456#新建源码存储目录mkdir -p /opt/software/sourcecd /opt/software/source#拉取源码git clone https://git-wip-us.apache.org/repos/asf/hive.git 进入拉取的源码目录，切换到自己想要的分支 bash123456#查看所有本地和远程分支，这里也可以切换到之前版本的分支cd hivegit branch -a#新建本地分支同步远程分支git checkout -b 3.1 origin/branch-3.1 如果想切换到特定release的源码，使用git tag命令 bash12345#查看所有taggit tag#切换到想要的tag，这里以release-3.1.2为例git checkout rel/release-3.1.2 三、查看编译说明一般来说，源码根目录都会有building.txt之类的文件作为编译说明，如果没有找到，也可以去官网查看编译说明。说明里一般都会注明前置要求，例如一些额外的编译环境要求等。 Hive没有前置要求，我们直接进入第四步 四、对源码做必要修改一般我们只有在框架不兼容的情况下我们需要重新编译，不兼容一般是由于框架依赖版本不一致造成的，一般我们只需要编辑框架的pom.xml文件修改依赖版本即可。但是有些依赖新版本和旧版本不兼容，此时我们就需要对源码进行更多的修改。这些修改最好在IDE中进行。 Hive的guava版本和Hadoop 3.1.3的不兼容，我们修改其为27.0-jre xml1234将&lt;guava.version&gt;19.0&lt;/guava.version&gt;修改为&lt;guava.version&gt;27.0-jre&lt;/guava.version&gt; 这个依赖新老版本就不兼容，修改版本后我们需要对源码进行必要修改。详细修改步骤会在另外一篇教程中讲述 五、编译准备工作全部做完，最后我们开始编译。一般的编译命令为： bash1mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=true 然后静待编译完成。这个过程会比较久，而且会从maven官网拉取大量jar包，所以要保证网络状况良好。 编译完成的Tar包的位置，各个框架都不一样，我们可以用下面的命令查找 bash1find ./ -name *.tar.gz Hive编译 拉取源码 bash12cd /opt/software/sourcegit clone https://git-wip-us.apache.org/repos/asf/hive.git 修改pom.xml，将guava的版本改为如下版本 Code1&lt;guava.version&gt;27.0-jre&lt;&#x2F;guava.version&gt; 修改以下文件中关于 com.google.common.util.concurrent.Futures#addCallback 的调用 src\\java\\org\\apache\\hadoop\\hive\\llap\\AsyncPbRpcProxy.java java123456789101112131415161718192021//173行Futures.addCallback( future, new ResponseCallback&lt;U&gt;( request.getCallback(), nodeId, this) ,executor);//278行Futures.addCallback(requestManagerFuture, new FutureCallback&lt;Void&gt;() &#123; @Override public void onSuccess(Void result) &#123; LOG.info(\"RequestManager shutdown\"); &#125; @Override public void onFailure(Throwable t) &#123; if (!(t instanceof CancellationException)) &#123; LOG.warn(\"RequestManager shutdown with error\", t); &#125; &#125;&#125;, requestManagerExecutor); src\\java\\org\\apache\\hadoop\\hive\\llap\\daemon\\impl\\AMReporter.java java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//162行Futures.addCallback(queueLookupFuture, new FutureCallback&lt;Void&gt;() &#123; @Override public void onSuccess(Void result) &#123; LOG.info(\"AMReporter QueueDrainer exited\"); &#125; @Override public void onFailure(Throwable t) &#123; if (t instanceof CancellationException &amp;&amp; isShutdown.get()) &#123; LOG.info(\"AMReporter QueueDrainer exited as a result of a cancellation after shutdown\"); &#125; else &#123; LOG.error(\"AMReporter QueueDrainer exited with error\", t); Thread.getDefaultUncaughtExceptionHandler().uncaughtException(Thread.currentThread(), t); &#125; &#125;&#125;, queueLookupExecutor);//266行Futures.addCallback(future, new FutureCallback&lt;Void&gt;() &#123; @Override public void onSuccess(Void result) &#123; LOG.info(\"Sent taskKilled for &#123;&#125;\", taskAttemptId); &#125; @Override public void onFailure(Throwable t) &#123; LOG.warn(\"Failed to send taskKilled for &#123;&#125;. The attempt will likely time out.\", taskAttemptId); &#125;&#125;, executor);//331行Futures.addCallback(future, new FutureCallback&lt;Void&gt;() &#123; @Override public void onSuccess(Void result) &#123; // Nothing to do. &#125; @Override public void onFailure(Throwable t) &#123; QueryIdentifier currentQueryIdentifier = amNodeInfo.getQueryIdentifier(); amNodeInfo.setAmFailed(true); LOG.warn(\"Heartbeat failed to AM &#123;&#125;. Marking query as failed. query=&#123;&#125;\", amNodeInfo.amNodeId, currentQueryIdentifier, t); queryFailedHandler.queryFailed(currentQueryIdentifier); &#125;&#125;, executor); src\\java\\org\\apache\\hadoop\\hive\\llap\\daemon\\impl\\LlapTaskReporter.java java12//131行Futures.addCallback(future, new HeartbeatCallback(errorReporter), heartbeatExecutor); src\\java\\org\\apache\\hadoop\\hive\\llap\\daemon\\impl\\TaskExecutorService.java java12345//178行Futures.addCallback(future, new WaitQueueWorkerCallback(), executionCompletionExecutorServiceRaw);//692行Futures.addCallback(future, wrappedCallback, executionCompletionExecutorService); src\\java\\org\\apache\\hadoop\\hive\\llap\\tezplugins\\LlapTaskSchedulerService.java java123456789//747行Futures.addCallback(nodeEnablerFuture, new LoggingFutureCallback(\"NodeEnablerThread\", LOG),nodeEnabledExecutor);//751行Futures.addCallback(delayedTaskSchedulerFuture, new LoggingFutureCallback(\"DelayedTaskSchedulerThread\", LOG),delayedTaskSchedulerExecutor);//755行Futures.addCallback(schedulerFuture, new LoggingFutureCallback(\"SchedulerThread\", LOG),schedulerExecutor); src\\java\\org\\apache\\hadoop\\hive\\ql\\exec\\tez\\WorkloadManager.java java12345678//1089行Futures.addCallback(future, FATAL_ERROR_CALLBACK, timeoutPool);//1923行Futures.addCallback(getFuture, this,timeoutPool);//1977行Futures.addCallback(waitFuture, this, timeoutPool); src\\test\\org\\apache\\hadoop\\hive\\ql\\exec\\tez\\SampleTezSessionState.java java123456789101112//121行Futures.addCallback(waitForAmRegFuture, new FutureCallback&lt;Boolean&gt;() &#123; @Override public void onSuccess(Boolean result) &#123; future.set(session); &#125; @Override public void onFailure(Throwable t) &#123; future.setException(t); &#125;&#125;,timeoutPool); 编译 bash1mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=true Tez编译 拉取源码 bash12cd /opt/software/sourcegit clone https://git-wip-us.apache.org/repos/asf/tez.git 安装Tez必要环境 bash1sudo yum install -y protobuf protobuf-static protobuf-devel 编译 查看编译说明，按照编译说明用下列命令编译 bash12cd tezmvn clean package -Dhadoop.version=3.1.3 -Phadoop28 -P\\!hadoop27 -DskipTests -Dmaven.javadoc.skip=true Phoenix编译 拉取源码 bash12cd /opt/software/sourcegit clone https://git-wip-us.apache.org/repos/asf/phoenix.git 编译 bash12cd phoenixmvn clean package -DskipTests -Dhbase.profile=2.2 -Dhbase.version=2.2.4 Spark编译 去spark官网下载源码，解压到/opt/software/source 进入该目录，编译 bash1./dev/make-distribution.sh --name without-hive --tgz -Pyarn -Phadoop-3.1 -Dhadoop.version=3.1.3 -Pparquet-provided -Porc-provided -Phadoop-provided","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"源码编译","slug":"大数据/源码编译","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hive","slug":"hive","permalink":"https://masteryang4.github.io/tags/hive/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"},{"name":"源码编译","slug":"源码编译","permalink":"https://masteryang4.github.io/tags/%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"},{"name":"phoenix","slug":"phoenix","permalink":"https://masteryang4.github.io/tags/phoenix/"},{"name":"tez","slug":"tez","permalink":"https://masteryang4.github.io/tags/tez/"}]},{"title":"HiveSQL之常用查询函数case","slug":"HiveSQL之常用查询函数case","date":"2020-05-26T15:53:58.000Z","updated":"2020-05-26T16:18:08.383Z","comments":true,"path":"2020/05/26/HiveSQL之常用查询函数case/","link":"","permalink":"https://masteryang4.github.io/2020/05/26/HiveSQL%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%9F%A5%E8%AF%A2%E5%87%BD%E6%95%B0case/","excerpt":"","text":"关键词：CASE WHEN THEN ELSE END数据准备 name dept_id sex 悟空 A 男 大海 A 男 宋宋 B 男 凤姐 A 女 婷姐 B 女 婷婷 B 女 需求求出不同部门男女各多少人。结果如下： Code12A 2 1B 1 2 创建本地emp_sex.txt，导入数据Code1234567[ys@hadoop102 datas]$ vim emp_sex.txt悟空 A 男大海 A 男宋宋 B 男凤姐 A 女婷姐 B 女婷婷 B 女 创建hive表并导入数据sql123456create table emp_sex(name string, dept_id string, sex string) row format delimited fields terminated by \"\\t\";load data local inpath '/opt/module/datas/emp_sex.txt' into table emp_sex; 查询数据sql12345678select dept_id, sum(case sex when '男' then 1 else 0 end) male_count, sum(case sex when '女' then 1 else 0 end) female_countfrom emp_sexgroup by dept_id; 首先注意CASE WHEN THEN ELSE END的缺一不可 也要注意sum函数的用法，sum(条件)是经常会用到的方法！！！ 比如sum(if XXX)就常在HiveSQL里面使用。 例如：sum(if(dt=&#39;2020-05-27&#39;, order_count,0 )) order_count，本质其实是一样的。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hive","slug":"大数据/hive","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hive","slug":"hive","permalink":"https://masteryang4.github.io/tags/hive/"},{"name":"SQL","slug":"SQL","permalink":"https://masteryang4.github.io/tags/SQL/"},{"name":"hivesql","slug":"hivesql","permalink":"https://masteryang4.github.io/tags/hivesql/"}]},{"title":"一段有趣的spark_aggregate代码","slug":"一段有趣的spark-aggregate代码","date":"2020-05-26T14:18:30.000Z","updated":"2020-05-29T14:14:17.186Z","comments":true,"path":"2020/05/26/一段有趣的spark-aggregate代码/","link":"","permalink":"https://masteryang4.github.io/2020/05/26/%E4%B8%80%E6%AE%B5%E6%9C%89%E8%B6%A3%E7%9A%84spark-aggregate%E4%BB%A3%E7%A0%81/","excerpt":"","text":"看到了一段非常有趣的关于spark中aggregate算子的代码，需要很细心才能给出正确答案。 在这里和大家分享。 代码示例scala12345678910111213141516import org.apache.spark.&#123;SparkConf, SparkContext&#125;object TrySpark &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setAppName(\"aggTest\").setMaster(\"local[*]\") val sc = new SparkContext(conf) val rdd = sc.makeRDD(Array(\"12\", \"234\", \"345\", \"4567\"), 2) val str: String = rdd.aggregate(\"0\")((a, b) =&gt; Math.max(a.length, b.length).toString, (x, y) =&gt; x + y) println(str) val str1: String = rdd.aggregate(\"\")((a, b) =&gt; Math.min(a.length, b.length).toString, (x, y) =&gt; x + y) println(str1) &#125;&#125; 前方高能输出结果1 Code1204311 输出结果2 Code1203411 惊不惊喜，刺不刺激（手动狗头）。 解析aggregate：行动算子，意为【聚合】 函数签名 def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U 函数说明 分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合 第一个括号内的参数为初始值 第二个括号中 第一个参数为分区内要执行的函数，初始值和分区内元素依次聚合 第二个参数为分区间要执行的函数，初始值和分区间元素依次聚合 代码详解： scala1val rdd = sc.makeRDD(Array(\"12\", \"234\", \"345\", \"4567\"), 2) scala1rdd.aggregate(\"0\")((a, b) =&gt; Math.max(a.length, b.length).toString, (x, y) =&gt; x + y) 首先注意rdd是两个分区，”12”, “234”一个分区，”345”, “4567”一个分区 执行分区内函数Math.max(a.length, b.length).toString 分区一 “0”，“12”执行函数，输出“2”，【注意：函数后面有个toString】【聚合：上一步输出作为下一步输入】 “2”，”234”执行函数，最终输出“3” 分区二 “0”，“345” =&gt; “3” “3”，”4567” =&gt; 最终 “4” 执行分区间函数(x, y) =&gt; x + y，其实就是一个字符串拼接，但是因为分区的原因 不一定哪个分区先执行完，所以会出现两种情况的字符串拼接：“034” or “043” scala1rdd.aggregate(\"\")((a, b) =&gt; Math.min(a.length, b.length).toString, (x, y) =&gt; x + y) rdd是两个分区，”12”, “234”一个分区，”345”, “4567”一个分区 执行分区内函数Math.min(a.length, b.length).toString 分区一 “”，“12”执行函数，输出“0”，【注意：函数后面有个toString】【聚合：上一步输出作为下一步输入】 “0”，”234”执行函数，最终输出“1”，【注意：“0”的长度是1】 分区二 “”，“345” =&gt; “0” “0”，”4567” =&gt; 最终 “1” 执行分区间函数(x, y) =&gt; x + y，字符串拼接，“”+“1”+“1” =&gt; “11”","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"}]},{"title":"spark常用算子join","slug":"spark常用算子join","date":"2020-05-25T11:52:38.000Z","updated":"2020-05-25T12:12:02.270Z","comments":true,"path":"2020/05/25/spark常用算子join/","link":"","permalink":"https://masteryang4.github.io/2020/05/25/spark%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90join/","excerpt":"","text":"简述JOIN函数签名 def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))] 函数说明 spark RDD 转换算子 (对照函数签名)在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素连接在一起的(K,(V,W))的RDD 重点示例 join leftOuterJoin rightOuterJoin fullOuterJoin scala1234567891011121314151617181920212223242526272829303132333435363738import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object JoinTest &#123; def main(args: Array[String]): Unit = &#123; //1.创建SparkConf val sparkConf: SparkConf = new SparkConf().setAppName(\"JoinTest\").setMaster(\"local[*]\") //2.创建SparkContext val sc = new SparkContext(sparkConf) //3.创建两个RDD val rdd1: RDD[(String, Int)] = sc.makeRDD(Array((\"a\", 1), (\"a\", 2), (\"b\", 1), (\"c\", 1))) val rdd2: RDD[(String, Int)] = sc.makeRDD(Array((\"a\", 1), (\"b\", 1), (\"b\", 2), (\"d\", 1))) //4.测试各种JOIN【 注意返回值 】 val result1: RDD[(String, (Int, Int))] = rdd1.join(rdd2) val result2: RDD[(String, (Int, Option[Int]))] = rdd1.leftOuterJoin(rdd2) val result3: RDD[(String, (Option[Int], Int))] = rdd1.rightOuterJoin(rdd2) val result4: RDD[(String, (Option[Int], Option[Int]))] = rdd1.fullOuterJoin(rdd2) //5.打印 result1.foreach(println) println(\"======================&gt;&gt;&gt;\") result2.foreach(println) println(\"======================&gt;&gt;&gt;\") result3.foreach(println) println(\"======================&gt;&gt;&gt;\") result4.foreach(println) //6.关闭连接 sc.stop() &#125;&#125; 输出结果： Code1234567891011121314151617181920212223(b,(1,1))(a,(1,1))(a,(2,1))(b,(1,2))&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt;&gt;(c,(1,None))(a,(1,Some(1)))(a,(2,Some(1)))(b,(1,Some(1)))(b,(1,Some(2)))&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt;&gt;(d,(None,1))(a,(Some(1),1))(a,(Some(2),1))(b,(Some(1),1))(b,(Some(1),2))&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;&gt;&gt;(d,(None,Some(1)))(c,(Some(1),None))(a,(Some(1),Some(1)))(a,(Some(2),Some(1)))(b,(Some(1),Some(1)))(b,(Some(1),Some(2)))","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"}]},{"title":"scala样例类转换成为JSON字符串","slug":"scala样例类转换成为JSON字符串","date":"2020-05-25T11:08:21.000Z","updated":"2020-05-26T01:01:19.891Z","comments":true,"path":"2020/05/25/scala样例类转换成为JSON字符串/","link":"","permalink":"https://masteryang4.github.io/2020/05/25/scala%E6%A0%B7%E4%BE%8B%E7%B1%BB%E8%BD%AC%E6%8D%A2%E6%88%90%E4%B8%BAJSON%E5%AD%97%E7%AC%A6%E4%B8%B2/","excerpt":"","text":"JSON常用方法Java中并没有内置JSON的解析，因此使用JSON需要借助第三方类库。 几个常用的 JSON 解析类库： Gson: 谷歌开发的 JSON 库，功能十分全面。 FastJson: 阿里巴巴开发的 JSON 库，性能十分优秀。 Jackson: 社区十分活跃且更新速度很快。 maven依赖： xml12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.47&lt;/version&gt;&lt;/dependency&gt; JSON 对象与字符串的相互转化 方法 作用 JSON.parseObject() 从字符串解析 JSON 对象 JSON.parseArray() 从字符串解析 JSON 数组 JSON.toJSONString(obj/array) 将 JSON 对象或 JSON 数组转化为字符串 示例： java1234567891011121314import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;public class JSONTest &#123; public static void main(String[] args) &#123; //从字符串解析JSON对象 JSONObject obj = JSON.parseObject(\"&#123;\\\"name\\\":\\\"ys\\\"&#125;\"); System.out.println(obj); //&#123;\"name\":\"ys\"&#125; //将JSON对象转化为字符串 String objStr = JSON.toJSONString(obj); System.out.println(objStr); //&#123;\"name\":\"ys\"&#125; &#125;&#125; Scala样例类转换成JSON字符串将Scala样例类转换成为JSON字符串，JSON.toJSONString(obj)会失效，所以使用如下方法： maven依赖（json4s —&gt; json for scala）： xml12345&lt;dependency&gt; &lt;groupId&gt;org.json4s&lt;/groupId&gt; &lt;artifactId&gt;json4s-native_2.11&lt;/artifactId&gt; &lt;version&gt;3.5.4&lt;/version&gt;&lt;/dependency&gt; scala1234import org.json4s.native.Serializationimplicit val formats=org.json4s.DefaultFormats //隐式转换val orderInfoJson: String = Serialization.write(orderInfo) 示例 scala123456789101112131415161718192021222324import com.alibaba.fastjson.&#123;JSON, JSONObject&#125;import com.atguigu.bean.UserInfoimport org.json4s.native.Serializationobject JsonStrTest &#123; def main(args: Array[String]): Unit = &#123; val userInfo = UserInfo(\"1001\",\"name1\",\"5\",\"2020-05-25\",\"male\") implicit val formats = org.json4s.DefaultFormats //println(JSON.toJSONString(userInfo)) //报错 val str = Serialization.write(userInfo) println(str) // &#123;\"id\":\"1001\",\"login_name\":\"name1\",\"user_level\":\"5\",\"birthday\":\"2020-05-25\",\"gender\":\"male\"&#125; val nObject: JSONObject = JSON.parseObject(str) //正常解析 println(nObject) // &#123;\"birthday\":\"2020-05-25\",\"login_name\":\"name1\",\"gender\":\"male\",\"user_level\":\"5\",\"id\":\"1001\"&#125; &#125;&#125; UserInfo.scala scala12345case class UserInfo(id:String, login_name:String, user_level:String, birthday:String, gender:String)","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"scala","slug":"大数据/scala","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/scala/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"scala","slug":"scala","permalink":"https://masteryang4.github.io/tags/scala/"}]},{"title":"[精]ElasticSearch总结与思考","slug":"精-ElasticSearch总结与思考","date":"2020-05-18T14:05:34.000Z","updated":"2020-06-09T15:20:01.583Z","comments":true,"path":"2020/05/18/精-ElasticSearch总结与思考/","link":"","permalink":"https://masteryang4.github.io/2020/05/18/%E7%B2%BE-ElasticSearch%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83/","excerpt":"","text":"简介概述 Elasticsearch，基于Lucene，隐藏复杂性，提供简单易用的RestfulAPI接口、JavaAPI接口（还有其他语言的API接口）。 Elasticsearch是一个实时分布式搜索和分析引擎。它用于全文搜索、结构化搜索、分析。 全文检索：将非结构化数据中的一部分信息提取出来,重新组织,使其变得有一定结构,然后对此有一定结构的数据进行搜索,从而达到搜索相对较快的目的。 倒排索引：简单举例：根据关键词找包含其的文章（正常思维：在文章中找关键词）。 结构化检索：我想搜索商品分类为日化用品的商品都有哪些，select * from products where category_id=’日化用品’。 数据分析：电商网站，最近7天牙膏这种商品销量排名前10的商家有哪些；新闻网站，最近1个月访问量排名前3的新闻版块是哪些。 可以作为一个大型分布式集群（数百台服务器）技术，处理PB级数据，服务大公司；也可以运行在单机上，服务小公司. 使用场景 维基百科，类似百度百科，牙膏，牙膏的维基百科，全文检索，高亮，搜索推荐。 The Guardian（国外新闻网站），类似搜狐新闻，用户行为日志（点击，浏览，收藏，评论）+ 社交网络数据（对某某新闻的相关看法），数据分析，给到每篇新闻文章的作者，让他知道他的文章的公众反馈（好，坏，热门，垃圾，鄙视，崇拜）。 Stack Overflow（国外的程序异常讨论论坛），IT问题，程序的报错，提交上去，有人会跟你讨论和回答，全文检索，搜索相关问题和答案，程序报错了，就会将报错信息粘贴到里面去，搜索有没有对应的答案。 GitHub（开源代码管理），搜索上千亿行代码。 国内：站内搜索（电商，招聘，门户，等等），IT系统搜索（OA，CRM，ERP，等等），数据分析（ES热门的一个使用场景）。 核心概念ElasticSearch与数据库类比 关系型数据库（如Mysql） 非关系型数据库（Elasticsearch） 数据库Database 索引Index 表Table 类型Type(6.0版本之后在一个索引下面只能有一个，7.0版本之后取消了Type) 数据行Row 文档Document(JSON格式) 数据列Column 字段Field 约束 Schema 映射Mapping 安装1）解压elasticsearch-6.6.0.tar.gz到/opt/module目录下 Code1[ys@hadoop102 software]$ tar -zxvf elasticsearch-6.6.0.tar.gz -C &#x2F;opt&#x2F;module&#x2F; 2）在/opt/module/elasticsearch-6.6.0路径下创建data文件夹 Code1[ys@hadoop102 elasticsearch-6.6.0]$ mkdir data 3）修改配置文件/opt/module/elasticsearch-6.6.0/config/elasticsearch.yml Code123[ys@hadoop102 config]$ pwd&#x2F;opt&#x2F;module&#x2F;elasticsearch-6.6.0&#x2F;config[ys@hadoop102 config]$ vim elasticsearch.yml yml1234567891011121314#-----------------------Cluster-----------------------cluster.name: my-application#-----------------------Node-----------------------node.name: node-102#-----------------------Paths-----------------------path.data: /opt/module/elasticsearch-6.6.0/datapath.logs: /opt/module/elasticsearch-6.6.0/logs#-----------------------Memory-----------------------bootstrap.memory_lock: falsebootstrap.system_call_filter: false#-----------------------Network-----------------------network.host: 192.168.9.102 #-----------------------Discovery-----------------------discovery.zen.ping.unicast.hosts: [\"192.168.9.102\"] （1）cluster.name 如果要配置集群需要两个节点上的elasticsearch配置的cluster.name相同，都启动可以自动组成集群，这里如果不改cluster.name则默认是cluster.name=my-application， （2）nodename随意取但是集群内的各节点不能相同 （3）修改后的每行前面不能有空格，修改后的“：”后面必须有一个空格 4）分发至hadoop103以及hadoop104，分发之后修改： Code1234567[ys@hadoop102 module]$ xsync elasticsearch-6.6.0&#x2F;node.name: node-103network.host: 192.168.9.103node.name: node-104network.host: 192.168.9.104 5）此时启动会报错，要配置linux系统环境（参考：http://blog.csdn.net/satiling/article/details/59697916） 6）启动Elasticsearch Code1[ys@hadoop102 elasticsearch-6.6.0]$ bin&#x2F;elasticsearch 7）测试elasticsearch Code12345678910111213141516171819[ys@hadoop102 elasticsearch-6.6.0]$ curl http:&#x2F;&#x2F;hadoop102:9200&#123; &quot;name&quot; : &quot;node-102&quot;, &quot;cluster_name&quot; : &quot;my-application&quot;, &quot;cluster_uuid&quot; : &quot;KOpuhMgVRzW_9OTjMsHf2Q&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;6.6.0&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;, &quot;build_hash&quot; : &quot;eb782d0&quot;, &quot;build_date&quot; : &quot;2018-06-29T21:59:26.107521Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;7.3.1&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 8）停止集群 Code1kill -9 进程号 9）群起脚本 Code1[ys@hadoop102 bin]$ vi es.sh shell123456789101112131415161718#!/bin/bashes_home=/opt/module/elasticsearchcase $1 in \"start\") &#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==============$i==============\" ssh $i \"source /etc/profile;$&#123;es_home&#125;/bin/elasticsearch &gt;/dev/null 2&gt;&amp;1 &amp;\" done&#125;;;\"stop\") &#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==============$i==============\" ssh $i \"ps -ef|grep $es_home |grep -v grep|awk '&#123;print \\$2&#125;'|xargs kill\" &gt;/dev/null 2&gt;&amp;1 done&#125;;;esac 可视化工具KibanaKibana的安装 1、将kibana压缩包上传到虚拟机指定目录 Code1[ys@hadoop102 software]$ tar -zxvf kibana-6.6.0-linux-x86_64.tar.gz -C &#x2F;opt&#x2F;module&#x2F; 2、修改相关配置，连接Elasticsearch Code1[ys@hadoop102 kibana]$ vim config&#x2F;kibana.yml yml12345678910# Kibana is served by a back end server. This setting specifies the port to use.server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is 'localhost', which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.server.host: \"192.168.9.102\"... ...... ...# The URL of the Elasticsearch instance to use for all your queries.elasticsearch.url: \"http://192.168.9.102:9200\" 3、启动Kibana Code1[ys@hadoop102 kibana]$ bin&#x2F;kibana 4、浏览器访问：hadoop102:5601 即可操作 操作命令行操作核心数据类型 字符串型：text(分词)、keyword(不分词) 数值型：long、integer、short、byte、double、float、half_float、scaled_float 日期类型：date Mapping1、手动创建 创建mapping Code1234567891011121314151617PUT my_index1&#123; &quot;mappings&quot;: &#123; &quot;_doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;username&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;pinyin&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 创建文档 Code1234PUT my_index1&#x2F;_doc&#x2F;1&#123; &quot;username&quot;:&quot;haha heihei&quot;&#125; 查询 Code12345678GET my_index1&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;username.pinyin&quot;: &quot;haha&quot; &#125; &#125;&#125; 2、自动创建 直接插入文档 Code123456PUT &#x2F;test_index&#x2F;_doc&#x2F;1&#123; &quot;username&quot;:&quot;alfred&quot;, &quot;age&quot;:1, &quot;birth&quot;:&quot;1991-12-15&quot;&#125; 查看mapping Code123456789101112131415161718192021222324252627GET &#x2F;test_index&#x2F;doc&#x2F;_mapping&#123; &quot;test_index&quot;: &#123; &quot;mappings&quot;: &#123; &quot;doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;birth&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;username&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; IK分词器分词器主要应用在中文上，在ES中字符串类型有keyword和text两种。keyword默认不进行分词，而text是将每一个汉字拆开称为独立的词，这两种都是不适用于生产环境。 keyword分词 Code1234GET _analyze&#123; &quot;keyword&quot;:&quot;我是程序员&quot;&#125; 结果展示（会报错error） text类型的分词 Code1234GET _analyze&#123; &quot;text&quot;:&quot;我是程序员&quot;&#125; 结果展示： Code123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;我&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 1, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;是&quot;, &quot;start_offset&quot;: 1, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;程&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 3, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;序&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 3 &#125;, &#123; &quot;token&quot;: &quot;员&quot;, &quot;start_offset&quot;: 4, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 4 &#125; ]&#125; IK分词器安装1）下载与安装的ES相对应的版本 2）解压elasticsearch-analysis-ik-6.6.0.zip，将解压后的IK文件夹拷贝到ES安装目录下的plugins目录下，并重命名文件夹为ik（什么名称都OK） Code1[ys@hadoop102 plugins]$ mkdir ik Code1[ys@hadoop102 software]$ unzip elasticsearch-analysis-ik-6.6.0.zip -d &#x2F;opt&#x2F;module&#x2F;elasticsearch-6.6.0&#x2F;plugins&#x2F;ik&#x2F; 3）分发分词器目录 Code1[ys@hadoop102 elasticsearch-6.6.0]$ xsync plugins&#x2F; 4）重新启动Elasticsearch，即可加载IK分词器 5）IK测试 ik_smart ：最少切分 ik_max_word：最细粒度划分 Code12345get _analyze&#123; &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;text&quot;:&quot;我是程序员&quot;&#125; Code12345678910111213141516171819202122232425&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;我&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;程序员&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125; ]&#125; ik_max_word Code1&quot;我&quot;,&quot;是&quot;,&quot;程序员&quot;,&quot;程序&quot;,&quot;员&quot; 检索文档【重点】向Elasticsearch增加数据 Code12345678PUT &#x2F;atguigu&#x2F;doc&#x2F;1&#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [&quot;sports&quot;, &quot;music&quot;]&#125; 查询数据 Code12# 协议方法 索引&#x2F;类型&#x2F;文档编号GET &#x2F;atguigu&#x2F;doc&#x2F;1 响应 Code1234567891011121314151617&#123; &quot;_index&quot;: &quot;atguigu&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &#x2F;&#x2F; 文档的原始数据JSON数据 &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125;&#125; 元数据查询Code1GET _cat&#x2F;indices 全文档检索Code12# 协议方法 索引&#x2F;类型&#x2F;_searchGET &#x2F;atguigu&#x2F;_doc&#x2F;_search 字段全值匹配检索[filter]Code123456789101112GET atguigu&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;about&quot;: &quot;I love to go rock climbing&quot; &#125; &#125; &#125; &#125;&#125; 字段分词匹配检索[match]Code12345678GET atguigu&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;about&quot;: &quot;I&quot; &#125; &#125;&#125; 字段模糊匹配检索[fuzzy]Code12345678910GET test&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;aa&quot;: &#123; &quot;value&quot;: &quot;我是程序&quot; &#125; &#125; &#125;&#125; 聚合检索Code1234567891011GET test&#x2F;_search&#123; &quot;aggs&quot;: &#123; &quot;groupby_aa&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;aa&quot;, &quot;size&quot;: 10 &#125; &#125; &#125;&#125; 分页检索Code123456GET movie_index&#x2F;movie&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot;: 1, &quot;size&quot;: 1&#125; 索引别名 _aliases索引别名就像一个快捷方式或软连接，可以指向一个或多个索引，也可以给任何一个需要索引名的API来使用。别名带给我们极大的灵活性，允许我们做下面这些： 1）给多个索引分组 (例如， last_three_months) 2）给索引的一个子集创建视图 3）在运行的集群中可以无缝的从一个索引切换到另一个索引 说白了就是功能更强大的视图 创建索引别名 建表时直接声明 Code12345678910111213141516171819202122232425262728293031PUT movie_chn_2020&#123; &quot;aliases&quot;: &#123; &quot;movie_chn_2020-query&quot;: &#123;&#125; &#125;, &quot;mappings&quot;: &#123; &quot;movie&quot;:&#123; &quot;properties&quot;: &#123; &quot;id&quot;:&#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;:&#123; &quot;type&quot;: &quot;text&quot; , &quot;analyzer&quot;: &quot;ik_smart&quot; &#125;, &quot;doubanScore&quot;:&#123; &quot;type&quot;: &quot;double&quot; &#125;, &quot;actorList&quot;:&#123; &quot;properties&quot;: &#123; &quot;id&quot;:&#123; &quot;type&quot;:&quot;long&quot; &#125;, &quot;name&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 为已存在的索引增加别名 Code123456POST _aliases&#123; &quot;actions&quot;: [ &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;, &quot;alias&quot;: &quot;movie_chn_2020-query&quot; &#125;&#125; ]&#125; 也可以通过加过滤条件缩小查询范围，建立一个子集视图 Code1234567891011121314POST _aliases&#123; &quot;actions&quot;: [ &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;, &quot;alias&quot;: &quot;movie_chn0919-query-zhhy&quot;, &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;actorList.id&quot;: &quot;3&quot; &#125; &#125; &#125; &#125; ]&#125; 查询别名：与使用普通索引没有区别 Code1GET movie_chn_2020-query&#x2F;_search 删除某个索引的别名 Code123456POST _aliases&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;, &quot;alias&quot;: &quot;movie_chn_2020-query&quot; &#125;&#125; ]&#125; 为某个别名进行无缝切换 Code1234567POST &#x2F;_aliases&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;, &quot;alias&quot;: &quot;movie_chn_2020-query&quot; &#125;&#125;, &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;movie_chn_yyyy&quot;, &quot;alias&quot;: &quot;movie_chn_2020-query&quot; &#125;&#125; ]&#125; 查询别名列表 Code1GET _cat&#x2F;aliases?v 索引模板Index Template 索引模板，顾名思义，就是创建索引的模具，其中可以定义一系列规则来帮助我们构建符合特定业务需求的索引的mappings和 settings，通过使用 Index Template 可以让我们的索引具备可预知的一致性。 常见的场景: 分割索引 分割索引就是根据时间间隔把一个业务索引切分成多个索引。比如把order_info 变成 order_info_20200101,order_info_20200102 ….. 这样做的好处有两个： 1、结构变化的灵活性：因为elasticsearch不允许对数据结构进行修改。但是实际使用中索引的结构和配置难免变化，那么只要对下一个间隔的索引进行修改，原来的索引位置原状。这样就有了一定的灵活性。 2、查询范围优化：因为一般情况并不会查询全部时间周期的数据，那么通过切分索引，物理上减少了扫描数据的范围，也是对性能的优化。 创建模板 Code123456789101112131415161718192021222324PUT _template&#x2F;template_movie2020&#123; &quot;index_patterns&quot;: [&quot;movie_test*&quot;], &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;aliases&quot; : &#123; &quot;&#123;index&#125;-query&quot;: &#123;&#125;, &quot;movie_test-query&quot;:&#123;&#125; &#125;, &quot;mappings&quot;: &#123; &quot;_doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;movie_name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; &#125; &#125; &#125; &#125;&#125; 其中 “index_patterns”: [“movie_test*”], 的含义就是凡是往movie_test开头的索引写入数据时，如果索引不存在，那么es会根据此模板自动建立索引。 在 “aliases” 中用{index}表示，获得真正的创建的索引名。 测试： Code12345POST movie_test_2020xxxx&#x2F;_doc&#123; &quot;id&quot;:&quot;333&quot;, &quot;name&quot;:&quot;zhang3&quot;&#125; 查看系统中已有的模板清单 Code1GET _cat&#x2F;templates 查看某个模板详情 Code123GET _template&#x2F;template_movie2020或者GET _template&#x2F;template_movie* JavaAPI操作maven依赖: xml1234567891011121314151617181920212223242526272829303132333435&lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpmime&lt;/artifactId&gt; &lt;version&gt;4.3.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.searchbox&lt;/groupId&gt; &lt;artifactId&gt;jest&lt;/artifactId&gt; &lt;version&gt;5.3.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna&lt;/artifactId&gt; &lt;version&gt;4.5.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.codehaus.janino&lt;/groupId&gt; &lt;artifactId&gt;commons-compiler&lt;/artifactId&gt; &lt;version&gt;2.7.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;versison&gt;6.6.0&lt;/version&gt;&lt;/dependency&gt; 单条写入数据 java1234567891011121314151617181920212223242526272829303132333435363738394041import com.ys.bean.Stu;import io.searchbox.client.JestClient;import io.searchbox.client.JestClientFactory;import io.searchbox.client.config.HttpClientConfig;import io.searchbox.core.Index;import java.io.IOException;public class ESWriter &#123; public static void main(String[] args) throws IOException &#123; //一、创建ES客户端对象 //1.1 创建ES客户端的工厂对象 JestClientFactory jestClientFactory = new JestClientFactory(); //1.2 创建配置信息 HttpClientConfig config = new HttpClientConfig.Builder(\"http://hadoop102:9200\").build(); jestClientFactory.setHttpClientConfig(config); //1.3 获取客户端对象 JestClient jestClient = jestClientFactory.getObject(); //二、写入数据 //2.1 创建Action对象 --&gt; Index Stu stu = new Stu(\"004\", \"少爷\"); Index index = new Index.Builder(stu) .index(\"stu_temp_01\") .type(\"_doc\") .id(\"1004\") .build(); //2.2 执行写入数据操作 jestClient.execute(index); //三、关闭资源 jestClient.shutdownClient(); &#125;&#125; 批量写入数据 java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import com.ys.bean.Stu;import io.searchbox.client.JestClient;import io.searchbox.client.JestClientFactory;import io.searchbox.client.config.HttpClientConfig;import io.searchbox.core.Bulk;import io.searchbox.core.Index;import java.io.IOException;public class ESWriterByBulk &#123; public static void main(String[] args) throws IOException &#123; //一、创建ES客户端对象 //1.1 创建ES客户端的工厂对象 JestClientFactory jestClientFactory = new JestClientFactory(); //1.2 创建配置信息 HttpClientConfig config = new HttpClientConfig.Builder(\"http://hadoop102:9200\").build(); jestClientFactory.setHttpClientConfig(config); //1.3 获取客户端对象 JestClient jestClient = jestClientFactory.getObject(); //二、批量写入 //2.1 准备数据 Stu stu1 = new Stu(\"008\", \"麻瓜\"); Stu stu2 = new Stu(\"009\", \"海格\"); //2.2 创建Bulk.Builder对象 Bulk.Builder builder = new Bulk.Builder(); //2.3 创建Index对象 Index index1 = new Index.Builder(stu1).id(\"1008\").build(); Index index2 = new Index.Builder(stu2).id(\"1009\").build(); //2.4 赋值默认的索引名称及类型名 builder.defaultIndex(\"stu_temp_01\"); builder.defaultType(\"_doc\"); //2.5 添加Index之Bulk builder.addAction(index1); builder.addAction(index2); //2.6 真正构建Bulk对象 Bulk bulk = builder.build(); //2.7 执行批量写入数据操作 jestClient.execute(bulk); //3.关闭连接 jestClient.shutdownClient(); &#125;&#125; 读取数据（这里不使用json串，可读性不好） java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import io.searchbox.client.JestClient;import io.searchbox.client.JestClientFactory;import io.searchbox.client.config.HttpClientConfig;import io.searchbox.core.Search;import io.searchbox.core.SearchResult;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.TermQueryBuilder;import org.elasticsearch.search.builder.SearchSourceBuilder;import java.io.IOException;import java.util.List;import java.util.Map;public class ESReader &#123; public static void main(String[] args) throws IOException &#123; //1.获取客户端对象 //1.1 创建ES客户端的工厂对象 JestClientFactory jestClientFactory = new JestClientFactory(); //1.2 创建配置信息 HttpClientConfig config = new HttpClientConfig.Builder(\"http://hadoop102:9200\").build(); jestClientFactory.setHttpClientConfig(config); //1.3 获取客户端对象 JestClient jestClient = jestClientFactory.getObject(); //2.读取数据 //2.0 创建查询条件 SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); BoolQueryBuilder boolQueryBuilder = new BoolQueryBuilder(); boolQueryBuilder.filter(new TermQueryBuilder(\"class_id\", \"190218\")); searchSourceBuilder.query(boolQueryBuilder); searchSourceBuilder.from(0); searchSourceBuilder.size(2); //2.1 创建Search对象 Search search = new Search.Builder(searchSourceBuilder.toString()) .addIndex(\"student\") .addType(\"_doc\") .build(); //2.2 执行查询操作 SearchResult searchResult = jestClient.execute(search); //2.3 解析searchResult System.out.println(\"查询数据\" + searchResult.getTotal() + \"条！\"); // [json对应map是常见操作] List&lt;SearchResult.Hit&lt;Map, Void&gt;&gt; hits = searchResult.getHits(Map.class); for (SearchResult.Hit&lt;Map, Void&gt; hit : hits) &#123; Map source = hit.source; for (Object key : source.keySet()) &#123; System.out.println(hit.id + \":\" + key.toString() + \":\" + source.get(key).toString()); &#125; System.out.println(\"*************\"); &#125; //3.关闭资源 jestClient.shutdownClient(); &#125;&#125; java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// Stu.javapublic class Stu &#123; private String id; private String name; public Stu() &#123; &#125; public Stu(String id, String name) &#123; this.id = id; this.name = name; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Stu stu = (Stu) o; if (id != null ? !id.equals(stu.id) : stu.id != null) return false; return name != null ? name.equals(stu.name) : stu.name == null; &#125; @Override public int hashCode() &#123; int result = id != null ? id.hashCode() : 0; result = 31 * result + (name != null ? name.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return \"Stu&#123;\" + \"id='\" + id + '\\'' + \", name='\" + name + '\\'' + '&#125;'; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"elasticsearch","slug":"大数据/elasticsearch","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/elasticsearch/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据库","slug":"数据库","permalink":"https://masteryang4.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://masteryang4.github.io/tags/elasticsearch/"}]},{"title":"MyISAM与InnoDB的区别(详)","slug":"MyISAM与InnoDB的区别-详","date":"2020-05-14T12:52:48.000Z","updated":"2020-05-14T13:47:24.563Z","comments":true,"path":"2020/05/14/MyISAM与InnoDB的区别-详/","link":"","permalink":"https://masteryang4.github.io/2020/05/14/MyISAM%E4%B8%8EInnoDB%E7%9A%84%E5%8C%BA%E5%88%AB-%E8%AF%A6/","excerpt":"","text":"MyISAM与InnoDB的区别（详）1.事务 InnoDB支持事务，MyISAM不支持。 对于InnoDB每一条SQL语言都默认封装成事务，自动提交，这样会影响速度，所以最好把多条SQL语言放在begin和commit之间，组成一个事务； 所以，博客中的《 MySQL事务相关 》一文，是基于InnoDB引擎的。 2.外键 InnoDB支持外键，而MyISAM不支持。 对一个包含外键的InnoDB表转为MYISAM会失败； 3.索引 InnoDB是聚集索引，使用B+Tree作为索引结构，数据文件是和（主键）索引绑在一起的（表数据文件本身就是按B+Tree组织的一个索引结构），必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。 MyISAM是非聚集索引，也是使用B+Tree作为索引结构，索引和数据文件是分离的（联系本文第9点），索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 也就是说：InnoDB的B+树主键索引的叶子节点就是数据文件，辅助索引的叶子节点是主键的值；而MyISAM的B+树主键索引和辅助索引的叶子节点都是数据文件的地址指针。 4.表的具体行数 InnoDB不保存表的具体行数，执行select count(*) from table时需要全表扫描。 而MyISAM用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快（注意不能加有任何WHERE条件）； 那么为什么InnoDB没有了这个变量呢？ ​ 因为InnoDB的事务特性，在同一时刻表中的行数对于不同的事务而言是不一样的，因此count统计会计算对于当前事务而言可以统计到的行数，而不是将总行数储存起来方便快速查询。InnoDB会尝试遍历一个尽可能小的索引除非优化器提示使用别的索引。如果二级索引不存在，InnoDB还会尝试去遍历其他聚簇索引. ​ 如果索引并没有完全处于InnoDB维护的缓冲区（Buffer Pool）中，count操作会比较费时。可以建立一个记录总行数的表并让你的程序在INSERT/DELETE时更新对应的数据。和上面提到的问题一样，如果此时存在多个事务的话这种方案也不太好用。如果得到大致的行数值已经足够满足需求可以尝试： SHOW TABLE STATUS 5.全文索引 Innodb不支持全文索引，而MyISAM支持全文索引，在涉及全文索引领域的查询效率上MyISAM速度更快高； 5.7以后的InnoDB支持全文索引了。 6.表压缩 MyISAM表格可以被压缩后进行查询操作,压缩表是不能进行修改的(除非先将表解除压缩，修改数据，然后再次压缩)。压缩表可以极大地减少磁盘空间占用，因此也可以减少磁盘I/O，从而提升查询性能，压缩表也支持索引，但索引也只是只读的。 7.锁粒度 InnoDB支持表、行(默认)级锁，而MyISAM支持表级锁。 InnoDB的行锁是实现在索引上的，而不是锁在物理行记录上。 潜台词是，如果访问没有命中索引，也无法使用行锁，将要退化为表锁 T_T。 8.主键 InnoDB表必须有主键（用户没有指定的话会自己找或生产一个主键），而Myisam可以没有 9.表数据文件存储 Innodb存储文件有frm、ibd，而Myisam是frm、MYD、MYI Innodb：frm是表定义文件，ibd是数据文件（共享表空间和单独表空间） Myisam：frm是表定义文件，myd是数据文件，myi是索引文件 索引选择 除非需要用到某些Innodb不具备的特性，并且没有其他办法可以代替，否则都应该优先选择innodb引擎。 参考文章 https://blog.csdn.net/qq_35642036/article/details/82820178 （里面的图片值得参考） https://www.cnblogs.com/timor0101/p/12883649.html","categories":[{"name":"SQL","slug":"SQL","permalink":"https://masteryang4.github.io/categories/SQL/"},{"name":"MySQL","slug":"SQL/MySQL","permalink":"https://masteryang4.github.io/categories/SQL/MySQL/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"数据库","slug":"数据库","permalink":"https://masteryang4.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://masteryang4.github.io/tags/MySQL/"}]},{"title":"MySQL事务相关","slug":"MySQL事务相关","date":"2020-05-14T09:10:57.000Z","updated":"2020-05-14T12:21:18.503Z","comments":true,"path":"2020/05/14/MySQL事务相关/","link":"","permalink":"https://masteryang4.github.io/2020/05/14/MySQL%E4%BA%8B%E5%8A%A1%E7%9B%B8%E5%85%B3/","excerpt":"","text":"事务四大特性（ACID）1、原子性（Atomicity）： 事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。 事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。 也就是说事务是一个不可分割的整体。 的基本单位 2、一致性（Consistency）： 事务开始前和结束后，数据库的完整性约束没有被破坏 。 比如 A 向 B 转账，不可能 A 扣了钱，B 却没收到。 3、隔离性（Isolation）： 同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。 比如 A 正在从一张银行卡中取钱，在 A 取钱的过程结束前，B 不能向这张卡转账。 4、持久性（Durability）： 事务完成后，事务对数据库的所有更新将被保存到数据库，不 能回滚。 MySQL事务隔离级别多个事务之间隔离的，相互独立的。 但是如果多个事务操作同一批数据，则会引发一些问题，设置不同的隔离级别就可以解决这些问题。 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted） 是 是 是 不可重复读（read-committed） 否 是 是 可重复读（repeatable-read） 否 否 是 串行化（serializable） 否 否 否 隔离级别越高，效率越低。 大多数数据库的默认级别就是不可重复读（Read committed），比如Sql Server , Oracle 【注意】MySQL的默认事务隔离级别是——可重复读 事务并发存在的问题1、脏读：事务 A 读取了事务 B 更新的数据，然后 B 回滚操作，那么 A 读取到的数据是脏数据。 （一个事务，读取到另一个事务中没有提交的数据） 2、不可重复读：事务A多次读取同一数据，事务B在事务A多次读取的过程中，对数据做了更新并提交，导致事务 A多次读取同一数据时，结果不一致 。 （在同一个事务中，两次读取到的数据不一样 ） 3、幻读：系统管理员 A 将数据库中所有学生的成绩从具体分数改为 ABCDE 等级，但是系统管理员 B 就在这个时候插入了一条具体分数的记录，当系统管理员 A 改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 （一个事务操作(DML)数据表中所有记录，另一个事务添加了一条数据，则第一个事务查询不到添加的数据） （ 一个事务(同一个read view)在前后两次查询同一范围的时候，后一次查询看到了前一次查询没有看到的行） 可重复读的隔离级别下使用了MVCC机制，select操作不会更新版本号，是快照读（历史版本）； insert、update和delete会更新版本号，是当前读（当前版本）。 幻读只在当前读下才会出现。 不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。 解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表等方法 幻读产生的原因： 行锁只能锁住行，即使把所有的行记录都上锁，也阻止不了新插入的记录。 解决幻读的其他方法： 将两行记录间的空隙加上锁，阻止新记录的插入；这个锁称为间隙锁。","categories":[{"name":"SQL","slug":"SQL","permalink":"https://masteryang4.github.io/categories/SQL/"},{"name":"MySQL","slug":"SQL/MySQL","permalink":"https://masteryang4.github.io/categories/SQL/MySQL/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"数据库","slug":"数据库","permalink":"https://masteryang4.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://masteryang4.github.io/tags/MySQL/"}]},{"title":"[精]zookeeper总结与思考","slug":"精-zookeeper总结与思考","date":"2020-05-14T08:30:13.000Z","updated":"2020-06-09T15:19:04.899Z","comments":true,"path":"2020/05/14/精-zookeeper总结与思考/","link":"","permalink":"https://masteryang4.github.io/2020/05/14/%E7%B2%BE-zookeeper%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83/","excerpt":"","text":"一、介绍概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。多作为集群提供服务的中间件. Zookeeper从设计模式角度来理解，是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生了变化，Zookeeper就负责通知已经在Zookeeper上注册的那些观察者做出相应的反应. 分布式系统: 分布式系统指由很多台计算机组成的一个整体。 这个整体一致对外,并且处理同一请求，系统对内透明，对外不透明。 内部的每台计算机都可以相互通信，例如使用RPC 或者是WebService。客户端向一个分布式系统发送的一次请求到接受到响应，有可能会经历多台计算机。 Zookeeper = 文件系统 + 通知机制 特点中心化集群，但是中心化集群易出现单点故障。 数据结构 应用场景提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。 二、安装及操作需要提前安装JDK 两种部署方式：本地模式（standalone），分布式模式 分布式安装部署 版本：zookeeper-3.4.10 1、规划 将在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。 2、解压安装 三台服务器分别解压：tar -zxvf zookeeper-3.4.10.tar.gz 解压后生成zookeeper-3.4.10目录 3、配置服务器编号 在zookeeper-3.4.10目录下创建zkData：mkdir -p zkData 进入目录：cd zkData 创建myid文件：touch myid 编辑文件：vim myid 在文件中添加与server对应的编号：比如hadoop02添加2； 在hadoop103、hadoop104上修改myid文件中内容为3、4 4、修改配置文件 zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg重命名为zoo.cfg：mv zoo_sample.cfg zoo.cfg 打开zoo.cfg文件：vim zoo.cfg 在文件中修改数据存储路径配置： dataDir=/opt/module/zookeeper-3.4.10/zkData 并且增加如下配置： #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 同步zoo.cfg配置文件到其他所有服务器 【配置参数解读】server.A=B:C:D A是一个数字，表示这个是第几号服务器【myid】； zk启动时读取myid文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口2888；【副本】 D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口3888。【选举信息】 【扩展】2181，客户端访问端口 5、相关操作 三台服务器在zookeeper-3.4.10下分别启动：bin/zkServer.sh start 查看状态：bin/zkServer.sh status shell123456789101112[ys@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower[ys@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: leader[ys@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower 客户端命令行操作启动客户端：bin/zkCli.sh 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] （详细信息）查看当前节点数据并能看到更新次数等数据 create 普通创建-s 含有序列-e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 三、内部原理【重点】选举机制【重点】 半数机制： 集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。 内部投票选举： Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。 【举例】五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。这些服务器依序启动，则： Code1234567891011121314151617181920212223242526因为一共5台服务器，只有超过半数以上，即最少启动3台服务器，集群才能正常工作。（1）服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成；服务器1状态保持为LOOKING；（2）服务器2启动，再发起一次选举。服务器1和2分别投自己一票，此时服务器1发现服务器2的id比自己大，更改选票投给服务器2；此时服务器1票数0票，服务器2票数2票，不够半数以上（3票），选举无法完成；服务器1，2状态保持LOOKING；（3）服务器3启动，发起一次选举。与上面过程一样，服务器1和2先投自己一票，然后因为服务器3id最大，两者更改选票投给为服务器3；此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数（3票），服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；（4）服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3；服务器4并更改状态为FOLLOWING；（5）服务器5启动，同4一样投票给3，此时服务器3一共5票，服务器5为0票；服务器5并更改状态为FOLLOWING；最终Leader是服务器3，状态为LEADING；其余服务器是Follower，状态为FOLLOWING。 参考文章： https://blog.csdn.net/weixin_43291055/article/details/95451357 选举机制文章推荐： https://www.cnblogs.com/shuaiandjun/p/9383655.html https://blog.csdn.net/wyqwilliam/article/details/83537139 节点类型 监听器原理【重点】 写数据流程 【案例】监听服务器节点动态上下线/zk工作机制 API操作：1、maven依赖 xml123456&lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.10&lt;/version&gt;&lt;/dependency&gt; 2、集群上创建/servers节点 shell12[zk: localhost:2181(CONNECTED) 10] create /servers \"servers\"Created /servers 3、服务器端向Zookeeper注册 java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import java.io.IOException;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.ZooDefs.Ids;public class DistributeServer &#123; private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = \"/servers\"; // 创建到zk的客户端连接 public void getConnect() throws IOException&#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; &#125; &#125;); &#125; // 注册服务器 public void registServer(String hostname) throws Exception&#123; String create = zk.create(parentNode + \"/server\", hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname +\" is online \"+ create); &#125; // 业务功能 public void business(String hostname) throws Exception&#123; System.out.println(hostname+\" is working ...\"); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 1获取zk连接 DistributeServer server = new DistributeServer(); server.getConnect(); // 2 利用zk连接注册服务器信息 server.registServer(args[0]); // 3 启动业务功能 server.business(args[0]); &#125;&#125; 4、客户端 java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;public class DistributeClient &#123; private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = \"/servers\"; // 创建到zk的客户端连接 public void getConnect() throws IOException &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 再次启动监听 try &#123; getServerList(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; // 获取服务器列表信息 public void getServerList() throws Exception &#123; // 1获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zk.getChildren(parentNode, true); // 2存储服务器信息列表 ArrayList&lt;String&gt; servers = new ArrayList&lt;&gt;(); // 3遍历所有节点，获取节点中的主机名称信息 for (String child : children) &#123; byte[] data = zk.getData(parentNode + \"/\" + child, false, null); servers.add(new String(data)); &#125; // 4打印服务器列表信息 System.out.println(servers); &#125; // 业务功能 public void business() throws Exception&#123; System.out.println(\"client is working ...\");Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 1获取zk连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 2获取servers的子节点信息，从中获取服务器信息列表 client.getServerList(); // 3业务进程启动 client.business(); &#125;&#125; 四、其他注意点：1、zk常用端口号： 2181，客户端访问端口2888，zk内部信息通讯（数据）3888，zk选举专用 2、zk不能越级创建节点； 且创建节点一般要带有数据（除非数据是null），否则创建会失败 shell1234567891011121314[zk: localhost:2181(CONNECTED) 1] create /ys/sss \"666\"Node does not exist: /ys/sss[zk: localhost:2181(CONNECTED) 2] create /ys \"666\" Created /ys...[zk: localhost:2181(CONNECTED) 16] create /ss nullCreated /ys [zk: localhost:2181(CONNECTED) 17] ls /[cluster, configs, controller, brokers, zookeeper, overseer, admin, isr_change_notification, controller_epoch, druid, aliases.json, live_nodes, collections, overseer_elect, spark, clusterstate.json, consumers, 【ss】, latest_producer_id_block, config, hbase, kylin][zk: localhost:2181(CONNECTED) 18] ls /ss[][zk: localhost:2181(CONNECTED) 19] get /ssnull... 常考面试题 请简述ZooKeeper的选举机制 半数机制：2n+1 10 台服务器：3 台 zk 20 台服务器：5 台 zk 100 台服务器：11 台 zk 【注意】台数并不是越多越好。 太多选举时间过长影响性能。 ZooKeeper的监听原理 ZooKeeper的常用命令 ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？ 部署方式单机模式、集群模式 角色：Leader和Follower 集群最少需要机器数：3","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"zookeeper","slug":"大数据/zookeeper","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/zookeeper/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://masteryang4.github.io/tags/zookeeper/"},{"name":"分布式","slug":"分布式","permalink":"https://masteryang4.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"[精]Redis总结与思考","slug":"精-Redis总结与思考","date":"2020-05-12T14:20:49.000Z","updated":"2020-06-09T15:19:27.345Z","comments":true,"path":"2020/05/12/精-Redis总结与思考/","link":"","permalink":"https://masteryang4.github.io/2020/05/12/%E7%B2%BE-Redis%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83/","excerpt":"","text":"Redis介绍及安装Redis简介1、Redis是最常用的非关系型数据库（NoSQL）——不依赖业务逻辑方式存储，而以简单的key-value模式存储。 常见的NoSQL数据库： ​ Memcached,Redis,MongoDB,HBase 2、Redis有16个库，编号为0~15，默认使用0号库。 3、Redis使用的是单线程+多路IO复用技术（Linux系统特有）。 Redis安装及启动1、Redis安装步骤： 首先保证有gcc-c++工具，否则先执行：yum install gcc-c++ 下载获得redis-3.2.5.tar.gz后将它放入Linux目录 解压命令:tar -zxvf redis-3.2.5.tar.gz 解压完成后进入目录:cd redis-3.2.5 在redis-3.2.5目录下执行make命令 在redis-3.2.5目录下执行make install命令 2、Redis默认安装目录：/usr/local/bin redis-benchmark：性能测试工具，可以在自己本子运行，看看自己本子性能如何(服务启动起来后执行) redis-check-aof：修复有问题的AOF文件 redis-check-rdb：修复有问题RDB文件 redis-sentinel：Redis集群使用 redis-server：Redis服务器启动命令 redis-cli：客户端，操作入口 3、Redis启动： 备份redis.conf：拷贝一份redis.conf到其他目录 修改redis.conf文件将里面的daemonize no 改成 yes(128行)，让服务在后台启动 启动命令：执行 redis-server /root/myredis/redis.conf 用客户端访问: redis-cli -p 6379 关闭：客户端中输入shutdown，redis-server进程就已关闭。之后Ctrl+c退出客户端即可。 Redis数据类型 常用五大数据类型：String,list,set,hash,zset 五大数据类型常用指令： 0、Key Key常用指令 keys * 查询当前库的所有键 exists &lt;key&gt; 判断某个键是否存在 type &lt;key&gt; 查看键对应的数据的类型 del &lt;key&gt; 删除某个键 expire &lt;key&gt; &lt;seconds&gt; 为键值设置过期时间，单位秒 ttl &lt;key&gt; 查看还有多少秒过期，-1表示永不过期，-2表示已过期 dbsize 查看当前数据库的key的数量 flushdb 清空当前库 flushall 通杀全部库 1、String String类型是二进制安全的。意味着Redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。 String类型是Redis最基本的数据类型，一个Redis中字符串value最多可以是512M String常用指令 get &lt;key&gt; 查询对应键值 set &lt;key&gt; &lt;value&gt; 添加键值对 append &lt;key&gt; &lt;value&gt; 将给定的&lt;value&gt;追加到原值的末尾 strlen &lt;key&gt; 获得值的长度 setnx &lt;key&gt; &lt;value&gt; 只有在 key 不存在时设置 key 的值 incr &lt;key&gt; 将 key 中储存的数字值增1。只能对数字值操作，如果为空，新增值为1 decr &lt;key&gt; 将 key 中储存的数字值减1。只能对数字值操作，如果为空，新增值为-1 incrby / decrby &lt;key&gt; &lt;步长&gt; 将 key 中储存的数字值增减。自定义步长 mset &lt;key1&gt; &lt;value1&gt; &lt;key2&gt; &lt;value2&gt; … 同时设置一个或多个 key-value对 mget &lt;key1&gt; &lt;key2&gt; &lt;key3&gt; … 同时获取一个或多个 value msetnx &lt;key1&gt; &lt;value1&gt; &lt;key2&gt; &lt;value2&gt; … 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。 getrange &lt;key&gt; &lt;起始位置&gt; &lt;结束位置&gt; 获得值的范围，类似java中的substring setrange &lt;key&gt; &lt;起始位置&gt; &lt;value&gt; 用 &lt;value&gt;覆写&lt;key&gt;所储存的字符串值，从&lt;起始位置&gt;开始 setex &lt;key&gt; &lt;过期时间&gt; &lt;value&gt; 设置键值的同时，设置过期时间，单位秒 getset &lt;key&gt; &lt;value&gt; 以新换旧，设置了新值同时获得旧值 2、List 单键多值 Redis 列表是简单的字符串列表，按照插入顺序排序。 它的底层实际是个双向链表，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。 List常用指令 lpush/rpush &lt;key&gt; &lt;value1&gt; &lt;value2&gt; … 从左边/右边插入一个或多个值 lpop/rpop &lt;key&gt; 从左边/右边吐出一个值。值在键在，值亡键亡。 rpoplpush &lt;key1&gt; &lt;key2&gt; 从&lt;key1&gt;列表右边吐出一个值，插到&lt;key2&gt;列表左边 lrange &lt;key&gt; &lt;start&gt; &lt;stop&gt; 按照索引下标获得元素(从左到右) lindex &lt;key&gt; &lt;index&gt; 按照索引下标获得元素(从左到右) llen &lt;key&gt; 获得列表长度 linsert &lt;key&gt; before &lt;value&gt; &lt;newvalue&gt; 在&lt;value&gt;的前面插入&lt;newvalue&gt; lrem &lt;key&gt; &lt;n&gt; &lt;value&gt; 从左边删除n个value(从左到右) 3、Set Redis的Set是string类型的无序集合 它底层其实是一个value为null的hash表,所以添加，删除，查找的复杂度都是O(1)。 Set常用指令 sadd &lt;key&gt; &lt;value1&gt; &lt;value2&gt; … 将一个或多个 member 元素加入到集合 key 当中，已经存在于集合的 member 元素将被忽略。 smembers &lt;key&gt; 取出该集合的所有值 sismember &lt;key&gt; &lt;value&gt; 判断集合&lt;key&gt;是否为含有该&lt;value&gt;值，有返回1，没有返回0 scard &lt;key&gt; 返回该集合的元素个数。 srem &lt;key&gt; &lt;value1&gt; &lt;value2&gt; … 删除集合中的某个元素。 spop &lt;key&gt; &lt;n&gt; 随机从该集合中吐出一个或多个值。 srandmember &lt;key&gt; &lt;n&gt; 随机从该集合中取出n个值。不会从集合中删除。 sinter &lt;key1&gt; &lt;key2&gt; 返回两个集合的交集元素。 sunion &lt;key1&gt; &lt;key2&gt; 返回两个集合的并集元素。 sdiff &lt;key1&gt; &lt;key2&gt; 返回两个集合的差集元素。 4、Hash Redis hash 是一个键值对集合。 Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 类似Java里面的Map&lt;String,Object&gt; Hash常用指令 hset &lt;key&gt; &lt;field&gt; &lt;value&gt; 给&lt;key&gt;集合中的&lt;field&gt;键赋值&lt;value&gt; hget &lt;key&gt; &lt;field&gt; 从&lt;key&gt;集合&lt;field&gt;取出 value hmset &lt;key&gt; &lt;field1&gt; &lt;value1&gt; &lt;field2&gt; &lt;value2&gt;… 批量设置hash的值 hexists key &lt;field&gt; 查看哈希表 key 中，给定域 field 是否存在 hkeys &lt;key&gt; 列出该hash集合的所有field hvals &lt;key&gt; 列出该hash集合的所有value hincrby &lt;key&gt; &lt;field&gt; &lt;increment&gt; 为哈希表 key 中的域 field 的值加上增量 increment hsetnx &lt;key&gt; &lt;field&gt; &lt;value&gt; 将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在 5、zset (sorted set) Redis有序集合zset与普通集合set非常相似，是一个没有重复元素的字符串集合。 有序集合的所有成员都关联了一个评分（score） ，这个评分（score）被用来按照从最低分到最高分的方式排序集合中的成员。（集合的成员是唯一的，但是评分可以是重复了的） 因为元素是有序的, 所以你也可以很快的根据评分（score）或者次序（position）来获取一个范围的元素。访问有序集合的中间元素也是非常快的,因此你能够使用有序集合作为一个没有重复成员的智能列表。 zset常用指令 zadd &lt;key&gt; &lt;score1&gt; &lt;value1&gt; &lt;score2&gt; &lt;value2&gt;… 将一个或多个 member 元素及其 score 值加入到有序集 key 当中 zrange &lt;key&gt; &lt;start&gt; &lt;stop&gt; [WITHSCORES] 返回有序集 key 中，下标在&lt;start&gt; &lt;stop&gt;之间的元素。带WITHSCORES，可以让分数一起和值返回到结果集。 zrangebyscore key min max [withscores] [limit offset count] 返回有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。有序集成员按 score 值递增(从小到大)次序排列 zrevrangebyscore key max min [withscores] [limit offset count] 同上，改为从大到小排列 zincrby &lt;key&gt; &lt;increment&gt; &lt;value&gt; 为元素的score加上增量 zrem &lt;key&gt; &lt;value&gt; 删除该集合下，指定值的元素 zcount &lt;key&gt; &lt;min&gt; &lt;max&gt; 统计该集合，分数区间内的元素个数 zrank &lt;key&gt; &lt;value&gt; 返回该值在集合中的排名，从0开始 Redis的Java客户端Jedismaven依赖： xml12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.8.1&lt;/version&gt;&lt;/dependency&gt; 注意事项： 禁用Linux的防火墙： 临时禁用：service iptables stop 关闭开机自启：chkconfig iptables off redis.conf中注释掉bind 127.0.0.1（61行） ,然后 protect-mode（80行）设置为 no。 Jedis测试连通性 java12345678public class Demo01 &#123; public static void main(String[] args) &#123; //连接本地的 Redis 服务 Jedis jedis = new Jedis(\"127.0.0.1\",6379); //查看服务是否运行，打出pong表示OK System.out.println(\"connection is OK==========&gt;: \"+jedis.ping()); &#125;&#125; Jedis-API: Key java12345678//keySet&lt;String&gt; keys = jedis.keys(\"*\");for (Iterator iterator = keys.iterator(); iterator.hasNext();) &#123; String key = (String) iterator.next(); System.out.println(key);&#125;System.out.println(\"jedis.exists====&gt;\"+jedis.exists(\"k2\"));System.out.println(jedis.ttl(\"k1\")); Jedis-API: String java12345System.out.println(jedis.get(\"k1\"));jedis.set(\"k4\",\"k4_Redis\");System.out.println(\"----------------------------------------\");jedis.mset(\"str1\",\"v1\",\"str2\",\"v2\",\"str3\",\"v3\");System.out.println(jedis.mget(\"str1\",\"str2\",\"str3\")); Jedis-API: List java1234List&lt;String&gt; list = jedis.lrange(\"mylist\",0,-1); for (String element : list) &#123; System.out.println(element); &#125; Jedis-API: Set java123456789jedis.sadd(\"orders\",\"jd001\");jedis.sadd(\"orders\",\"jd002\");jedis.sadd(\"orders\",\"jd003\");Set&lt;String&gt; set1 = jedis.smembers(\"orders\");for (Iterator iterator = set1.iterator(); iterator.hasNext();) &#123; String string = (String) iterator.next(); System.out.println(string);&#125;jedis.srem(\"orders\",\"jd002\"); Jedis-API: hash[注意] java1234567891011jedis.hset(\"hash1\",\"userName\",\"lisi\");System.out.println(jedis.hget(\"hash1\",\"userName\"));Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;(); //【注意】map.put(\"telphone\",\"13810169999\");map.put(\"address\",\"atguigu\");map.put(\"email\",\"abc@163.com\");jedis.hmset(\"hash2\",map);List&lt;String&gt; result = jedis.hmget(\"hash2\", \"telphone\",\"email\");for (String element : result) &#123; System.out.println(element);&#125; Jedis-API: zset java123456789jedis.zadd(\"zset01\",60d,\"v1\");jedis.zadd(\"zset01\",70d,\"v2\");jedis.zadd(\"zset01\",80d,\"v3\");jedis.zadd(\"zset01\",90d,\"v4\");Set&lt;String&gt; s1 = jedis.zrange(\"zset01\",0,-1);for (Iterator iterator = s1.iterator(); iterator.hasNext();) &#123; String string = (String) iterator.next(); System.out.println(string);&#125; Redis事务 Redis事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 Redis事务的主要作用就是串联多个命令防止别的命令插队 悲观锁(Pessimistic Lock)，顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 乐观锁(Optimistic Lock)， 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量。Redis就是利用这种check-and-set机制实现事务的。 三特性： 1、单独的隔离操作 事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 2、没有隔离级别的概念 队列中的命令没有提交之前都不会实际的被执行，因为事务提交前任何指令都不会被实际执行，也就不存在“事务内的查询要看到事务里的更新，在事务外查询不能看到”这个让人万分头痛的问题 3、不保证原子性 Redis同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚 Redis持久化1、RDB （Redis DataBase） 在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里。 备份是如何执行的： Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。 关于fork：在Linux程序中，fork()会产生一个和父进程完全相同的子进程，但子进程在此后多会exec系统调用，出于效率考虑，Linux中引入了“写时复制技术”，一般情况父进程和子进程会共用同一段物理内存，只有进程空间的各段的内容要发生变化时，才会将父进程的内容复制一份给子进程。 在redis.conf中配置文件名称，默认为dump.rdb RDB优缺点： 优点 节省磁盘空间 恢复速度快 rdb的缺点 虽然Redis在fork时使用了写时拷贝技术,但是如果数据庞大时还是比较消耗性能。 在备份周期在一定间隔时间做一次备份，所以如果Redis意外down掉的话，就会丢失最后一次快照后的所有修改。 2、AOF （Append Of File） 以日志的形式来记录每个写操作，将Redis执行过的所有写指令记录下来(读操作不记录)，只许追加文件但不可以改写文件，Redis启动之初会读取该文件重新构建数据，换言之，Redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。 AOF默认不开启，需要手动在配置文件中配置 可以在redis.conf中配置文件名称，默认为 appendonly.aof AOF和RDB同时开启，系统默认取AOF的数据 AOF文件故障恢复： AOF文件的保存路径，同RDB的路径一致。 如遇到AOF文件损坏，可通过 redis-check-aof --fix appendonly.aof 进行恢复 Rewrite： AOF采用文件追加方式，文件会越来越大为避免出现此种情况，新增了重写机制,当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集.可以使用命令bgrewriteaof。 AOF优缺点： 优点： 备份机制更稳健，丢失数据概率更低。 可读的日志文本，通过操作AOF稳健，可以处理误操作。 缺点： 比起RDB占用更多的磁盘空间。 恢复备份速度要慢。 每次读写都同步的话，有一定的性能压力。 存在个别Bug，造成恢复不能。 用哪个好呢 官方推荐两个都启用。 如果对数据不敏感，可以选单独用RDB。 不建议单独用 AOF，因为可能会出现Bug。 如果只是做纯内存缓存，可以都不用。 Redis主从复制概念：主从复制，就是主机数据更新后根据配置和策略，自动同步到备机的master/slaver机制，Master以写为主，Slave以读为主。 用处：读写分离，性能扩展。容灾快速回复。 Code1234567891011配从(服务器)不配主(服务器):- 拷贝多个redis.conf文件include- 开启daemonize yes- Pid文件名字pidfile- 指定端口port- Log文件名字- Dump.rdb名字dbfilename- Appendonly 关掉或者换名字info replication:打印主从复制的相关信息slaveof &lt;ip&gt; &lt;port&gt; :成为某个实例的从服务器 一主二仆模式： 复制原理： 每次从机联通后，都会给主机发送sync指令 主机立刻进行存盘操作，发送RDB文件，给从机 从机收到RDB文件后，进行全盘加载 之后每次主机的写操作，都会立刻发送给从机，从机执行相同的命令 薪火相传： 上一个slave可以是下一个slave的Master，slave同样可以接收其他slaves的连接和同步请求，那么该slave作为了链条中下一个的master, 可以有效减轻master的写压力,去中心化降低风险。 用 slaveof &lt;ip&gt; &lt;port&gt; 中途变更转向:会清除之前的数据，重新建立拷贝最新的 风险是一旦某个slave宕机，后面的slave都没法备份 反客为主： 当一个master宕机后，后面的slave可以立刻升为master，其后面的slave不用做任何修改。。 用 slaveof no one 将从机变为主机。 哨兵模式(sentinel)反客为主的自动版，能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库。 Code1234567891、配置哨兵：调整为一主二仆模式自定义的&#x2F;myredis目录下新建sentinel.conf文件在配置文件中填写内容： sentinel monitor mymaster 127.0.0.1 6379 1其中mymaster为监控对象起的服务器名称， 1 为 至少有多少个哨兵同意迁移的数量。 2、启动哨兵执行redis-sentinel &#x2F;myredis&#x2F;sentinel.conf 故障恢复： 1、新主登基 从下线的主服务的所有从服务里面挑选一个从服务，将其转成主服务选择条件依次为：（1）选择优先级靠前的（2）选择偏移量最大的（3）选择runid最小的从服务 2、群仆俯首 挑选出新的主服务之后，sentinel 向原主服务的从服务发送 slaveof 新主服务 的命令，复制新master 3、旧主俯首 当已下线的服务重新上线时，sentinel会向其发送slaveof命令，让其成为新主的从 优先级在redis.conf中slave-priority 100偏移量是指获得原主数据最多的每个redis实例启动后都会随机生成一个40位的runid","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Redis","slug":"大数据/Redis","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Redis/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据库","slug":"数据库","permalink":"https://masteryang4.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://masteryang4.github.io/tags/Redis/"},{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://masteryang4.github.io/tags/JavaWeb/"}]},{"title":"JUnit常用注解","slug":"JUnit常用注解","date":"2020-05-12T11:49:00.000Z","updated":"2020-05-12T12:14:44.974Z","comments":true,"path":"2020/05/12/JUnit常用注解/","link":"","permalink":"https://masteryang4.github.io/2020/05/12/JUnit%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3/","excerpt":"","text":"JUnit常用注解JUnit是 Java平台最常用的测试框架 。 本文重点阐述JUnit4版本的@Before、@After、@BeforeClass、@AfterClass四个注解。 JUnit4，JUnit5注解对比 JUnit4 JUnit5 功能 @BeforeClass @BeforeAll 在当前类的所有测试方法之前执行。注解在【静态方法】上。 @AfterClass @AfterAll 在当前类中的所有测试方法之后执行。注解在【静态方法】上。 @Before @BeforeEach 在每个测试方法之前执行。注解在【非静态方法】上。 @After @AfterEach 在每个测试方法之后执行。注解在【非静态方法】上。 为什么 JUnit中@BeforeClass和@AfterClass标注的方法必须是static的 ？ 其实和JUnit的运行机制有关： 在JUnit中：每运行一个@Test方法，就会为该测试类新建一个新的实例。所以@BeforeClass和@AfterClass必须是static的，因为运行他们的时候，测试类还没有实例化。 这种设计有助于提高测试方法之间的独立性，因为每个@Test执行的时候，都新建了一个实例，这样的话，可以避免测试方法之间重用各个@Test方法里面的变量值。 示例： java12345678910111213141516import org.junit.Test;public class JUintDemo &#123; int i = 2; @Test public void test1() &#123; int i = 1; System.out.println(\"test1 i=\" + i); //test1 i=1 &#125; @Test public void test2() &#123; System.out.println(\"test2 i=\" + i); //test2 i=2 &#125;&#125; 代码示例java1234567891011121314151617181920212223242526272829303132333435import org.junit.*;public class JunitTest &#123; @BeforeClass //【静态方法】 public static void beforeClass() &#123; System.out.println(\"before class:begin this class================\"); &#125; @AfterClass //【静态方法】 public static void afterClass() &#123; System.out.println(\"after class:end this class=================\"); &#125; @Before public void before() &#123; System.out.println(\"before:begin test\"); &#125; @After public void after() &#123; System.out.println(\"after:end test\"); &#125; @Test public void Test() &#123; System.out.println(\"[this is a test!]\"); &#125; @Test public void Test2() &#123; System.out.println(\"[this is another test!!!!!]\"); &#125;&#125; 执行整个JunitTest文件，输出结果： Code12345678before class:begin this class&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;before:begin test[this is a test!]after:end testbefore:begin test[this is another test!!!!!]after:end testafter class:end this class&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 小结一整个JUnit4的单元测试用例执行顺序为： ​ @BeforeClass -&gt; @Before -&gt; @Test -&gt; @After -&gt; @AfterClass; 每一个单独的测试方法的调用顺序为： ​ @Before -&gt; @Test -&gt; @After;","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"JUnit","slug":"JUnit","permalink":"https://masteryang4.github.io/tags/JUnit/"},{"name":"单元测试","slug":"单元测试","permalink":"https://masteryang4.github.io/tags/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/"}]},{"title":"[SparkSQL]UDAF自定义聚合函数","slug":"SparkSQL-UDAF自定义聚合函数","date":"2020-05-05T13:30:07.000Z","updated":"2020-05-05T13:45:04.384Z","comments":true,"path":"2020/05/05/SparkSQL-UDAF自定义聚合函数/","link":"","permalink":"https://masteryang4.github.io/2020/05/05/SparkSQL-UDAF%E8%87%AA%E5%AE%9A%E4%B9%89%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0/","excerpt":"","text":"[SparkSQL]UDAF自定义聚合函数SparkSql中，用户可以设定自己的自定义聚合函数（UserDefinedAggregateFunction）。 需求：实现平均年龄 user.json 文件： json123&#123;\"username\": \"lisi\",\"userage\": 40&#125;&#123;\"username\": \"zhangsan\",\"userage\": 30&#125;&#123;\"username\": \"wangwu\",\"userage\":20&#125; UDAF - 弱类型scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import org.apache.spark.SparkConfimport org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types.&#123;DataType, DoubleType, LongType, StructField, StructType&#125;import org.apache.spark.sql.&#123;DataFrame, Row, SparkSession&#125;import org.apache.spark.util.AccumulatorV2object SparkSQL_UDAF01 &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"sparksql\") val spark = SparkSession.builder().config(sparkConf).getOrCreate() // TODO 读取JSON数据 val df: DataFrame = spark.read.json(\"input/user.json\") // TODO 使用自定义聚合函数实现年龄的平均值计算 // buffer // select avg(age) from user // 创建自定义函数 val udaf = new MyAvgAgeUDAF // 注册UDAF函数 spark.udf.register(\"avgAge\", udaf) df.createTempView(\"user\") spark.sql(\"select avgAge(userage) from user\").show spark.close &#125; /* * TODO 自定义聚合函数（UDAF） * 1. 继承UserDefinedAggregateFunction * 2. 重写方法 */ class MyAvgAgeUDAF extends UserDefinedAggregateFunction &#123; // TODO 传入聚合函数的数据结构 // 1 =&gt; age =&gt; Long override def inputSchema: StructType = &#123; StructType(Array( StructField(\"age\", LongType) )) &#125; // TODO 用于计算的缓冲区的数据结构 override def bufferSchema: StructType = &#123; StructType(Array( StructField(\"totalage\", LongType), StructField(\"totalcnt\", LongType) )) &#125; // TODO 输出结果的类型 override def dataType: DataType = DoubleType // TODO 函数稳定性（幂等性） // 给函数相同的输入值，计算结果也相同 override def deterministic: Boolean = true // TODO 用于计算的缓冲区初始化 override def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = 0L buffer(1) = 0L &#125; // TODO 将输入的值更新到缓冲区中 override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; buffer(0) = buffer.getLong(0) + input.getLong(0) buffer(1) = buffer.getLong(1) + 1L &#125; // TODO 合并缓冲区 // MutableAggregationBuffer 继承了Row override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0) buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1) &#125; // TODO 计算结果 override def evaluate(buffer: Row): Any = &#123; buffer.getLong(0).toDouble / buffer.getLong(1) &#125; &#125;&#125; 输出： sql12345+---------------------+|myavgageudaf(userage)|+---------------------+| 30.0|+---------------------+ UDAF - 强类型scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;DataFrame, Dataset, Encoder, Encoders, SparkSession, TypedColumn&#125;import org.apache.spark.sql.expressions.Aggregatorobject UDAF02 &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"myudaf\") val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate() import spark.implicits._ val df: DataFrame = spark.read.json(\"input/user.json\") //封装为DataSet val ds: Dataset[User01] = df.as[User01] //创建聚合函数 var myAgeUdtf1 = new MyAveragUDAF1 //将聚合函数转换为查询的列 val col: TypedColumn[User01, Double] = myAgeUdtf1.toColumn //查询 ds.select(col).show() &#125; //输入数据类型 case class User01(username: String, userage: Long) //缓存类型 case class AgeBuffer(var sum: Long, var count: Long) /** * 定义类继承org.apache.spark.sql.expressions.Aggregator * 重写类中的方法 */ class MyAveragUDAF1 extends Aggregator[User01, AgeBuffer, Double] &#123; override def zero: AgeBuffer = &#123; AgeBuffer(0L, 0L) &#125; override def reduce(b: AgeBuffer, a: User01): AgeBuffer = &#123; b.sum = b.sum + a.userage b.count = b.count + 1 b &#125; override def merge(b1: AgeBuffer, b2: AgeBuffer): AgeBuffer = &#123; b1.sum = b1.sum + b2.sum b1.count = b1.count + b2.count b1 &#125; override def finish(buff: AgeBuffer): Double = &#123; buff.sum.toDouble / buff.count &#125; //DataSet默认额编解码器，用于序列化，固定写法 //自定义类型就是produce 自带类型根据类型选择 override def bufferEncoder: Encoder[AgeBuffer] = &#123; Encoders.product &#125; override def outputEncoder: Encoder[Double] = &#123; Encoders.scalaDouble &#125; &#125;&#125; 输出： sql12345+-----------------------------------------------------+|MyAveragUDAF1(com.atguigu.sparksql.UDAF_qiang$User01)|+-----------------------------------------------------+| 30.0|+-----------------------------------------------------+","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"},{"name":"sparksql","slug":"sparksql","permalink":"https://masteryang4.github.io/tags/sparksql/"}]},{"title":"HashMap文章推荐","slug":"HashMap文章推荐","date":"2020-04-30T15:42:39.000Z","updated":"2020-05-09T13:27:03.153Z","comments":true,"path":"2020/04/30/HashMap文章推荐/","link":"","permalink":"https://masteryang4.github.io/2020/04/30/HashMap%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90/","excerpt":"","text":"HashMap文章推荐Java 8系列之重新认识HashMap 【强烈推荐】来自美团技术团队，里面的参考文章也非常好 《吊打面试官》系列-HashMap 《吊打面试官》系列-ConcurrentHashMap &amp; HashTable 来自敖丙（蘑菇街大佬），从面试官角度阐述关键技术点，十分硬核，全是干货。 一个HashMap跟面试官扯了半个小时 面试者角度阐述HashMap。 有空闲时间的话，我自己也会出一篇，甚至是一系列的HashMap文章， 比如 源码分析， 知识点总结， 常考面试题归档 等等","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"集合","slug":"Java/集合","permalink":"https://masteryang4.github.io/categories/Java/%E9%9B%86%E5%90%88/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"hashmap","slug":"hashmap","permalink":"https://masteryang4.github.io/tags/hashmap/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"Java IO相关总结归纳","slug":"Java-IO相关总结归纳","date":"2020-04-29T14:12:12.000Z","updated":"2020-04-29T14:14:13.243Z","comments":true,"path":"2020/04/29/Java-IO相关总结归纳/","link":"","permalink":"https://masteryang4.github.io/2020/04/29/Java-IO%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93%E5%BD%92%E7%BA%B3/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"[spark]十一种方式实现WordCount","slug":"spark-十一种方式实现WordCount","date":"2020-04-27T14:23:10.000Z","updated":"2020-04-27T15:12:53.079Z","comments":true,"path":"2020/04/27/spark-十一种方式实现WordCount/","link":"","permalink":"https://masteryang4.github.io/2020/04/27/spark-%E5%8D%81%E4%B8%80%E7%A7%8D%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0WordCount/","excerpt":"","text":"[Spark]十一种方式实现WordCount使用Spark中的11种方法实现经典的WordCount算法。 其中，10种SparkRDD（算子）+ 1种自定义累加器实现。 特朗普：没人比我更懂WordCount！（滑稽） Why WordCount？ 大数据中最经典的算法，相当于编程语言中的“Hello World”。 在大数据处理中，大多数复杂的问题通常被拆分成一个个小问题，这些小问题一般都是基于WordCount算法。所以，WordCount是重中之重，是大数据处理算法的基石。 10种Spark算子实现scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable/** * spark-使用十种[算子]实现wordcount */object RDDWordcount &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark\") val sc = new SparkContext(sparkConf) // val rdd = sc.textFile(\"input/wc.txt\").flatMap(datas =&gt; &#123; // datas.split(\" \") // &#125;) val rdd = sc.makeRDD(List(\"hadoop\", \"hello\", \"spark\", \"hello\", \"scala\", \"hello\", \"scala\", \"spark\")) println(\"=================1====================\") rdd.countByValue().foreach(println) println(\"=================2====================\") rdd.map((_, 1)).countByKey().foreach(println) println(\"=================3====================\") rdd.map((_, 1)).reduceByKey(_ + _).collect().foreach(println) println(\"=================4====================\") rdd.map((_, 1)).groupByKey().mapValues(_.size).collect().foreach(println) println(\"=================5====================\") rdd.map((_, 1)).aggregateByKey(0)(_ + _, _ + _).collect().foreach(println) println(\"=================6====================\") rdd.map((_, 1)).foldByKey(0)(_ + _).collect().foreach(println) println(\"=================7====================\") rdd.map((_, 1)).combineByKey( (num: Int) =&gt; num, (x: Int, y: Int) =&gt; &#123; x + y &#125;, (x: Int, y: Int) =&gt; &#123; x + y &#125; ).collect().foreach(println) println(\"=================8====================\") rdd.map((_, 1)).groupBy(_._1).map(kv =&gt; &#123; (kv._1, kv._2.size) &#125;).collect().foreach(println) println(\"=================9====================\") rdd.aggregate(mutable.Map[String, Int]())( (map, word) =&gt; &#123; map(word) = map.getOrElse(word, 0) + 1 map &#125;, (map1, map2) =&gt; &#123; map1.foldLeft(map2)( (finalMap, kv) =&gt; &#123; finalMap(kv._1) = finalMap.getOrElse(kv._1, 0) + kv._2 finalMap &#125; ) &#125; ).foreach(println) println(\"=================10====================\") rdd.map(s =&gt; mutable.Map(s -&gt; 1)).fold(mutable.Map[String, Int]())( (map1, map2) =&gt; &#123; map1.foldLeft(map2)( (finalMap, kv) =&gt; &#123; finalMap(kv._1) = finalMap.getOrElse(kv._1, 0) + kv._2 finalMap &#125; ) &#125; ).foreach(println) sc.stop() &#125;&#125; 输出结果： scala1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950=================1====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================2====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================3====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================4====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================5====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================6====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================7====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================8====================(hello,3)(spark,2)(hadoop,1)(scala,2)=================9====================(hadoop,1)(spark,2)(scala,2)(hello,3)=================10====================(hadoop,1)(spark,2)(scala,2)(hello,3) 自定义累加器实现scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import org.apache.spark.rdd.RDDimport org.apache.spark.util.AccumulatorV2import org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutableobject MyAccTest &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setAppName(\"acc\").setMaster(\"local[*]\") val sc: SparkContext = new SparkContext(conf) // TODO Spark - 自定义累加器 - wordcount // 累加器可以不使用shuffle就完成数据的聚合功能 val rdd: RDD[String] = sc.makeRDD(List(\"hadoop spark\", \"hello\", \"spark\", \"hello\", \"scala\", \"hello\", \"scala\", \"spark\")) // TODO 1. 创建累加器 val acc = new WordCountAccumulator // TODO 2. 向Spark注册累加器 sc.register(acc, \"wordcount\") // TODO 3. 使用累加器 rdd.foreach( words =&gt; &#123; val ws = words.split(\" \") ws.foreach( word =&gt; &#123; acc.add(word) &#125; ) &#125; ) println(acc.value) //Map(hadoop -&gt; 1, spark -&gt; 3, scala -&gt; 2, hello -&gt; 3) sc.stop() &#125; // 自定义累加器 Map&#123;(Word - Count), (Word - Count)&#125; // 1, 继承AccumulatorV2, 定义泛型 // IN : 向累加器传递的值的类型 , Out : 累加器的返回结果类型 // 2. 重写方法 class WordCountAccumulator extends AccumulatorV2[String, mutable.Map[String, Int]] &#123; var innerMap = mutable.Map[String, Int]() // TODO 累加器是否初始化 // Z override def isZero: Boolean = innerMap.isEmpty // TODO 复制累加器 override def copy(): AccumulatorV2[String, mutable.Map[String, Int]] = &#123; new WordCountAccumulator &#125; // TODO 重置累加器 override def reset(): Unit = &#123; innerMap.clear() &#125; // TODO 累加数据 override def add(word: String): Unit = &#123; val cnt = innerMap.getOrElse(word, 0) innerMap.update(word, cnt + 1) &#125; // TODO 合并累加器 override def merge(other: AccumulatorV2[String, mutable.Map[String, Int]]): Unit = &#123; // 两个Map的合并 var map1 = this.innerMap var map2 = other.value innerMap = map1.foldLeft(map2)( (map, kv) =&gt; &#123; val k = kv._1 val v = kv._2 map(k) = map.getOrElse(k, 0) + v map &#125; ) &#125; // TODO 获取累加器的值，就是累加器的返回结果 override def value: mutable.Map[String, Int] = innerMap &#125;&#125; 输出结果： scala1Map(spark -&gt; 3, hadoop -&gt; 1, scala -&gt; 2, hello -&gt; 3)","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"大数据/spark","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"spark","slug":"spark","permalink":"https://masteryang4.github.io/tags/spark/"},{"name":"scala","slug":"scala","permalink":"https://masteryang4.github.io/tags/scala/"},{"name":"wordcount","slug":"wordcount","permalink":"https://masteryang4.github.io/tags/wordcount/"}]},{"title":"kafka高效读写数据","slug":"kafka高效读写数据","date":"2020-04-27T12:28:01.000Z","updated":"2020-05-09T13:26:09.502Z","comments":true,"path":"2020/04/27/kafka高效读写数据/","link":"","permalink":"https://masteryang4.github.io/2020/04/27/kafka%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE/","excerpt":"","text":"kafka高效读写数据一、分布式集群Kafka本身是分布式集群；同时采用分区技术，并发度高。 zookeeper在kafka中的作用：kafka集群中有一个broker会被选举成controller，负责管理集群broker的上下线，所有的topic分区副本分配和leader选举等工作。controller的管理工作都依赖于zk。 二、顺序写磁盘Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。 三、零复制技术kafka零复制技术示意图： java复制技术示意图： （仅仅复制文件，没有对于文件的应用，效率很低。 文件要经过操作系统层（OS层）Buffer缓存传给java应用层输入流，输入流再将数据写到输出流，输出流将数据写到OS层缓存，缓存在将数据写到新的文件。。。） 因为java复制技术在拷贝文件时效率较低，所以对上图做出优化，如下图所示： （应用层通知操作系统层：仅仅是复制文件，所以操作系统层就不会将数据传给应用层，直接在操作系统层复制文件即可。）","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"kafka","slug":"大数据/kafka","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"kafka","slug":"kafka","permalink":"https://masteryang4.github.io/tags/kafka/"}]},{"title":"flume总结与思考","slug":"精-flume总结与思考","date":"2020-04-24T15:50:53.000Z","updated":"2020-05-14T08:34:21.325Z","comments":true,"path":"2020/04/24/精-flume总结与思考/","link":"","permalink":"https://masteryang4.github.io/2020/04/24/%E7%B2%BE-flume%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"flume","slug":"大数据/flume","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flume/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"教程","slug":"教程","permalink":"https://masteryang4.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"flume","slug":"flume","permalink":"https://masteryang4.github.io/tags/flume/"}]},{"title":"kafka分区分配策略","slug":"kafka分区分配策略","date":"2020-04-23T09:09:56.000Z","updated":"2020-05-09T13:25:52.931Z","comments":true,"path":"2020/04/23/kafka分区分配策略/","link":"","permalink":"https://masteryang4.github.io/2020/04/23/kafka%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/","excerpt":"","text":"kafka分区分配策略kafka系列总结之：kafka分区分配策略[转载&amp;归纳] kafka是由Apache软件基金会开发的一个开源流处理平台，由Scala和Java编写。 Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据 kafka官网： kafka.apache.org kafka分区分配策略文章索引 1、 Kafka分区分配策略（1）——RangeAssignor 2、 Kafka分区分配策略（2）——RoundRobinAssignor和StickyAssignor 3、 Kafka分区分配策略（3）——自定义分区分配策略 4、 Kafka分区分配策略（4）——分配的实施 [注]作者为 《深入理解Kafka:核心设计与实践原理》 的作者：朱忠华老师 作者更多kafka技术文章： https://blog.csdn.net/u013256816/category_6500871.html 作者个人博客： http://honeypps.com/ 作者CSDN博客： https://blog.csdn.net/u013256816","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"kafka","slug":"大数据/kafka","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"kafka","slug":"kafka","permalink":"https://masteryang4.github.io/tags/kafka/"}]},{"title":"scala中的flatMap和foldLeft函数","slug":"scala中的flatMap和foldLeft函数","date":"2020-04-21T14:34:32.000Z","updated":"2020-04-27T15:10:43.047Z","comments":true,"path":"2020/04/21/scala中的flatMap和foldLeft函数/","link":"","permalink":"https://masteryang4.github.io/2020/04/21/scala%E4%B8%AD%E7%9A%84flatMap%E5%92%8CfoldLeft%E5%87%BD%E6%95%B0/","excerpt":"","text":"scala中的flatMap和foldLeft函数scala由于其函数式编程的特性，在大数据的处理中被广泛使用。 此文针对scala集合中两个常用的，却不太好理解的函数进行示例讲解。 flatMapscala中最重要的函数之一，映射扁平化 把握以下三点即可： 1、flatMap = map + flatten 2、什么类型调用的flatMap方法，则返回的也是什么类型 3、先对集合中的每个元素进行map， ​ 再对map后的每个元素（map后的每个元素必须还是集合）中的每个元素进行flatten [注] 进行map的对象可以是只含一层的集合，但进行flatten操作的对象必需是至少含两层的集合 map和flatten示例： scala123456789101112131415object Test0001 &#123; def main(args: Array[String]): Unit = &#123; val list = List(1,2,3,4) // 集合映射 println(\"map =&gt; \" + list.map(x=&gt;&#123;x*2&#125;)) //map =&gt; List(2, 4, 6, 8) println(\"map =&gt; \" + list.map(x=&gt;x*2)) //map =&gt; List(2, 4, 6, 8) println(\"map =&gt; \" + list.map(_*2)) //map =&gt; List(2, 4, 6, 8) // 集合扁平化 val list1 = List( List(1,2), List(3,4) ) println(\"flatten =&gt;\" + list1.flatten) //flatten =&gt;List(1, 2, 3, 4) &#125;&#125; flatMap示例一： scala123val words = Set(\"scala\", \"spark\", \"hadoop\")val result = words.flatMap(x =&gt; x.toUpperCase)println(result) //Set(A, L, P, C, H, K, R, O, D, S) flatMap示例二： scala123456val tuples: List[(String, Int)] = List((\"Hello Scala\", 4), (\"Hello Spark\", 2))val strings: List[String] = tuples.map(t=&gt;&#123;(t._1+\" \")*t._2&#125;)//List(Hello Scala Hello Scala Hello Scala Hello Scala , Hello Spark Hello Spark )val flatMapList: List[String] = strings.flatMap(t=&gt;&#123;t.split(\" \")&#125;)//List(Hello, Scala, Hello, Scala, Hello, Scala, Hello, Scala, Hello, Spark, Hello, Spark) flatMap示例三： scala1234567val linesList = List((\"Hello Scala\", 4), (\"Hello Spark\", 2))val flatMapList: List[(String, Int)] = linesList.flatMap(t =&gt; &#123; val line: String = t._1 val words = line.split(\" \") words.map(w =&gt; (w, t._2))&#125;)println(flatMapList) //List((Hello,4), (Scala,4), (Hello,2), (Spark,2)) 根据上述三个原则即可算出函数结果。 foldLeft集合折叠函数，fold、foldRight底层都是基于foldLeft函数。 所以本文用到的函数可以不用严格区分，主要阐述其原理。 scala1def fold[A1 &gt;: A](z: A1)(op: (A1, A1) =&gt; A1): A1 = foldLeft(z)(op) 就是将集合的数据和集合之外的数据进行聚合操作。 fold方法有函数柯里化，有2个参数列表 第一个参数列表：集合之外的数据 第二个参数列表：表示计算规则 fold示例一： scala12345val list = List(1, 2, 3, 4)// 集合折叠println(\"fold =&gt; \" + list.fold(0)(_+_)) //10// 集合折叠(左)println(\"foldLeft =&gt; \" + list.foldLeft(0)(_+_)) //10 fold示例二： scala123456789101112131415161718192021object Scala21_Collection_Method4 &#123; def main(args: Array[String]): Unit = &#123; // 将两个Map集合进行合并(merge)处理 val map1 = mutable.Map(\"a\" -&gt; 1, \"b\" -&gt; 2, \"c\" -&gt; 3) val map2 = mutable.Map(\"a\" -&gt; 4, \"d\" -&gt; 5, \"c\" -&gt; 6) // Map( \"a\"-&gt;5, \"b\"-&gt;2, \"c\"-&gt;9 ,\"d\"-&gt;5) val map3 = map2.foldLeft(map1)( (map, kv) =&gt; &#123; val k = kv._1 val v = kv._2 //map.update(k, map.getOrElse(k, 0) + v) map(k) = map.getOrElse(k, 0) + v map &#125; ) println(map3) //Map(b -&gt; 2, d -&gt; 5, a -&gt; 5, c -&gt; 9) println(map1) //Map(b -&gt; 2, d -&gt; 5, a -&gt; 5, c -&gt; 9) println(map2) //Map(d -&gt; 5, a -&gt; 4, c -&gt; 6) &#125;&#125; 原理示意图如下： 总结： 其实，在foldleft函数中，第二个参数规定的就是， foldleft第一个参数和foldleft调用者的第一个元素的运算规则 可以用如下公式理解： a. foldLeft( b )( (b,a的第一个元素)=&gt;{} ) （对应上面示意图：红色块为b，蓝色块为a） 只不过在此公式中b和a的第一个元素都是动态变化的： ​ b一直在迭代，a会继续往后顺序取后面的值。 ​ 其实函数最终返回值就是b的值（上面的例子map1和map3相等也能证明这一点，本质就是map1把值赋给了map3），且a不发生改变。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"scala","slug":"大数据/scala","permalink":"https://masteryang4.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/scala/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"scala","slug":"scala","permalink":"https://masteryang4.github.io/tags/scala/"}]},{"title":"Java空指针问题的本质","slug":"Java空指针问题的本质","date":"2020-04-18T15:58:28.000Z","updated":"2020-04-18T16:00:16.725Z","comments":true,"path":"2020/04/18/Java空指针问题的本质/","link":"","permalink":"https://masteryang4.github.io/2020/04/18/Java%E7%A9%BA%E6%8C%87%E9%92%88%E9%97%AE%E9%A2%98%E7%9A%84%E6%9C%AC%E8%B4%A8/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"面试题：sleep和wait的区别","slug":"sleep和wait的区别小结","date":"2020-04-14T08:01:30.000Z","updated":"2020-04-16T15:49:47.457Z","comments":true,"path":"2020/04/14/sleep和wait的区别小结/","link":"","permalink":"https://masteryang4.github.io/2020/04/14/sleep%E5%92%8Cwait%E7%9A%84%E5%8C%BA%E5%88%AB%E5%B0%8F%E7%BB%93/","excerpt":"","text":"Java中sleep和wait方法的区别 sleep和wait都能使线程处于阻塞状态，但二者有着本质区别。 代码示例java123456789101112131415161718public class test_thread &#123; public static void main(String[] args) throws Exception &#123; Thread t1 = new Thread(); Thread t2 = new Thread(); //【本质区别】静态方法和成员方法 //【静态方法】，绑定的是类。休眠的线程不是t1，是当前运行的main线程 //和对象都没有关系，所以不存在什么对象锁 t1.sleep(1000); Thread.sleep(1000); //【成员方法】，等待的线程就是t2 //有同步/synchronized关键字才能拿到对象锁。 t2.wait(); t2.wait(1000);//wait也可以加等待时间 //【扩展】scala中的伴生对象就是对静态语法的模拟 &#125;&#125; 总结 【核心】静态方法、成员方法 sleep是Thread类的静态方法。sleep的作用是让线程休眠道制定的时间，在时间到达时恢复，也就是说sleep将在接到时间到达事件事恢复线程执行。 wait是Object的方法，也就是说可以对任意一个对象调用wait方法，调用wait方法将会属将调用者的线程挂起，直到其他线程调用同一个对象的notify方法才会重新激活调用者。 sleep方法没有释放锁（lock），而wait方法释放了锁，使得其他线程可以使用同步控制块或者方法。 【使用范围】 wait，notify和notifyAll只能在同步控制方法或者同步控制块里面使用， 而sleep可以在任何地方使用","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"多线程","slug":"Java/多线程","permalink":"https://masteryang4.github.io/categories/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"Git使用小结","slug":"Git使用小结","date":"2020-04-12T16:25:50.000Z","updated":"2020-06-07T16:02:10.436Z","comments":true,"path":"2020/04/13/Git使用小结/","link":"","permalink":"https://masteryang4.github.io/2020/04/13/Git%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93/","excerpt":"","text":"Git使用小结小结Git常用指令，以及如何将本地代码同步/更新到Github的常用指令 一、初始配置git安装完成后，需要设置一下，在命令行输入 Code12$ git config --global user.name &quot;Your Name&quot;$ git config --global user.email &quot;email@example.com&quot; –global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。 二、常用指令进入到自己的项目文件下右键选择Git Bash Here打开git客户端 初始化项目： Code1git init 将文件添加到本地仓库： Code1git add 将文件提交到仓库 Code1git commit -m &quot;注释内容&quot; 关联远程项目（你的远程仓库地址） Code1git remote add origin https:&#x2F;&#x2F;github.com&#x2F;xxxx&#x2F;xxx.git 本地推送到远程（ 第一次推送master分支的所有内容） 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 Code1git push -u origin master 查看Git状态 Code1git status 三、更新文件到GithubCode1234git addgit commit -m &quot;注释内容&quot;git pull origin master #从远程抓取分支，使用git pull，如果有冲突，要先处理冲突git push origin master 查看远程库信息： Code1git remote -v 会显示可以抓取和推送的origin的地址。如果没有推送权限，就看不到push的地址。","categories":[{"name":"Git&Github","slug":"Git-Github","permalink":"https://masteryang4.github.io/categories/Git-Github/"}],"tags":[{"name":"Git&Github","slug":"Git-Github","permalink":"https://masteryang4.github.io/tags/Git-Github/"}]},{"title":"关于i=i++的分析与思考","slug":"关于i-i-的分析与思考","date":"2020-04-10T16:00:30.000Z","updated":"2020-04-18T15:22:45.874Z","comments":true,"path":"2020/04/11/关于i-i-的分析与思考/","link":"","permalink":"https://masteryang4.github.io/2020/04/11/%E5%85%B3%E4%BA%8Ei-i-%E7%9A%84%E5%88%86%E6%9E%90%E4%B8%8E%E6%80%9D%E8%80%83/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"关于HashMap的两道小面试题","slug":"关于HashMap的两道小面试题","date":"2020-04-10T16:00:16.000Z","updated":"2020-04-18T15:23:25.435Z","comments":true,"path":"2020/04/11/关于HashMap的两道小面试题/","link":"","permalink":"https://masteryang4.github.io/2020/04/11/%E5%85%B3%E4%BA%8EHashMap%E7%9A%84%E4%B8%A4%E9%81%93%E5%B0%8F%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"集合","slug":"Java/集合","permalink":"https://masteryang4.github.io/categories/Java/%E9%9B%86%E5%90%88/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"String_StringBuffer_StringBuilder分析总结","slug":"String_StringBuffer_StringBuilder分析总结","date":"2020-04-10T16:00:03.000Z","updated":"2020-04-24T15:48:19.991Z","comments":true,"path":"2020/04/11/String_StringBuffer_StringBuilder分析总结/","link":"","permalink":"https://masteryang4.github.io/2020/04/11/String_StringBuffer_StringBuilder%E5%88%86%E6%9E%90%E6%80%BB%E7%BB%93/","excerpt":"","text":"String_StringBuffer_StringBuilder分析总结本文对Java语言中的String，StringBuffer，StringBuilder类进行分析对比， 并String类型进行简单原理分析。 String，StringBuffer，StringBuilder的区别1、可变与不可变 String类中使用字符数组保存字符串，如下就是，因为有“final”修饰符，所以可以知道string对象是不可变的。 java1private final char value[]; StringBuilder与StringBuffer都继承自AbstractStringBuilder类，在AbstractStringBuilder中也是使用字符数组保存字符串，如下就是，可知这两种对象都是可变的。 java1char[] value; 2、是否多线程安全 String中的对象是不可变的，也就可以理解为常量，显然线程安全。 AbstractStringBuilder是StringBuilder与StringBuffer的公共父类，定义了一些字符串的基本操作，如expandCapacity、append、insert、indexOf等公共方法。 StringBuilder并没有对方法进行加同步锁，所以是非线程安全的。 ​ StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。看如下源码： java12345678public synchronized StringBuffer reverse() &#123; super.reverse(); return this;&#125;public int indexOf(String str) &#123; return indexOf(str, 0); //存在 public synchronized int indexOf(String str, int fromIndex) 方法&#125; 3、StringBuilder与StringBuffer共同点 StringBuilder与StringBuffer有公共父类AbstractStringBuilder(抽象类)。 抽象类与接口的其中一个区别是：抽象类中可以定义一些子类的公共方法，子类只需要增加新的功能，不需要重复写已经存在的方法；而接口中只是对方法的申明和常量的定义。 StringBuilder、StringBuffer的方法都会调用AbstractStringBuilder中的公共方法，如super.append(…)。只是StringBuffer会在方法上加synchronized关键字，进行同步。 如果程序不是多线程的，那么使用StringBuilder效率高于StringBuffer。 String相关String类部分源码： java123456789101112131415161718192021222324252627282930313233343536public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; //... public String() &#123; this.value = \"\".value; &#125; public String(String original) &#123; this.value = original.value; this.hash = original.hash; &#125; public String(char value[]) &#123; this.value = Arrays.copyOf(value, value.length); &#125; //... public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h; &#125; 说明： private final char value[];说明String不可变 其实不可变指的是其字符串内容不可变，字符串对象的地址其实是可以改变的，示例如下： java1234String a = \"ABCabc\";System.out.println(\"a = \" + a); //a = ABCabca = a.replace('A', 'a');System.out.println(\"a = \" + a); //a = aBCabc ​ 这个例子的本质是，字符串对象a指向了一个新的字符串数组。 如果真的要去修改String内容的话，其实也是可以的，使用反射机制就可以实现，示例如下： java123456789101112131415161718192021public static void testReflection() throws Exception &#123; //创建字符串\"Hello World\"， 并赋给引用s String s = \"Hello World\"; System.out.println(\"s = \" + s); //Hello World //获取String类中的value字段 Field valueFieldOfString = String.class.getDeclaredField(\"value\"); //改变value属性的访问权限 valueFieldOfString.setAccessible(true); //获取s对象上的value属性的值 char[] value = (char[]) valueFieldOfString.get(s); //改变value所引用的数组中的第5个字符 value[5] = '_'; System.out.println(\"s = \" + s); //Hello_World&#125; 参考博客： https://www.cnblogs.com/leskang/p/6110631.html https://www.cnblogs.com/xudong-bupt/p/3961159.html","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"Java中final关键字小结","slug":"Java中final关键字小结","date":"2020-04-10T15:59:41.000Z","updated":"2020-04-16T15:44:18.891Z","comments":true,"path":"2020/04/10/Java中final关键字小结/","link":"","permalink":"https://masteryang4.github.io/2020/04/10/Java%E4%B8%ADfinal%E5%85%B3%E9%94%AE%E5%AD%97%E5%B0%8F%E7%BB%93/","excerpt":"","text":"Java中final关键字小结一、final、finally、finalize的区别final： 修饰符（关键字）有三种用法：修饰类、变量和方法。 修饰类时，意味着它不能再派生出新的子类，即不能被继承，因此它和 abstract 是反义词。 修饰变量时，该变量使用中不被改变，必须在声明时给定初值，在引用中只能读取不可修改，即为常量。（下一节代码示例） 修饰方法时，也同样只能使用，不能在子类中被重写。 finally: 通常放在 try…catch 的后面构造最终执行代码块，这就意味着程序无论正常执行还是发生异常，这里的代码只要 JVM 不关闭都能执行，可以将释放外部资源的代码写在finally块中。 finalize： Object 类中定义的方法。 Java 中允许使用 finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。 这个方法是由垃圾收集器在销毁对象时调用的，通过重写 finalize() 方法可以整理系统资源或者执行其他清理工作。 二、代码示例java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class zd_important_test_nbst &#123; public static void main(String[] args) &#123; UserTest userTest = new UserTest(); System.out.println(userTest.getA()); UserTest userTest1 = new UserTest(6); System.out.println(userTest1.getA()); &#125;&#125;class UserTest &#123; /** * final修饰的变量，要么一开始就初始化（饿汉式），要么就在构造方法里初始化（懒汉式）。 * 一旦初始化完成，就不能修改。 * * 【String中同样，使用private final修饰char[]】所以String是不可变的。 * （通过反射可以破坏其不可变性） * 其他博客会提到上述内容。 */ private final int a; public UserTest() &#123; super(); a = 1; &#125; public UserTest(int a) &#123; this.a = a; &#125; public int getA() &#123; return a; &#125; //会报错，因为a是final// public void setA(int b) &#123;// this.a = b;// &#125; /** * 以下为idea默认生成的hashcode和equals，可忽略 * * 在Object的源码中，hashcode是native方法，使用c语言实现的，综合类的信息计算出的hashcode值 * equals底层就是“==” */ @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; UserTest userTest = (UserTest) o; return a == userTest.a; &#125; @Override public int hashCode() &#123; return Objects.hash(a); &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"Java静态代码块的加载时机","slug":"Java静态代码块的加载时机","date":"2020-04-10T15:57:07.000Z","updated":"2020-04-18T15:22:19.726Z","comments":true,"path":"2020/04/10/Java静态代码块的加载时机/","link":"","permalink":"https://masteryang4.github.io/2020/04/10/Java%E9%9D%99%E6%80%81%E4%BB%A3%E7%A0%81%E5%9D%97%E7%9A%84%E5%8A%A0%E8%BD%BD%E6%97%B6%E6%9C%BA/","excerpt":"","text":"Java静态代码块的加载时机在java中，静态代码块其实并不是随着类的加载而加载。","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"SQL的执行顺序问题","slug":"SQL的执行顺序问题","date":"2020-04-10T15:44:24.000Z","updated":"2020-04-14T13:41:32.432Z","comments":true,"path":"2020/04/10/SQL的执行顺序问题/","link":"","permalink":"https://masteryang4.github.io/2020/04/10/SQL%E7%9A%84%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%E9%97%AE%E9%A2%98/","excerpt":"","text":"SQL的执行顺序问题众所周知，sql的执行顺序： sql1from... where...group by... having....select ... order by... limit 但是，有一个“bug”，在 MySQL 中： sql1SELECT title, COUNT(title) AS t FROM table GROUP BY title HAVING t &gt;= 2 这样的语句是可以执行的。 正常来说，having在select之前执行，但是却可以使用select的别名，这是为什么呢？ 查阅了一切资料，做出如下解释： 解释一 mysql的处理方式是中间生成虚拟表（或者叫临时表），而这个虚拟表的生成的列靠的就是select。 所以猜测类似having之后的操作，其实内部已经根据select生成了虚拟表，列自然也是as后的。 解释二 之所以MYSQL可以这么做是因为MYSQL用的是临时表， 在having前已经产生了数据，所以可以用别名，但SQL Sever不可以，SQL是在having后才Select。","categories":[{"name":"SQL","slug":"SQL","permalink":"https://masteryang4.github.io/categories/SQL/"},{"name":"MySQL","slug":"SQL/MySQL","permalink":"https://masteryang4.github.io/categories/SQL/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://masteryang4.github.io/tags/MySQL/"}]},{"title":"LeetCode经典10道题","slug":"LeetCode经典10道题","date":"2020-03-22T15:36:52.000Z","updated":"2020-06-02T15:12:37.693Z","comments":true,"path":"2020/03/22/LeetCode经典10道题/","link":"","permalink":"https://masteryang4.github.io/2020/03/22/LeetCode%E7%BB%8F%E5%85%B810%E9%81%93%E9%A2%98/","excerpt":"","text":"LeetCode题目精选1. 两数之和链接：https://leetcode-cn.com/problems/two-sum/ 给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 Code1234给定 nums &#x3D; [2, 7, 11, 15], target &#x3D; 9因为 nums[0] + nums[1] &#x3D; 2 + 7 &#x3D; 9所以返回 [0, 1] 题解： java12345678910111213class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; int complement = target - nums[i]; if (map.containsKey(complement)) &#123; return new int[] &#123; map.get(complement), i &#125;; &#125; map.put(nums[i], i); &#125; throw new IllegalArgumentException(\"No two sum solution\"); &#125;&#125; 2. 爬楼梯链接：https://leetcode-cn.com/problems/climbing-stairs/ 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 示例 1： Code12345输入： 2输出： 2解释： 有两种方法可以爬到楼顶。1. 1 阶 + 1 阶2. 2 阶 示例 2： Code123456输入： 3输出： 3解释： 有三种方法可以爬到楼顶。1. 1 阶 + 1 阶 + 1 阶2. 1 阶 + 2 阶3. 2 阶 + 1 阶 题解： java1234567891011121314public class Solution &#123; public int climbStairs(int n) &#123; if (n == 1) &#123; return 1; &#125; int[] dp = new int[n + 1]; dp[1] = 1; dp[2] = 2; for (int i = 3; i &lt;= n; i++) &#123; dp[i] = dp[i - 1] + dp[i - 2]; &#125; return dp[n]; &#125;&#125; 3. 翻转二叉树链接：https://leetcode-cn.com/problems/invert-binary-tree/ 翻转一棵二叉树。 示例： 输入： Code12345 4 &#x2F; \\ 2 7 &#x2F; \\ &#x2F; \\1 3 6 9 输出： Code12345 4 &#x2F; \\ 7 2 &#x2F; \\ &#x2F; \\9 6 3 1 题解： java12345678910public TreeNode invertTree(TreeNode root) &#123; if (root == null) &#123; return null; &#125; TreeNode right = invertTree(root.right); TreeNode left = invertTree(root.left); root.left = right; root.right = left; return root;&#125; 4. 反转链表链接：https://leetcode-cn.com/problems/reverse-linked-list/ 反转一个单链表。 示例: Code12输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 题解： java1234567891011public ListNode reverseList(ListNode head) &#123; ListNode prev = null; ListNode curr = head; while (curr != null) &#123; ListNode nextTemp = curr.next; curr.next = prev; prev = curr; curr = nextTemp; &#125; return prev;&#125; 5. LRU缓存机制链接：https://leetcode-cn.com/problems/lru-cache/ 运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。 获取数据 get(key) - 如果密钥 (key) 存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。写入数据 put(key, value) - 如果密钥不存在，则写入其数据值。当缓存容量达到上限时，它应该在写入新数据之前删除最近最少使用的数据值，从而为新的数据值留出空间。 进阶: 你是否可以在 O(1) 时间复杂度内完成这两种操作？ 示例: Code1234567891011LRUCache cache &#x3D; new LRUCache( 2 &#x2F;* 缓存容量 *&#x2F; );cache.put(1, 1);cache.put(2, 2);cache.get(1); &#x2F;&#x2F; 返回 1cache.put(3, 3); &#x2F;&#x2F; 该操作会使得密钥 2 作废cache.get(2); &#x2F;&#x2F; 返回 -1 (未找到)cache.put(4, 4); &#x2F;&#x2F; 该操作会使得密钥 1 作废cache.get(1); &#x2F;&#x2F; 返回 -1 (未找到)cache.get(3); &#x2F;&#x2F; 返回 3cache.get(4); &#x2F;&#x2F; 返回 4 题解： java12345678910111213141516171819202122232425262728class LRUCache extends LinkedHashMap&lt;Integer, Integer&gt;&#123; private int capacity; public LRUCache(int capacity) &#123; super(capacity, 0.75F, true); this.capacity = capacity; &#125; public int get(int key) &#123; return super.getOrDefault(key, -1); &#125; public void put(int key, int value) &#123; super.put(key, value); &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) &#123; return size() &gt; capacity; &#125;&#125;/** * LRUCache 对象会以如下语句构造和调用: * LRUCache obj = new LRUCache(capacity); * int param_1 = obj.get(key); * obj.put(key,value); */ 6. 最长回文子串链接：https://leetcode-cn.com/problems/longest-palindromic-substring/ 给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。 示例 1： Code123输入: &quot;babad&quot;输出: &quot;bab&quot;注意: &quot;aba&quot; 也是一个有效答案。 示例 2： Code12输入: &quot;cbbd&quot;输出: &quot;bb&quot; 题解： java1234567891011121314151617181920212223public String longestPalindrome(String s) &#123; if (s == null || s.length() &lt; 1) return \"\"; int start = 0, end = 0; for (int i = 0; i &lt; s.length(); i++) &#123; int len1 = expandAroundCenter(s, i, i); int len2 = expandAroundCenter(s, i, i + 1); int len = Math.max(len1, len2); if (len &gt; end - start) &#123; start = i - (len - 1) / 2; end = i + len / 2; &#125; &#125; return s.substring(start, end + 1);&#125;private int expandAroundCenter(String s, int left, int right) &#123; int L = left, R = right; while (L &gt;= 0 &amp;&amp; R &lt; s.length() &amp;&amp; s.charAt(L) == s.charAt(R)) &#123; L--; R++; &#125; return R - L - 1;&#125; 7. 有效的括号链接：https://leetcode-cn.com/problems/valid-parentheses/ 给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串，判断字符串是否有效。 有效字符串需满足： 1. 左括号必须用相同类型的右括号闭合。 2. 左括号必须以正确的顺序闭合。 注意空字符串可被认为是有效字符串。 示例 1: Code12输入: &quot;()&quot;输出: true 示例 2: Code12输入: &quot;()[]&#123;&#125;&quot;输出: true 示例 3: Code12输入: &quot;(]&quot;输出: false 示例 4: Code12输入: &quot;([)]&quot;输出: false 示例 5: Code12输入: &quot;&#123;[]&#125;&quot;输出: true 题解： java1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123; // Hash table that takes care of the mappings. private HashMap&lt;Character, Character&gt; mappings; // Initialize hash map with mappings. This simply makes the code easier to read. public Solution() &#123; this.mappings = new HashMap&lt;Character, Character&gt;(); this.mappings.put(')', '('); this.mappings.put('&#125;', '&#123;'); this.mappings.put(']', '['); &#125; public boolean isValid(String s) &#123; // Initialize a stack to be used in the algorithm. Stack&lt;Character&gt; stack = new Stack&lt;Character&gt;(); for (int i = 0; i &lt; s.length(); i++) &#123; char c = s.charAt(i); // If the current character is a closing bracket. if (this.mappings.containsKey(c)) &#123; // Get the top element of the stack. If the stack is empty, set a dummy value of '#' char topElement = stack.empty() ? '#' : stack.pop(); // If the mapping for this bracket doesn't match the stack's top element, return false. if (topElement != this.mappings.get(c)) &#123; return false; &#125; &#125; else &#123; // If it was an opening bracket, push to the stack. stack.push(c); &#125; &#125; // If the stack still contains elements, then it is an invalid expression. return stack.isEmpty(); &#125;&#125; 8. 数组中的第K个最大元素链接：https://leetcode-cn.com/problems/kth-largest-element-in-an-array/ 在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。 示例 1: Code12输入: [3,2,1,5,6,4] 和 k &#x3D; 2输出: 5 示例 2: Code12输入: [3,2,3,1,2,4,5,5,6] 和 k &#x3D; 4输出: 4 说明: 你可以假设 k 总是有效的，且 1 ≤ k ≤ 数组的长度。 题解： java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.util.Random;class Solution &#123; int [] nums; public void swap(int a, int b) &#123; int tmp = this.nums[a]; this.nums[a] = this.nums[b]; this.nums[b] = tmp; &#125; public int partition(int left, int right, int pivot_index) &#123; int pivot = this.nums[pivot_index]; // 1. move pivot to end swap(pivot_index, right); int store_index = left; // 2. move all smaller elements to the left for (int i = left; i &lt;= right; i++) &#123; if (this.nums[i] &lt; pivot) &#123; swap(store_index, i); store_index++; &#125; &#125; // 3. move pivot to its final place swap(store_index, right); return store_index; &#125; public int quickselect(int left, int right, int k_smallest) &#123; /* Returns the k-th smallest element of list within left..right. */ if (left == right) // If the list contains only one element, return this.nums[left]; // return that element // select a random pivot_index Random random_num = new Random(); int pivot_index = left + random_num.nextInt(right - left); pivot_index = partition(left, right, pivot_index); // the pivot is on (N - k)th smallest position if (k_smallest == pivot_index) return this.nums[k_smallest]; // go left side else if (k_smallest &lt; pivot_index) return quickselect(left, pivot_index - 1, k_smallest); // go right side return quickselect(pivot_index + 1, right, k_smallest); &#125; public int findKthLargest(int[] nums, int k) &#123; this.nums = nums; int size = nums.length; // kth largest is (N - k)th smallest return quickselect(0, size - 1, size - k); &#125;&#125; 9. 实现 Trie (前缀树)实现一个 Trie (前缀树)，包含 insert, search, 和 startsWith 这三个操作。 示例: Code12345678Trie trie &#x3D; new Trie();trie.insert(&quot;apple&quot;);trie.search(&quot;apple&quot;); &#x2F;&#x2F; 返回 truetrie.search(&quot;app&quot;); &#x2F;&#x2F; 返回 falsetrie.startsWith(&quot;app&quot;); &#x2F;&#x2F; 返回 truetrie.insert(&quot;app&quot;); trie.search(&quot;app&quot;); &#x2F;&#x2F; 返回 true 说明: 你可以假设所有的输入都是由小写字母 a-z 构成的。 保证所有输入均为非空字符串。 题解： java1234567891011121314151617181920212223242526272829303132333435363738394041class Trie &#123; private TrieNode root; public Trie() &#123; root = new TrieNode(); &#125; // Inserts a word into the trie. public void insert(String word) &#123; TrieNode node = root; for (int i = 0; i &lt; word.length(); i++) &#123; char currentChar = word.charAt(i); if (!node.containsKey(currentChar)) &#123; node.put(currentChar, new TrieNode()); &#125; node = node.get(currentChar); &#125; node.setEnd(); &#125; // search a prefix or whole key in trie and // returns the node where search ends private TrieNode searchPrefix(String word) &#123; TrieNode node = root; for (int i = 0; i &lt; word.length(); i++) &#123; char curLetter = word.charAt(i); if (node.containsKey(curLetter)) &#123; node = node.get(curLetter); &#125; else &#123; return null; &#125; &#125; return node; &#125; // Returns if the word is in the trie. public boolean search(String word) &#123; TrieNode node = searchPrefix(word); return node != null &amp;&amp; node.isEnd(); &#125;&#125; 10. 编辑距离链接：https://leetcode-cn.com/problems/edit-distance/ 给定两个单词 word1 和 word2，计算出将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 1. 插入一个字符 2. 删除一个字符 3. 替换一个字符 示例 1: Code123456输入: word1 &#x3D; &quot;horse&quot;, word2 &#x3D; &quot;ros&quot;输出: 3解释: horse -&gt; rorse (将 &#39;h&#39; 替换为 &#39;r&#39;)rorse -&gt; rose (删除 &#39;r&#39;)rose -&gt; ros (删除 &#39;e&#39;) 示例 2: Code12345678输入: word1 &#x3D; &quot;intention&quot;, word2 &#x3D; &quot;execution&quot;输出: 5解释: intention -&gt; inention (删除 &#39;t&#39;)inention -&gt; enention (将 &#39;i&#39; 替换为 &#39;e&#39;)enention -&gt; exention (将 &#39;n&#39; 替换为 &#39;x&#39;)exention -&gt; exection (将 &#39;n&#39; 替换为 &#39;c&#39;)exection -&gt; execution (插入 &#39;u&#39;) 题解： java1234567891011121314151617181920212223242526272829303132333435class Solution &#123; public int minDistance(String word1, String word2) &#123; int n = word1.length(); int m = word2.length(); // if one of the strings is empty if (n * m == 0) return n + m; // array to store the convertion history int [][] d = new int[n + 1][m + 1]; // init boundaries for (int i = 0; i &lt; n + 1; i++) &#123; d[i][0] = i; &#125; for (int j = 0; j &lt; m + 1; j++) &#123; d[0][j] = j; &#125; // DP compute for (int i = 1; i &lt; n + 1; i++) &#123; for (int j = 1; j &lt; m + 1; j++) &#123; int left = d[i - 1][j] + 1; int down = d[i][j - 1] + 1; int left_down = d[i - 1][j - 1]; if (word1.charAt(i - 1) != word2.charAt(j - 1)) left_down += 1; d[i][j] = Math.min(left, Math.min(down, left_down)); &#125; &#125; return d[n][m]; &#125;&#125;","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://masteryang4.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"面试","slug":"面试","permalink":"https://masteryang4.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://masteryang4.github.io/tags/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"HashMap底层实现源码分析","slug":"HashMap底层实现源码分析","date":"2020-03-16T15:21:13.000Z","updated":"2020-05-09T13:27:22.937Z","comments":true,"path":"2020/03/16/HashMap底层实现源码分析/","link":"","permalink":"https://masteryang4.github.io/2020/03/16/HashMap%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"HashMap底层实现原理0.样例数据java123456789101112131415161718192021222324252627282930313233343536373839import java.util.HashMap;import java.util.Iterator;import java.util.Map;import java.util.Set;public class CollectionTest &#123; public static void main(String[] args) &#123; //唯一的工作初始化负债因子（this.loadFactor = DEFAULT_LOAD_FACTOR）为0.75f Map&lt;String,Integer&gt; map = new HashMap&lt;&gt;(); int count = 1; //添加kv for (char i = 65; i &lt; 91; i++) &#123; map.put(String.valueOf(i),count); count++; &#125; //第一种遍历方式 Set&lt;String&gt; keySet = map.keySet(); Iterator&lt;String&gt; iterator = keySet.iterator(); while (iterator.hasNext()) &#123; String key = iterator.next(); System.out.println(key+ \" =&gt; \" + map.get(key)); &#125; System.out.println(\"******************************\"); //第二种遍历方式 Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; iteratorMap = map.entrySet().iterator(); while (iteratorMap.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; mapEntry = iteratorMap.next(); System.out.println(mapEntry); &#125; System.out.println(\"******************************\"); //第三种遍历方式 for (Map.Entry&lt;String, Integer&gt; entry : map.entrySet()) &#123; System.out.println(entry.getKey() + \" =&gt; \" + entry.getValue()); &#125; &#125;&#125; 1. 类信息java1public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; 2. 基本属性java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125private static final long serialVersionUID = 362498820763181265L; //序列化版本号static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 默认容量16(左移4位相当于乘以2的4次方)static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//最大容量（1073741824）static final float DEFAULT_LOAD_FACTOR = 0.75f;//默认负载因子static final int TREEIFY_THRESHOLD = 8; //链表节点转换红黑树节点的阈值static final int UNTREEIFY_THRESHOLD = 6; //红黑树节点转换链表节点的阈值static final int MIN_TREEIFY_CAPACITY = 64;// 转红黑树时, table的最小长度// 基本hash节点, 继承自Entry，此时的Node节点就是相当于Entry节点的实现static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + \"=\" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125;transient Node&lt;K,V&gt;[] table; //hashMap数组的表示transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; //entry节点transient int size; //数组长度transient int modCount; //添加的元素个数int threshold; //合理的初始化数组长度，根据tableSizeFor()得到，用于手动设置时使用final float loadFactor; //负载因子，用于手动设置时使用//构造器一：定义Node[]数组初始长度public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); //为Node[]数组设置负债因子 this.loadFactor = loadFactor; //为Node[]数组设置一个合理的值 this.threshold = tableSizeFor(initialCapacity);&#125;//初始化Node[]数组长度，根据传入的值以2的n次方对数组进行扩容//（例如：存入传入值为9，数组容量为16，在(8,16]范围内将不会再次扩容）。static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= 1 &lt;&lt; 30) ? 1 &lt;&lt; 30 : n + 1;&#125;//构造器二：调用HashMap(int initialCapacity, float loadFactor)构造器public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;//构造器三：仅创建HashMap对象，并初始化负债因子为0.75fpublic HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR;&#125;// 红黑树节点static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; /** * Returns root of tree containing this node. */ final TreeNode&lt;K,V&gt; root() &#123; for (TreeNode&lt;K,V&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; &#125; //...&#125; 3. hash算法HashMap定位数组索引位置，直接决定了hash方法的离散性能。下面是定位哈希桶数组的源码： java123456789101112131415161718192021222324252627// 计算key的hash值static final int hash(Object key) &#123; int h; // 1.先拿到key的hashCode值,基本数据类型会使用其包装类重载的hashCode()方法去计算hash值，引用数据类型根据是否重写去计算 // 2.将hashCode的高16位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; // 将(tab.length - 1) 与 hash值进行&amp;运算 int index = (tab.length - 1) &amp; hash;&#125;//对值进行Hash计算public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = valu /** * 当KEY值为A测试数据，A的hash为: 31 * hash + ANSI码值65 * 当KEY值为AB测试数据，AB的hash为：31 * 65 + 66 */ for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h;&#125; HashMap底层数组的长度总是2的n次方，并且取模运算为“h mod table.length”，对应上面的公式，可以得到该运算等同于“h &amp; (table.length - 1)”。这是HashMap在速度上的优化，因为&amp;比%具有更高的效率。 在JDK1.8的实现中，还优化了高位运算的算法，将hashCode的高16位与hashCode进行异或运算，主要是为了在table的length较小的时候，让高位也参与运算，并且不会有太大的开销。 4. get方法java1234567891011121314151617181920212223242526272829303132//调用的GET方法public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;//实际执行的GET方法final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // table不为空 &amp;&amp; table长度大于0 &amp;&amp; table索引位置(根据hash值计算出)节点不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // first的key等于传入的key则返回first对象 if (first.hash == hash &amp;&amp; ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //first的key不等于传入的key则说明是链表，向下遍历 if ((e = first.next) != null) &#123; // 判断是否为TreeNode，是则为红黑树 // 如果是红黑树节点，则调用红黑树的查找目标节点方法getTreeNode if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //走下列步骤表示是链表，循环至节点的key与传入的key值相等 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; //找不到符合的返回空 return null;&#125; 5. put方法java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364//掉用的PUT方法，hash(key)调用本例中的hash()方法public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; //实际执行的PUT方法 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // table是否为空或者length等于0, 如果是则调用resize方法进行初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 通过hash值计算索引位置, 如果table表该索引位置节点为空则新增一个 if ((p = tab[i = (n - 1) &amp; hash]) == null) // 将索引位置的头节点赋值给p tab[i] = newNode(hash, key, value, null); else &#123; // table表该索引位置不为空 Node&lt;K,V&gt; e; K k; //判断p节点的hash值和key值是否跟传入的hash值和key值相等 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果相等, 则p节点即为要查找的目标节点，赋值给e // 判断p节点是否为TreeNode, 如果是则调用红黑树的putTreeVal方法查找目标节点 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 走到这代表p节点为普通链表节点 else &#123; // 遍历此链表, binCount用于统计节点数 for (int binCount = 0; ; ++binCount) &#123; //p.next为空代表目标节点不存在 if ((e = p.next) == null) &#123; //新增一个节点插入链表尾部 p.next = newNode(hash, key, value, null); //如果节点数目超过8个，调用treeifyBin方法将该链表转换为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //e节点的hash值和key值都与传入的相等, 则e即为目标节点,跳出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // e不为空则代表根据传入的hash值和key值查找到了节点,将该节点的value覆盖,返回oldValue if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); // 用于LinkedHashMap return oldValue; &#125; &#125; //map修改次数加1 ++modCount; //map节点数加1，如果超过阀值，则扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); // 用于LinkedHashMap return null;&#125; 从上面的源码分析可以看出 1、如果节点已经存在，则更新原值 2、如果节点不存在，则插入数组中，如果数组已经有值，则判断是非是红黑树，如果是，则调用红黑树方法插入 3、如果插入的是链表，插入尾部，然后判断节点数是否超过8，如果超过，则转换为红黑树 4、先插入的数据，后面判断是否超过阀值再进行的扩容 putTreeVal,插入红黑树方法就不看了，看下treeifyBin方法，该方法是将链表转化为红黑树, java123456789101112131415161718192021222324final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; // table为空或者table的长度小于64, 进行扩容 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); // 根据hash值计算索引值, 遍历该索引位置的链表 else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); // 链表节点转红黑树节点 if (tl == null) // tl为空代表为第一次循环 hd = p; // 头结点 else &#123; p.prev = tl; // 当前节点的prev属性设为上一个节点 tl.next = p; // 上一个节点的next属性设置为当前节点 &#125; tl = p; // tl赋值为p, 在下一次循环中作为上一个节点 &#125; while ((e = e.next) != null); // e指向下一个节点 // 将table该索引位置赋值为新转的TreeNode的头节点 if ((tab[index] = hd) != null) hd.treeify(tab); // 以头结点为根结点, 构建红黑树 &#125;&#125; 可以看到，会先判断tab的节点数是否超过64，如果没超过，则进行扩容，如果超过了才会转换为红黑树 可以得到两个结论 1、什么时候转换为红黑树 当链表数目超过8,并且map节点数量超过64，才会转换为红黑树 2、什么时候扩容（前提是map数目没有超过最大容量值 1&lt;&lt;30 ） 新增节点时，发生了碰撞，并且节点数目超过阀值 新增节点时，发生了碰撞，节点数量木有超过阀值，但是链表数目&gt;8,map节点&lt;64时 再看下resize()方法 java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788final Node&lt;K,V&gt;[] resize() &#123; //oldTab保存未扩容的tab Node&lt;K,V&gt;[] oldTab = table; //oldTab最大容量 int oldCap = (oldTab == null) ? 0 : oldTab.length; //oldTab阀值 int oldThr = threshold; int newCap, newThr = 0; //如果老map有值 if (oldCap &gt; 0) &#123; // 老table的容量超过最大容量值，设置阈值为Integer.MAX_VALUE，返回老表 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; //老table的容量没有超过最大容量值，将新容量赋值为老容量*2，如果新容量&lt;最大容量并且老容量&gt;=16, 则将新阈值设置为原来的两倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // 老表的容量为0, 老表的阈值大于0, 是因为初始容量被放入阈值 newCap = oldThr; // 则将新表的容量设置为老表的阈值 else &#123; //老表的容量为0, 老表的阈值为0, 则为空表，设置默认容量和阈值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 如果新阈值为空, 则通过新的容量*负载因子获得新阈值 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; // 将当前阈值赋值为刚计算出来的新的阈值 threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 将当前的表赋值为新定义的表 // 如果老表不为空, 则需遍历将节点赋值给新表 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; // 将索引值为j的老表头节点赋值给e oldTab[j] = null; //将老表的节点设置为空, 以便垃圾收集器回收空间 // 如果e.next为空, 则代表老表的该位置只有1个节点, // 通过hash值计算新表的索引位置, 直接将该节点放在该位置 if (e.next == null) // newTab[e.hash &amp; (newCap - 1)] = e; //e.next不为空,判断是否是红黑树 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //是普通链表 else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; //如果e的hash值与老表的容量进行与运算为0,则扩容后的索引位置跟老表的索引位置一样 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; //如果e的hash值与老表的容量进行与运算为1,则扩容后的索引位置为: // 老表的索引位置＋oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; // 最后一个节点的next设为空 newTab[j] = loHead; // 将原索引位置的节点设置为对应的头结点 &#125; if (hiTail != null) &#123; hiTail.next = null; // 最后一个节点的next设为空 newTab[j + oldCap] = hiHead; // 将索引位置为原索引+oldCap的节点设置为对应的头结点 &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 可以看出，扩容时，节点重hash只分布在原索引位置与原索引+oldCap位置，为什么呢 假设老表的容量为16，即oldCap=16，则新表容量为16*2=32，假设节点1的hash值为0000 0000 0000 0000 0000 1111 0000 1010，节点2的hash值为0000 0000 0000 0000 0000 1111 0001 1010，则节点1和节点2在老表的索引位置计算如下图计算1，由于老表的长度限制，节点1和节点2的索引位置只取决于节点hash值的最后4位。再看计算2，计算2为新表的索引计算，可以知道如果两个节点在老表的索引位置相同，则新表的索引位置只取决于节点hash值倒数第5位的值，而此位置的值刚好为老表的容量值16，此时节点在新表的索引位置只有两种情况：原索引位置和原索引+oldCap位置（在此例中即为10和10+16=26）。由于结果只取决于节点hash值的倒数第5位，而此位置的值刚好为老表的容量值16，因此此时新表的索引位置的计算可以替换为计算3，直接使用节点的hash值与老表的容量16进行位于运算，如果结果为0则该节点在新表的索引位置为原索引位置，否则该节点在新表的索引位置为原索引+oldCap位置。 6. remove()方法java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; // 如果table不为空并且根据hash值计算出来的索引位置不为空, 将该位置的节点赋值给p if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; // 如果p的hash值和key都与入参的相同, 则p即为目标节点, 赋值给node if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; // 否则向下遍历节点 if (p instanceof TreeNode) // 如果p是TreeNode则调用红黑树的方法查找节点 node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; do &#123; // 遍历链表查找符合条件的节点 // 当节点的hash值和key与传入的相同,则该节点即为目标节点 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; // 赋值给node, 并跳出循环 break; &#125; p = e; // p节点赋值为本次结束的e &#125; while ((e = e.next) != null); // 指向像一个节点 &#125; &#125; // 如果node不为空(即根据传入key和hash值查找到目标节点)，则进行移除操作 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) // 如果是TreeNode则调用红黑树的移除方法 ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); // 走到这代表节点是普通链表节点 // 如果node是该索引位置的头结点则直接将该索引位置的值赋值为node的next节点 else if (node == p) tab[index] = node.next; // 否则将node的上一个节点的next属性设置为node的next节点, // 即将node节点移除, 将node的上下节点进行关联(链表的移除) else p.next = node.next; ++modCount; // 修改次数+1 --size; // table的总节点数-1 afterNodeRemoval(node); // 供LinkedHashMap使用 return node; // 返回被移除的节点 &#125; &#125; return null;&#125; 7. JDK1.7和1.8的区别1、JDK1.7的时候使用的是数组+ 单链表的数据结构。但是在JDK1.8及之后时，使用的是数组+链表+红黑树的数据结构（当链表的深度达到8的时候，也就是默认阈值，就会自动扩容把链表转成红黑树的数据结构来把时间复杂度从O（n）变成O（logN）提高了效率） 2、JDK1.7用的是头插法，而JDK1.8及之后使用的都是尾插法，那么他们为什么要这样做呢？因为JDK1.7是用单链表进行的纵向延伸，当采用头插法时会容易出现逆序且环形链表死循环问题。但是在JDK1.8之后是因为加入了红黑树使用尾插法，能够避免出现逆序且链表死循环的问题。 3、扩容后数据存储位置的计算方式也不一样：1. 在JDK1.7的时候是直接用hash值和需要扩容的二进制数进行&amp;（这里就是为什么扩容的时候为啥一定必须是2的多少次幂的原因所在，因为如果只有2的n次幂的情况时最后一位二进制数才一定是1，这样能最大程度减少hash碰撞）（hash值 &amp; length-1），而在JDK1.8的时候直接用了JDK1.7的时候计算的规律，也就是扩容前的原始位置+扩容的大小值=JDK1.8的计算方式，而不再是JDK1.7的那种异或的方法。但是这种方式就相当于只需要判断Hash值的新增参与运算的位是0还是1就直接迅速计算出了扩容后的储存方式。 4、jdk1.7 先扩容再put ，jdk1.8 先put再扩容","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"集合","slug":"Java/集合","permalink":"https://masteryang4.github.io/categories/Java/%E9%9B%86%E5%90%88/"}],"tags":[{"name":"源码","slug":"源码","permalink":"https://masteryang4.github.io/tags/%E6%BA%90%E7%A0%81/"},{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"},{"name":"hashmap","slug":"hashmap","permalink":"https://masteryang4.github.io/tags/hashmap/"}]},{"title":"Comparable和Comparator底层源码分析","slug":"Comparable和Comparator底层源码分析","date":"2020-03-15T15:52:58.000Z","updated":"2020-04-18T15:17:34.133Z","comments":true,"path":"2020/03/15/Comparable和Comparator底层源码分析/","link":"","permalink":"https://masteryang4.github.io/2020/03/15/Comparable%E5%92%8CComparator%E5%BA%95%E5%B1%82%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"1. Comparable源码分析1.1创建Java工程，实现Comparable接口java123456789101112131415161718192021222324252627282930313233343536373839404142434445import java.io.Serializable;//实现Serializable，标识该类可被序列化//实现Comparable接口，让此类可以利用Collections.sort()进行排序public class User&lt;T extends User&gt; implements Serializable,Comparable&lt;T&gt;&#123; private String name; private int age; private transient String address;//transient修饰，标识该类序列化时此字段不需要进行存储 public User(String name)&#123; this.name = name; &#125; public User(String name,int age,String address)&#123; this(name); this.age = age; this.address = address; &#125; public String getName() &#123; return name; &#125; public int getAge() &#123; return age; &#125; public String getAddress() &#123; return address; &#125; @Override public int compareTo(T o) &#123; //在此处打上断点，方便进行调试 int returnInt = 0; if(age&gt;o.getAge())&#123; returnInt=1; &#125;else if(age==o.getAge())&#123; returnInt=0; &#125;else if(age&lt;o.getAge())&#123; returnInt=-1; &#125; return returnInt; &#125;&#125; 1.2 编写测试类java123456789101112131415161718192021222324252627282930313233import java.util.ArrayList;import java.util.Collections;import java.util.List;public class TestComparable &#123; public static void main(String[] args) &#123; User u1 = new User(\"caililiang1\",20,\"hubei1\"); User u2 = new User(\"caililiang2\",30,\"hubei2\"); User u3 = new User(\"caililiang3\",25,\"hubei3\"); User u4 = new User(\"caililiang4\",28,\"hubei4\"); User u5 = new User(\"caililiang5\",23,\"hubei5\"); List&lt;User&gt; list = new ArrayList&lt;User&gt;(); list.add(u1); list.add(u2); list.add(u3); list.add(u4); list.add(u5); for(int i=0;i&lt;list.size();i++)&#123; User u =list.get(i); System.out.println(u.getName()+\"---&gt;\"+u.getAge()); &#125; System.out.println(\"排序后---------------------\"); //在此处打上断点，方便进行调试 Collections.sort(list); for(int i=0;i&lt;list.size();i++)&#123; User u =list.get(i); System.out.println(u.getName()+\"---&gt;\"+u.getAge()); &#125; &#125;&#125; 1.3 Collections类中的泛型方法sort()java1234567// 此处 &lt;T extends Comparable&lt;? super T&gt;&gt; 的意思是：// 1.&lt;T extends Comparable&gt;表示比较对象的类必须是Comparable 的子类。// 2.Comparable&lt;? super T&gt;表示是Comparable实现类及以上。public static &lt;T extends Comparable&lt;? super T&gt;&gt; void sort(List&lt;T&gt; list) &#123; //调用List接口中的sort()方法 list.sort(null); &#125; 1.4 List接口中的默认方法sort()java1234567891011// 由于本例中采用的是ArrayList集合，ArrayList集合对List接口中的sort()方法进行了重写，// 因此实际在DeBug的过程中会执行ArrayLIst类中的sort()方法 default void sort(Comparator&lt;? super E&gt; c) &#123; Object[] a = this.toArray(); Arrays.sort(a, (Comparator) c); ListIterator&lt;E&gt; i = this.listIterator(); for (Object e : a) &#123; i.next(); i.set((E) e); &#125; &#125; 1.5 ArrayList集合中的方法sort()java1234567891011@Override@SuppressWarnings(\"unchecked\")public void sort(Comparator&lt;? super E&gt; c) &#123; final int expectedModCount = modCount; //此方法直接调用Arrays类中sort()方法 Arrays.sort((E[]) elementData, 0, size, c); if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; modCount++;&#125; 1.6 Arrays类中的sort()方法java12345678910111213141516171819202122public static &lt;T&gt; void sort(T[] a, int fromIndex, int toIndex, Comparator&lt;? super T&gt; c) &#123; //在1.3中传入的 c值为null,所以调用sort(a, fromIndex, toIndex)方法 if (c == null) &#123; sort(a, fromIndex, toIndex); &#125; else &#123; rangeCheck(a.length, fromIndex, toIndex); if (LegacyMergeSort.userRequested) legacyMergeSort(a, fromIndex, toIndex, c); else TimSort.sort(a, fromIndex, toIndex, c, null, 0, 0); &#125;&#125;public static void sort(Object[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); if (LegacyMergeSort.userRequested) //归并排序 legacyMergeSort(a, fromIndex, toIndex); else //二进制插入排序 ComparableTimSort.sort(a, fromIndex, toIndex, null, 0, 0);&#125; 解析：源码里首先判断是否采用传统的排序方法,LegacyMergeSort.userRequested属性默认为false,也就是说默认选中 ComparableTimSort.sort(a)方法(传统归并排序在1.5及之前是默认排序方法，1.5之后默认执行ComparableTimSort.sort()方法。除非程序中强制要求使用传统归并排序,语句如下：System.setProperty(“java.util.Arrays.useLegacyMergeSort”, “true”))继续看 ComparableTimSort.sort(a)源码 1.7 ComparableTimSort类中的sort()方法java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687static void sort(Object[] a, int lo, int hi, Object[] work, int workBase, int workLen) &#123; assert a != null &amp;&amp; lo &gt;= 0 &amp;&amp; lo &lt;= hi &amp;&amp; hi &lt;= a.length; //nRemaining表示没有排序的对象个数，方法执行前，如果这个数小于2，就不需要排序了。 //如果2&lt;= nRemaining &lt;=32,即MIN_MERGE的初始值，表示需要排序的数组是小数组 //可以使用mini-TimSort方法进行排序，否则需要使用归并排序。 int nRemaining = hi - lo; if (nRemaining &lt; 2) return; // Arrays of size 0 and 1 are always sorted // If array is small, do a \"mini-TimSort\" with no merges if (nRemaining &lt; MIN_MERGE) &#123; //调用重写的compareTo()方法 int initRunLen = countRunAndMakeAscending(a, lo, hi); //只看这一句 binarySort(a, lo, hi, lo + initRunLen); return; &#125; ...... &#125;//这里才是真正的调用compareTo()方法对当前对象进行比较 private static int countRunAndMakeAscending(Object[] a, int lo, int hi) &#123; assert lo &lt; hi; int runHi = lo + 1; if (runHi == hi) return 1; // Find end of run, and reverse range if descending if (((Comparable) a[runHi++]).compareTo(a[lo]) &lt; 0) &#123; // 降序排列 while (runHi &lt; hi &amp;&amp; ((Comparable) a[runHi]).compareTo(a[runHi - 1]) &lt; 0) runHi++; reverseRange(a, lo, runHi); &#125; else &#123;// 升序排列 while (runHi &lt; hi &amp;&amp; ((Comparable) a[runHi]).compareTo(a[runHi - 1]) &gt;= 0) runHi++; &#125; return runHi - lo; &#125;//这里才是真正的进行排序。 private static void binarySort(Object[] a, int lo, int hi, int start) &#123; assert lo &lt;= start &amp;&amp; start &lt;= hi; if (start == lo) start++; for ( ; start &lt; hi; start++) &#123; Comparable pivot = (Comparable) a[start]; // Set left (and right) to the index where a[start] (pivot) belongs int left = lo; int right = start; assert left &lt;= right; /* * Invariants: * pivot &gt;= all in [lo, left). * pivot &lt; all in [right, start). */ while (left &lt; right) &#123; int mid = (left + right) &gt;&gt;&gt; 1; if (pivot.compareTo(a[mid]) &lt; 0) right = mid; else left = mid + 1; &#125; assert left == right; /* * The invariants still hold: pivot &gt;= all in [lo, left) and * pivot &lt; all in [left, start), so pivot belongs at left. Note * that if there are elements equal to pivot, left points to the * first slot after them -- that's why this sort is stable. * Slide elements over to make room for pivot. */ int n = start - left; // The number of elements to move // Switch is just an optimization for arraycopy in default case switch (n) &#123; case 2: a[left + 2] = a[left + 1]; case 1: a[left + 1] = a[left]; break; default: System.arraycopy(a, left, a, left + 1, n); &#125; a[left] = pivot; &#125; &#125; 2. Comparator源码分析2.1 创建JavaBeanjava123456789101112131415161718192021222324252627282930import java.io.Serializable;public class People implements Serializable &#123; private String name; private int age; private transient String address;//transient修饰，标识该类序列化时此字段不需要进行存储 public People(String name)&#123; this.name = name; &#125; public People(String name,int age,String address)&#123; this(name); this.age = age; this.address = address; &#125; public String getName() &#123; return name; &#125; public int getAge() &#123; return age; &#125; public String getAddress() &#123; return address; &#125;&#125; 2.2 创建外部比较器java123456789101112131415import java.util.Comparator;public class PeopleComparator&lt;T extends People&gt; implements Comparator&lt;T&gt; &#123; public int compare(T o1, T o2) &#123; int returnInt = 0; if(o1.getAge()&gt;o2.getAge())&#123; returnInt = 1; &#125;else if(o1.getAge()==o2.getAge())&#123; returnInt = 0; &#125;else if(o1.getAge()&lt;o2.getAge())&#123; returnInt = -1; &#125; return returnInt; &#125;&#125; 2.3 创建测试类java12345678910111213141516171819202122232425262728293031323334import java.util.ArrayList;import java.util.Collections;import java.util.List;public class TestComparator &#123; public static void main(String[] args) &#123; People u1 = new People(\"caililiang1\",20,\"hubei1\"); People u2 = new People(\"caililiang2\",30,\"hubei2\"); People u3 = new People(\"caililiang3\",25,\"hubei3\"); People u4 = new People(\"caililiang4\",28,\"hubei4\"); People u5 = new People(\"caililiang5\",23,\"hubei5\"); List&lt;People&gt; list = new ArrayList&lt;People&gt;(); list.add(u1); list.add(u2); list.add(u3); list.add(u4); list.add(u5); for(int i=0;i&lt;list.size();i++)&#123; People u =list.get(i); System.out.println(u.getName()+\"---&gt;\"+u.getAge()); &#125; System.out.println(\"排序后---------------------\"); Collections.sort(list,new PeopleComparator()); for(int i=0;i&lt;list.size();i++)&#123; People u =list.get(i); System.out.println(u.getName()+\"---&gt;\"+u.getAge()); &#125; &#125;&#125; 2.4 Collections类中的泛型方法sort()java12345 @SuppressWarnings(&#123;\"unchecked\", \"rawtypes\"&#125;)//此处调用的是sort方法的重载方法，与案例一中不同 public static &lt;T&gt; void sort(List&lt;T&gt; list, Comparator&lt;? super T&gt; c) &#123; list.sort(c); &#125; 2.5 ArrayList集合中的方法sort()java12345678910@Override@SuppressWarnings(\"unchecked\")public void sort(Comparator&lt;? super E&gt; c) &#123; final int expectedModCount = modCount; Arrays.sort((E[]) elementData, 0, size, c); if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; modCount++;&#125; 2.6 Arrays类中的sort()方法java123456789101112public static &lt;T&gt; void sort(T[] a, int fromIndex, int toIndex, Comparator&lt;? super T&gt; c) &#123; if (c == null) &#123; sort(a, fromIndex, toIndex); &#125; else &#123; rangeCheck(a.length, fromIndex, toIndex); if (LegacyMergeSort.userRequested) legacyMergeSort(a, fromIndex, toIndex, c); else //本次进入这里进行排序 TimSort.sort(a, fromIndex, toIndex, c, null, 0, 0); &#125;&#125; 2.7 TimSort类下的sort()方法java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758static &lt;T&gt; void sort(T[] a, int lo, int hi, Comparator&lt;? super T&gt; c, T[] work, int workBase, int workLen) &#123; assert c != null &amp;&amp; a != null &amp;&amp; lo &gt;= 0 &amp;&amp; lo &lt;= hi &amp;&amp; hi &lt;= a.length; int nRemaining = hi - lo; if (nRemaining &lt; 2) return; // Arrays of size 0 and 1 are always sorted // If array is small, do a \"mini-TimSort\" with no merges if (nRemaining &lt; MIN_MERGE) &#123; int initRunLen = countRunAndMakeAscending(a, lo, hi, c); binarySort(a, lo, hi, lo + initRunLen, c); return; &#125; private static &lt;T&gt; void binarySort(T[] a, int lo, int hi, int start, Comparator&lt;? super T&gt; c) &#123; assert lo &lt;= start &amp;&amp; start &lt;= hi; if (start == lo) start++; for ( ; start &lt; hi; start++) &#123; T pivot = a[start]; // Set left (and right) to the index where a[start] (pivot) belongs int left = lo; int right = start; assert left &lt;= right; /* * Invariants: * pivot &gt;= all in [lo, left). * pivot &lt; all in [right, start). */ while (left &lt; right) &#123; int mid = (left + right) &gt;&gt;&gt; 1; if (c.compare(pivot, a[mid]) &lt; 0) right = mid; else left = mid + 1; &#125; assert left == right; /* * The invariants still hold: pivot &gt;= all in [lo, left) and * pivot &lt; all in [left, start), so pivot belongs at left. Note * that if there are elements equal to pivot, left points to the * first slot after them -- that's why this sort is stable. * Slide elements over to make room for pivot. */ int n = start - left; // The number of elements to move // Switch is just an optimization for arraycopy in default case switch (n) &#123; case 2: a[left + 2] = a[left + 1]; case 1: a[left + 1] = a[left]; break; default: System.arraycopy(a, left, a, left + 1, n); &#125; a[left] = pivot; &#125;&#125; 3. 总结 Comparable 此接口强行对实现它的每个类的对象进行整体排序。这种排序被称为类的自然排序，类的compareTo()方法被称为它的自然比较方法。 实现此接口的对象列表（集合和数组）可以通过 Collections.sort和 Arrays.sort 进行自动排序。实现此接口的对象可以用作有序映射中的键或有序集合中的元素，无需指定比较器。 Arrays.sort(people) Comparator 是比较器，排序时，需要新建比较器对象，将比较器和对象一起传递过去就可以比大小，可称为“外部排序”。比较器是定义在要比较对象的外部的, 必须要重写compare()方法，而需要比较的类的结构不需要有任何变化。并且在Comparator 里面用户可以自己实现复杂的可以通用的逻辑,使其可以匹配一些比较简单的对象,那样就可以节省很多重复劳动了。 Arrays.sort(people,new PersonCompartor()); 关于两个类的具体应用场景可以理解为，自己在创建一个工程时可以使用Comparable进行排序，当工程创建完毕时添加新的排序功能时，可以使用Comparator，无需改变类的结构。","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://masteryang4.github.io/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"源码","slug":"源码","permalink":"https://masteryang4.github.io/tags/%E6%BA%90%E7%A0%81/"},{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]},{"title":"Hexo博客安装及部署","slug":"Hexo博客安装及部署","date":"2020-03-12T06:40:31.000Z","updated":"2020-04-18T15:53:17.599Z","comments":true,"path":"2020/03/12/Hexo博客安装及部署/","link":"","permalink":"https://masteryang4.github.io/2020/03/12/Hexo%E5%8D%9A%E5%AE%A2%E5%AE%89%E8%A3%85%E5%8F%8A%E9%83%A8%E7%BD%B2/","excerpt":"安装nodejs node -v #查看node版本npm -v #查看npm版本npm install -g cnpm –registry=http://registry.npm.taobao.org #安装淘宝的cnpm 管理器cnpm -v #查看cnpm版本 hexo安装及配置 hexo -v #查看hexo版本mkdir blog #创建blog目录cd blog #进入blog目录sudo hexo init #生成博客 初始化博客 hexo s #启动本地博客服务http://localhost:4000/ #本地访问地址hexo n “我的第一篇文章” #创建新的文章 在blog目录下","text":"安装nodejs node -v #查看node版本npm -v #查看npm版本npm install -g cnpm –registry=http://registry.npm.taobao.org #安装淘宝的cnpm 管理器cnpm -v #查看cnpm版本 hexo安装及配置 hexo -v #查看hexo版本mkdir blog #创建blog目录cd blog #进入blog目录sudo hexo init #生成博客 初始化博客 hexo s #启动本地博客服务http://localhost:4000/ #本地访问地址hexo n “我的第一篇文章” #创建新的文章 在blog目录下 hexo clean #清理hexo g #生成#Github创建一个新的仓库 YourGithubName.github.iocnpm install –save hexo-deployer-git #在blog目录下安装git部署插件 配置_config.yml yml123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:type: gitrepo: https://github.com/YourGithubName/YourGithubName.github.io.gitbranch: master 部署到Github仓库里 hexo d https://YourGithubName.github.io/ #访问这个地址可以查看博客 yilia主题配置 下载yilia主题到本地 git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 修改hexo根目录下的 _config.yml 文件 yml1theme: yilia 部署到github hexo clean #清理一下 hexo g #生成 hexo d #部署到远程Github仓库 查看博客 ： https://YourGithubName.github.io/","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://masteryang4.github.io/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://masteryang4.github.io/tags/Hexo/"},{"name":"博客","slug":"博客","permalink":"https://masteryang4.github.io/tags/%E5%8D%9A%E5%AE%A2/"}]},{"title":"ArrayList底层实现源码分析(JDK1.8)","slug":"ArrayList底层实现源码分析_JDK1.8","date":"2020-03-12T03:30:45.000Z","updated":"2020-04-18T15:16:40.883Z","comments":true,"path":"2020/03/12/ArrayList底层实现源码分析_JDK1.8/","link":"","permalink":"https://masteryang4.github.io/2020/03/12/ArrayList%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90_JDK1.8/","excerpt":"1. 类信息1public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; 2. 基本属性","text":"1. 类信息java1public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; 2. 基本属性 java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//定义序列化ID，主要是为了表示不同的版本的兼容性private static final long serialVersionUID = 8683452581122892189L;//默认的数组存储容量(ArrayList底层是数组结构)private static final int DEFAULT_CAPACITY = 10;//当指定数组的容量为0时使用这个常量赋值private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;//默认空参构造函数时使用这个常量赋值private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;//真正存放数据的对象数组，transient标识不被序列化transient Object[] elementData;//数组中的真实元素个数，该值小于或等于elementData.lengthprivate int size;//最大数组长度：0x7fffffff - 8private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;//构造器一：创建具有初始化长度的listpublic ArrayList(int initialCapacity) &#123; //对传入的值进行合法检测 if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125;&#125;//构造器二：默认空参构造器public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125;//构造器三：创建具有初始化值的集合，可传入的集合类型父类是Collection即可，此处是多态的一个应用public ArrayList(Collection&lt;? extends E&gt; c) &#123; //将传入的集合转化为数组 elementData = c.toArray(); //判断elementData数组长度 if ((size = elementData.length) != 0) &#123; // elementData转化的数组如果不是Object的子类，就对当前数组进行复制，重新赋值给elementData if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123;//如果数组长度为0，复制为EMPTY_ELEMENTDATA this.elementData = EMPTY_ELEMENTDATA; &#125;&#125; 3. add(E e) 方法ArrayList集合创建时，默认初始化长度为0，通过add( )方法在添加元素时对数组长度进行动态赋值。添加第一个元素时，长度为10。当添加的元素个数超过10时，会进行首次扩容，容量为原数组长度的1.5倍。 java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 //此方法是添加元素的方法，另外还有一个重载方法 public boolean add(E e) &#123; //调用ensureCapacityInternal方法，初始化数组长度（默认为10） ensureCapacityInternal(size + 1); //为数组复制 elementData[size++] = e; return true; &#125;//初始化数组长度，默认值为10 private void ensureCapacityInternal(int minCapacity) &#123; //判断如果数组长度为0，对长度进行初始化 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; //从默认数组长度（10）和添加的元素个数（添加第一个元素时size=0,minCapacity=size+1）中取出最大值 //作为数组初始化长度 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; //再次确定数组容量 ensureExplicitCapacity(minCapacity); &#125; //再次确定数组容量 private void ensureExplicitCapacity(int minCapacity) &#123; //对数组元素个数进行统计 modCount++; //如果数组长度超过10，就对数组长度进行扩容 //那第一次扩容举例：minCapacity值为11，DEFAULT_CAPACITY值为10 if (minCapacity - elementData.length &gt; 0) //对数组进行扩容，默认为老数组的1.5倍 grow(minCapacity); &#125; //对数组进行扩容，默认为老数组的1.5倍 private void grow(int minCapacity) &#123; //老数组容量：minCapacity int oldCapacity = elementData.length; //新数组容量：是老数组长度的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //对新数组容量进行合法检测 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //MAX_ARRAY_SIZE：0x7fffffff - 8 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) //如果超过最大数组长度，再次进行扩容 newCapacity = hugeCapacity(minCapacity); //对原数组进行复制 elementData = Arrays.copyOf(elementData, newCapacity); &#125; private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) throw new OutOfMemoryError(); //三元运算符，如果超过最大数组长度返回Integer最大值：0x7fffffff return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; 4. add (int idnex,E element)从源码中可以看出，与add(E e)方法大致一致，主要的差异是增加了一行代码：System.arraycopy(elementData, index, elementData, index + 1, size - index)，从index位置开始以及之后的数据，整体拷贝到index+1开始的位置，然后再把新加入的数据放在index这个位置，而之前的数据不需要移动。 java123456789101112131415161718//在指定位置添加元素public void add(int index, E element) &#123; //判断index是否在范围内 rangeCheckForAdd(index); //与add(E e)方法一致，对数组长度进行初始化 ensureCapacityInternal(size + 1); //对原数组从index位置进行拷贝，复制到index+1的位置，elementData[index]此时为空 //System.arraycopy是一个native方法，意味着这个方法是C/C++语言实现的，我们无法再以普通的方式去查看这些方法了 System.arraycopy(elementData, index, elementData, index + 1, size - index); //为该下标赋值 elementData[index] = element; size++;&#125;//判断index是否在范围内的具体实现private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; arraycopy(elementData, index, elementData, index + 1, size - index)函数中各个参数对应的意义：（原数组，原数组的开始位置，目标数组，目标数组的开始位置，拷贝的个数） 5. remove(int index)java12345678910111213141516171819202122232425262728293031323334353637383940414243444546 //移除指定index下的元素 public E remove(int index) &#123; //index是否合法检测 rangeCheck(index); modCount++; //指定index下的元素 E oldValue = elementData(index); //移除后数组长度 int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); //为最后一个元素赋值为null elementData[--size] = null; return oldValue; &#125; //返回指定index下的元素E elementData(int index) &#123; return (E) elementData[index]; &#125; //根据元素（对象）移除该元素 public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; //类似于remove()方法 private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; &#125; remove方法与add正好是一个相反的操作，移除一个元素，会影响到一批数字的位置移动，所以也是比较耗性能。核心代码都是调用了java.lang.System.arraycopy(Object src, int srcPos, Object dest, int destPos, int length)方法 6. get(int index)java123456//根据指定下标获取元素值public E get(int index) &#123; rangeCheck(index); return elementData(index);&#125; 7. set(int index, E element)java12345678//修改指定index下的元素值public E set(int index, E element) &#123; rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; 8. clear()java12345678910//清空所有元素public void clear() &#123; modCount++; // clear to let GC do its work for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0;&#125; 9. contains(Object o)java1234567891011121314151617//查询是否包含某个元素public boolean contains(Object o) &#123; return indexOf(o) &gt;= 0;&#125;//具体的实现方法，如果不包含返回-1public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; 10. 总结 基于数组实现的List在随机访问和遍历的效率比较高，但是往指定位置加入元素或者删除指定位置的元素效率比较低。","categories":[{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/categories/Java/"},{"name":"集合","slug":"Java/集合","permalink":"https://masteryang4.github.io/categories/Java/%E9%9B%86%E5%90%88/"}],"tags":[{"name":"源码","slug":"源码","permalink":"https://masteryang4.github.io/tags/%E6%BA%90%E7%A0%81/"},{"name":"易错点","slug":"易错点","permalink":"https://masteryang4.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"Java","slug":"Java","permalink":"https://masteryang4.github.io/tags/Java/"}]}]}